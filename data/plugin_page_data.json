[
  {
    "normalized_name": "napari-mcp",
    "name": "napari-mcp",
    "display_name": "Napari MCP",
    "version": "0.0.1a0",
    "created_at": "2025-09-15",
    "modified_at": "2025-09-15",
    "authors": [
      "Ilan Theodoro"
    ],
    "author_emails": [
      "Ilan Theodoro <ilan.silva@czbiohub.org>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-mcp/",
    "home_github": "https://github.com/royerlab/napari-mcp#readme",
    "home_other": null,
    "summary": "MCP server for remote control of napari viewers via Model Context Protocol",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "fastmcp>=2.7.0",
      "napari>=0.5.5",
      "pyqt6>=6.5.0",
      "qtpy>=2.4.1",
      "Pillow>=10.3.0",
      "imageio>=2.34.0",
      "numpy>=1.26.0",
      "pytest>=8.4.0; extra == \"test\"",
      "pytest-asyncio>=0.23.0; extra == \"test\"",
      "pytest-cov>=4.0.0; extra == \"test\"",
      "pytest-qt>=4.0.0; extra == \"test\"",
      "pytest-xdist>=3.5.0; extra == \"test\"",
      "pytest-timeout>=2.2.0; extra == \"test\"",
      "pytest-benchmark>=4.0.0; extra == \"test\"",
      "pytest-mock>=3.12.0; extra == \"test\"",
      "pytest-random-order>=1.1.0; extra == \"test\"",
      "pytest-forked>=1.6.0; extra == \"test\"",
      "hypothesis>=6.100.0; extra == \"test\"",
      "napari[pyqt6,testing]; extra == \"test\"",
      "tox; extra == \"test\"",
      "ruff>=0.12.10; extra == \"dev\"",
      "mypy>=1.17.0; extra == \"dev\"",
      "types-Pillow>=10.0.0; extra == \"dev\"",
      "pre-commit>=4.3.0; extra == \"dev\"",
      "bandit>=1.8.6; extra == \"dev\"",
      "safety>=3.6.0; extra == \"dev\"",
      "black>=24.0.0; extra == \"dev\"",
      "napari-mcp[dev,test]; extra == \"all\""
    ],
    "package_metadata_description": "# Napari MCP Server\n\n[![Tests](https://github.com/royerlab/napari-mcp/workflows/Tests/badge.svg)](https://github.com/royerlab/napari-mcp/actions)\n[![codecov](https://codecov.io/gh/royerlab/napari-mcp/graph/badge.svg?token=E1WY58V877)](https://codecov.io/gh/royerlab/napari-mcp)\n[![PyPI version](https://badge.fury.io/py/napari-mcp.svg)](https://badge.fury.io/py/napari-mcp)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nMCP server for remote control of napari viewers via Model Context Protocol (MCP). Perfect for AI-assisted analysis with Claude Desktop.\n\n## üöÄ Quick Start\n\n### Option 1: Install from PyPI (Recommended)\n```bash\n# Install the package\npip install napari-mcp\n\n# Run the server\nnapari-mcp\n```\n\n### Option 2: Install from GitHub (Latest)\n```bash\n# Install directly from GitHub\nuv pip install git+https://github.com/royerlab/napari-mcp.git\n\n# Run the server\nnapari-mcp\n```\n\n### Option 3: Development Install\n```bash\n# Clone and install\ngit clone https://github.com/royerlab/napari-mcp.git\ncd napari-mcp\nuv pip install -e .\n\n# Run the server\nnapari-mcp\n```\n\n**Claude Desktop config (After Installation):**\n```json\n{\n  \"mcpServers\": {\n    \"napari\": {\n      \"command\": \"napari-mcp\"\n    }\n  }\n}\n```\n\n**Alternative config (GitHub install):**\n```json\n{\n  \"mcpServers\": {\n    \"napari\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\", \"--with\", \"git+https://github.com/royerlab/napari-mcp.git\",\n        \"napari-mcp\"\n      ]\n    }\n  }\n}\n```\n\n**Why this approach?**\n- ‚úÖ **Zero Installation** - No pip install, no virtual environments\n- ‚úÖ **Single File** - Easy to share, version, and deploy\n- ‚úÖ **Auto Dependencies** - uv handles all dependencies automatically\n- ‚úÖ **Direct GitHub Execution** - Run latest version directly from repo without downloading\n- ‚úÖ **Always Up-to-Date** - GitHub URL ensures you get the latest version\n- ‚úÖ **Reproducible** - Same dependencies every time\n\n## ü§ñ Multi-LLM Support\n\nWorks with multiple AI assistants and IDEs:\n\n| Application | Status | Setup Method |\n|-------------|--------|--------------|\n| **Claude Desktop** | ‚úÖ Full Support | Manual config (recommended) |\n| **Claude Code** | ‚úÖ Full Support | `fastmcp install claude-code` |\n| **Cursor** | ‚úÖ Full Support | `fastmcp install cursor` |\n| **ChatGPT** | üü° Limited | Remote deployment only |\n\n**‚Üí See [LLM_INTEGRATIONS.md](LLM_INTEGRATIONS.md) for complete setup guides**\n\n## üîß Alternative Installation Methods\n\n### Traditional Package Installation\n\n```bash\n# Clone and install\ngit clone https://github.com/royerlab/napari-mcp.git\ncd napari-mcp\npip install -e .\n\n# Run\nnapari-mcp\n```\n\nClaude Desktop config for installed version:\n```json\n{\n  \"mcpServers\": {\n    \"napari\": {\n      \"command\": \"napari-mcp\"\n    }\n  }\n}\n```\n\n### Development Installation\n\n```bash\n# With uv (recommended for development)\nuv pip install -e \".[test,dev]\"\n\n# With pip\npip install -e \".[test,dev]\"\n```\n\n**Requirements:**\n- Python 3.10+\n- napari 0.5.5+\n- Qt Backend (PyQt6 installed automatically)\n\n## üõ† Available MCP Tools\n\n### Session Information\n- `session_information()` - Get comprehensive session info including viewer state, layers, system details\n\n### Layer Management\n- `list_layers()` - Get all layers and their properties\n- `add_image(path, name?, colormap?, blending?, channel_axis?)` - Add image layer from file\n- `add_labels(path, name?)` - Add segmentation labels from file\n- `add_points(points, name?, size?)` - Add point annotations\n- `remove_layer(name)` - Remove layer by name\n- `rename_layer(old_name, new_name)` - Rename layer\n- `set_layer_properties(...)` - Modify layer visibility, opacity, colormap, etc.\n- `reorder_layer(name, index?|before?|after?)` - Change layer order\n- `set_active_layer(name)` - Set selected layer\n\n### Viewer Controls\n- `init_viewer(title?, width?, height?)` - Create or configure viewer\n- `close_viewer()` - Close viewer window\n- `start_gui(focus?)` - Start GUI event loop\n- `stop_gui()` - Stop GUI event loop\n- `is_gui_running()` - Check GUI status\n- `reset_view()` - Reset camera to fit all data\n- `set_zoom(zoom)` - Set zoom level\n- `set_camera(center?, zoom?, angle?)` - Position camera\n- `set_ndisplay(2|3)` - Switch between 2D/3D display\n- `set_dims_current_step(axis, value)` - Navigate dimensions (time, Z-stack)\n- `set_grid(enabled?)` - Enable/disable grid view\n\n### Utilities\n- `screenshot(canvas_only?)` - Capture PNG image as base64\n- `execute_code(code)` - Run Python with access to viewer, napari, numpy\n- `install_packages(packages, ...)` - Install Python packages dynamically\n\n## ‚ö†Ô∏è **IMPORTANT SECURITY WARNING**\n\n**This server includes powerful tools that allow arbitrary code execution:**\n\n- **`execute_code()`** - Runs any Python code in the server environment\n- **`install_packages()`** - Installs any Python package via pip\n\n**Security Implications:**\n- ‚úÖ **Safe for local development** with trusted AI assistants like Claude\n- ‚ùå **NEVER expose to untrusted networks** or public internet\n- ‚ùå **Do not use in production environments** without proper sandboxing\n- ‚ùå **Can access your filesystem, network, and install malware**\n\n**Recommended Usage:**\n- Use only on `localhost` connections\n- Run in isolated virtual environments\n- Only use with trusted AI assistants\n\n## üìñ Usage Examples\n\n### Basic Layer Operations\n\n**Add and manipulate images:**\n```\nAsk Claude: \"Add a sample image to napari and set its colormap to 'viridis'\"\n```\n\n**Work with annotations:**\n```\nAsk Claude: \"Create some point annotations at coordinates [[100,100], [200,200]] and make them size 10\"\n```\n\n### Advanced Analysis\n\n**Execute custom code:**\n```\nAsk Claude: \"Execute this code to create a synthetic image:\nimport numpy as np\ndata = np.random.random((512, 512))\nviewer.add_image(data, name='random_noise')\"\n```\n\n**Install packages on-demand:**\n```\nAsk Claude: \"Install scipy and create a Gaussian filtered version of the current image\"\n```\n\n### View Management\n\n**Control the camera:**\n```\nAsk Claude: \"Reset the view, then zoom to 2x and center on coordinates [256, 256]\"\n```\n\n**Switch display modes:**\n```\nAsk Claude: \"Switch to 3D display mode and take a screenshot\"\n```\n\n## üß™ Testing\n\n```bash\n# Run basic tests (fast, no GUI)\n./run_tests.sh\n\n# Run with real GUI tests (requires display)\n./run_realgui_tests.sh\n\n# Run with coverage\npytest --cov=src --cov-report=html tests/ -m \"not realgui\"\n```\n\n## ü§ù Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes with tests\n4. Run pre-commit hooks: `pre-commit run --all-files`\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n**Development setup:**\n```bash\ngit clone https://github.com/royerlab/napari-mcp.git\ncd napari-mcp\nuv pip install -e \".[test,dev]\"\npre-commit install\n```\n\n## üìã Architecture\n\nThe server architecture consists of:\n\n- **FastMCP Server**: Handles MCP protocol communication\n- **Napari Integration**: Manages viewer lifecycle and operations\n- **Qt Event Loop**: Asynchronous GUI event processing\n- **Tool Layer**: Exposes napari functionality as MCP tools\n\nKey design decisions:\n- **Thread-safe**: All napari operations are serialized through locks\n- **Non-blocking**: Qt event loop runs asynchronously\n- **Stateful**: Maintains viewer state across tool calls\n- **Extensible**: Easy to add new tools and functionality\n\n## üìö Resources\n\n- **[QUICKSTART.md](QUICKSTART.md)** - Get running in 2 minutes\n- **[LLM_INTEGRATIONS.md](LLM_INTEGRATIONS.md)** - Complete guide for Claude Desktop, Claude Code, Cursor, ChatGPT\n- **[Model Context Protocol](https://modelcontextprotocol.io/)** - MCP specification\n- **[FastMCP](https://github.com/jlowin/fastmcp)** - Python MCP framework\n- **[napari](https://napari.org/)** - Multi-dimensional image viewer\n- **[Claude Desktop](https://claude.ai/download)** - AI assistant with MCP support\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- [napari team](https://napari.org/team/) for the excellent imaging platform\n- [FastMCP](https://github.com/jlowin/fastmcp) for the MCP framework\n- [Anthropic](https://www.anthropic.com/) for Claude and MCP development\n- [astral-sh](https://astral.sh/) for uv dependency management\n\n---\n\n**Built with ‚ù§Ô∏è for the microscopy and AI communities**\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MCP Server Control"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "mitoclass",
    "name": "mitoclass",
    "display_name": "Mitoclass",
    "version": "1.0.0",
    "created_at": "2025-07-30",
    "modified_at": "2025-09-13",
    "authors": [
      "Jules Malard",
      "David Rousseau",
      "Arnaud Chevrollier"
    ],
    "author_emails": [
      "\"Jules Malard",
      "David Rousseau",
      "Arnaud Chevrollier\" <malardjules2@gmail.com>"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/mitoclass/",
    "home_github": "https://github.com/Jmlr2/MitoClassif",
    "home_other": null,
    "summary": "Mitoclass is a napari plugin for classifying mitochondrial morphology from microscopy images: it allows preprocessing data, predicting classes (connected, fragmented, intermediate), visualizing overlays and 3D summaries, and managing a prediction history.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tifffile",
      "tensorflow",
      "pandas",
      "scikit-learn",
      "plotly",
      "napari[all]",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "\n# <img src=\"https://raw.githubusercontent.com/Jmlr2/MitoClassif/main/assets/mitoclass.png\" alt=\"MitoClass logo\" height=\"60\" style=\"vertical-align: middle;\"> Mitoclass\n\n[![License: GPL‚ÄØv3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/mitoclass.svg)](https://pypi.org/project/mitoclass/)\n[![Python‚ÄØ‚â•‚ÄØ3.10](https://img.shields.io/badge/python-%3E%3D3.10-blue.svg)]()\n[![napari‚Äëhub](https://img.shields.io/badge/napari--hub-mitoclass-orange.svg)](https://github.com/napari/napari-hub)\n\n<p align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/Jmlr2/MitoClassif/main/assets/imhorphen.png\" alt=\"IMHORPHEN\" height=\"70\" style=\"margin: 0 20px;\">\n  <img src=\"https://raw.githubusercontent.com/Jmlr2/MitoClassif/main/assets/LARIS.png\" alt=\"LARIS\" height=\"70\" style=\"margin: 0 20px;\">\n  <img src=\"https://raw.githubusercontent.com/Jmlr2/MitoClassif/main/assets/ua.png\" alt=\"Universit√© d'Angers\" height=\"70\" style=\"margin: 0 20px;\">\n</p>\n\n---\n\n## 1&nbsp;&nbsp;Overview\n\n**Mitoclass** is a *napari* plugin for the qualitative assessment of mitochondrial network morphology.\nInference is **patch‚Äëwise**: each 2‚ÄëD patch‚Äîobtained from a maximum‚Äëintensity projection of 3‚ÄëD stacks‚Äîis classified as **connected**, **fragmented**, or **intermediate**.\n\n---\n\n## 2&nbsp;&nbsp;Key features\n\n| Module | Description |\n|--------|-------------|\n| Patch‚Äëbased inference | Analyse an image folder *or* the active napari layer. |\n| RGBA heatmaps | Overlay prediction maps as semi‚Äëtransparent layers in napari. |\n| Global statistics | Compute the proportion of pixels assigned to each morphology and identify the dominant class. |\n| 3‚ÄëD graph | Interactive Plotly scatter plot of connected / fragmented / intermediate proportions per image. |\n\n---\n\n## 3&nbsp;&nbsp;Requirements\n\n* **Python**¬†‚â•¬†3.10\n* **OS**¬†: Windows, Linux or macOS\n* **Hardware**¬†: CPU is sufficient; GPU (CUDA¬†11+) is recommended for large datasets\n\n---\n\n## 4&nbsp;&nbsp;Installation\n\n### 4.1¬†¬†PyPI\n\n```bash\npip install mitoclass\n```\n\n### 4.2¬†¬†Reproducible *conda* environment\n\n```bash\nconda create -n mitoclass python=3.10\nconda activate mitoclass\n\n# (Optional) GPU acceleration\nconda install -c conda-forge cudnn=8.9 cuda11.8 tensorflow\n\npip install mitoclass\n```\n\n*Apple¬†Silicon*: install `tensorflow-macos`.\n\n### 4.3¬†¬†Pre‚Äëtrained model\n\nDownload the model (`.h5`) from\n<https://github.com/Jmlr2/MitoClassif/releases>\n\n---\n\n## 5&nbsp;&nbsp;Usage\n\n### 5.1¬†¬†Graphical interface\n\n```bash\nnapari\n```\n\n1. Open **Plugins ‚Üí Mitoclass**.\n2. Select the four required paths:\n\n   | Field | Purpose |\n   |-------|---------|\n   | **Input dir** | Folder of images to analyse (`.tif`, `.tiff`, `.stk`, `.png`). |\n   | **Output dir** | Destination folder for CSV and graph files. |\n   | **Heatmaps dir** | Folder where heatmaps (`*_map.tif`) will be written. |\n   | **Model file** | Pre‚Äëtrained Keras model (`.h5`). |\n\n3. Click **Run inference**. A progress bar tracks the number of processed images.\n4. After completion:\n   * **Show heatmaps** adds the newly generated `*_map.tif` layers to napari.\n   * **Show 3D graph** opens `graph3d.html`, displaying the connected/fragmented/intermediate proportions.\n\n*Tip*: Without an *Input dir* you may run **Infer active layer**; results are still saved to *Output dir* and *Heatmaps dir*.\n\n### 5.2¬†¬†Output structure\n\n| Folder | File(s) | Content |\n|--------|---------|---------|\n| **Output dir** | `predictions.csv` | Pixel proportion for each class (*connected*, *fragmented*, *intermediate*) and the dominant morphology, one line per image. |\n|                | `graph3d.html` | Interactive 3‚ÄëD Plotly graph of class proportions. |\n| **Heatmaps dir** | `*_map.tif` | One RGBA heatmap per image, ready to overlay in napari. |\n\n---\n\n## 6&nbsp;&nbsp;Licence\n\nThis project is released under the **GNU¬†GPL‚ÄØv3** licence.\nSee the [LICENSE](LICENSE) file for details.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Mitoclass"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-convpaint",
    "name": "napari-convpaint",
    "display_name": "napari ConvPaint",
    "version": "0.8.0rc1",
    "created_at": "2023-11-01",
    "modified_at": "2025-09-12",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-convpaint/",
    "home_github": "https://github.com/guiwitz/napari-convpaint",
    "home_other": null,
    "summary": "A plugin for segmentation by pixel classification using pre-trained neural networks for feature extraction",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "catboost",
      "einops",
      "joblib",
      "magicgui",
      "napari",
      "napari-annotation-project",
      "napari-guitils",
      "numpy",
      "pandas",
      "pyyaml",
      "qtpy",
      "scikit-learn",
      "scikit-image",
      "tifffile",
      "torch",
      "torchvision>=0.13.0",
      "zarr",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "cellpose; extra == \"cellpose\"",
      "ilastik-napari; extra == \"ilastik\"",
      "pyqt5; extra == \"qt\""
    ],
    "package_metadata_description": "\n[![License](https://img.shields.io/pypi/l/napari-convpaint.svg?color=green)](https://github.com/guiwitz/napari-convpaint/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-convpaint.svg?color=green)](https://pypi.org/project/napari-convpaint)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-convpaint.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-convpaint/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-convpaint/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-convpaint/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-convpaint)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-convpaint)](https://napari-hub.org/plugins/napari-convpaint)\n\n\n\n![overview conv-paint](/images/overview_github.png)\nThis napari plugin can be used to segment objects or structures in images based on a few brush strokes providing examples of the classes. Based on the same idea as other tools like ilastik, its main strength is that it can use features from pretrained neural networks like VGG16 or DINOV2, enabling the segmentation of more complex images.\n\n**Find more information and tutorials in the [docs](https://guiwitz.github.io/napari-convpaint/) or read the [preprint](https://doi.org/10.1101/2024.09.12.610926).**\n\n\n![overview conv-paint](/images/network_github.png)\n\n## Installation\n\nYou can install `napari-convpaint` via [pip]\n\n    pip install napari-convpaint\n\nTo install latest development version :\n\n    pip install git+https://github.com/guiwitz/napari-convpaint.git\n\n\n## Example use case: Tracking shark body parts in a movie\nThese are the scribble annotations provided for training:\n![](./images/shark_annot.png)\n\nAnd this is the resulting Convpaint segmentation:\n<video src=\"https://github.com/user-attachments/assets/6a2be1fe-25cc-4af1-9f50-aab9bc4123d9\"></video>\n\nCheck out the documentation or the paper for more usecases!\n\n## API\n\nYou can now use the API in a fashion very similar to the napari plugin. The ConvpaintModel class combines a feature extractor and a classifier model, and holds all the parameters defining the model. Initialize a ConvpaintModel object, train its classifier and use it to segment an image:\n\n```Python\ncp_model = ConvpaintModel(\"dino\") # alternatively use vgg, cellpose or gaussian\ncp_model.train(image, annotations)\nsegmentation = cp_model.segment(image)\n```\n\nThere are many other options, such as predicting all class probabilities (see below) and we will update the documentation and notebook examples soon. In the meantime feel free to test it yourself.\n\n```Python\nprobas = cp_model.predict_probas(image)\n```\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-convpaint\" is free and open source software\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-convpaint/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n## Authors\n\nThe idea behind this napari plugin was first developed by [Lucien Hinderling](https://hinderling.github.io) in the group of [Olivier Pertz](https://www.pertzlab.net/), at the Institute of Cell Biology, University of Bern. Pertz lab obtained a CZI napari plugin development grant with the title [\"Democratizing Image Analysis with an Easy-to-Train Classifier\"](https://chanzuckerberg.com/science/programs-resources/imaging/napari/democratizing-image-analysis-with-an-easy-to-train-classifier/) which supported the adaptation of the initial concept as a napari plugin called napari-convpaint. The plugin has been developed by [Guillaume Witz<sup>1</sup>](https://guiwitz.github.io/blog/about/), [Roman Schwob<sup>1,2</sup>](https://github.com/quasar1357) and Lucien Hinderling<sup>2</sup> with much appreciated assistance of [Benjamin Gr√§del<sup>2</sup>](https://x.com/benigraedel), [Maciej Dobrzy≈Ñski<sup>2</sup>](https://macdobry.net), Mykhailo Vladymyrov<sup>1</sup> and Ana Stojiljkoviƒá<sup>1</sup>.\n\n<sup>1</sup>[Data Science Lab](https://www.dsl.unibe.ch/), University of Bern \\\n<sup>2</sup>[Pertz Lab](https://www.pertzlab.net/), Institute of Cell Biology, University of Bern \n\n## Cite Convpaint\n\nIf you find Convpaint useful in your research, please consider citing our work. Please also cite any Feature Extractor you have used in Convpaint, such as [ilastik](https://github.com/ilastik/ilastik-napari), [cellpose](https://cellpose.readthedocs.io/en/latest/) or [DINOv2](https://github.com/facebookresearch/dinov2).\n\nConvpaint:\n```\n@article {Hinderling2024,\n\tauthor = {Hinderling, Lucien and Witz, Guillaume and Schwob, Roman and Stojiljkoviƒá, Ana and Dobrzy≈Ñski, Maciej and Vladymyrov, Mykhailo and Frei, Jo√´l and Gr√§del, Benjamin and Frismantiene, Agne and Pertz, Olivier},\n\ttitle = {Convpaint - Interactive pixel classification using pretrained neural networks},\n\telocation-id = {2024.09.12.610926},\n\tdoi = {10.1101/2024.09.12.610926},\n\tjournal = {bioRxiv},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tyear = {2024},\n}\n```\nSuggested citations for feature extractors:\n```\n@article {Berg2019,\n\tauthor = {Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and Straehle, Christoph N. and Kausler, Bernhard X. and Haubold, Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I. and Xu, Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong and Koethe, Ullrich and Hamprecht, Fred A. and Kreshuk, Anna},\n\ttitle = {ilastik: interactive machine learning for (bio)image analysis.},\n\tissn = {1548-7105},\n\turl = {https://doi.org/10.1038/s41592-019-0582-9},\n\tdoi = {10.1038/s41592-019-0582-9},\n\tjournal = {Nature Methods},\n\tpublisher = {Springer Nature},\n\tyear = {2019},\n\tjournal = {Nature Methods},\n}\n```\n```\n@article {Stringer2021,\n\tauthor = {Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu Marius},\n\ttitle = {Cellpose: a generalist algorithm for cellular segmentation.},\n\telocation-id = {s41592-020-01018-x},\n\tdoi = {10.1038/s41592-020-01018-x},\n\tjournal = {Nature Methods},\n\tpublisher = {Springer Nature},\n\tyear = {2021},\n}\n```\n```\n@article {oquab2024dinov2learningrobustvisual,\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \n      author={Maxime Oquab and Timoth√©e Darcet and Th√©o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv√© Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2024},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2304.07193}\n}\n\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ConvPaint"
    ],
    "contributions_sample_data": [
      "Label cell 3D"
    ]
  },
  {
    "normalized_name": "orchardseg",
    "name": "segtree",
    "display_name": "SegTree",
    "version": "0.0.2",
    "created_at": "2025-09-10",
    "modified_at": "2025-09-11",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@univ-angers.fr"
    ],
    "license": "Copyright (c) 2025, Herearii M...",
    "home_pypi": "https://pypi.org/project/orchardseg/",
    "home_github": "https://github.com/hereariim/segtree",
    "home_other": null,
    "summary": "Individual tree segmentation",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10.16",
    "package_metadata_requires_dist": [
      "torch>=2.3.1",
      "torchvision>=0.18.1",
      "numpy",
      "magicgui",
      "qtpy",
      "aiofiles==23.2.1",
      "altair==5.5.0",
      "annotated-types==0.7.0",
      "antlr4-python3-runtime==4.9.3",
      "anyio==4.9.0",
      "attrs==25.3.0",
      "certifi==2025.1.31",
      "charset-normalizer==3.4.1",
      "click==8.1.8",
      "coloredlogs==15.0.1",
      "contourpy==1.3.1",
      "cycler==0.12.1",
      "fastapi==0.115.12",
      "ffmpy==0.5.0",
      "filelock==3.18.0",
      "flatbuffers==25.2.10",
      "fonttools==4.56.0",
      "fsspec==2025.3.2",
      "gradio==4.29.0",
      "gradio-client==0.16.1",
      "gradio-imageslider==0.0.20",
      "h11==0.14.0",
      "httpcore==1.0.7",
      "httpx==0.28.1",
      "huggingface-hub==0.30.1",
      "humanfriendly==10.0",
      "hydra-core==1.3.2",
      "idna==3.10",
      "imageio==2.37.0",
      "importlib-resources==6.5.2",
      "iopath==0.1.10",
      "jinja2==3.1.6",
      "joblib==1.4.2",
      "jsonschema==4.23.0",
      "jsonschema-specifications==2024.10.1",
      "kiwisolver==1.4.8",
      "lazy-loader==0.4",
      "markdown-it-py==3.0.0",
      "markupsafe==2.1.5",
      "matplotlib==3.10.1",
      "mdurl==0.1.2",
      "mpmath==1.3.0",
      "narwhals==1.33.0",
      "networkx==3.4.2",
      "numpy==1.26.4",
      "omegaconf==2.3.0",
      "onnx==1.17.0",
      "onnxruntime==1.21.0",
      "opencv-python==4.11.0.86",
      "orjson==3.10.16",
      "pandas==2.2.3",
      "pillow==10.4.0",
      "portalocker==3.1.1",
      "protobuf==6.30.2",
      "py-cpuinfo==9.0.0",
      "pycocotools==2.0.8",
      "pydantic==2.11.1",
      "pydantic-core==2.33.0",
      "pydub==0.25.1",
      "pyparsing==3.2.3",
      "pyreadline3==3.5.4",
      "python-multipart==0.0.20",
      "pytz==2025.2",
      "pyyaml==6.0.2",
      "referencing==0.36.2",
      "regex==2024.11.6",
      "requests==2.32.3",
      "rich==14.0.0",
      "rpds-py==0.24.0",
      "ruff==0.11.2",
      "safetensors==0.5.3",
      "scikit-image==0.25.2",
      "scikit-learn==1.6.1",
      "scipy==1.15.2",
      "seaborn==0.13.2",
      "semantic-version==2.10.0",
      "shellingham==1.5.4",
      "sniffio==1.3.1",
      "starlette==0.46.1",
      "sympy==1.13.1",
      "threadpoolctl==3.6.0",
      "tifffile==2025.3.30",
      "timm==1.0.15",
      "tokenizers==0.21.1",
      "tomlkit==0.12.0",
      "tqdm==4.67.1",
      "transformers==4.50.3",
      "typer==0.15.2",
      "typing-inspection==0.4.0",
      "tzdata==2025.2",
      "ultralytics==8.3.99",
      "ultralytics-thop==2.0.14",
      "urllib3==2.3.0",
      "uvicorn==0.34.0",
      "websockets==11.0.3",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# üå≥ Segtree\n\n[![License BSD-3](https://img.shields.io/pypi/l/segtree.svg?color=green)](https://github.com/hereariim/segtree/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/segtree.svg?color=green)](https://pypi.org/project/orchardseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/segtree.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/segtree/workflows/tests/badge.svg)](https://github.com/hereariim/segtree/actions)\n[![codecov](https://codecov.io/gh/hereariim/segtree/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/segtree)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/segtree)](https://napari-hub.org/plugins/segtree)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nSegtree is a plugin designed for the segmentation of individual trees from imagery. It isolates tree in ovelapping tree context.\n\n**Input:** RGB images or preprocessed point clouds.\n\n**Output:** Binary masks of segmentated tree.\n\n![alt text](<Screenshot from 2025-09-10 17-01-24.png>)\n\n## How it works\n\nThe user provides as input a **color image** and, optionally, a **trunk detector mask**.\nWith a single click, the plugin automatically places prompt points on the detected trunk labels. These prompts are then used by [SAM2 HQ](https://github.com/SysCV/sam-hq/blob/main/sam-hq2/README.md#citing-hq-sam-2) (Segment Anything Model v2, High Quality mode) to segment the tree foliage associated with each trunk.\n\n![alt text](<Screenshot from 2025-09-10 15-30-09.png>)\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Article : Individual Segmentation of Intertwined Apple Trees in a Row via Prompt Engineering\n\nMETUAREA, Herearii, LAURENS, Fran√ßois, GUERRA, Walter, et al. [Individual Segmentation of Intertwined Apple Trees in a Row via Prompt Engineering](https://www.mdpi.com/1424-8220/25/15/4721). Sensors, 2025, vol. 25, no 15, p. 4721.\n\n## Citing segtree\n\nIf you find segtree useful in your research, please star ‚≠ê this repository and consider citing üìù:\n\n```\n@article{metuarea2025individual,\n  title={Individual Segmentation of Intertwined Apple Trees in a Row via Prompt Engineering},\n  author={Metuarea, Herearii and Laurens, Fran{\\c{c}}ois and Guerra, Walter and Lozano, Lidia and Patocchi, Andrea and Van Hoye, Shauny and Dutagaci, Helin and Labrosse, Jeremy and Rasti, Pejman and Rousseau, David},\n  journal={Sensors},\n  volume={25},\n  number={15},\n  pages={4721},\n  year={2025},\n  publisher={MDPI}\n}\n```\n\n## Installation\n\nYou can install `orchardseg` via [pip]:\n\n    pip install orchardseg\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/segtree.git\n\n\n## Contact\n\nImhorphen team, bioimaging research group\n\n42 rue George Morel, Angers, France\n\n- Pr David Rousseau, david.rousseau@univ-angers.fr\n- Herearii Metuarea, herearii.metuarea@univ-angers.fr\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"segtree\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/hereariim/segtree/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Individual Tree Segmentation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "orseg",
    "name": "orseg",
    "display_name": "Orseg",
    "version": "0.0.2",
    "created_at": "2025-09-10",
    "modified_at": "2025-09-11",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@univ-angers.fr"
    ],
    "license": "Copyright (c) 2025, Herearii M...",
    "home_pypi": "https://pypi.org/project/orseg/",
    "home_github": "https://github.com/hereariim/orseg",
    "home_other": null,
    "summary": "Segtree is a plugin designed for the segmentation of individual trees from imagery. It isolates tree in ovelapping tree context.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10.16",
    "package_metadata_requires_dist": [
      "torch>=2.3.1",
      "torchvision>=0.18.1",
      "numpy",
      "magicgui",
      "qtpy",
      "aiofiles==23.2.1",
      "altair==5.5.0",
      "annotated-types==0.7.0",
      "antlr4-python3-runtime==4.9.3",
      "anyio==4.9.0",
      "attrs==25.3.0",
      "certifi==2025.1.31",
      "charset-normalizer==3.4.1",
      "click==8.1.8",
      "coloredlogs==15.0.1",
      "contourpy==1.3.1",
      "cycler==0.12.1",
      "fastapi==0.115.12",
      "ffmpy==0.5.0",
      "filelock==3.18.0",
      "flatbuffers==25.2.10",
      "fonttools==4.56.0",
      "fsspec==2025.3.2",
      "gradio==4.29.0",
      "gradio-client==0.16.1",
      "gradio-imageslider==0.0.20",
      "h11==0.14.0",
      "httpcore==1.0.7",
      "httpx==0.28.1",
      "huggingface-hub==0.30.1",
      "humanfriendly==10.0",
      "hydra-core==1.3.2",
      "idna==3.10",
      "imageio==2.37.0",
      "importlib-resources==6.5.2",
      "iopath==0.1.10",
      "jinja2==3.1.6",
      "joblib==1.4.2",
      "jsonschema==4.23.0",
      "jsonschema-specifications==2024.10.1",
      "kiwisolver==1.4.8",
      "lazy-loader==0.4",
      "markdown-it-py==3.0.0",
      "markupsafe==2.1.5",
      "matplotlib==3.10.1",
      "mdurl==0.1.2",
      "mpmath==1.3.0",
      "narwhals==1.33.0",
      "networkx==3.4.2",
      "numpy==1.26.4",
      "omegaconf==2.3.0",
      "onnx==1.17.0",
      "onnxruntime==1.21.0",
      "opencv-python==4.11.0.86",
      "orjson==3.10.16",
      "pandas==2.2.3",
      "pillow==10.4.0",
      "portalocker==3.1.1",
      "protobuf==6.30.2",
      "py-cpuinfo==9.0.0",
      "pycocotools==2.0.8",
      "pydantic==2.11.1",
      "pydantic-core==2.33.0",
      "pydub==0.25.1",
      "pyparsing==3.2.3",
      "pyreadline3==3.5.4",
      "python-multipart==0.0.20",
      "pytz==2025.2",
      "pyyaml==6.0.2",
      "referencing==0.36.2",
      "regex==2024.11.6",
      "requests==2.32.3",
      "rich==14.0.0",
      "rpds-py==0.24.0",
      "ruff==0.11.2",
      "safetensors==0.5.3",
      "scikit-image==0.25.2",
      "scikit-learn==1.6.1",
      "scipy==1.15.2",
      "seaborn==0.13.2",
      "semantic-version==2.10.0",
      "shellingham==1.5.4",
      "sniffio==1.3.1",
      "starlette==0.46.1",
      "sympy==1.13.1",
      "threadpoolctl==3.6.0",
      "tifffile==2025.3.30",
      "timm==1.0.15",
      "tokenizers==0.21.1",
      "tomlkit==0.12.0",
      "tqdm==4.67.1",
      "transformers==4.50.3",
      "typer==0.15.2",
      "typing-inspection==0.4.0",
      "tzdata==2025.2",
      "ultralytics==8.3.99",
      "ultralytics-thop==2.0.14",
      "urllib3==2.3.0",
      "uvicorn==0.34.0",
      "websockets==11.0.3",
      "napari[all]; extra == \"all\""
    ],
    "package_metadata_description": "# orseg\n\n[![License BSD-3](https://img.shields.io/pypi/l/orseg.svg?color=green)](https://github.com/hereariim/orseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/orseg.svg?color=green)](https://pypi.org/project/orseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/orseg.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/orseg/workflows/tests/badge.svg)](https://github.com/hereariim/orseg/actions)\n[![codecov](https://codecov.io/gh/hereariim/orseg/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/orseg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/orseg)](https://napari-hub.org/plugins/orseg)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nSegtree is a plugin designed for the segmentation of individual trees from imagery. It isolates tree in ovelapping tree context.\n\n![alt text](<src/orseg/Screenshot from 2025-09-10 17-01-24.png>)\n\n## How it works\n\nThe user provides as input a **color image** and, optionally, a **trunk detector mask**.\nWith a single click, the plugin automatically places prompt points on the detected trunk labels. These prompts are then used by [SAM2 HQ](https://github.com/SysCV/sam-hq/blob/main/sam-hq2/README.md#citing-hq-sam-2) (Segment Anything Model v2, High Quality mode) to segment the tree foliage associated with each trunk.\n![alt text](<src/orseg/Screenshot from 2025-09-10 15-30-09.png>)\n\n## Article : Individual Segmentation of Intertwined Apple Trees in a Row via Prompt Engineering\n\nMETUAREA, Herearii, LAURENS, Fran√ßois, GUERRA, Walter, et al. [Individual Segmentation of Intertwined Apple Trees in a Row via Prompt Engineering](https://www.mdpi.com/1424-8220/25/15/4721). Sensors, 2025, vol. 25, no 15, p. 4721.\n\n## Citing segtree\n\nIf you find segtree useful in your research, please star ‚≠ê this repository and consider citing üìù:\n\n```\n@article{metuarea2025individual,\n  title={Individual Segmentation of Intertwined Apple Trees in a Row via Prompt Engineering},\n  author={Metuarea, Herearii and Laurens, Fran{\\c{c}}ois and Guerra, Walter and Lozano, Lidia and Patocchi, Andrea and Van Hoye, Shauny and Dutagaci, Helin and Labrosse, Jeremy and Rasti, Pejman and Rousseau, David},\n  journal={Sensors},\n  volume={25},\n  number={15},\n  pages={4721},\n  year={2025},\n  publisher={MDPI}\n}\n```\n\n## Installation\n\nYou can install `orseg` via [pip]:\n\n```\npip install orseg\n```\n\nIf napari is not already installed, you can install `orseg` with napari and Qt via:\n\n```\npip install \"orseg[all]\"\n```\n\n\nTo install latest development version :\n\n```\npip install git+https://github.com/hereariim/orseg.git\n```\n\n## Contact\n\nImhorphen team, bioimaging research group\n\n42 rue George Morel, Angers, France\n\n- Pr David Rousseau, david.rousseau@univ-angers.fr\n- Herearii Metuarea, herearii.metuarea@univ-angers.fr\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"orseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/hereariim/orseg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Individual Tree Segmentation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-dmc-brainmap",
    "name": "napari-dmc-brainmap",
    "display_name": "dmc_brainmap",
    "version": "0.1.7b11",
    "created_at": "2025-02-18",
    "modified_at": "2025-09-09",
    "authors": [
      "Felix Jung"
    ],
    "author_emails": [
      "jung.neurosc@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-dmc-brainmap/",
    "home_github": null,
    "home_other": "None",
    "summary": "DMC-BrainMap is an end-to-end tool for multi-feature brain mapping across species",
    "categories": [],
    "package_metadata_requires_python": "==3.10.*",
    "package_metadata_requires_dist": [
      "numpy==1.26.4",
      "pandas==2.0.1",
      "matplotlib==3.8.3",
      "seaborn==0.12.2",
      "scikit-learn==1.4.1.post1",
      "scikit-image==0.22.0",
      "scikit-spatial==7.2.0",
      "tifffile==2023.2.28",
      "magicgui==0.8.1",
      "qtpy==2.4.1",
      "opencv-python==4.9.0.80",
      "natsort==8.4.0",
      "imagecodecs==2024.1.1",
      "mergedeep==1.3.4",
      "aicsimageio==4.14.0",
      "aicspylibczi==3.1.2",
      "aicssegmentation==0.5.3",
      "distinctipy==1.3.4",
      "bg_atlasapi==1.0.2",
      "shapely==2.0.1"
    ],
    "package_metadata_description": "\n# napari-dmc-brainmap\n*DMC-BrainMap is an end-to-end tool for multi-feature brain mapping across species.*  \nThis [napari](https://napari.org/stable/) plugin was generated with [Cookiecutter](https://github.com/cookiecutter/cookiecutter) using napari's [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) template.\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-dmc-brainmap.svg?color=green)](https://github.com/hejDMC/napari-dmc-brainmap/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-dmc-brainmap.svg?color=green)](https://pypi.org/project/napari-dmc-brainmap)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dmc-brainmap.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-dmc-brainmap)](https://napari-hub.org/plugins/napari-dmc-brainmap)\n\n\n## Quick start\nA detailed guide and tutorial can be found on the [Wiki pages of this repo](https://github.com/hejDMC/napari-dmc-brainmap/wiki).\n\n### Installation\n\nDMC-BrainMap is a plugin for [napari](https://napari.org/stable/). Hence, you first need to install napari and subsequently the DMC-BrainMap plugin via the plugin manager. To install napari, we recommend to install napari into a clean virtual environment using *conda* or *venv*. Please refer to the [napari installation guide](https://napari.org/stable/tutorials/fundamentals/installation.html#napari-installation) for more information and [for information on installing napari as a bundled app](https://napari.org/stable/tutorials/fundamentals/installation.html#napari-installation).  \n\n#### Step 1: Setup the virtual environment (Python 3.10)\n\n```\nconda create -y -n napari-env -c conda-forge python=3.10\nconda activate napari-env\n```\n\n#### Step 2: Install napari\n\n```\npython -m pip install \"napari[all]\"\n```\n\n#### Step 3: Install napari-dmc-brainmap\n\nYou can install `napari-dmc-brainmap` via the napari plugin manager or via [pip](https://pypi.org/project/napari-dmc-brainmap/):\n\n    pip install napari-dmc-brainmap\n\n### Usage\n\nPlease refer to the Wiki pages for detailed instructions and a short tutorial on how to use DMC-BrainMap. When working with DMC-BrainMap on your own data, please keep the following points in mind:\n- DMC-BrainMap requires single-channel 16-bit .tif/.tiff images to work (in principle 8-bit also work)\n- DMC-BrainMap requires that your data is organized by animals in separate folders (you can pool data later down the lane)\n- DMC-BrainMap uses 5 channel labels (`dapi`, `green`, `n3`, `cy3`, `cy5`) corresponding to blue, green, orange, red and far red channels. *However, these are only labels, you can assign them as you please. Hence, you can use DMC-BrainMap also for non-fluorescence data given you converted your images to single-channel 16-bit .tif/.tiff images*. Please contact us if you need to use more than 5 channels.\n- It is essential that you structure your data in the following way (hierarchical organization, same name for images in different channels, channel labels are selected by you), **otherwise DMC-BrainMap won't work**:\n```\nanimal_id-001\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄstitched\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄdapi\n‚îÇ   |    ‚îÇ   animal_id-001_001.tiff\n‚îÇ   |    ‚îÇ   animal_id-001_002.tiff\n|   ‚îÇ    |   animal_id-001_003.tiff\n‚îÇ   |    ‚îÇ   animal_id-001_004.tiff\n‚îÇ   |    ‚îÇ   ...\n‚îÇ   ‚îÇ   \n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄgreen\n‚îÇ       ‚îÇ   animal_id-001_001.tiff\n‚îÇ       ‚îÇ   animal_id-001_002.tiff\n‚îÇ       ‚îÇ   animal_id-001_003.tiff\n‚îÇ       ‚îÇ   animal_id-001_004.tiff\n‚îÇ       ‚îÇ   ...\n‚îÇ   \nanimal_id-2\n‚îÇ   ...\n```\n\n## Documentation\nDocumentation on DMC-BrainMap's source code can be found on the project's [Read the Docs page](https://napari-dmc-brainmap.readthedocs.io/en/latest/#).\n\n## Seeking help or contributing\n\nDMC-BrainMap is an open-source project, and we welcome contributions of all kinds. If you have any questions, feedback, or suggestions, please feel free to open an issue on this repository. \n\n## License\n\nDistributed under the terms of the [BSD-3](https://github.com/teamdigitale/licenses/blob/master/BSD-3-Clause) license,\n\"napari-dmc-brainmap\" is free and open source software\n\n## Citing DMC-BrainMap\n\nIf you use DMC-BrainMap in your scientific work, please cite:\n```\nJung, F., Cao, X., Heymans, L., Carl√©n, M. (2025) \"DMC-BrainMap - an open-source, end-to-end tool for multi-feature brain mapping across species\", bioRxiv, https://doi.org/10.1101/2025.02.19.639009\n```\n\nBibTeX:\n\n``` bibtex\n@article{Jung2025x,\n   author = {Felix Jung and Xiao Cao and Loran Heymans and Marie Carlen},\n   doi = {10.1101/2025.02.19.639009},\n   journal = {bioRxiv},\n   month = {2},\n   title = {DMC-BrainMap - an open-source, end-to-end tool for multi-feature brain mapping across species},\n   url = {http://biorxiv.org/lookup/doi/10.1101/2025.02.19.639009},\n   year = {2025},\n}\n```\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Create params file (general info -- ALWAYS start with this!)",
      "Stitch tif images",
      "Padding of stitched images to match atlas resolution",
      "Preprocessing",
      "Registration",
      "Segmentation",
      "Create results file",
      "Visualize data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-vascilia",
    "name": "napari-vascilia",
    "display_name": "VASCilia",
    "version": "1.4.0",
    "created_at": "2024-06-28",
    "modified_at": "2025-09-08",
    "authors": [
      "Yasmin Kassim"
    ],
    "author_emails": [
      "ymkgz8@mail.missouri.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-vascilia/",
    "home_github": "https://github.com/ucsdmanorlab/Napari-VASCilia",
    "home_other": null,
    "summary": "A plugin for deep learning-based 3D analysis of cochlear hair cell stereocilia bundles.",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "numpy==1.26.4",
      "scikit-learn==1.3.2",
      "opencv-python",
      "matplotlib",
      "imagecodecs",
      "tifffile",
      "napari[all]",
      "readlif",
      "czitools==0.4.1",
      "npe2",
      "colormap==1.1.0",
      "segmentation-models-pytorch==0.3.3",
      "pretrainedmodels==0.7.4"
    ],
    "package_metadata_description": "# VASCilia (Vision Analysis StereoCilia): A Napari Plugin for Deep Learning-Based 3D Analysis of Cochlear Hair Cell Stereocilia Bundles \n\n\n\n<p align=\"left\">\n\n  <img src=\"images/logo_3d.png\" alt=\"VASCilia Logo\" width=\"170\">\n\n</p>\n\n\n\n## Documentation\n\n\n\n**Check out the full documentation for this project at [this link](https://ucsdmanorlab.github.io/Napari-VASCilia/).**\n\n\n\nExplore the complexities of the cochlea with VASCilia, a Napari plugin created to aid in the 3D segmentation and quantification of stereocilia bundles. Equipped with a range of thoughtful features, VASCilia stands for (Vision Analysis StereoCilia) and it provides a supportive tool for auditory research, including:  \n\n1. Slice Selection: Easily navigate through 3D stacks to find the slices that matter most for your research.\n\n2. Stack Rotation: Adjust the orientation of your stack to facilitate better analysis.\n\n3. 3D Instance Segmentation: Identify and assess individual bundles with clear separation using deep learning.\n\n4. Bundle Deletion: Remove unwanted bundles to streamline your dataset.\n\n5. Regional Classification: identify whether the region is from BASE, MIDDLE, or APEX in the cochlea using deep learning.\n\n6. Hair Cell Differentiation: Distinguish between Inner Hair Cells and Outer Hair Cells with confidence using deep learning.\n\n7. Measurement Analysis: Calculate various measurements such as volume, centroid location, and surface area.\n\n8. Fluorescence Intensity Analysis: Assess the intensity of signal or protein with detailed precision.\n\n9. 3D Bundle Height Calculation: Measure the 3D distance from the peak to the base of each bundle, according to your sample's resolution.\n\n10. Bundle orientation: Determine bundle orientation for all hair cells based on two strategies: Height-only and Height&Distance.\n\n\n\nVASCilia &#x2764;&#xfe0f; is a valuable resource for the ear research community &#128066;, simplifying the complexity of measurement and analysis. It comes with a suite of pre-trained models to facilitate 3D segmentation, cell type identification and regional classification.\n\n\n\nFurthermore, we are committed to supporting research growth with a comprehensive training section for those looking to explore different staining techniques or develop new segmentation models through annotation and refinement.\n\n\n\nVASCilia is here to support researchers in their quest for deeper understanding and innovation in the study of cochlear structures.  \n\n*[click the image to see a highlights reel of the plugin](https://youtu.be/MwMOxJQ_elo)*  \n\n\n\n[![Watch the video](images/main_pipeline.png)](https://youtu.be/MwMOxJQ_elo)\n\n\n\n*[Click me to see a video demo of the entire workflow](https://youtu.be/mNPJ1g0vEW8)*  \n\n\n\n## How to install : \n\nSTEP1[Install WSL]:  \n\n1. Open the Command Prompt and install the Ubuntu 20.04 Distribution by simply copy paste this command  \n\nwsl --install -d Ubuntu-20.04\n\n2. After the setup successfully completes, reboot your computer. Open Ubuntu by typing \"Ubuntu\" in the search bar. A pop-up window for Ubuntu will appear. To check if CUDA and the GPU are correctly installed and available, type nvidia-smi in the terminal  \n\n\n\nSTEP2[Download the deep learning trained models]:\n\n1. Download the VASCilia_trained_models from https://www.dropbox.com/scl/fo/jsvldda8yvma3omfijxxn/ALeDfYUbiOuj69Flbc728rs?rlkey=mtilfz33qiizpul7uyisud5st&st=41kjlbw0&dl=0 \n\nnow you should have a folder called 'models'\n\n\n\n- üìÅ **models** `[Trained models]`\n\n    - üìÅ **cell_type_identification_model** `[has weights for cell type identification IHC vs OHC]`\n\n    - üìÅ **new_seg_model** `[incase you fine tune the existing model, the new model will be stored here]`\n\n    - üìÅ **region_prediction** `[has weights for region prediction]`\n\n    - üìÅ **seg_model**  `[has the weights for the 3D instance segmentation model]`\n\n    - üìÅ **Train_predict_stereocilia_exe** `[executible needed by the plugin to segment and retrain the model using WSL]`  \n\n    - üìÅ **ZFT_trim_model** `[deep learning model weights for z focus tracker algorithm]`  \n\n    - üìÅ **rotation_correction_model** `[deep learning model weights for correcting the orientation of the stack]`  \n\n \n\nSTEP3[download one dataset to test VASCilia]:  \n\ndownload one sample from our datasets to try in this link https://www.dropbox.com/scl/fo/pg3i39xaf3vtjydh663n9/h?rlkey=agtnxau73vrv3ism0h55eauek&dl=0  \n\ncreate a folder, called raw_data folder and put the downloaded dataset inside the raw_data folder\n\n\n\n  - üìÅ **raw_data** `[raw data (stacks) is placed here]`\n\n    - üìÑ Litter 12 Mouse 4 MIDDLE - delBUNdelCAP_Airyscan Processing.czi\n\n   \n\nAlso create another folder called processed_data in which the plugin will use to store the results of the analysis\n\n  \n\n  - üìÅ **processed_data** `[processed data will be stored here]`\n\n\n\n## Instructions for Cloning the Repository [You can do either Option A or Option B]:\n\n## Option A: From source (recommended for dev):  \n\n```sh\n\ngit clone https://github.com/ucsdmanorlab/Napari-VASCilia.git\n\ncd Napari-VASCilia\n\nconda create -y -n napari-VASCilia -c conda-forge python=3.10    \n\nconda activate napari-VASCilia    \n\npip install -r requirements.txt\n\npip install -e .\n\nnapari  \n\n```\n\n## Option B: From PyPI (end users):\n\n```sh\n\nconda create -y -n napari-VASCilia -c conda-forge python=3.10    \n\nconda activate napari-VASCilia \n\npip install --extra-index-url https://download.pytorch.org/whl/cu113 torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113\n\npip install napari-vascilia\n\nnapari  \n\n```\n\nPost-installation:  \n\n1. Activate the plugin through Plugins -> VASCilia UI (Napari-VASCilia).\n\n2. This will generate the config.json file at C:/Users/Username/.napari-vascilia/config.json. Update the paths in config.json as needed.\n\nconfig.json will be generated upon running the plugin for the first time.\n\n\n\n- üìÅ C:/Users/Username/   [your home folder]\n\n  - üìÅ **.napari-vascilia** `[Folder_path]`\n\n    - üìÑ **config.json**\n\n\n\nPlease update the /.../ portion according to your paths:\n\n\n\n```sh\n\n{\n\n    \"rootfolder\": \"C:/Users/.../processed_data/\",\n\n    \"wsl_executable\": \"C:/Users/.../models/Train_predict_stereocilia_exe/Train_Predict_stereocilia_exe_v2\",\n\n    \"model\": \"C:/Users/.../models/seg_model/stereocilia_v7/\",\n\n    \"model_output_path\": \"C:/Users/.../models/new_seg_model/stereocilia_v8/\",\n\n    \"model_region_prediction\": \"C:/Users/.../models/region_prediction/resnet50_best_checkpoint_resnet50_balancedclass.pth\",\n\n    \"model_celltype_identification\": \"C:/Users/.../models/cell_type_identification_model/\",\n\n    \"ZFT_trim_model\": \"C:/Users/.../models/ZFT_trim_model/\",\n\n    \"rotation_correction_model\": \"C:/Users/.../models/rotation_correction_model/\",\n\n    \"green_channel\": 0,\n\n    \"red_channel\": 1,\n\n    \"blue_channel\": -1,\n\n    \"signal_intensity_channel\": 0,\n\n    \"subtract_background\": True,\n\n    \"dilate_labels\": False,\n\n    \"flag_to_upscale\": False,\n\n    \"flag_to_downscale\": False,\n\n    \"flag_to_pad\": False,\n\n    \"resize_dimension\": 1500,\n\n    \"pad_dimension\": 2000,\n\n    \"force_manual_resolution\": 0,\n\n    \"button_width\": 60,\n\n    \"button_height\": 18\n\n}\n\n```\n\n\n\nCongratulations :) &#127881;, now you can enjoy working with the plugin. \n\n\n\n## Unique about VASCilia :  \n\nVASCilia saves all the intermediate results and the variables inside a pickle file while the user is using it in a very effiecint way. That allows a super fast uploading for the analysis if the user or their supervisor wants to keep working or review the analysis steps.  \n\n*[Click me to learn how to upload a z-stack](https://youtu.be/Sxm_fsjoWL0)*  \n\n\n\n## How to use VASCilia :  \n\n*[Click me to see a video demo of the entire workflow](https://youtu.be/mNPJ1g0vEW8)*  \n\n\n\nThere are several buttons inside the blugin in the right hand side of Napari:\n\n\n\n1. 'Open CZI Cochlea Files and Preprocess' button: read the CZI file.\n\n2. 'Upload Processed CZI Stack' button: Incase you already have processed the stack, then just uplead your Analysis_state.pkl that usually has all the variables needed to upload your analysis\n\n3. 'Trim Full Stack' button: this button allows you to choose only the slices of interest (has been automated in v_1_1_0)\n\n4. \"Rotate' buttom: this button allows to rotate the stack to have proper analysis (has been automated in v_1_1_0)  \n\n5. Segment with 3DBundleSeg: it is a two steps algorithm (2D detection + multi-object assignment algorithm across all slices) to produce robust 3D detection. 3DBundleSeg is the first instance segmentation model for stereocilia bundles in the literature. It is trained on P5 and P21 3D stacks (thousands of 2D instances) and it produces highly acccurate boundary delineation even in the most challenging datasets. Here are some examples:  \n\n\n\n<p align=\"center\">\n\n  <strong>3DBundleSeg can tackle challenged cases</strong>  \n\n  <br>\n\n  <img src=\"images/challenged_cases_gray.png\" width=\"100%\">\n\n</p>\n\n\n\n<p align=\"center\">\n\n  <strong>Multi-object assignment algorithm to produce robust 3D detection</strong>  \n\n  <br>\n\n  <img src=\"images/3DBundleSeg.png\" width=\"100%\">\n\n</p>\n\n\n\n\n\n6. Delete Label 'button': delete the unwanted detection if it is near the boundary or for any other reason.\n\n7. Calculate measurments 'button': calculate different measurments from the detected bundles and store them in csv file\n\n8. Calculate Bundle Height 'button': compute the 3D distance from the highest point in the 3D detection of each bundle to it's base. This calculation will consider the sample resolution.\n\n9. Perform Cell Clustering 'button': find the IHC, OHC1, OHC2, and OHC3 using either GMM, Kmeans or Deep Learning. Those layers will be added to the plugin to be used during the analysis. \n\n10. Compute Fluorescence Intensity 'button': produce plots and CSV files that has the accumelated intensity and mean intensity for the fluorescence signal.\n\n11. Predict Region 'button': Predict whether the region is from the BASE, MIDDLE, or APEX region using a RESNET50 trained model. \n\n12. Compute Orientation: It computes the orientation using two strategies.\n\n\n\n<p align=\"center\">\n\n  <strong>Bundle Height with top and bottom adjustable points in red and green, orientation with two points in magenta, and bundle ID in green</strong>  \n\n  <br>\n\n  <img src=\"images/Bundles.png\" width=\"50%\">\n\n</p>\n\n\n\n<p align=\"center\">\n\n  <strong>Cell type identification (IHC1 in yellow, OHC1 in cyan, OHC2 in green, and OHC3 in magenta)</strong>  \n\n  <br>\n\n  <img src=\"images/clustering.png\" width=\"50%\">\n\n</p>\n\n\n\n13. Training Section.\n\n\n\n<p align=\"center\">\n\n  <strong>Training section</strong>  \n\n  <br>\n\n  <img src=\"images/Training_section2.png\" width=\"40%\">\n\n</p>\n\n\n\nThe training section is for the research ear community incase their datasets are little different than ours then they can easily create their cround truth, train a new model and use it in the plugin\n\n1. Create/Save Ground Truth 'button': this button will create a new layer to draw new ground truth and save them as variables inside the plugin\n\n2. Generate Ground Truth Mask 'button': this button will save all the generated masks after finish annotating to a new folder. \n\n3. Display Stored Ground Truth 'button': this button will display the stored masks in the plugin.\n\n4. Copy Segmentation Masks to Ground Truth 'button': this button helps in speeding up the annotation process by copying what our trained model is producing sothat the annotator will only correct the wrong part.\n\n5. Move Ground Truth to Training Folder 'button': this button will move all the annotated ground truth to the training folder to start the training process. \n\n6. Check Training Data 'button': this button checks the training data whether they follow the format needed by the architecture. It checks whether there are training and valiation folders and it reads every single file to make sure it doesn't have redundant or no information. It gives warning messages incase it finds an issue.\n\n7. Train New Model for 3DBundleSeg 'button': this button will start the training.\n\n\n\nVASCilia also equipped with two more buttons for resetting (to facilitate transitions between analyzing several stacks) and also exit VASCilia.  \n\nWe are still working on the documentation, so this gihub will be continiuosly updated.\n\n\n\n## Multi-Batch Processing Feature: Required File\n\nThe **Multi-Batch Processing** feature in this package requires an additional file 'file_names_for_batch_processing.csv' to be in the same path of your rootfolder in your config file. \n\n### Download the File\n\nYou can download the csv file from the following link and don't forget to change the paths https://www.dropbox.com/scl/fo/pg3i39xaf3vtjydh663n9/h?rlkey=agtnxau73vrv3ism0h55eauek&dl=0  \n\n:\n\n\n\n## Testing Other Lab Data  \n\nLiberman Data *[Click me to see a video demo of the entire workflow](https://youtu.be/PIG3q7G6Xr0)*  \n\nArtur Indzhykulian Data *[Click me to see a video demo of the entire workflow](https://youtu.be/WseYK4Zn-3o)*  \n\n\n\n## Paper and Citation\n\n\n\nThis work will be submitted very soon. If you want to read or cite the paper &#128522;, you can find it [here](https://doi.org/10.1101/2024.06.17.599381).  \n\n\n\nKassim, Y. M., Rosenberg, D. B., Renero, A., Das, S., Rahman, S., Al Shammaa, I., Salim, S., Huang, Z., Huang, K., Ninoyu, Y., Friedman, R. A., Indzhykulian, A. A., & Manor, U. (2024). VASCilia (Vision Analysis StereoCilia): A Napari Plugin for Deep Learning-Based 3D Analysis of Cochlear Hair Cell Stereocilia Bundles. bioRxiv. https://doi.org/10.1101/2024.06.17.599381\n\n\n\n## Project Authors and Contacts\n\n\n\n**Python Implementation of this repository:** Dr. Yasmin M. Kassim    \n\n**Contact:** ykassim@ucsd.edu, ymkgz8@mail.missouri.edu  \n\nYasmin Kassim was responsible for the plugin design, fully implemented all functions in Python, wrote the manuscript,\n\nproofread the ground truth data, created all figures, and established the GitHub repository and codebase.\n\n\n\n**Stacks used in this study imaged by:** Dr. David Rosenberg   \n\n\n\n**Height bundle ground truth analyses**: Samprita Das and Alma Renero.  \n\n\n\n**StereoCilia Bundles Ground Truth**: 55 (P5 and P21) 3D stacks were manually annotated by Yasmin Kassim and five undergraduate students using the CVAT annotation tool. This is an extremely challenging process, as each 3D stack might have up to 60 bundles in a 3D setting, which could translate to around 1000 bundles in a 2D setting across all frames. The students involved in this effort are:  \n\n**Samia Rahman, Ibraheem Al Shammaa, Samer Salim, Zhuoling Huang, and Kevin Huang**.\n\n\n\nThis dataset will be the first annotated dataset in the literature to 3D segment the stereocilia bundles and it will be published and available for the ear research community with the publication of this paper.\n\n\n\n**Other Lab Support**:   \n\nYuzuru Ninoyu assisted with some of the imaging data, with Rick Friedman‚Äôs supervision and support.   \n\nArtur Indzhykulian provided additional imaging data for testing.  \n\n\n\n**Lab Supervisor:** Dr. Uri Manor   \n\nThe Principal Investigator, conceived and supervised the project, and provided critical\n\nrevisions and updates to the manuscript.  \n\n\n\n**Contact:** u1manor@UCSD.EDU  \n\n**Department:** Cell and Development Biology Department/ UCSD  \n\n**Lab Website:** https://manorlab.ucsd.edu/\n\n\n\n\n\n\n\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "VASCilia UI"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "axondeepseg",
    "name": "AxonDeepSeg",
    "display_name": "AxonDeepSeg",
    "version": "5.2.0",
    "created_at": "2025-02-28",
    "modified_at": "2025-09-05",
    "authors": [
      "NeuroPoly Lab",
      "Polytechnique Montreal"
    ],
    "author_emails": [
      "\"NeuroPoly Lab",
      "Polytechnique Montreal\" <axondeepseg@googlegroups.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/axondeepseg/",
    "home_github": "https://github.com/axondeepseg/axondeepseg",
    "home_other": null,
    "summary": "Axon/Myelin segmentation using AI",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": "<3.13,>=3.11",
    "package_metadata_requires_dist": [
      "numpy<2",
      "scipy",
      "scikit-image!=0.25.0,!=0.25.1",
      "tabulate",
      "pandas",
      "matplotlib",
      "mpld3",
      "tqdm",
      "requests",
      "pillow!=9.0.0",
      "imageio>=2.28.0",
      "pytest",
      "pytest-cov",
      "prettytable",
      "jupyter",
      "openpyxl",
      "qtconsole<5.4.2",
      "napari[all]",
      "acvl_utils!=0.2.1",
      "nnunetv2==2.2.1",
      "loguru",
      "torch<2.4.0",
      "pydicom<3",
      "pytest-qt",
      "magicgui",
      "qtpy"
    ],
    "package_metadata_description": "\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/axondeepseg/doc-figures/blob/main/logo/logo_ads-dark-alpha.png?raw=true\" width=\"385\">\n  <img alt=\"ADS logo (simplified image of segmented axons/myelin in blue and red beside the text 'AxonDeepSeg')\" src=https://github.com/axondeepseg/doc-figures/blob/main/logo/logo_ads-alpha.png?raw=true\" width=\"385\">\n</picture>\n\n\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/neuropoly/axondeepseg/master?filepath=notebooks%2Fgetting_started.ipynb)\n[![Build Status](https://github.com/axondeepseg/axondeepseg/actions/workflows/run_tests.yaml/badge.svg)](https://github.com/axondeepseg/axondeepseg/actions/workflows/run_tests.yaml)\n[![Documentation Status](https://readthedocs.org/projects/axondeepseg/badge/?version=stable)](http://axondeepseg.readthedocs.io/en/latest/?badge=latest)\n[![Coverage Status](https://coveralls.io/repos/github/axondeepseg/axondeepseg/badge.svg?branch=master)](https://coveralls.io/github/axondeepseg/axondeepseg?branch=master)\n[![Twitter Follow](https://img.shields.io/twitter/follow/axondeepseg.svg?style=social&label=Follow)](https://twitter.com/axondeepseg)\n\nSegment axon and myelin from microscopy data using deep learning. Written in Python. Using the TensorFlow framework.\nBased on a convolutional neural network architecture. Pixels are classified as either axon, myelin or background.\n\nFor more information, see the [documentation website](http://axondeepseg.readthedocs.io/).\n\n![alt tag](https://github.com/axondeepseg/doc-figures/blob/main/animations/napari.gif?raw=true)\n\n\n\n## Help\n\nWhether you are a newcomer or an experienced user, we will do our best to help and reply to you as soon as possible. Of course, please be considerate and respectful of all people participating in our community interactions.\n\n* If you encounter difficulties during installation and/or while using AxonDeepSeg, or have general questions about the project, you can start a new discussion on the [AxonDeepSeg GitHub Discussions forum](https://github.com/neuropoly/axondeepseg/discussions). We also encourage you, once you've familiarized yourself with the software, to continue participating in the forum by helping answer future questions from fellow users!\n* If you encounter bugs during installation and/or use of AxonDeepSeg, you can open a new issue ticket on the [AxonDeepSeg GitHub issues webpage](https://github.com/neuropoly/axondeepseg/issues).\n\n\n\n\n### Napari plugin\n\nA tutorial demonstrating the basic features of our plugin for Napari is hosted on YouTube, and can be viewed by clicking [this link](https://www.youtube.com/watch?v=zibDbpko6ko).\n\n## References\n\n**AxonDeepSeg**\n\n* [Lubrano et al. *Deep Active Leaning for Myelin Segmentation on Histology Data.* Montreal Artificial Intelligence and Neuroscience 2019](https://arxiv.org/abs/1907.05143) - \\[[**source code**](https://github.com/neuropoly/deep-active-learning)\\]\n* [Zaimi et al. *AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks.* Scientific Reports 2018](https://www.nature.com/articles/s41598-018-22181-4)\n* [Collin et al. *Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images*. preprint](https://arxiv.org/abs/2409.11552v1) - \\[[**source code**](https://github.com/axondeepseg/model_seg_generalist)]\n\n**Applications**\n\n* [Tabarin et al. *Deep learning segmentation (AxonDeepSeg) to generate axonal-property map from ex vivo human optic chiasm using light microscopy.* ISMRM 2019](https://www.ismrm.org/19/program_files/DP23.htm) - \\[[**source code**](https://github.com/thibaulttabarin/UnAxSeg)\\]\n* [Lousada et al. *Characterization of cortico-striatal myelination in the context of pathological Repetitive Behaviors.*  International Basal Ganglia Society (IBAGS) 2019](http://www.ibags2019.com/key4register/images/client/863/files/Abstractbook1405.pdf)\n* [Duval et al. *Axons morphometry in the human spinal cord.* NeuroImage 2019](https://www.sciencedirect.com/science/article/pii/S1053811918320044)\n* [Yu et al. *Model-informed machine learning for multi-component T2 relaxometry.* Medical Image Analysis 2021](https://www.sciencedirect.com/science/article/pii/S1361841520303042) - \\[[**source code**](https://github.com/thomas-yu-epfl/Model_Informed_Machine_Learning)\\]\n\n**Reviews**\n\n* [Riordon et al. *Deep learning with microfluidics for biotechnology.* Trends in Biotechnology 2019](https://www.sciencedirect.com/science/article/pii/S0167779918302452)\n\n## Citation\n\nIf you use this work in your research, please cite it as follows:\n\nZaimi, A., Wabartha, M., Herman, V., Antonsanti, P.-L., Perone, C. S., & Cohen-Adad, J. (2018). AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks. Scientific Reports, 8(1), 3816. Link to paper: https://doi.org/10.1038/s41598-018-22181-4.\n\nCopyright (c) 2018 NeuroPoly (Polytechnique Montreal)\n\n## Licence\n\nThe MIT License (MIT)\n\nCopyright (c) 2018 NeuroPoly, √âcole Polytechnique, Universit√© de Montr√©al\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n## Contributors\n\nPierre-Louis Antonsanti, Stoyan Asenov, Mathieu Boudreau, Oumayma Bounou, Marie-H√©l√®ne Bourget, Julien Cohen-Adad, Victor Herman, Melanie Lubrano, Antoine Moevus, Christian Perone, Vasudev Sharma, Thibault Tabarin, Maxime Wabartha, Aldo Zaimi.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ADS"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-flim-phasor-plotter",
    "name": "napari-flim-phasor-plotter",
    "display_name": "FLIM phasor plotter",
    "version": "0.2.2",
    "created_at": "2023-08-17",
    "modified_at": "2025-09-05",
    "authors": [
      "Marcelo L. Zoccoler",
      "Cornelia Wetzker"
    ],
    "author_emails": [
      "marzoccoler@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-flim-phasor-plotter/",
    "home_github": "https://github.com/zoccoler/napari-flim-phasor-plotter",
    "home_other": null,
    "summary": "A plugin that performs phasor plot from TCSPC FLIM data.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari>=0.4.19",
      "napari-clusters-plotter<0.9.0,>=0.8.1",
      "ptufile",
      "sdtfile",
      "natsort",
      "rocket-fft",
      "dask",
      "zarr",
      "napari-segment-blobs-and-things-with-membranes",
      "napari-skimage-regionprops",
      "scikit-image>=0.20.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-flim-phasor-plotter\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-flim-phasor-plotter.svg?color=green)](https://github.com/zoccoler/napari-flim-phasor-plotter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-flim-phasor-plotter.svg?color=green)](https://pypi.org/project/napari-flim-phasor-plotter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-flim-phasor-plotter.svg?color=green)](https://python.org)\n[![tests](https://github.com/zoccoler/napari-flim-phasor-plotter/workflows/tests/badge.svg)](https://github.com/zoccoler/napari-flim-phasor-plotter/actions)\n[![codecov](https://codecov.io/gh/zoccoler/napari-flim-phasor-plotter/branch/main/graph/badge.svg)](https://codecov.io/gh/zoccoler/napari-flim-phasor-plotter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-flim-phasor-plotter)](https://napari-hub.org/plugins/napari-flim-phasor-plotter)\n[![DOI](https://zenodo.org/badge/578127094.svg)](https://zenodo.org/doi/10.5281/zenodo.12620955)\n\nNapari-flim-phasor-plotter is a [napari](https://napari.org/stable/) plugin to interactively load and show raw fluorescence lifetime imaging microscopy (FLIM) single images and series and generate phasor plots. These are Fourier transforms of the decay data being visualized using the [napari-clusters-plotter](https://github.com/BiAPoL/napari-clusters-plotter) plotter, adapted to suit the FLIM context. This allows qualitative and quantitative downstream analysis of FLIM images.  \n\n----------------------------------\n\n## Quick demo\n\n![](https://github.com/zoccoler/napari-flim-phasor-plotter/raw/main/images/napari_FLIM_phasor_calculator_Demo.gif)\n\n## Documentation\n\nPlease check our [documentation](https://napari-flim-phasor-plotter.readthedocs.io/en/stable/) for more details on how to install and use this plugin.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-flim-phasor-plotter\" is free and open source software. \n\nIf you use this plugin in a publication, please cite us: https://doi.org/10.5281/zenodo.12620956\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/zoccoler/napari-flim-phasor-plotter/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.PTU",
      "*.tif",
      "*.ptu",
      "*.zarr",
      "*.sdt",
      "*.SDT"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Calculate Phasors",
      "Convert Folder (Stack) to zarr",
      "Convert Folder (Stack) to ome-tif",
      "Convert Single File to ome-tif",
      "Apply binning to TCSPC FLIM data",
      "Phasor Plotter Widget",
      "Split N Largest Clusters Labels",
      "Manual Label Extraction",
      "Smooth Cluster Mask"
    ],
    "contributions_sample_data": [
      "Seminal Receptacle (2D Raw FLIM)",
      "Hazelnut (2D Raw FLIM)",
      "Hazelnut (3D Raw FLIM)",
      "Lifetime Cat (2D Raw Synthetic FLIM)"
    ]
  },
  {
    "normalized_name": "fishfeats",
    "name": "fishfeats",
    "display_name": "fishfeats",
    "version": "1.2.2",
    "created_at": "2025-08-19",
    "modified_at": "2025-09-04",
    "authors": [
      "Ga√´lle Letort"
    ],
    "author_emails": [],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/fishfeats/",
    "home_github": "https://github.com/gletort/FishFeats",
    "home_other": null,
    "summary": "Napari plugin for RNA-Fish+cells analysis pipeline",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari<0.6.5",
      "numpy",
      "imaris-ims-file-reader",
      "czifile",
      "tifffile",
      "munkres",
      "matplotlib",
      "scikit-image",
      "opencv-python-headless",
      "lxml",
      "packaging",
      "big-fish>=0.6.2; extra == \"full\"",
      "epyseg; extra == \"full\"",
      "cellpose[distributed]; extra == \"full\"",
      "stardist; extra == \"full\""
    ],
    "package_metadata_description": "# Fish&Feats ![snap](./docs/imgs/snap.png)\n\n[![License BSD-3](https://img.shields.io/pypi/l/fishfeats.svg?color=green)](https://github.com/gletort/FishFeats/-/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/fishfeats.svg?color=green)](https://pypi.org/project/fishfeats)\n[![Python Version](https://img.shields.io/pypi/pyversions/fishfeats.svg?color=green)](https://python.org)\n\n[Napari](https://napari.org/stable/) plugin to quantify 3D cells in a tissue and their smRNA-Fish or other RNA contents.\n\n[main.webm](https://github.com/user-attachments/assets/7eda5fa8-3241-4af8-b392-bc3e64aa31b9)\n\n\nFishFeats offers several flexible options to analyse 3D cells and RNA counts, from segmentation of apical cells and nuclei to hierarchical clustering of cells based on their RNA contents. \nInstallation/Usage are all described in the [documentation](https://gletort.github.io/FishFeats/).\n\n![main interface](./docs/imgs/Main_snapshot.png)\n\n## Installation\n\nPlease refer to our [documentation page](https://gletort.github.io/FishFeats/Installation/) for more details on the installation.\n\n`FishFeats` is distributed as a pip module on pypi.\nIt can be installed by typing in a python virtual environement:\n```\npip install fishfeats\n``` \n\nSome options of `Fishfeats` rely on dependencies that are not required by default, so to not force a lot of dependencies installation.\nIf you want to do directly the installation with **all dependencies**, type:\n``` \npip install `fishfeats[full]`\n```\n\n## Usage\n\nYou can launch `fishfeats` in Napari by going to `Plugins>fishfeats>Start`.\nIt will open a file dialog box asking you to select the image that you want to analyze. \nRefer to the [documentation](https://gletort.github.io/FishFeats/) for presentation of the different steps possible in the pipeline.\n\n\n## Test dataset\n\nYou can find in this zenodo repository freely available images that can be used to test our pipeline.\nAll the steps are fully documented in the online [documentation](https://gletort.github.io/FishFeats/) and can be performed with one of these test images.\n\nExample of analysis you can do with `FishFeats` are detailled step-by-step [here](https://gletort.github.io/FishFeats/Step-by-step/) and can be followed with the test image.\n\n## License\n\nFishfeats is distributed freely under the BSD-3 license.\n\n\n[napari]: https://github.com/napari/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Start",
      "Classify segmented cells",
      "Hierarchical clustering",
      "Start from open layer(s)",
      "Start multiscale",
      "Open fishfeats documentation",
      "Convert previous results"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-omero-downloader-cci",
    "name": "napari-omero-downloader-cci",
    "display_name": "Omero Downloader CCI",
    "version": "0.2.3",
    "created_at": "2025-09-04",
    "modified_at": "2025-09-04",
    "authors": [
      "Simon Leclerc"
    ],
    "author_emails": [
      "simon.leclerc@gu.se"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-omero-downloader-cci/",
    "home_github": "https://github.com/CCI-GU-Sweden/napari-omero-downloader-cci",
    "home_other": null,
    "summary": "A plugin that allows napari to connect to the Omero CCI server to visualize and download image data",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "dask",
      "napari[all]; extra == \"all\""
    ],
    "package_metadata_description": "# napari-omero-downloader-cci\n\n[![License MIT](https://img.shields.io/pypi/l/napari-omero-downloader-cci.svg?color=green)](https://github.com/CCI-GU-Sweden/napari-omero-downloader-cci/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-omero-downloader-cci.svg?color=green)](https://pypi.org/project/napari-omero-downloader-cci)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-omero-downloader-cci.svg?color=green)](https://python.org)\n[![tests](https://github.com/CCI-GU-Sweden/napari-omero-downloader-cci/workflows/tests/badge.svg)](https://github.com/CCI-GU-Sweden/napari-omero-downloader-cci/actions)\n[![codecov](https://codecov.io/gh/CCI-GU-Sweden/napari-omero-downloader-cci/branch/main/graph/badge.svg)](https://codecov.io/gh/CCI-GU-Sweden/napari-omero-downloader-cci)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-omero-downloader-cci)](https://napari-hub.org/plugins/napari-omero-downloader-cci)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA plugin that allows napari to connect to the Omero CCI server to visualize and download image data\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template] (None).\n\n## Installation\n\n### First napari installation\n\nFirst install miniconda from conda forge: [https://conda-forge.org/download/].\n\nRecommanded, create an environment for napari, bundling both napari and omero\n\n```\nconda create -n napari -c conda-forge napari omero-py pyqt --yes\nconda activate napari\nnapari\n```\n\n### Already python installed napari\n\nIn this case, activate your environment and install Omero:\n\n```\nconda install -c conda-forge omero-py --yes\n```\n\n### Plugin installation\n\nYou can install `napari-omero-downloader-cci` via [pip]:\n\n```\npip install napari-omero-downloader-cci\n```\n\nTo install latest development version :\n\n```\npip install git+https://github.com/CCI-GU-Sweden/napari-omero-downloader-cci.git\n```\n\n‚Äîor, during development‚Äî\n\n```\npip install -e .\n```\n\n## Running the plugin after Installation\n\nYou will need to open the conda-forge program, then:\n\n```\nconda activate napari\nnapari\n```\n\nThe plug will be in the plugin tab.\n\n## Warning about standalone napari\n\nA standalone version of napari is available, and the plugin will be available on the napari hub. However, installation through the standalone app is not recommanded, since it relies on pip which does not distribute system ready dependancy for Ice.\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-omero-downloader-cci\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/CCI-GU-Sweden/napari-omero-downloader-cci/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "OmeroDownloaderWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "ndev-settings",
    "name": "ndev-settings",
    "display_name": "ndev settings",
    "version": "0.2.0",
    "created_at": "2025-09-04",
    "modified_at": "2025-09-04",
    "authors": [
      "Tim Monko"
    ],
    "author_emails": [
      "timmonko@gmail.com"
    ],
    "license": "Copyright (c) 2025, Tim Monko\n...",
    "home_pypi": "https://pypi.org/project/ndev-settings/",
    "home_github": "https://github.com/ndev-kit/ndev-settings",
    "home_other": null,
    "summary": "Reusable settings and customization widget for the ndev-kit",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "magicgui",
      "magic-class",
      "pyyaml",
      "napari[all]; extra == \"all\""
    ],
    "package_metadata_description": "# ndev-settings\n\n[![License BSD-3](https://img.shields.io/pypi/l/ndev-settings.svg?color=green)](https://github.com/ndev-kit/ndev-settings/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/ndev-settings.svg?color=green)](https://pypi.org/project/ndev-settings)\n[![Python Version](https://img.shields.io/pypi/pyversions/ndev-settings.svg?color=green)](https://python.org)\n[![tests](https://github.com/ndev-kit/ndev-settings/workflows/tests/badge.svg)](https://github.com/ndev-kit/ndev-settings/actions)\n[![codecov](https://codecov.io/gh/ndev-kit/ndev-settings/branch/main/graph/badge.svg)](https://codecov.io/gh/ndev-kit/ndev-settings)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/ndev-settings)](https://napari-hub.org/plugins/ndev-settings)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nReusable settings and customization widget for the ndev-kit\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template] v1.1.0.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `ndev-settings` via [pip]:\n\n```\npip install ndev-settings\n```\n\nIf napari is not already installed, you can install `ndev-settings` with napari and Qt via:\n\n```\npip install \"ndev-settings[all]\"\n```\n\n\nTo install latest development version :\n\n```\npip install git+https://github.com/ndev-kit/ndev-settings.git\n```\n\n## Use with external libraries\n\nExternal libraries can provide their settings in YAML format with the same structure as your main `ndev_settings.yaml`.\n\n**Step 1**: Create a YAML file in the external library (e.g., `ndev_settings.yaml`):\n\n```yaml\nndevio_Reader:\n  preferred_reader:\n    default: bioio-ome-tiff\n    dynamic_choices:\n      fallback_message: No readers found\n      provider: bioio.readers\n    tooltip: Preferred reader to use when opening images\n    value: bioio-ome-tiff\n  scene_handling:\n    choices:\n    - Open Scene Widget\n    - View All Scenes\n    - View First Scene Only\n    default: Open Scene Widget\n    tooltip: How to handle files with multiple scenes\n    value: View First Scene Only\n  clear_layers_on_new_scene:\n    default: false\n    tooltip: Whether to clear the viewer when selecting a new scene\n    value: false\n\nndevio_Export:\n  canvas_scale:\n    default: 1.0\n    max: 100.0\n    min: 0.1\n    tooltip: Scales exported figures and screenshots by this value\n    value: 1.0\n  override_canvas_size:\n    default: false\n    tooltip: Whether to override the canvas size when exporting canvas screenshot\n    value: false\n  canvas_size:\n    default: !!python/tuple\n    - 1024\n    - 1024\n    tooltip: Height x width of the canvas when exporting a screenshot\n    value: !!python/tuple\n    - 1024\n    - 1024\n```\n\n**Step 2**: Register the entry point in `pyproject.toml`:\n\n```toml\n[project.entry-points.\"ndev_settings.manifest\"]\nndevio = \"ndevio:ndev_settings.yaml\"\n```\n\n**Step 3**: Use the autogenerated widget in napari!\n\n![external settings contributed automatically to the ndev-settings widget](./resources/widget.png)\n\n## Usage Example\n\n```python\nfrom ndev_settings import get_settings\n\nsettings = get_settings()\n\n# Access settings from main file\nprint(settings.Canvas.canvas_scale)\n\n# Access settings from external libraries (if installed)\nprint(settings.Reader.preferred_reader)  # From ndevio\nprint(settings.Export.compression_level)  # From ndevio\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"ndev-settings\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/ndev-kit/ndev-settings/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ndev Settings"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "epicure",
    "name": "epicure",
    "display_name": "EpiCure",
    "version": "0.2.17",
    "created_at": "2024-10-18",
    "modified_at": "2025-09-03",
    "authors": [
      "Ga√´lle Letort"
    ],
    "author_emails": [],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/epicure/",
    "home_github": null,
    "home_other": null,
    "summary": "Napari plugin to manually correct epithelia segmentation in movies",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari<=0.4.19",
      "numpy",
      "magicgui",
      "qtpy",
      "pyqtwebengine",
      "scikit-image",
      "scipy",
      "opencv_python_headless",
      "roifile",
      "xlsxwriter",
      "plotly",
      "kaleido",
      "imagecodecs",
      "edt",
      "packaging",
      "laptrack",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# EpiCure\n\n[![License BSD-3](https://img.shields.io/pypi/l/epicure.svg?color=green)](https://gitlab.pasteur.fr/gletort/epicure/-/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/epicure.svg?color=green)](https://pypi.org/project/epicure)\n[![Python Version](https://img.shields.io/pypi/pyversions/epicure.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/epicure)](https://napari-hub.org/plugins/epicure)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13952184.svg)](https://doi.org/10.5281/zenodo.13952184)\n\n![EpiCure logo](https://gitlab.pasteur.fr/gletort/epicure/-/raw/main/imgs/epicure_logo.png?raw=True \"EpiCure logo\")\n\n**Napari plugin to ease manual correction of epithelia segmentation in movies.**\n\nTo analyse individual cell trajectory from epithelia movies marked for cell-cell junctions, a very precise segmentation and tracking is required.\nSeveral tools such as TissuAnalyzer, Epyseg, CellPose or Dist2Net perform very good segmentation (~5% of errors). \nHowever, this still implies a high amount of cells to correct manually. \nEpiCure allows to decrease the burden of this task. \nSeveral features are proposed to ease the manual correction of the segmented movies, such as error detection, numerous shortcuts for editing the segmentation, option for tracking, display and measure/export options.\nEpiCure detect segmentation errors by taking advantage of temporal information. \nWhen a correction is done at a given frame, EpiCure relink the track to adjust for the changes.\n\n\n > **Documentation in the [wiki](https://gitlab.pasteur.fr/gletort/epicure/-/wikis/Home)**\n\n<p align=\"center\">\n![EpiCure interface](https://gitlab.pasteur.fr/gletort/epicure/-/raw/main/imgs/EpiGen.png?raw=True \"EpiCure interface\")\n</p>\n\n## Installation\n\n### Install plugin\nTo install EpiCure on a fresh python virtual environment, type inside the environement:\n```\npip install epicure\n``` \n\nThen launch `Napari`, and the plugin should be visible in the `Plugins` list.\n\nIf you already have an environment with `Napari` installed, you can also install it directly in `Napari>Plugins>Install/Uninstall plugins`\n\n### Install code\nTo have the code to be able to modify it, clone this repository. You can use `pip install -e .` so that everytime you update the code, the plugin will be updated. \n\n## Dependencies\n\nThe input files of EpiCure can be already tracked or not.\nTracking options are proposed in EpiCure:\n* Laptrack centroids\n* Laptrack overlaps\n\n## Usage\nRefer to the [wiki](https://gitlab.pasteur.fr/gletort/epicure/-/wikis/Home) for documentation of the different steps possible in the pipeline.\n\n## References\n\nIf you use EpiCure, thank you for citing our work: \n\nEpiCure is not published yet, you can cite it using Zenodo for now: https://doi.org/10.5281/zenodo.13952184\n\n\n## Issues\nIssues have been disactivated to avoid spammed issues. To report an issue or ask for development, please contact us directly by email.\n\n\nThis [napari] plugin was initialized with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[file an issue]: https://github.com/gletort/epicure/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Start EpiCure",
      "Concatenate EpiCured movies",
      "Open EpiCure documentation",
      "Edit Preferences"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-epyseg",
    "name": "napari-epyseg",
    "display_name": "Napari-EpySeg",
    "version": "0.0.8",
    "created_at": "2025-02-12",
    "modified_at": "2025-09-03",
    "authors": [
      "Ga√´lle Letort"
    ],
    "author_emails": [],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-epyseg/",
    "home_github": "https://github.com/gletort/napari-epyseg",
    "home_other": null,
    "summary": "Napari plugin to segment epithelia with EpySeg",
    "categories": [
      "Segmentation"
    ],
    "package_metadata_requires_python": "<3.11",
    "package_metadata_requires_dist": [
      "epyseg",
      "napari<=0.6.1",
      "numpy",
      "magicgui",
      "tifffile",
      "pillow",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "[![DOI](https://zenodo.org/badge/931444556.svg)](https://doi.org/10.5281/zenodo.16910935)\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n\n# napari-epyseg\n\nPlugin to run [EpySeg](https://github.com/baigouy/EPySeg) directly in [Napari](https://napari.org/stable/).\nCan handle temporal data (movie) or single time point (image).\n\n## Installation\n\nTo install it from Napari, go into the `Plugins` menu, select `Install/Uninstall Plugins..` and look for napari-epyseg in the plugins list.\n\nTo install it directly outside of napari, create/reuse and activate a python environement, e.g `epyseg_env` and install it with `pip`:\n```\npip install napari-epyseg\n```\n\n> [!IMPORTANT]\n> `epyseg` is compatible until python 3.10 (included), but not for versions of python above. Thus, `napari-epyseg` is also compatible with python versions until 3.10\n\n## Usage\n\nIn Napari, go to `Plugins>napari-epyseg` to start it.\nIt will open an interface in the left part of the main window.\n\n![interface_image](./imgs/napepy-interface.png)\n\nYou must select the layer (image or movie, single color channel) on which to run `EpySeg`.\nFor this, in the `Pick an Image` parameter, select the corresponding layer (you should open the image/movie independantly of the plugin).\n\nTo run `EpySeg` with default parameters, press directly `Segment` once you have selected the image.\nWhen processing is finished, a new layer called `Segmentation` will be added in the right panel of the interface.\nYou can save the result with the `Save segmentation` button that appears on the left part of the interface. \nChoose where to save the file and the file name with the `Segmentation filename` parameter, and click the button to save it.\n\n![results_image](./imgs/result.png)\n\n## Remark\n\nThis plugin was tested on python 3.10, with epyseg version 0.1.52, napari version 0.4.19, tensorflow 2.14\n\n## Troubleshooting\n\n* On Linux Ubuntu, python 3.10, epyseg don't run with tensorflow 2.15 and output the error: `DNN not found`. Downgrading tensorflow to 2.14 worked.\n* `tifffile versions` compability: some versions of tifffile where not compatible with epyseg, but recent versions are now fine. `tifffile <= 2021.11.2` are fine and `tifffile==2025.1.10` also.\n\n## License\n\nThis plugin is distributed under the BDS-3 license.\nIf you use this plugin, please cite [Epyseg](https://journals.biologists.com/dev/article/147/24/dev194589/226105/EPySeg-a-coding-free-solution-for-automated) and the plugin with its doi: [![DOI](https://zenodo.org/badge/931444556.svg)](https://doi.org/10.5281/zenodo.16910935)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Napari-EpySeg"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-skimage",
    "name": "napari-skimage",
    "display_name": "napari skimage",
    "version": "0.6.0",
    "created_at": "2024-05-29",
    "modified_at": "2025-09-03",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "Copyright (c) 2024, Guillaume ...",
    "home_pypi": "https://pypi.org/project/napari-skimage/",
    "home_github": "https://github.com/guiwitz/napari-skimage",
    "home_other": null,
    "summary": "A plugin to apply scikit-image operations",
    "categories": [
      "Image Processing",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-skimage\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-skimage.svg?color=green)](https://github.com/guiwitz/napari-skimage/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-skimage.svg?color=green)](https://pypi.org/project/napari-skimage)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-skimage.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-skimage/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-skimage/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-skimage/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-skimage)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-skimage)](https://napari-hub.org/plugins/napari-skimage)\n[![launch - renku](https://renkulab.io/renku-badge.svg)](https://renkulab.io/projects/guillaume.witz1/napari-skimage/sessions/new?autostart=1)\n\nnapari-skimage gives easy access to scikit-image functions in napari. The main goal of the plugin is to allow new users of napari, especially without coding experience, to easily explore basic image processing, in a similar way to what is possible in Fiji.\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Philosophy\n\nThe plugin is still in early development and does not cover all functions of scikit-image. If you are interested in a specific function, please open an issue or a pull request. scikit-image functions are turned into interactive widgets mostly via magicgui, a tool that allows to create GUIs from functions in a simple way (in particular not requiring Qt knowledge). The code avoids on purpose complex approaches, e.g. to automate the creation of widgets, in order to keep the code simple and easy to understand for beginners.\n\n## Installation\n\nYou can install `napari-skimage` via [pip]:\n\n    pip install napari-skimage\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/guiwitz/napari-skimage.git\n\n## Usage\n\nThe plugin function can be accessed under ```Plugins -> napari-skimage```. Each function will appear as a widget on the right of the napari window. Some functions such as ```Gaussian Filter``` give access to a single operation and its options. Some functions such as ```Thresholding``` give access to variants of the same operation via a dropdown menu. Currently the plugin does not support multi-channel processing and will consider those as stacks. At the moment, the plugin offers access to the following operation types.\n\n### Filtering\n\nA set of classical filters: Gaussian, Prewitt, Laplace etc. as well as rank filters such as median, minimum, maximum etc.\n\n![Gaussian filter](docs/gaussian.png)\n\n### Thresholding\nA set of thresholding methods: Otsu, Li, Yen etc.\n![Thresholding](docs/thresholding.png)\n\n### Binary morphological operations\nA set of binary morphology operations: binary erosion, binary dilation etc.\n![Binary morphological operations](docs/binary_morphology.png)\n\n### Morphological operations\nA set of morphological operations: erosion, dilation, opening, closing etc.\n![Morphological operations](docs/morphology.png)\n\n### Restoration\nA set of restoration operations such as rolling ball, or non-local means denoising.\n![Restoration](docs/denoise_nl.png)\n\n### Mathematics \nIn addition the plugin provides a set of simple mathematical operators to:\n- operate on single images e.g. square, square root, log etc.\n- operate on two images e.g. add, subtract, multiply etc.\n![Mathematics](docs/simple_maths.png)\n\n## Code structure\n\nEach set of functions is grouped in a separate module. For example all filtering operations are grouped in ```src/napari_skimge/skimage_filter_widget.py```. A set of test in ```src/_tests/test_basic_widgets.py``` simply check that each widget can be created and generated an output of the correct size using the default settings.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-skimage\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-skimage/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Farid filter",
      "Prewitt filter",
      "Laplace filter",
      "Gaussian filter",
      "Frangi filter",
      "Median filter",
      "Butterworth filter",
      "Automated Threshold",
      "Manual Threshold",
      "Binary Morphology",
      "Morphology",
      "Label connected components",
      "Simple maths",
      "Crop rectangle",
      "Image pairs maths",
      "Conversion",
      "Rank filters",
      "Rolling ball restoration",
      "Denoise nl means restoration",
      "Peak Local Max",
      "Marching Cubes",
      "Marching Cubes (labels)",
      "Regionprops (labels)",
      "Axis operations"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "brainglobe-registration",
    "name": "brainglobe-registration",
    "display_name": "BrainGlobe Registration",
    "version": "0.0.5",
    "created_at": "2025-01-27",
    "modified_at": "2025-09-02",
    "authors": [
      "Brainglobe Developers"
    ],
    "author_emails": [
      "Brainglobe Developers <hello@brainglobe.info>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/brainglobe-registration/",
    "home_github": "https://github.com/brainglobe/brainglobe-registration",
    "home_other": null,
    "summary": "A napari plugin for registration to a  BrainGlobe atlas.",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "napari!=0.6.0,>=0.4.18",
      "bayesian-optimization",
      "brainglobe-atlasapi",
      "brainglobe-utils>=0.4.3",
      "dask",
      "dask-image",
      "fancylog",
      "itk-elastix",
      "lxml_html_clean",
      "numpy",
      "pandas",
      "pytransform3d",
      "qtpy",
      "qt-niu",
      "scikit-image",
      "scipy",
      "tifffile",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "black; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "setuptools_scm; extra == \"dev\"",
      "pyqt5; extra == \"dev\""
    ],
    "package_metadata_description": "# brainglobe-registration\n\n[![License BSD-3](https://img.shields.io/pypi/l/brainglobe-registration.svg?color=green)](https://github.com/brainglobe/brainglobe-registration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/brainglobe-registration.svg?color=green)](https://pypi.org/project/brainglobe-registration)\n[![Python Version](https://img.shields.io/pypi/pyversions/brainglobe-registration.svg?color=green)](https://python.org)\n[![tests](https://github.com/brainglobe/brainglobe-registration/workflows/tests/badge.svg)](https://github.com/brainglobe/brainglobe-registration/actions)\n[![codecov](https://codecov.io/gh/brainglobe/brainglobe-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/brainglobe/brainglobe-registration)\n\nRegistration to a BrainGlobe atlas using Elastix\n\n----------------------------------\n\n> [!WARNING]\n> This tool is in very early development. The interface may change and some features are not yet available.\n\nA [napari] plugin for registering images to a BrainGlobe atlas.\n\n![brainglobe-registration](./imgs/brainglobe_registration_main.png)\n\n## Usage\n\n1. Open `napari`.\n2. [Install the plugin](#Installation).\n3. Open the widget by selecting `Plugins > BrainGlobe Registration` in the napari menu bar near the\ntop left of the window.\n![brainglobe-registration-plugin](./imgs/brainglobe_registration_plugin_window.png)\nThe `BrainGlobe Registration` plugin will appear on the right hand side of the napari window.\n4. Open the image you want to register in napari (a sample 2D image can be found by selecting `File > Open Sample > Sample Brain Slice`).\n5. Select the atlas you want to register to from the dropdown menu.\n![brainglobe-registration-atlas-selection](./imgs/brainglobe_registration_atlas_selection.png)\nThe atlas will appear in the napari viewer. Select the approximate `Z` slice of the atlas that you want to register to,\nusing the slider at the bottom of the napari viewer.\n![brainglobe-registration-atlas-selection](./imgs/brainglobe_registration_atlas_selection_2.png)\n6. Adjust the sample image to roughly match the atlas image.\nYou can do this by adjusting X and Y translation as well as rotating around the centre of the image.\nYou can overlay the two images by toggling `Grid` mode in the napari viewer (Ctrl+G).\nYou can then adjust the color map and opacity of the atlas image to make manual alignment easier.\n![brainglobe-registration-overlay](./imgs/brainglobe_registration_overlay.png)\nThe sample image can be reset to its original position and orientation by clicking `Reset Image` in the `BrainGlobe Registration` plugin window.\n7. Select the transformations you want to use from the dropdown menu. Set the transformation type to empty to remove a step.\nSelect from one of the three provided default parameter sets (elastix, ARA, or IBL). Customise the parameters further in the\n`Parameters` tab.\n8. Click `Run` to register the image. The registered image will appear in the napari viewer.\n![brainglobe-registration-registered](./imgs/brainglobe_registration_registered.png)\n![brainglobe-registration-registered](./imgs/brainglobe_registration_registered_stacked.png)\n\n## Installation\n\nWe strongly recommend to use a virtual environment manager (like `conda` or `venv`). The installation instructions below\nwill not specify the Qt backend for napari, and you will therefore need to install that separately. Please see the\n[`napari` installation instructions](https://napari.org/stable/tutorials/fundamentals/installation.html) for further advice on this.\n\nYou can install `brainglobe-registration` via [pip]:\n\n    pip install brainglobe-registration\n\nor [via napari](https://napari.org/stable/plugins/start_using_plugins/finding_and_installing_plugins.html).\n\nTo install the latest development version :\n\n    pip install git+https://github.com/brainglobe/brainglobe-registration.git\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"brainglobe-registration\" is free and open source software\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## Citation\nIf you find this package useful, and use it in your research, please cite the following:\n> Igor Tatarnikov, Alessandro Felder, Kimberly Meechan, & Adam Tyson. (2025). brainglobe/brainglobe-registration. Zenodo. https://doi.org/10.5281/zenodo.14750325\n\n## Acknowledgements\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/brainglobe/brainglobe-registration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BrainGlobe Registration"
    ],
    "contributions_sample_data": [
      "2D coronal mouse brain section",
      "3D mouse brain"
    ]
  },
  {
    "normalized_name": "napari-lattice",
    "name": "napari-lattice",
    "display_name": "Lattice Lightsheet Analysis",
    "version": "1.0.5",
    "created_at": "2022-07-19",
    "modified_at": "2025-09-02",
    "authors": [
      "Pradeep Rajasekhar",
      "Lachlan Whitehead",
      "Robert Haase",
      "Michael Milton"
    ],
    "author_emails": [],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-lattice/",
    "home_github": "https://github.com/BioimageAnalysisCoreWEHI/napari_lattice",
    "home_other": null,
    "summary": "Napari plugin for analysing and visualizing lattice lightsheet and Oblique Plane Microscopy data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "aicsimageio>=4.6.3",
      "dask[distributed]",
      "fsspec>=2022.8.2",
      "importlib_resources",
      "lls_core",
      "magic-class>=0.7.5",
      "magicgui<0.8.0",
      "napari-aicsimageio>=0.7.2",
      "napari-workflow-inspector",
      "napari-workflows>=0.2.8",
      "napari[all]>=0.4.11",
      "npy2bdv",
      "ome-types<0.6.0",
      "numpy<2",
      "psutil",
      "pyclesperanto_prototype>=0.20.0",
      "pydantic",
      "qtpy",
      "typing_extensions>=4.7.0",
      "rich",
      "StrEnum",
      "xarray",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\""
    ],
    "package_metadata_description": null,
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.h5"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Lattice Lightsheet Analysis"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "samcell-napari",
    "name": "samcell-napari",
    "display_name": "samcell-napari",
    "version": "1.1.3",
    "created_at": "2025-04-02",
    "modified_at": "2025-09-02",
    "authors": [
      "Saahil Sanganeriya"
    ],
    "author_emails": [
      "saahilsanganeria666@gmail.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/samcell-napari/",
    "home_github": null,
    "home_other": null,
    "summary": "A napari plugin for cell segmentation with SAMCell 1.0",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari>=0.4.14",
      "numpy>=1.21.0",
      "torch>=1.9",
      "transformers>=4.26.0",
      "scikit-image>=0.19.0",
      "opencv-python>=4.5.0",
      "scipy>=1.7.0",
      "pandas>=1.3.0",
      "tqdm>=4.60.0",
      "safetensors>=0.3.0",
      "timm>=0.6.0",
      "openpyxl>=3.0.0",
      "safetensors>=0.3.0; extra == \"safetensors\"",
      "black; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\""
    ],
    "package_metadata_description": "# samcell-napari\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI](https://img.shields.io/pypi/v/samcell-napari)](https://pypi.org/project/samcell-napari/)\n\nA napari plugin for cell segmentation using the Segment Anything Model (SAM) foundation model.\n\n![SAMCell Segmentation Example](https://github.com/saahilsanganeriya/samcell-napari/raw/main/docs/images/samcell-napari.jpg)\n\n## Description\n\nSAMCell-napari provides an intuitive interface for segmenting cells in microscopy images using deep learning. It leverages the power of the Segment Anything Model (SAM) adapted specifically for biological cell segmentation, providing accurate results with minimal tuning.\n\n### Key Features:\n- Simple, user-friendly interface within napari\n- Compatible with SAMCell models in multiple formats (`.pt`, `.bin`, `.safetensors`)\n- Support for both SAM-ViT-Base and SAM-ViT-Large model architectures\n- Adjustable segmentation parameters for fine-tuning\n- Real-time visualization of results\n- Distance map visualization for analyzing cell proximity\n- Full integration with napari's layer system\n- Enhanced sliding window algorithm with advanced blending for seamless segmentation of large images\n\n### What's New in v1.0.0:\n- Support for multiple model file formats (`.pt`, `.bin`, `.safetensors`)\n- Improved sliding window algorithm with smooth blending between crops\n- Better handling of small images and edge cases\n- Enhanced error recovery and logging\n- Multiple threshold testing capability\n- Optimized default thresholds for better segmentation results\n- Support for both SAM-ViT-Base and SAM-ViT-Large model variants\n\n## Installation\n\nYou can install `samcell-napari` via [pip]:\n\n```bash\npip install samcell-napari\n```\n\nTo install latest development version:\n\n```bash\npip install git+https://github.com/saahilsanganeriya/samcell-napari.git\n```\n\n## Usage\n\n1. Start napari\n   ```bash\n   napari\n   ```\n\n2. Load your image in napari\n\n3. Open the SAMCell plugin:\n   ```\n   Plugins > samcell-napari > SAMCell Segmentation\n   ```\n\n4. Provide the path to your SAMCell model file (supports `.pt`, `.bin`, or `.safetensors` formats)\n   - You can download pre-trained models from the [official SAMCell release page](https://github.com/saahilsanganeriya/SAMCell/releases/tag/v1)\n\n5. Adjust parameters if needed:\n   - Cell peak threshold: Higher values detect fewer cells (default: 0.47)\n   - Cell fill threshold: Lower values create larger cells (default: 0.09)\n   - Crop size: Size of image crops for processing (default: 256)\n\n6. Click \"Run Segmentation\"\n\n7. View the segmentation results in napari as a Labels layer\n\n## Requirements\n\n- Python 3.8 or higher\n- napari 0.4.14 or higher\n- PyTorch 1.9 or higher\n- transformers 4.20.0 or higher\n- CUDA-capable GPU recommended for faster processing\n\n## Model Compatibility\n\nThe plugin is compatible with SAMCell model files in multiple formats:\n- PyTorch model files (`.pt`)\n- Binary model files (`.bin`) - including the standard `pytorch_model.bin`\n- SafeTensors files (`.safetensors`) - a safer alternative to PyTorch's pickle-based format\n\nThe plugin supports models based on:\n- SAM-ViT-Base architecture - Primary model type\n- SAM-ViT-Large architecture - Fallback if a model doesn't load with base architecture\n\nPre-trained models can be downloaded from the [official SAMCell release page](https://github.com/saahilsanganeriya/SAMCell/releases/tag/v1).\n\nRecommended models include:\n- SAMCell1.0-Cellpose-cyto: Trained on the Cellpose cytoplasm dataset\n- SAMCell1.0-livecell: Trained on the LiveCELL dataset\n\nThese models are part of the release assets for the paper \"SAMCell: Generalized Label-Free Biological Cell Segmentation with Segment Anything\".\n\n## How It Works\n\nSAMCell operates using an enhanced sliding window approach to process large images:\n\n1. The image is divided into overlapping crops with intelligent handling of image boundaries\n2. Each crop is processed through a SAM-based model\n3. A distance map is created, representing cell centers and boundaries\n4. The crops are stitched back together with smooth blending for seamless transitions\n5. The distance map is processed to extract individual cell masks using watershed segmentation\n6. Results are displayed in napari as labels\n\n## Technical Details\n\n### Model Type Detection\n\nThe plugin intelligently determines the appropriate SAM model architecture:\n1. First tries to load the model with SAM-ViT-Base architecture\n2. If that fails, automatically falls back to SAM-ViT-Large\n3. This ensures maximum compatibility with various pre-trained models\n\n### Sliding Window Algorithm\n\nThe plugin uses an advanced sliding window algorithm that:\n- Handles images of any size, including those smaller than the crop size\n- Creates appropriate overlaps between crops to ensure no cells are missed\n- Uses a cosine-based blending mask to create smooth transitions between crops\n- Fills any potential gaps using nearest neighbor interpolation\n\n### Multiple Threshold Testing\n\nFor researchers who want to optimize segmentation parameters, the plugin includes a batch processing capability to test multiple threshold combinations at once (available via the API).\n\n## Contributing\n\nContributions are very welcome! Please feel free to submit a Pull Request.\n\n## License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n## Citation\n\nIf you use this plugin in your research, please cite:\n\n```\n@article{samcell2023,\n  title={SAMCell: Generalized Label-Free Biological Cell Segmentation with Segment Anything},\n  author={...},\n  journal={...},\n  year={2023}\n}\n``` \n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SAMCell Segmentation"
    ],
    "contributions_sample_data": [
      "Sample Cell Image (2D)"
    ]
  },
  {
    "normalized_name": "brainglobe-napari-io",
    "name": "brainglobe-napari-io",
    "display_name": "brainglobe-napari-io",
    "version": "0.4.0",
    "created_at": "2021-03-12",
    "modified_at": "2025-09-01",
    "authors": [
      "Adam Tyson"
    ],
    "author_emails": [
      "Adam Tyson <hello@brainglobe.info>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/brainglobe-napari-io/",
    "home_github": "https://github.com/brainglobe/brainglobe-napari-io",
    "home_other": null,
    "summary": "Read and write files from the BrainGlobe computational neuroanatomy suite into napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "brainglobe-atlasapi>=2.0.1",
      "brainglobe-space>=1.0.0",
      "brainglobe-utils>=0.4.2",
      "napari>=0.6.1",
      "tifffile>=2020.8.13",
      "numpy",
      "pandas",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "black; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "setuptools_scm; extra == \"dev\"",
      "pyqt5; extra == \"dev\""
    ],
    "package_metadata_description": "# napari-brainglobe-io\n\n[![License](https://img.shields.io/pypi/l/brainglobe-napari-io.svg?color=green)](https://github.com/brainglobe/brainglobe-napari-io/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/brainglobe-napari-io.svg?color=green)](https://pypi.org/project/brainglobe-napari-io)\n[![Python Version](https://img.shields.io/pypi/pyversions/brainglobe-napari-io.svg?color=green)](https://python.org)\n[![tests](https://github.com/brainglobe/brainglobe-napari-io/workflows/tests/badge.svg)](https://github.com/brainglobe/brainglobe-napari-io/actions)\n[![codecov](https://codecov.io/gh/brainglobe/brainglobe-napari-io/branch/main/graph/badge.svg)](https://codecov.io/gh/brainglobe/brainglobe-napari-io)\n\nVisualise cellfinder and brainreg results with napari\n\n\n----------------------------------\n\n\n## Installation\nThis package is likely already installed\n(e.g. with cellfinder, brainreg or another napari plugin), but if you want to\ninstall it again, either use the napari plugin install GUI or you can\ninstall `brainglobe-napari-io` via [pip]:\n\n    pip install brainglobe-napari-io\n\n## Usage\n* Open napari (however you normally do it, but typically just type `napari` into your terminal, or click on your desktop icon)\n\n### brainreg\n#### Sample space\nDrag your [brainreg](https://github.com/brainglobe/brainreg) output directory (the one with the log file) onto the napari window.\n\nVarious images should then open, including:\n* `Registered image` - the image used for registration, downsampled to atlas resolution\n* `atlas_name` - e.g. `allen_mouse_25um` the atlas labels, warped to your sample brain\n* `Boundaries` - the boundaries of the atlas regions\n\nIf you downsampled additional channels, these will also be loaded.\n\nMost of these images will not be visible by default. Click the little eye icon to toggle visibility.\n\n_N.B. If you use a high resolution atlas (such as `allen_mouse_10um`), then the files can take a little while to load._\n\n![sample_space](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/sample_space.gif)\n\n\n#### Atlas space\n`napari-brainreg` also comes with an additional plugin, for visualising your data\nin atlas space.\n\nThis is typically only used in other software, but you can enable it yourself:\n* Open napari\n* Navigate to `Plugins` -> `Plugin Call Order`\n* In the `Plugin Sorter` window, select `napari_get_reader` from the `select hook...` dropdown box\n* Drag `brainreg_read_dir_atlas_space` (the atlas space viewer plugin) above `brainreg_read_dir` (the normal plugin) to ensure that the atlas space plugin is used preferentially.\n\n\n### cellfinder\n#### Load cellfinder XML file\n* Load your raw data (drag and drop the data directories into napari, one at a time)\n* Drag and drop your cellfinder XML file (e.g. `cell_classification.xml`) into napari.\n\n#### Load cellfinder directory\n* Load your raw data (drag and drop the data directories into napari, one at a time)\n* Drag and drop your cellfinder output directory into napari.\n\nThe plugin will then load your detected cells (in yellow) and the rejected cell\ncandidates (in blue). If you carried out registration, then these results will be\noverlaid (similarly to the loading brainreg data, but transformed to the\ncoordinate space of your raw data).\n\n![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_data.gif)\n**Loading raw data**\n\n![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_results.gif)\n**Loading cellfinder results**\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.xml"
    ],
    "contributions_writers_filename_extensions": [
      ".xml"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tardis-em",
    "name": "napari-tardis-em",
    "display_name": "TARDIS-em napari plugin",
    "version": "0.3.19",
    "created_at": "2024-07-22",
    "modified_at": "2025-09-01",
    "authors": [
      "Robert Kiewisz"
    ],
    "author_emails": [
      "rkiewisz@nysbc.org"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tardis-em/",
    "home_github": null,
    "home_other": "None",
    "summary": "Tomogram and micrograph segmentation with TARDIS-em",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "setuptools>=70.3.0",
      "napari>=0.5.0",
      "torch>=2.2.2",
      "tardis_em[nd2]>=0.3.18",
      "numpy>=2.0.0",
      "matplotlib>=3.9.1",
      "qtpy>=2.4.1",
      "pyqt5>=5.15.10",
      "opencv-python",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# Napari plugin for TARDIS-em\n\nNapari [gen2] plugin for Cry-EM and Cryo-ET micrograph segmentation with TARDIS-em.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.nd2",
      "*.am",
      "*.rec",
      "*.tif",
      "*.csv",
      "*.mrc"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Train TARDIS CNN",
      "Predict TARDIS CNN",
      "Predict Microtubules 3D",
      "Predict Membrane 3D",
      "Predict Membrane 2D",
      "Predict Actin 3D",
      "Fiber Edit",
      "Fiber Analysis",
      "TIRF analysis"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-prism",
    "name": "napari-prism",
    "display_name": "napari prism",
    "version": "0.1.7",
    "created_at": "2024-11-14",
    "modified_at": "2025-08-29",
    "authors": [
      "Rafael Tubelleza"
    ],
    "author_emails": [
      "rafaelrtubelleza@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-prism/",
    "home_github": "https://github.com/clinicalomx/napari-prism",
    "home_other": null,
    "summary": "A Python package for the inteRactive and Integrated analySis of Multiplexed tissue microarrays",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "spatialdata>=0.5.0",
      "imagecodecs",
      "napari[all]",
      "napari_matplotlib>=3.0.0",
      "napari_spatialdata>=0.5.7",
      "dask<2024.11.12",
      "matplotlib",
      "PyComplexHeatmap",
      "scikit-learn",
      "cellpose>=3.0.10",
      "scanpy>=1.10.0",
      "harmonypy",
      "phenograph",
      "squidpy",
      "kneed",
      "xarray",
      "pyometiff",
      "forestplot",
      "scikit-survival",
      "spatialdata-plot>=0.2.11",
      "requests",
      "dask-cudf-cu12==24.10.*; extra == \"gpu\"",
      "rapids-singlecell[rapids12]; extra == \"gpu\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-lazy-fixtures; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "qtpy; extra == \"testing\"",
      "ipykernel; extra == \"docs\"",
      "ipython; extra == \"docs\"",
      "myst-nb>=1.1; extra == \"docs\"",
      "myst-parser; extra == \"docs\"",
      "sphinx>=4; extra == \"docs\"",
      "sphinx-autodoc-typehints; extra == \"docs\"",
      "sphinx-book-theme>=1; extra == \"docs\"",
      "sphinx-copybutton; extra == \"docs\"",
      "sphinx-qt-documentation; extra == \"docs\"",
      "sphinxcontrib-bibtex>=1; extra == \"docs\"",
      "sphinx-tabs; extra == \"docs\"",
      "sphinxext-opengraph; extra == \"docs\""
    ],
    "package_metadata_description": "# PRISM: A **P**ython package for the inte**R**active and **I**ntegrated analy**S**is of **M**ultiplexed tissue microarrays\n\n<!--\n#FUTURE: package logo\n-->\n\n[![License MIT](https://img.shields.io/pypi/l/napari-prism.svg?color=green)](https://github.com/clinicalomx/napari-prism/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-prism.svg?color=green)](https://pypi.org/project/napari-prism)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-prism.svg?color=green)](https://python.org)\n[![tests](https://github.com/clinicalomx/napari-prism/workflows/tests/badge.svg)](https://github.com/clinicalomx/napari-prism/actions)\n[![codecov](https://codecov.io/gh/clinicalomx/napari-prism/branch/main/graph/badge.svg)](https://codecov.io/gh/clinicalomx/napari-prism)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-prism)](https://napari-hub.org/plugins/napari-prism)\n\n**NOTE: PRISM is still in heavy development.**\nPRISM or napari-prism is a package and [napari] plugin designed for interactively processing, analysing and visualising multiplxed tissue microarrays.\n\nCurrently, end-to-end capabilities (i.e. starting from importing the raw image file, to basic spatial analysis of annotated cells) are available for images generated from the\nAkoya Phenocycler‚Ñ¢-Fusion platform. However, the modular structure of the\npackage allows for usage at any stage of processing and/or analysis, given a pre-built SpatialData object using readers from either\n[spatialdata-io] or [sopa].\n\nPRISM uses [spatialdata] as the core data framework, allowing for:\n\n1. The rich integration of tools from the ([scverse]) Python bioinformatics ecosystem with highly interactive graphical user interfaces from [napari] and [napari-spatialdata].\n2. The storage of images, shapes, annotations and their linked `AnnData` objects in a standardized, FAIR-compliant data structure, addressing the non-standard and fragmented organization of files before, during, and after a multiplexed image analysis pipeline.\n\nThe package was designed to be used completely within the [napari] application and therefore require little to no knowledge of Python programming. Documentation for usage via the API is currently in progress.\n\n## Requirements\n\nInstall [miniconda] or anaconda.\n\nOpen the conda terminal and create a simple environment:\n\n```bash\nconda create -n prism python -c conda-forge\n```\n\nActivate the environment before executing the instructions in the Installation section.\n\n```bash\nconda activate prism\n```\n\n### List of Dependencies (Version specific)\n\n```\npython>=3.10\nspatialdata>=0.5.0\nspatialdata-plot>=0.2.11\nnapari[all]\nnapari_matplotlib>=3.0.0\nnapari_spatialdata>=0.5.7\ndask<2024.11.12\ncellpose>=3.0.10\nscanpy>=1.10.0\nxarray\n```\n\n## Installation: CPU only\n\nInstall this package via [pip]:\n\n```bash\npip install napari-prism\n```\n\nInstall the latest development version:\n\n```bash\npip install git+https://github.com/clinicalomx/napari-prism.git@main\n```\n\n## Installation: GPU-accelerated\n\n### General computations with RAPIDS and rapids-singlecell\n\nGeneral larger scale and/or computationally demanding functions can be accelerated with the [NVIDIA RAPIDS suite](https://rapids.ai/). We utilise some packages from this suite, as well as the GPU-accelerated implementation of scanpy with [rapids-singlecell].\n\n1. [Check and configure the system requirements from RAPIDS](https://docs.rapids.ai/install/#system-req).\n    - Currently, only Linux distributions (or Windows systems with WSL2) are supported.\n    - Install the [CUDA12.2](https://developer.nvidia.com/cuda-12-2-2-download-archive) or [CUDA12.5](https://developer.nvidia.com/cuda-12-5-1-download-archive) toolkit.\n2. Install the package together with [RAPIDS] and [rapids-singlecell] via [pip]:\n\n```bash\npip install napari-prism[gpu] --extra-index-url=https://pypi.nvidia.com\n```\n\n### Cell segmentation with Cellpose\n\nTo run [cellpose] on the GPU, install the [CUDA version of PyTorch](https://pytorch.org/get-started/locally/). You may need to [remove any installed CPU versions of PyTorch](https://github.com/MouseLand/cellpose?tab=readme-ov-file#gpu-version-cuda-on-windows-or-linux).\n\nThe new [cellpose] v4 adopts the SAM foundation model for generalization. While this may reduce the need for fine-tuning, we find that this needs higher end GPUs with more VRAM. If this is an issue, consider installing Cellpose 3, i.e:\n\n```bash\npip install cellpose<4\n```\n\n## Getting Started\n\nTo start using `napari-prism`, please see the [tutorials](https://napari-prism.readthedocs.io/en/latest/notebooks/getting_started.html#):\n\n- [Getting started](https://napari-prism.readthedocs.io/en/latest/notebooks/getting_started.html)\n- To learn how to interactively analyse raw .qptiff TMAs, see [TMA Image Analysis](https://napari-prism.readthedocs.io/en/latest/notebooks/tma_usage.html)\n- To learn how to interactively analyse AnnData-contained SpatialData objects, see [Anndata Analysis](https://napari-prism.readthedocs.io/en/latest/notebooks/adata_usage.html)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-prism\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Known Issues\n\nAdding shapes like `tma_envelopes` may cause segmentation faults (https://github.com/napari/napari/issues/6709). A workaround is to uninstall triangle (`pip uninstall triangle`)\n\n## Citation\n\nRafael Tubelleza, Aaron Kilgallon, Chin Wee Tan, James Monkman, John F Fraser, Arutha Kulasinghe, PRISM: a Python package for interactive and integrated analysis of multiplexed tissue microarrays, NAR Genomics and Bioinformatics, Volume 7, Issue 3, September 2025, lqaf114, https://doi.org/10.1093/nargab/lqaf114\n\n[napari]: https://github.com/napari/napari\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[file an issue]: https://github.com/clinicalomx/napari-prism/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[PyTorch]: https://pytorch.org/\n[cellpose]: https://github.com/MouseLand/cellpose\n[RAPIDS]: https://rapids.ai/\n[rapids-singlecell]: https://github.com/scverse/rapids_singlecell\n[spatialdata]: https://github.com/scverse/spatialdata/tree/main\n[napari-spatialdata]: https://github.com/scverse/napari-spatialdata/tree/main\n[spatialdata-io]: https://github.com/scverse/spatialdata-io\n[sopa]: https://github.com/gustaveroussy/sopa\n[scverse]: https://scverse.org/\n[miniconda]: https://www.anaconda.com/docs/getting-started/miniconda/install\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.qptiff",
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TMA Image Analysis",
      "AnnData Analysis"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "ndev-themes",
    "name": "ndev-themes",
    "display_name": "ndev themes",
    "version": "0.1.0",
    "created_at": "2025-08-29",
    "modified_at": "2025-08-29",
    "authors": [
      "Tim Monko"
    ],
    "author_emails": [
      "timmonko@gmail.com"
    ],
    "license": "Copyright (c) 2025, Tim Monko\n...",
    "home_pypi": "https://pypi.org/project/ndev-themes/",
    "home_github": "https://github.com/ndev-kit/ndev-themes",
    "home_other": null,
    "summary": "napari themes for the ndev-kit, for fun",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# ndev-themes\n\n[![License BSD-3](https://img.shields.io/pypi/l/ndev-themes.svg?color=green)](https://github.com/ndev-kit/ndev-themes/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/ndev-themes.svg?color=green)](https://pypi.org/project/ndev-themes)\n[![Python Version](https://img.shields.io/pypi/pyversions/ndev-themes.svg?color=green)](https://python.org)\n[![tests](https://github.com/ndev-kit/ndev-themes/workflows/tests/badge.svg)](https://github.com/ndev-kit/ndev-themes/actions)\n[![codecov](https://codecov.io/gh/ndev-kit/ndev-themes/branch/main/graph/badge.svg)](https://codecov.io/gh/ndev-kit/ndev-themes)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/ndev-themes)](https://napari-hub.org/plugins/ndev-themes)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nnapari themes for the ndev-kit, for fun\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\nThis plugin provides themes for napari, which can be selected in the napari settings (Preferences > Appearance). This is intended to be a fun way to test that napari plugin themes are working as expected and for learning to share this information and support users. You still may find these useful!\n\n### Cozy Theme\n\nA warm and cozy theme with brownish colors, intended to introduce a gray canvas for different visualizations.\n\n![ndev-cozy theme](./resources/ndev-cozy.png)\n\n## Installation\n\nYou can install `ndev-themes` via [pip]:\n\n```bash\npip install ndev-themes\n```\n\nIf napari is not already installed, you can install `ndev-themes` with napari and Qt via:\n\n```bash\npip install \"ndev-themes[all]\"\n```\n\n\nTo install latest development version :\n\n```bash\npip install git+https://github.com/ndev-kit/ndev-themes.git\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"ndev-themes\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/ndev-kit/ndev-themes/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "brainways",
    "name": "brainways",
    "display_name": "Brainways",
    "version": "0.1.16.3",
    "created_at": "2024-11-14",
    "modified_at": "2025-08-28",
    "authors": [
      "Ben Kantor"
    ],
    "author_emails": [
      "benkantor@mail.tau.ac.il"
    ],
    "license": "GPL-3.0",
    "home_pypi": "https://pypi.org/project/brainways/",
    "home_github": "https://github.com/bkntr/brainways",
    "home_other": null,
    "summary": "Brainways",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "aicsimageio[base-imageio]==4.14.0",
      "aicspylibczi",
      "brainglobe-atlasapi",
      "click",
      "dacite",
      "datasets",
      "fsspec",
      "huggingface-hub",
      "importlib-resources",
      "itk-elastix",
      "kornia",
      "napari[all]>=0.5.0",
      "natsort",
      "networkx",
      "numpy<2.0.0",
      "opencv-contrib-python-headless",
      "opencv-python-headless",
      "openpyxl",
      "pandas",
      "paquo",
      "qtpy",
      "scikit-image",
      "scikit-learn",
      "scikit-posthocs",
      "stardist",
      "statsmodels",
      "tensorflow<2.20",
      "toml",
      "torch",
      "torchvision",
      "lightning",
      "tqdm",
      "scyjava",
      "jpype1==1.5.0",
      "albumentations==2.0.5",
      "jsonargparse<5.0.0,>=4.0.0",
      "timm<2.0.0,>=1.0.0",
      "pre-commit; extra == \"dev\"",
      "scipy-stubs; extra == \"dev\"",
      "py; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-mock; extra == \"testing\"",
      "pytest-qt<4.1.0; extra == \"testing\"",
      "tox; extra == \"testing\""
    ],
    "package_metadata_description": "# Brainways\n\n[![DOI](https://img.shields.io/badge/DOI-10.1101/2023.05.25.542252-green.svg)](https://doi.org/10.1101/2023.05.25.542252)\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/brainways.svg?color=green)](https://github.com/bkntr/brainways/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/brainways.svg?color=green)](https://pypi.org/project/brainways)\n[![Python Version](https://img.shields.io/pypi/pyversions/brainways.svg?color=green)](https://python.org)\n[![tests](https://github.com/bkntr/brainways/workflows/tests/badge.svg)](https://github.com/bkntr/brainways/actions)\n[![codecov](https://codecov.io/gh/bkntr/brainways/branch/main/graph/badge.svg)](https://codecov.io/gh/bkntr/brainways)\n[![Documentation Status](https://readthedocs.org/projects/brainways/badge/?version=latest)](https://brainways.readthedocs.io/en/latest/?badge=latest)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/brainways)](https://napari-hub.org/plugins/brainways)\n\n## Overview\n\nBrainways is an AI-powered tool designed for the automated analysis of brain-wide activity networks from fluorescence imaging in coronal slices. It streamlines the process of registration, cell quantification, and statistical comparison between experimental groups, all accessible through a user-friendly interface without requiring programming expertise. For advanced users, Brainways also offers a flexible Python backend for customization.\n\n![Brainways User Interface Demo](assets/brainways-ui.gif)\n\n## Key Features\n\nBrainways simplifies complex analysis workflows into manageable steps:\n\n1.  **Rigid Registration:** Aligns coronal slices to a 3D reference atlas.\n2.  **Non-rigid Registration:** Refines alignment to account for individual variations and tissue distortions.\n3.  **Cell Detection:** Automatically identifies cells using the [StarDist](https://github.com/stardist/stardist) algorithm.\n4.  **Quantification:** Counts cells within defined brain regions.\n5.  **Statistical Analysis:**\n    *   Performs ANOVA contrast analysis between experimental conditions.\n    *   Conducts Partial Least Squares (PLS) analysis.\n    *   Generates network graphs visualizing brain-wide activity patterns.\n\n## Getting Started\n\n!!! note \"Windows GPU Support Pre-installation\"\n    If you plan to use Brainways with GPU acceleration on Windows, you must install GPU-compatible versions of PyTorch and TensorFlow *before* installing Brainways. Follow the instructions on the [PyTorch](https://pytorch.org/get-started/locally/) and [TensorFlow](https://www.tensorflow.org/install/pip) websites. Once these dependencies are met, proceed with the Brainways installation below.\n\nInstall and launch the Brainways user interface using pip:\n\n```bash\npip install brainways\nbrainways ui\n```\n\nFor a detailed walkthrough, please refer to our [Getting Started Guide](https://brainways.readthedocs.io/en/latest/02_getting_started/).\n\n!!! tip \"Achieving Reliable Results\"\n    To ensure the best possible outcomes with Brainways, we highly recommend reviewing our [Best Practices Guide](https://brainways.readthedocs.io/en/latest/04_best_practices/).\n\n## Architecture\n\nBrainways is built as a monorepo containing two primary components:\n\n*   `brainways`: The core library housing all backend functionalities, including registration algorithms, quantification logic, and statistical tools. It can be used programmatically via Python for custom workflows. The automatic registration model inference code resides within the `brainways.model` subpackage.\n*   `brainways.ui`: A [napari](https://napari.org/stable/) plugin providing the graphical user interface for interactive analysis.\n\n## Development Status\n\nBrainways is under active development by Ben Kantor at the Bartal Lab, Tel Aviv University, Israel. Check out our [releases page](https://github.com/bkntr/brainways/releases) for the latest updates.\n\n## Citation\n\nIf Brainways contributes to your research, please cite our publication: [Kantor and Bartal (2025)](https://doi.org/10.1038/s41386-025-02105-3).\n\n```bibtex\n@article{kantor2025mapping,\n    title={Mapping brain-wide activity networks: brainways as a tool for neurobiological discovery},\n    author={Kantor, Ben and Ruzal, Keren and Ben-Ami Bartal, Inbal},\n    journal={Neuropsychopharmacology},\n    pages={1--11},\n    year={2025},\n    publisher={Springer International Publishing Cham}\n}\n```\n\n## License\n\nBrainways is distributed under the terms of the [GNU GPL v3.0] license. It is free and open-source software.\n\n## Issues and Support\n\nEncountering problems? Please [file an issue] on our GitHub repository with a detailed description of the problem.\n\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[file an issue]: https://github.com/bkntr/brainways/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.bwp"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Brainways"
    ],
    "contributions_sample_data": [
      "Sample project",
      "Annotated sample project"
    ]
  },
  {
    "normalized_name": "napari-3d-counter",
    "name": "napari-3d-counter",
    "display_name": "3D Counter",
    "version": "0.5.4",
    "created_at": "2023-10-18",
    "modified_at": "2025-08-28",
    "authors": [
      "Peter Newstein"
    ],
    "author_emails": [
      "peternewstein@gmail.com"
    ],
    "license": "GPL-3.0-or-later",
    "home_pypi": "https://pypi.org/project/napari-3d-counter/",
    "home_github": "https://github.com/pnewstein/napari-3d-counter",
    "home_other": null,
    "summary": "A simple plugin for counting objects in 3D images",
    "categories": [
      "Measurement",
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "pandas",
      "scikit-image",
      "napari==0.6.4",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-3d-counter\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-3d-counter.svg?color=green)](https://github.com/pnewstein/napari-3d-counter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-3d-counter.svg?color=green)](https://pypi.org/project/napari-3d-counter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-3d-counter.svg?color=green)](https://python.org)\n[![tests](https://github.com/pnewstein/napari-3d-counter/workflows/tests/badge.svg)](https://github.com/pnewstein/napari-3d-counter/actions)\n[![codecov](https://codecov.io/gh/pnewstein/napari-3d-counter/branch/main/graph/badge.svg)](https://codecov.io/gh/pnewstein/napari-3d-counter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-3d-counter)](https://napari-hub.org/plugins/napari-3d-counter)\n\nA simple plugin for counting objects in 3D images\n\n![small](https://github.com/pnewstein/napari-3d-counter/assets/30813691/9d524c31-f23b-4b34-bcb6-ec3bb415cdae)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-3d-counter` via conda\n    \n    conda install -c conda-forge napari-3d-counter pyqt\n\nor pip\n\n    pip install napari-3d-counter\n\n\n\nTo install latest development version:\n\n    pip install git+https://github.com/pnewstein/napari-3d-counter.git\n\n\n##  Count3D Usage\n\nFirst launch napari with the `napari` command or interactively through ipython\n```python\nimport napari\nviewer = napari.Veiwer()\n```\nCount3D can be launched from the plugin menu of napari, or through the command palette (ctrl shift P)\n\n\n### Adding a cell\n\nYou can add a cell of the currently selected cell type by clicking on the viewer.\n\n- Ensure that `Point adder` layer is selected\n- Ensure that `Add points` tool is selected\n- Click on the viewer where you would like the point to be added\n\nThe counter on the current cell type's button will be incremented\n\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/745d495e-1d18-43dd-aa5e-e9ecd835cdae\n\n\n### Changing cell type\n\nYou can change the currently selected cell type by clicking on that cell type's\nbutton. This change will be reflected in the GUI. Additionally, the keyboard\nshortcut for that cell type can be used. Keyboard shortcuts are listed on the\nbutton, and are \"q\", \"w\", \"e\", \"r\", \"t\", \"y\" by default\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/844d04ce-2795-4226-a98b-d5fe5a0b131e\n\n\n### Undo last added cell\n\nThe undo button (shortcut u) will remove last added cell, regardless of\ncell type\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/c04ca5e3-9f48-4dd5-89e5-a9866b353e03\n\n\n### Remove a particular cell\n\nTo remove a particular cell, change to the layer containing the cell you would\nlike to remove. Then select the `select points` tool to select the points to\ndelete, then use `Delete selected points` to delete those points\n\nThis change will be reflected in the counts.\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/d0787cba-9b23-46d5-9cd3-21a4ad73460a\n\n\n\n### Change appearance of a cell type\n\nChanges to the name or edge color of a points layer will be reflected in the\npreviously added points, as well as the GUI. Features that are editable in this way include:\n    - face color\n    - edge color\n    - symbol\n    - size\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/6c495270-d4c4-473e-9091-8d2e0f8e2764\n\n\n### Save configuration\n\nUse the `Make launch_cell_count.py` button to create a python script that will\nlaunch napari with 3DCounter added to the dock and current cell type appearances\nalready loaded\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/3448652d-3064-4900-8bbe-e88d75667108\n\n\n### Save cells\n\nUse the \"Save cells\" button to save the cell coordinates for all layers into a\ncsv file\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/38b30f2a-cc83-46c2-8b19-4d44715c07c5\n\n\n### Load cells\n\nUse the \"Load cells\" button to load the cells from a csv file into new layers\n\n\nhttps://github.com/pnewstein/napari-3d-counter/assets/30813691/7df74688-85b1-4b61-aa51-dab179763832\n\n\n### Launch with saved configuration\n\nTo run Count3D with custom configuration, paste the following code into your napari ipython console\n\n```python\nfrom napari_3d_counter import Count3D, CellTypeConfig\n\ncell_type_config = [\n    # The first cell type is called \"cq+eve+\" and should be green\n    CellTypeConfig(\n        name=\"cq+eve+\",\n        color=\"g\"\n    ),\n    # The first cell type is called \"cq+eve-\" and should be cyan\n    CellTypeConfig(\n        name=\"cq+eve-\",\n        color=\"c\"\n    ),\n    # The first cell type is called \"cq-eve+\" and should be red\n    CellTypeConfig(\n        name=\"cq-eve+\",\n        color=\"r\"\n    ),\n]\n# Launch the plugin with configuration\nviewer.window.add_dock_widget(Count3D(viewer, cell_type_config=cell_type_config))\n```\n\n##  Auxiliary plugins\n### Reconstruct Selected\nOne use case of Napari 3D Counter is to visualize a subset of labeled cells.\nFor example, automated process label your cells of interest as well as a set of\noff-target cells, and you would like to visualize only your cells of interest.\nThis can be accomplished by using Napari 3D Counter to count your cells of\ninterest, and some other process to create labels (perhaps\n[nsbatwm](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes))\nand using Reconstruct Selected to create a new image layer of those labels\nwhich have been counted as a particular cell type.\n\n### Ingress Points\nThis plugin takes a points layer and adds the points to the selected cell_type\nlayer. This can be useful if you want to manually count cells after cell identification.\n\n### Split on Shapes\nThis plugin can be used to subset a cell type into several groups based on their\nx-y location. Simply draw a shape that surrounds your cells (perhaps in a\nsegment) and run this plugin to get a list of cells of each type in each shape.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-3d-counter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/pnewstein/napari-3d-counter/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Count 3D",
      "Reconstruct Selected",
      "Ingress Points",
      "Split on Shapes"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "brainrender-napari",
    "name": "brainrender-napari",
    "display_name": "brainrender",
    "version": "0.1.3",
    "created_at": "2023-10-13",
    "modified_at": "2025-08-27",
    "authors": [
      "Alessandro Felder"
    ],
    "author_emails": [
      "Alessandro Felder <a.felder@ucl.ac.uk>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/brainrender-napari/",
    "home_github": "https://github.com/brainglobe/brainrender-napari",
    "home_other": null,
    "summary": "A napari plugin to render BrainGlobe atlases and associated data as layers.",
    "categories": [],
    "package_metadata_requires_python": ">=3.11.0",
    "package_metadata_requires_dist": [
      "brainglobe-atlasapi>=2.2.0",
      "brainglobe-utils>=0.4.3",
      "meshio",
      "napari>=0.6.1",
      "numpy",
      "qtpy",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "black; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "setuptools_scm; extra == \"dev\"",
      "pyqt5; extra == \"dev\""
    ],
    "package_metadata_description": "# brainrender-napari\n\n[![License BSD-3](https://img.shields.io/pypi/l/brainrender-napari.svg?color=green)](https://github.com/brainglobe/brainrender-napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/brainrender-napari.svg?color=green)](https://pypi.org/project/brainrender-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/brainrender-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/brainglobe/brainrender-napari/workflows/tests/badge.svg)](https://github.com/brainglobe/brainrender-napari/actions)\n[![codecov](https://codecov.io/gh/brainglobe/brainrender-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/brainglobe/brainrender-napari)\n\nVisualisation and management of BrainGlobe atlases in napari.\n\n----------------------------------\n\nA napari plugin to visualise and manage BrainGlobe atlases. `brainrender-napari` aims to port the functionality of [`brainrender`](https://github.com/brainglobe/brainrender) to [`napari`](https://napari.org/stable/).\n![add-region-brainrender-napari](https://github.com/brainglobe/brainrender-napari/assets/10500965/24fd3752-0ba7-4f47-aabf-5de22ff0f69b)\n\n## Usage\n\nCheck out the [\"Visualising an atlas in napari\"](https://brainglobe.info/tutorials/visualise-atlas-napari.html) tutorial in the BrainGlobe documentation.\n\n## Installation\n\nWe strongly recommend to use a virtual environment manager (like `conda` or `venv`). The installation instructions below will not specify the Qt backend for napari, and you will therefore need to install that separately. Please see [the `napari` installation instructions](https://napari.org/stable/tutorials/fundamentals/installation.html) for further advice on this.\n\nYou can install `brainrender-napari` via [pip]:\n\n    pip install brainrender-napari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/brainglobe/brainrender-napari.git\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"brainrender-napari\" is free and open source software\n\n\n## Acknowledgements\n\nThis [@napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template and the [Neuroinformatics Unit's template](https://github.com/neuroinformatics-unit/python-cookiecutter).\n\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[file an issue]: https://github.com/brainglobe/brainrender-napari/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Brainrender",
      "Manage atlas versions"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tree-rings",
    "name": "napari-tree-rings",
    "display_name": "Napari Tree Rings",
    "version": "0.1.5",
    "created_at": "2025-08-11",
    "modified_at": "2025-08-26",
    "authors": [
      "Volker Baecker",
      "Thi-Thu-Khiet Dang"
    ],
    "author_emails": [
      "volker.baecker@mri.cnrs.fr",
      "dangthithukhiet7988@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tree-rings/",
    "home_github": null,
    "home_other": "None",
    "summary": "A tool to delineate bark, pith and xylem annual rings and to measure their property parameters on circular sections of tree trunks.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "brightest-path-lib",
      "tree-ring-analyzer",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "appdirs",
      "scyjava",
      "pyperclip",
      "shapelysmooth",
      "imagej",
      "napari-imagej",
      "scikit-learn",
      "torch",
      "torchvision",
      "matplotlib",
      "opencv-python-headless",
      "pint",
      "tifffile",
      "tensorflow==2.17.0",
      "python-polylabel",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "qtpy; extra == \"testing\"",
      "tree-ring-analyzer; extra == \"testing\"",
      "matplotlib; extra == \"testing\"",
      "brightest-path-lib; extra == \"testing\"",
      "numpy; extra == \"testing\"",
      "scikit-image; extra == \"testing\"",
      "opencv-python-headless; extra == \"testing\"",
      "pint; extra == \"testing\"",
      "tifffile; extra == \"testing\"",
      "tensorflow==2.17.0; extra == \"testing\"",
      "python-polylabel; extra == \"testing\"",
      "scikit-learn; extra == \"testing\"",
      "sphinx_rtd_theme; extra == \"docs\"",
      "myst_parser; extra == \"docs\"",
      "sphinx_tabs; extra == \"docs\"",
      "sphinx; extra == \"docs\""
    ],
    "package_metadata_description": "# napari-tree-rings\n\n[![License MIT](https://img.shields.io/pypi/l/napari-tree-rings.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/napari-tree-rings/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tree-rings.svg?color=green)](https://pypi.org/project/napari-tree-rings)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tree-rings.svg?color=green)](https://python.org)\n[![tests](https://github.com/MontpellierRessourcesImagerie/napari-tree-rings/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/napari-tree-rings/actions)\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-tree-rings/branch/main/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-tree-rings)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tree-rings)](https://napari-hub.org/plugins/napari-tree-rings)\n\nA tool to delineate bark, pith and xylem annual rings and to measure their property parameters on circular sections of tree trunks.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## How to use it?\nUsers can export the segmentation findings and estimate bark, ring borders, and pith with ease using the Napari Tree Rings plugin:\n- Run button on the Segment Rings tag: find the rings in just one image.\n- Run Batch button on the Batch Segment Trunk tag: runs all the images in the folder. \n\nUsers can also modify certain parameters, including the batch size. The interface's goal is to assist biologists without having programming expertise by being user-friendly.\n\nIf accessible, the unit of micrometres will be used to determine the parameters; if not, pixels will be used. The calculated parameters are made up of:\n- bbox: The bounding box‚Äôs minimum and maximum coordinates on the horizontal and vertical axes.\n- perimeter: perimeter of the region, measured as the length of the contour.\n- area: Region‚Äôs area.\n- area_convex: Area of the convex hull image, which is the smallest convex polygon enclosing the region.\n- axis_major_length: Length of the ring boundaries‚Äô major axis.\n- axis_minor_length: Length of the ring boundaries‚Äô minor axis.\n- eccentricity: The eccentricity, which ranges from 0 to 1, is the focal distance divided by the major axis length. When the eccentricity is zero, the region becomes a circle.\n- feret_diameter_max: The maximum Feret's diameter, which is the largest distance between points across the convex hull.\n- orientation: Angle between the major axis and the vertical axis, measured in radians and ranging from -pi/2 to pi/2 anticlockwise.\n- area_growth: The area between the two ring boundaries that experiences growth over a year (except the cases of pith and bark).\n\n- For more details, check the [detailed documentation](https://montpellierressourcesimagerie.github.io/napari-tree-rings).\n\n## Installation\n\nYou can install `napari-tree-rings` via [pip]:\n\n    pip install napari-tree-rings\n\n\n## Adding other measurements\nIf you would like to add other measurements while running batch, you can modify `BatchSegmentTrunk.run` in the `src/napari_tree_rings/image/process.py`. There is an example of `area_growth` for you to see and refer to.\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-tree-rings\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segment Trunk"
    ],
    "contributions_sample_data": [
      "Napari Tree Rings"
    ]
  },
  {
    "normalized_name": "napari-flowreg",
    "name": "napari-flowreg",
    "display_name": "Flow-Registration Motion Correction and Analysis for 2P Imaging",
    "version": "0.1.0a2",
    "created_at": "2025-08-22",
    "modified_at": "2025-08-25",
    "authors": [
      "Philipp Flotho"
    ],
    "author_emails": [
      "Philipp Flotho <Philipp.Flotho@uni-saarland.de>"
    ],
    "license": "CC BY-NC-SA 4.0",
    "home_pypi": "https://pypi.org/project/napari-flowreg/",
    "home_github": "https://github.com/FlowRegSuite/napari-flowreg",
    "home_other": null,
    "summary": "napari plugin for Flow-Registration motion correction",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "pyflowreg[vis]",
      "qtpy",
      "numpy",
      "scipy>=1.10.0",
      "matplotlib>=3.5.0",
      "pytest>=7.0; extra == \"testing\"",
      "pytest-cov>=4.0; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pytest-xvfb; extra == \"testing\"",
      "npe2>=0.7.0; extra == \"testing\"",
      "PySide6>=6.5; extra == \"testing\"",
      "h5py; extra == \"testing\"",
      "tifffile; extra == \"testing\"",
      "psutil; extra == \"testing\"",
      "tomli>=2.0.1; extra == \"testing\"",
      "napari[all]>=0.4.18; extra == \"testing\"",
      "napari[all]>=0.4.18; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "black; extra == \"dev\"",
      "ruff; extra == \"dev\""
    ],
    "package_metadata_description": "[![PyPI - Version](https://img.shields.io/pypi/v/napari-flowreg)](https://pypi.org/project/napari-flowreg/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/napari-flowreg)](https://pypi.org/project/napari-flowreg/)\n[![PyPI - License](https://img.shields.io/pypi/l/napari-flowreg)](LICENSE)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-flowreg)](https://pypistats.org/packages/napari-flowreg)\n[![GitHub Actions](https://github.com/FlowRegSuite/napari-flowreg/actions/workflows/pypi-release.yml/badge.svg)](https://github.com/FlowRegSuite/napari-flowreg/actions/workflows/pypi-release.yml)\n\n## üöß Under Development\n\nThis project is still in an **alpha stage**. Expect rapid changes, incomplete features, and possible breaking updates between releases. \n\n- The API may evolve as we stabilize core functionality.  \n- Documentation and examples are incomplete.  \n- Feedback and bug reports are especially valuable at this stage.\n\n# <img src=\"https://raw.githubusercontent.com/FlowRegSuite/pyflowreg/refs/heads/main/img/flowreglogo.png\" alt=\"FlowReg logo\" height=\"64\"> napari-FlowReg\n\nThis repository contains the napari wrapper for the Flow-Registration toolbox, which is a toolbox for the compensation and stabilization of multichannel microscopy videos. \nThe publication for this toolbox can be found [here](https://doi.org/10.1002/jbio.202100330) and the project website with video results [here](https://www.snnu.uni-saarland.de/flow-registration/).\n\n**Related projects**\n- PyFlowReg: https://github.com/FlowRegSuite/pyflowreg\n- Original Flow-Registration repo: https://github.com/FlowRegSuite/flow_registration\n- ImageJ/Fiji plugin: https://github.com/FlowRegSuite/flow_registration_IJ\n\n\n![Fig1](https://raw.githubusercontent.com/FlowRegSuite/pyflowreg/refs/heads/main/img/bg.jpg)\n\n\n## Installation via pip and conda\n\nTo install the plugin via conda, you can create a new environment and install `napari` along with the plugin:\n\n    conda create -n flowreg -c conda-forge python=3.12.0 napari\n\nYou can then install `napari-flowreg` via [pip]:\n\n    pip install napari[all] napari-flowreg\n\nor from the directly from the GitHub repository:\n\n    pip install git+https://github.com/flowregsuite/napari-flowreg.git\n\n\n## Dataset\n\nThe dataset which we used for our evaluations is available as [2-Photon Movies with Motion Artifacts](https://drive.google.com/drive/folders/1fPdzQo5SiA-62k4eHF0ZaKJDt1vmTVed?usp=sharing).\n\n## Citation\n\nDetails on the original method and video results can be found [here](https://www.snnu.uni-saarland.de/flow-registration/).\n\nIf you use parts of this code or the plugin for your work, please cite\n\n> ‚ÄúPyflowreg,‚Äù (in preparation), 2025.\n\nand \n\n> P. Flotho, S. Nomura, B. Kuhn and D. J. Strauss, ‚ÄúSoftware for Non-Parametric Image Registration of 2-Photon Imaging Data,‚Äù J Biophotonics, 2022. [doi:https://doi.org/10.1002/jbio.202100330](https://doi.org/10.1002/jbio.202100330)\n\nBibTeX entry\n```\n@article{flotea2022a,\n    author = {Flotho, P. and Nomura, S. and Kuhn, B. and Strauss, D. J.},\n    title = {Software for Non-Parametric Image Registration of 2-Photon Imaging Data},\n    year = {2022},\n  journal = {J Biophotonics},\n  doi = {https://doi.org/10.1002/jbio.202100330}\n}\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Flow-Registration"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ome-zarr-navigator",
    "name": "napari-ome-zarr-navigator",
    "display_name": "napari OME-Zarr Navigator",
    "version": "0.2.1",
    "created_at": "2024-07-19",
    "modified_at": "2025-08-25",
    "authors": [
      "Fabio Steffen and Joel Luethi"
    ],
    "author_emails": [
      "fabio.steffen@uzh.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-ome-zarr-navigator/",
    "home_github": "https://github.com/fractal-analytics-platform/napari-ome-zarr-navigator",
    "home_other": null,
    "summary": "A plugin to browse OME-Zarr plates by conditions and load images, labels and features from ROIs",
    "categories": [
      "Dataset",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari-ome-zarr",
      "ome-zarr",
      "wget",
      "ngio<0.3,>=0.2.4",
      "numcodecs!=0.14.0,!=0.14.1,!=0.16",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-ome-zarr-navigator <img align=\"right\" height=\"150\" src=\"https://raw.githubusercontent.com/fractal-napari-plugins-collection/napari-ome-zarr-navigator/master/docs/images/navigator_logo.png\">\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-ome-zarr-navigator.svg?color=green)](https://github.com/fractal-analytics-platform/napari-ome-zarr-navigator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ome-zarr-navigator.svg?color=green)](https://pypi.org/project/napari-ome-zarr-navigator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ome-zarr-navigator.svg?color=green)](https://python.org)\n[![tests](https://github.com/fractal-analytics-platform/napari-ome-zarr-navigator/workflows/tests/badge.svg)](https://github.com/fractal-analytics-platform/napari-ome-zarr-navigator/actions)\n[![codecov](https://codecov.io/gh/fractal-analytics-platform/napari-ome-zarr-navigator/branch/main/graph/badge.svg)](https://codecov.io/gh/fractal-analytics-platform/napari-ome-zarr-navigator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ome-zarr-navigator)](https://napari-hub.org/plugins/napari-ome-zarr-navigator)\n\nA plugin to browse OME-Zarr plates by conditions and load images, labels and features from ROIs\n\n\n## Usage\n\nThe ImageBrowser recognizes OME-Zarr plates that were loaded via [napari-ome-zarr](https://github.com/ome/napari-ome-zarr) and allows the selection of wells in the multiwell plate.\n\n<img width=\"1624\" alt=\"navigator_1\" src=\"https://github.com/user-attachments/assets/86c0c0d5-df4c-4579-8719-c36efe67485c\">\n\nThe ImageBrowser allows to zoom to a given well (\"Go to well\") & puts a white bounding box around the selected well.\n\n<img width=\"1624\" alt=\"navigator_2\" src=\"https://github.com/user-attachments/assets/13ead72a-de0e-4b03-8051-b9759af4a131\">\n\nUsing prototype `condition tables` (to be defined better, see sample data provided by the plugin orthe example in the test data below), the ImageBrowser allows for selecting subsets of the well list based on conditions defined in the `condition table`. The [operetta-compose Fractal task package](https://github.com/leukemia-kispi/operetta-compose) provides a task to create such condition tables.\n\n\n<img width=\"1624\" alt=\"navigator_3\" src=\"https://github.com/user-attachments/assets/f8cb9311-49ef-43fd-9358-4193ffb58877\">\n\nThe ROI loader (formerly [available separately as a napari plugin](https://github.com/jluethi/napari-ome-zarr-roi-loader)) can be used standalone or integrated with the ImageBrowser. If a well is selected from the ImageBrowser, all the images in that well can be loaded via the ROI loader.\nThis supports:\n- Loading images from different multiplexing acquisitions\n- Loading any ROI based on [Fractal ROI tables](https://fractal-analytics-platform.github.io/fractal-tasks-core/tables/#roi-tables)\n- Loading label images\n- Loading feature measurements (based on [Fractal feature tables](https://fractal-analytics-platform.github.io/fractal-tasks-core/tables/#feature-tables) in the AnnData format)\n\nThis approach of loading label images and feature data has been optimized for and tested with the [napari feature classifier](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier).\n\n<img width=\"1624\" alt=\"navigator_4\" src=\"https://github.com/user-attachments/assets/0548c8f1-23c3-4c55-9388-c818bb94bbc3\">\n\nThis plugin is meant to work well for OME-Zarr files generated by [Fractal](https://fractal-analytics-platform.github.io/).\n\n----------------------------------\n\n\n## Test data\n\nTest data is available at https://zenodo.org/records/11262587\n\n\n## Installation\n\nYou can install `napari-ome-zarr-navigator` via [pip]:\n\n    pip install napari-ome-zarr-navigator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/fractal-analytics-platform/napari-ome-zarr-navigator.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-ome-zarr-navigator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/fractal-analytics-platform/napari-ome-zarr-navigator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Image Browser",
      "ROI Loader"
    ],
    "contributions_sample_data": [
      "hiPSC",
      "leukemia"
    ]
  },
  {
    "normalized_name": "napari-czitools",
    "name": "napari-czitools",
    "display_name": "CziReadTools",
    "version": "0.0.4",
    "created_at": "2025-08-20",
    "modified_at": "2025-08-24",
    "authors": [
      "Sebastian Rhode"
    ],
    "author_emails": [
      "sebrhode@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-czitools/",
    "home_github": "https://github.com/sebi06/napari-czitools",
    "home_other": null,
    "summary": "Plugin to read CZI image files and their metadata",
    "categories": [
      "IO"
    ],
    "package_metadata_requires_python": "<3.13,>=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "superqt",
      "scikit-image",
      "pyqtgraph",
      "czitools>=0.11.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "- [napari-czitools](#napari-czitools)\n  - [Installation](#installation)\n  - [Supported Operating Systems](#supported-operating-systems)\n  - [Usage - Core Functionalities](#usage---core-functionalities)\n    - [Open Complete CZI Files](#open-complete-czi-files)\n    - [Open CZI Sample Data](#open-czi-sample-data)\n      - [CellDivision 5D Stack](#celldivision-5d-stack)\n      - [Neurons 3D Stack](#neurons-3d-stack)\n      - [AiryScan 3D Stack](#airyscan-3d-stack)\n      - [Wellplate Data](#wellplate-data)\n    - [Advanced CZI Reader (CziReadTools) plugin](#advanced-czi-reader-czireadtools-plugin)\n  - [Current Limitations](#current-limitations)\n    - [Future plans](#future-plans)\n  - [Contributing](#contributing)\n  - [License](#license)\n  - [Issues](#issues)\n- [Disclaimer](#disclaimer)\n\n# napari-czitools\n\n[![License MIT](https://img.shields.io/pypi/l/napari-czitools.svg?color=green)](https://github.com/sebi06/napari-czitools/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-czitools.svg?color=green)](https://pypi.org/project/napari-czitools)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-czitools.svg?color=green)](https://python.org)\n[![tests](https://github.com/sebi06/napari-czitools/workflows/tests/badge.svg)](https://github.com/sebi06/napari-czitools/actions)\n[![codecov](https://codecov.io/gh/sebi06/napari-czitools/branch/main/graph/badge.svg)](https://codecov.io/gh/sebi06/napari-czitools)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-czitools)](https://napari-hub.org/plugins/napari-czitools)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nPlugin to read CZI image file and metadata\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n![napari-czitools - Read CZI Metadata and load image Data](https://github.com/sebi06/napari-czitools/raw/main/readme_images/title_pic.png)\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-czitools` via [pip]:\n\n    pip install napari-czitools\n\nTo install latest development version :\n\n    pip install git+https://github.com/sebi06/napari-czitools.git\n\n## Supported Operating Systems\n\nCurrently this only tested on:\n\n- Linux\n- Windows\n\nMacOS is not supported yet out of the box yet, but [czitools] uses [pylibCZIrw]. But it should be possible to install it manually: [MaxOS wheels for pylibCZIrw] (read and write CZI files on MacOS).\n\n## Usage - Core Functionalities\n\nThe plugin provides a reader for CZI files and allows to load the image data into [napari]. It also reads the metadata from the CZI file and displays it in the metadata panel of [napari].\n\n### Open Complete CZI Files\n\n- Open complete CZI Files and display the metadata in Napari using the [czitools] package\n\n![Open complete CZI file](./readme_images/file_open_mdtable_lls7.png)\n\n- Open different CZI Image sample data\n- if not found locally in current directory `../src/napari_czitools/sample_data` it will be opened from remote repository (might be slow)\n\n![Open sample data](https://github.com/sebi06/napari-czitools/raw/main/readme_images/open_sample1.png)\n\n### Open CZI Sample Data\n\n#### CellDivision 5D Stack\n\n![Sample Data - 5D Stack](https://github.com/sebi06/napari-czitools/raw/main/readme_images/open_sample_5D.png)\n\n#### Neurons 3D Stack\n\n![Sample Data - 3D Stack](https://github.com/sebi06/napari-czitools/raw/main/readme_images/open_sample_3D.png)\n\n#### AiryScan 3D Stack\n\n![Sample Data - AiryScan 3D Stack](https://github.com/sebi06/napari-czitools/raw/main/readme_images/open_sample_airyscan.png)\n\n#### Wellplate Data\n\n![Sample Data - Wellpate](https://github.com/sebi06/napari-czitools/raw/main/readme_images/open_sample_wellplate.png)\n\n### Advanced CZI Reader (CziReadTools) plugin\n\nSelect the plugin to show the UI in the right panel of the Napari UI via \"Plugins > Advanced CZI Reader (CziReadTools)\"\n\n1) Select the CZI file to read its metadata\n2) Once the metadata are read the display can be toggled between a **table** and a **tree view**\n3) The metadata will update the dimension double-range sliders and enable reading the pixel data\n\n<img src=\"https://github.com/sebi06/napari-czitools/raw/main/readme_images/reader_adv1.png\" alt=\"Advanced CZI Reader - Plugin\" style=\"width:30%; height:auto;\">\n\n1) Metadata will be shown as a **table** or as a **tree view**\n2) The **Load Pixel Data** button will be enabled once the metadata is read\n3) The **Dimension Sliders** will be enabled and allow to select an range to be read for all available dimensions\n\n<img src=\"https://github.com/sebi06/napari-czitools/raw/main/readme_images/reader_adv2.png\" alt=\"Advanced CZI Reader - Plugin\" style=\"width:80%; height:auto;\">\n\n- The dimensions slider allow to define size of CZI subset to be read\n- This allows to read parts of a CZI image dataset\n- Important - when reading a subset the metadata will still reflects the size of the complete CZI\n\n![Advanced CZI Reader - Plugin](./readme_images/load_pixel1.png)\n\n- Example for reading a subset\n  - Timepoints (4-7): 4 slices or T=4\n  - Channels (0-0): 1 slice or CH=1\n  - Z-Plane (7-10): 4 slices or Z=4\n\n![Advanced CZI Reader - Plugin](./readme_images/load_pixel2.png)\n\n## Current Limitations\n\nThe plugin is still in its very early stage, therefor expect bugs and breaking changes\n\n- reading CZI with multiple scenes only works when the scenes have equal size\n- opening the sample CZI files will not display the CZI metadata right now\n\n### Future plans\n\n- allow reading individual scenes when scenes have different sizes\n- upgrade [pylibCZIrw] to allow use [bioio-czi] for even better reading\n- export of metadata table\n\nFeedback is always welcome!\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-czitools\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n# Disclaimer\n\nThe software & scripts are free to use for everybody. The author undertakes no warranty concerning the use of this plugins and scripts. Use them on your own risk.\n\nBy using this plugin you agree to this disclaimer.\n\nVersion: 2025.08.20\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n[file an issue]: https://github.com/sebi06/napari-czitools/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[czitools]: https://pypi.org/project/czitools/\n[pylibCZIrw]: https://pypi.org/project/pylibCZIrw/\n[MaxOS wheels for pylibCZIrw]: https://pypi.scm.io/#/package/pylibczirw\n[bioio-czi]: https://pypi.org/project/bioio-czi/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.czi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Advanced CZI Reader"
    ],
    "contributions_sample_data": [
      "Celldivision 5D dataset",
      "WellPlate 6D dataset",
      "Z-Stack dataset",
      "Airyscan Z-Stack dataset",
      "HE Stain dataset"
    ]
  },
  {
    "normalized_name": "napari-nd-annotator",
    "name": "napari-nD-annotator",
    "display_name": "Annotation Toolbox",
    "version": "0.3.2",
    "created_at": "2022-06-01",
    "modified_at": "2025-08-21",
    "authors": [
      "David Bauer",
      "Jozsef Molnar",
      "Dominik Hirling"
    ],
    "author_emails": [
      "dbauer@brc.hu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-nd-annotator/",
    "home_github": "https://github.com/bauerdavid/napari-nD-annotator",
    "home_other": null,
    "summary": "A toolbox for annotating objects one by one in nD",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magic-class",
      "qtpy",
      "opencv-python",
      "matplotlib",
      "napari>=0.4.11",
      "scikit-image>=0.19",
      "SimpleITK",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "numpy; extra == \"testing\"",
      "napari-bbox; extra == \"bbox\"",
      "minimal-surface; extra == \"ms\"",
      "napari-bbox; extra == \"all\"",
      "minimal-surface; extra == \"all\""
    ],
    "package_metadata_description": "# napari-nD-annotator\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-nD-annotator.svg?color=green)](https://github.com/bauerdavid/napari-nD-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nD-annotator.svg?color=green)](https://pypi.org/project/napari-nD-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nD-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/bauerdavid/napari-nD-annotator/workflows/tests/badge.svg)](https://github.com/bauerdavid/napari-nD-annotator/actions)\n[![codecov](https://codecov.io/gh/bauerdavid/napari-nD-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/bauerdavid/napari-nD-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nD-annotator)](https://napari-hub.org/plugins/napari-nD-annotator)\n\n**A toolbox for annotating objects one by one in nD.**\n\nThis plugin contains some tools to make 2D/3D (and technically any dimensional) annotation easier.\nMain features:\n * auto-filling labels\n * label slice interpolation (geometric mean, RPSV representation)\n * minimal contour segmentation\n\nIf the <code>[napari-bbox]</code> plugin is also installed (see [Installation](#installation)), you can also\n * list objects annotated with bounding boxes \n * visualize selected objects from different projections\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-nD-annotator` via [pip]:\n\n    pip install napari-nD-annotator\n\nThe plugin is also available in [napari-hub], to install it directly from napari, please refer to\n[plugin installation instructions] at the official [napari] website.\n\n\n### Optional packages\nThere are some functionalities which require additional Python packages.\n\n#### Bounding boxes\nThe bounding box and object list functionality requires the <code>[napari-bbox]</code> Python package.\nIf you want to use these features, install <code>[napari-bbox]</code> separately either using [pip] or directly from napari.\nYou can also install it together with this plugin:\n```\npip install napari-nD-annotator[bbox]\n```\n\n#### Minimal surface\nTo use the minimal surface method, you will need the <code>[minimal-surface]</code> Python package as well. Please install it using [pip]:\n\nSeparately:\n```\npip install minimal-surface\n```\n\nOr bundled with the plugin:\n```\npip install napari-nD-annotator[ms]\n```\n> [!WARNING]\n> The <code>[minimal-surface]</code> package is only available for Windows at the time. We are actively working on bringing it to Linux and Mac systems as well.\n\n#\n\nIf you would like to install all optional packages, use\n```\npip install napari-nD-annotator[all]\n```\n###\nIf any problems occur during installation or while using the plugin, please [file an issue].\n\n## Usage\nYou can start napari with the plugin's widgets already opened as:\n\n    napari -w napari-nD-annotator \"Object List\" \"Annotation Toolbox\"\n\n\n### Bounding boxes\nThe main idea is to create bounding boxes around objects we want to annotate, crop them, and annotate them one by one. This has mainly two advantages when visualizing in 3D:\n\n1. We don't have to load the whole data into memory\n2. The surrounding objects won't occlude the annotated ones, making it easier to check the annotation.\n\nBounding boxes can be created from the `Object list` widget. The dimensionality of the bounding box layer will be determined from the image layer. As bounding boxes are created, a small thumbnail will be displayed.\n\nThe proposed pipeline goes as follows:\n\n 1. Create a bounding box layer\n 2. Select data parts using the bounding boxes\n 3. Select an object from the object list\n 4. Annotate the object\n 5. Repeat from 3.\n\n### Slice interpolation\nThe `Interpolation` tab contains tools for estimating missing annotation slices from existing ones. There are multiple options:\n * Geometric: the interpolation will be determined by calculating the average of the corresponding contour points.\n * RPSV: A more sophisticated average contour calculation, see the preprint [here](https://arxiv.org/pdf/1901.02823.pdf).\n * Distance-based: a signed distance transform is applied to the annotations. The missing slices will be filled in using their \nweighted sum.\n\n> **Note**: Geometric and RPSV interpolation works only when there's a single connected mask on each slice. If you want to \n> interpolate disjoint objects (*e.g.* dividing cells), use distance based interpolation instead.\n\n> **Note**: Distance-based interpolation might give bad results if some masks are too far away from each other on the same slice\n> and there's a big offset compared to the other slice used in the interpolation. If you get unsatisfactory results, try\n> annotating more slices (skip less frames).\n\nhttps://user-images.githubusercontent.com/36735863/188876826-1771acee-93ba-4905-982e-bfb459329659.mp4\n\n### Minimal contour\nThis plugin can estimate a minimal contour, which is calculated from a point set on the edges of the object, which are provided by the user. This contour will follow some kind of image feature (pixels with high gradient or high/low intensity).\nFeatures:\n * With a single click a new point can be added to the set. This will also extend the contour with the curve shown in red\n * A double click will close the curve by adding both the red and gray curves to the minimal contour\n * When holding `Shift`, the gray and red highlight will be swapped, so the other curve can be added to the contour\n * With the `Ctrl` button down a straight line can be added instead of the minimal path\n * If the anchor points were misplaced, the last point can be removed by right-clicking, or the whole point set can be cleared by pressing `Esc`\n * The `Param` value at the widget will decide, how strongly should the contour follow edges on the image. Higher value means higher sensitivity to image data, while a lower value will be closer to straight lines.\n * Different features can be used, like image gradient or pixel intensities, and also user-defined features (using Python)\n   * the image is accessed as the `image` variable, and the features should be stored in the `features` variable in the small code editor widget\n\nThis functionality can be used by selecting the `Minimal Contour` tab in the `Annotation Toolbox` widget, which will create a new layer called `Anchors`.\n\n> **Warning**: Do not remove the `Anchors` layer!\n\n> **Warning**: Some utility layers appear in the layer list when using the plugin. These are marked with a lock (:lock:) symbol.\n> __Do not remove them or modify their data, as this will most probably break the plugin!__ However, you can change their appearance,\n> *e.g.* their color settings.\n\n#### Intensity-based:\n\nhttps://user-images.githubusercontent.com/36735863/191023482-0dfafb5c-003a-47f6-a21b-8582a4e3930f.mp4\n\n#### Gradient-based:\n\nhttps://user-images.githubusercontent.com/36735863/191024941-f20f63a0-8281-47d2-be22-d1ec34fe1f5d.mp4\n\n#### Custom feature:\n\nhttps://user-images.githubusercontent.com/36735863/191025028-3f807bd2-1f2e-40d2-800b-48af820a7dbe.mp4\n\n### Shortcuts\n\n| Action                                        | Mouse               | Keyboard       |\n|-----------------------------------------------|---------------------|----------------|\n| Increment selected label                      | `Shift + Wheel ‚¨ÜÔ∏è`  | `E`            |\n| Decrement selected label                      | `Shift + Wheel ‚¨áÔ∏è`  | `Q`            |\n| Previous slice                                | `Ctrl + Wheel ‚¨ÜÔ∏è`\\* | `A`            |\n| Next slice                                    | `Ctrl + Wheel ‚¨áÔ∏è`\\* | `D`            |\n| Increase paint brush size of labels layer     | `Alt + Wheel ‚¨ÜÔ∏è`    | `W`            |\n| Decrease paint brush size of labels layer     | `Alt + Wheel ‚¨áÔ∏è`    | `S`            |\n| Interpolate                                   | -                   | `Ctrl+I`       |\n| Change between 'Anchors' and the labels layer | -                   | `Ctrl+Tab`     |\n| Jump to layer `#i`                            | -                   | `Ctrl+'i'`\\*\\* |\n\n> *Built-in functionality of [napari]\n> \n> **`i`: 0-9\n\n> **Note**: you can check the list of available shortcuts by clicking the `?` button in the bottom right corner of the main widget.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-nD-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[napari-hub]: https://napari-hub.org/\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[plugin installation instructions]: https://napari.org/plugins/find_and_install_plugin.html\n[file an issue]: https://github.com/bauerdavid/napari-nD-annotator/issues/new/choose\n[napari-bbox]: https://github.com/bauerdavid/napari-bbox\n[minimal-surface]: https://pypi.org/project/minimal-surface\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotation Toolbox",
      "Slice Interpolator",
      "Object List"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-video",
    "name": "napari_video",
    "display_name": "video",
    "version": "0.2.13",
    "created_at": "2021-02-27",
    "modified_at": "2025-08-20",
    "authors": [
      "Jan Clemens"
    ],
    "author_emails": [
      "clemensjan@googlemail.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-video/",
    "home_github": "https://github.com/janclemenslab/napari-video",
    "home_other": null,
    "summary": "napari plugin for reading videos.",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "numpy",
      "pyvideoreader"
    ],
    "package_metadata_description": "# napari-video\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari_video)](https://napari-hub.org/plugins/napari_video)\n\nNapari plugin for working with videos.\n\nRelies on [pyvideoreader](https://pypi.org/project/pyvideoreader/) as a backend which itself uses [opencv](https://opencv.org) for reading videos.\n\n## Installation\n```shell\npip install napari[all] napari_video\n```\n\n## Usage\nFrom a terminal:\n```shell\nnapari video.avi\n```\n\nOr from within python:\n```shell\nimport napari\nfrom napari_video.napari_video import VideoReaderNP\n\npath='video.mp4'\nvr = VideoReaderNP(path)\nwith napari.gui_qt():\n    viewer = napari.view_image(vr, name=path)\n```\n\n## Internals\n`napari_video.napari_video.VideoReaderNP` exposes a video with a numpy-like interface, using opencv as a backend.\n\nFor instance, open a video:\n```python\nvr = VideoReaderNP('video.avi')\nprint(vr)\n```\n```\nvideo.avi with 60932 frames of size (920, 912, 3) at 100.00 fps\n```\nThen\n\n- `vr[100]` will return the 100th frame as a numpy array with shape `(902, 912, 3)`.\n- `vr[100:200:10]` will return 10 frames evenly spaced between frame number 100 and 200 (shape `(10, 902, 912, 3)`).\n- Note that by default, single-frame and slice indexing return 3D and 4D arrays, respectively. To consistently return 4D arrays, open the video with `remove_leading_singleton=False`. `vr[100]` will then return a `(1, 902, 912, 3)` array.\n- We can also request specific ROIs and channels. For instance, `vr[100:200:10,100:400,800:850,1]` will return an array with shape `(10, 300, 50, 1)`.\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mov",
      "*.mp4",
      "*.avi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "neurogenesis-napari",
    "name": "neurogenesis-napari",
    "display_name": "TUM.ai + Helmholtz",
    "version": "0.1.0a1.post3",
    "created_at": "2025-07-29",
    "modified_at": "2025-08-20",
    "authors": [
      "TUM.ai"
    ],
    "author_emails": [
      "\"TUM.ai\" <contact@tum-ai.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/neurogenesis-napari/",
    "home_github": "https://github.com/tum-ai/neurogenesis_napari",
    "home_other": null,
    "summary": "A napari plugin to segment and classify cells.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "cellpose==3.1.1.2",
      "huggingface-hub>=0.33.0",
      "magicgui",
      "napari-czifile2",
      "napari[pyqt5]",
      "numpy==1.26.4",
      "opencv-python==4.11.0.86",
      "pandas==2.3.0",
      "qtpy",
      "sam2==1.1.0",
      "scikit-image==0.25.2",
      "scikit-learn==1.2.2",
      "tifffile<=2023.4.12",
      "torch==2.7.1"
    ],
    "package_metadata_description": "# TUMai Helmholtz Neurogenesis Napari Plugin\n\n[![License MIT](https://img.shields.io/pypi/l/neurogenesis-napari.svg?color=green)](LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/neurogenesis-napari.svg?color=green)](https://pypi.org/project/neurogenesis-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/neurogenesis-napari.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/neurogenesis-napari)](https://napari-hub.org/plugins/neurogenesis-napari)\n\nThis plugin provides one-click color normalization, denoising, Cellpose-based nuclear segmentation and cell classification.\n\n## Key Features\n\n| Widget | Function | Input | Output |\n|--------|----------|-------|---------|\n| **Normalize + Denoise** | Color normalization and denoising | Bright-field image | Processed image |\n| **Segment** | Nuclear segmentation | DAPI/nuclear stain | Masks, centroids, bounding boxes |\n| **Segment + Classify** | End-to-end cell analysis | 4-channel images | Cell segmentation + classification |\n\n## Quick Start\n\n### Installation\n\n```bash\npip install neurogenesis-napari\n```\n\nOr install through napari:\n1. Open napari\n2. Go to `Plugins` ‚Üí `Install/Uninstall Plugins`\n3. Search for **\"TumAI Histology Toolkit\"**\n4. Click Install\n\n### Basic Usage\n\n1. **Load your images** into napari\n2. **Select the appropriate widget** from the `Plugins` menu\n3. **Choose your image layers** from the dropdown menus\n4. **Click the action button** to process\n\nThe plugin will automatically download required AI models on first use.\n\n---\n\n## Widget Documentation\n\n### Normalize + Denoise\n\n**Purpose**: Standardizes color variations and reduces noise in bright-field images.\n\n#### Usage\n1. Load a bright-field image into napari\n2. Open `Plugins` ‚Üí `Normalize and Denoise`\n3. Select your bright-field image from the **BF** dropdown\n4. Click **\"Normalize + Denoise\"**\n\n#### What it does\n- **Color Normalization**: Adjusts colors against an internal reference to standardize appearance across different images/scanners\n- **Denoising**: Removes noise while preserving important cellular structures\n- **Output**: Creates a new layer named `{original_name}_denoised`\n\n---\n\n### Segment\n\n**Purpose**: Detects and segments individual cell nuclei using Cellpose.\n\n#### Usage\n1. Load a nuclear staining image (DAPI) into napari\n2. Open `Plugins` ‚Üí `Segment`\n3. Select your nuclear image from the **DAPI** dropdown\n4. Optionally adjust:\n   - **GPU**: Enable for faster processing\n   - **Model**: Choose Cellpose model (`cyto3` default)\n5. Click **\"Segment Nuclei\"**\n\n#### What it does\n- **Segmentation**: Uses Cellpose to identify individual nuclei\n- **Creates 3 new layers**:\n  - `{name}_masks`: Segmentation masks\n  - `{name}_centroids`: Center points of each detected cell\n  - `{name}_bboxes`: Bounding boxes around each cell\n\n---\n\n### Segment + Classify\n\n**Purpose**: Complete pipeline that segments nuclei AND classifies cell types in multi-channel images.\n\n#### Usage\n1. Load a **4-channel image** into napari as separate layers:\n   - **DAPI**: Nuclear staining\n   - **Tuj1**: Œ≤-III-tubulin\n   - **RFP**: Red fluorescent protein marker\n   - **BF**: Bright-field\n2. Open `Plugins` ‚Üí `Segment and Classify`\n3. Select each channel from the respective dropdowns\n4. Choose **Reuse cached**:\n   - **True**: Reuse previous segmentation (faster) from the segment widget\n   - **False**: Perform fresh segmentation\n5. Click **\"Segment + Classify\"**\n\n#### What it does\n1. **Segmentation**: Does segmentation same as the segment widget above\n2. **Feature extraction**: Uses a Variational Autoencoder (VAE) to extract features\n3. **Classification**: Nearest-centroid classifier assigns cell types\n\n#### Output\nCreates colored polygons for detected cells based on type:\n- **üü£ Astrocytes** (magenta polygons)\n- **‚ö´ Dead Cells** (gray polygons)\n- **üîµ Neurons** (cyan polygons)\n- **üü¢ OPCs** (lime polygons)\n\nThe classification results can be edited.\n\n---\n\n### Supported Image Formats\n- `.czi` (via napari-czifile2)\n- `.png`, `.jpg`\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Normalize and Denoise",
      "Segment",
      "Segment and Classify"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "vessqc",
    "name": "VessQC",
    "display_name": "VessQC",
    "version": "0.8.0",
    "created_at": "2025-02-18",
    "modified_at": "2025-08-20",
    "authors": [
      "Peter Lampen"
    ],
    "author_emails": [
      "Peter Lampen <lampen@isas.de>"
    ],
    "license": "Copyright (c) 2024, Peter Lamp...",
    "home_pypi": "https://pypi.org/project/vessqc/",
    "home_github": "https://github.com/MMV-Lab/VessQC",
    "home_other": null,
    "summary": "Quality control plugin for vessel segmentation in napari",
    "categories": [
      "Segmentation",
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "joblib>=1.2",
      "napari>=0.4.18",
      "numpy>=1.23",
      "qtpy>=2.3",
      "scipy>=1.9",
      "SimpleITK>=2.2",
      "tifffile>=2023.2.28",
      "pyqt5; extra == \"qt\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\""
    ],
    "package_metadata_description": "# VessQC\n\n[![License BSD-3](https://img.shields.io/pypi/l/VessQC.svg?color=green)](https://github.com/MMV-Lab/VessQC/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/VessQC.svg?color=green)](https://pypi.org/project/VessQC)\n[![Python Version](https://img.shields.io/pypi/pyversions/VessQC.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/VessQC/workflows/tests/badge.svg)](https://github.com/MMV-Lab/VessQC/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/VessQC/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/VessQC)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/VessQC)](https://napari-hub.org/plugins/VessQC)\n\nA plugin for a vessel quality check\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `VessQC` via [pip]:\n\n    pip install VessQC\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MMV-Lab/VessQC.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"VessQC\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MMV-Lab/VessQC/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Vessel Quality Check"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-isolate-cell",
    "name": "napari-isolate-cell",
    "display_name": "Isolate Cell from Segmentation",
    "version": "0.1.2",
    "created_at": "2025-08-06",
    "modified_at": "2025-08-15",
    "authors": [
      "Sergio Bernal Garcia"
    ],
    "author_emails": [
      "smb2318@columbia.edu"
    ],
    "license": "Copyright (c) 2025, Sergio Ber...",
    "home_pypi": "https://pypi.org/project/napari-isolate-cell/",
    "home_github": "https://github.com/serg-bg/napari-isolate-cell",
    "home_other": null,
    "summary": "Isolate soma‚Äëspecific arbors & export SWC.",
    "categories": [
      "Segmentation",
      "Annotation",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "napari[all]",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scipy",
      "kimimaro",
      "tifffile",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-isolate-cell\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-isolate-cell.svg?color=green)](https://github.com/serg-bg/napari-isolate-cell/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-isolate-cell.svg?color=green)](https://python.org)\n[![tests](https://github.com/serg-bg/napari-isolate-cell/workflows/tests/badge.svg)](https://github.com/serg-bg/napari-isolate-cell/actions)\n[![codecov](https://codecov.io/gh/serg-bg/napari-isolate-cell/branch/main/graph/badge.svg)](https://codecov.io/gh/serg-bg/napari-isolate-cell)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n\nA [napari] plugin to isolate single cell morphologies (e.g., neurons) from label volumes based on a user click, automatically read image scale, and export the isolated structure as TIFF and correctly scaled SWC files.\n\n![Demo of napari-cell-isolate plugin](images/napari-cell-isolate-demo.gif)\n\n----------------------------------\n\n## Overview\n\nThis plugin helps streamline the process of extracting individual cell structures from dense segmentations, such as those produced by deep learning models like nnUNet.\n\n**Key Features:**\n\n*   **Click-Based Isolation:** Simply click on the soma (or any part) of the cell you want to isolate in a Napari Labels layer.\n*   **Automatic Scale Detection:** Reads ZYX scale information directly from TIFF metadata (standard tags or ImageJ metadata) and applies it to the loaded Napari layer.\n*   **Anisotropy Awareness:** Automatically populates the widget's Anisotropy fields based on the detected image scale.\n*   **Outputs:**\n    *   Adds the isolated cell as a new Labels layer in Napari, preserving the original scale.\n    *   Saves the isolated label volume as a TIFF file.\n    *   Saves the skeletonized structure as an SWC file with coordinates reflecting the original image's physical scale (micrometers).\n*   **Configurable Parameters:** Adjust morphological closing radius (defaults to 0 for dense segmentations) and skeleton dust threshold.\n\n## Workflow\n\n![Workflow diagram](images/One-click_cell_isolation_RESPAN.png)\n\n## Installation\n\n### For Users (Recommended)\n```bash\npip install napari-isolate-cell\n```\n\nOr using [uv](https://github.com/astral-sh/uv) (faster):\n```bash\nuv pip install napari-isolate-cell\n```\n\n### For Developers\n```bash\ngit clone https://github.com/serg-bg/napari-isolate-cell.git\ncd napari-isolate-cell\npip install -e .[testing]\n```\n\n## Usage\n\n1. **Launch napari** and open your 3D segmentation (`.tif` file)\n2. **Open plugin**: `Plugins` ‚Üí `napari-isolate-cell` ‚Üí `Isolate Cell Arbor`\n3. **Select your labels layer** from the dropdown\n4. **Click \"Activate Click Isolation\"**\n5. **Click any cell** in the viewer to isolate it\n\n**Outputs:**\n- New labels layer with isolated cell\n- `isolated_outputs/` folder containing:\n  - `.tif` - Isolated cell volume\n  - `.swc` - Skeleton with physical coordinates (¬µm)\n\n**Parameters:**\n- **Morphological Closing**: Default 0 (increase to bridge small gaps)\n- **Dust Threshold**: Default 100 (minimum skeleton branch size in voxels)\n- **Anisotropy**: Auto-detected from TIFF metadata\n\n## Requirements\n\n*   Python >= 3.10\n*   napari\n*   NumPy\n*   scikit-image\n*   SciPy\n*   tifffile\n*   magicgui\n*   qtpy\n\n(See `pyproject.toml` for specific version constraints)\n\n## Contributing\n\nContributions are very welcome. Please file an issue to discuss potential changes or features first. Tests can be run with [pytest] (`pip install -e .[testing]` then `pytest`). Please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-isolate-cell\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[napari hub]: https://napari-hub.org/\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[pytest]: https://docs.pytest.org/\n[file an issue]: https://github.com/serg-bg/napari-isolate-cell/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.TIFF",
      "*.TIF"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Isolate Cell Arbor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spatialdata",
    "name": "napari-spatialdata",
    "display_name": "napari spatialdata",
    "version": "0.5.7",
    "created_at": "2022-07-06",
    "modified_at": "2025-08-15",
    "authors": [
      "giovanni palla"
    ],
    "author_emails": [
      "giov.pll@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-spatialdata/",
    "home_github": "https://github.com/scverse/napari-spatialdata",
    "home_other": null,
    "summary": "Interactive visualization of spatial omics data with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "anndata",
      "click",
      "cycler",
      "dask<=2024.11.2,>=2024.4.1",
      "geopandas",
      "loguru",
      "matplotlib",
      "napari>=0.6.2",
      "napari-matplotlib",
      "numba",
      "numpy",
      "packaging",
      "pandas",
      "pillow",
      "pyqtgraph",
      "qtpy",
      "scanpy",
      "scipy",
      "shapely",
      "scikit-learn",
      "spatialdata>=0.2.6",
      "superqt",
      "typing_extensions>=4.8.0",
      "vispy",
      "xarray",
      "xarray-datatree",
      "bermuda; extra == \"bermuda\"",
      "loguru; extra == \"test\"",
      "pytest; extra == \"test\"",
      "pytest-cov; extra == \"test\"",
      "pytest-mock; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "pre-commit>=2.9.0; extra == \"test\"",
      "sphinx>=4.5; extra == \"doc\"",
      "sphinx-book-theme>=1.0.0; extra == \"doc\"",
      "myst-parser; extra == \"doc\"",
      "sphinxcontrib-bibtex>=1.0.0; extra == \"doc\"",
      "sphinx-autodoc-typehints>=1.11.0; extra == \"doc\"",
      "sphinx-autobuild; extra == \"doc\"",
      "scanpydoc; extra == \"doc\"",
      "ipykernel; extra == \"doc\"",
      "ipython; extra == \"doc\"",
      "sphinx-copybutton; extra == \"doc\"",
      "sphinx-qt-documentation; extra == \"doc\"",
      "myst-nb; extra == \"doc\"",
      "squidpy; extra == \"doc\"",
      "pydantic>2; extra == \"readthedocs\"",
      "spatialdata>=0.1.0-pre0; extra == \"pre\"",
      "napari[pyqt5]; extra == \"all\""
    ],
    "package_metadata_description": "![SpatialData banner](https://github.com/scverse/spatialdata/blob/main/docs/_static/img/spatialdata_horizontal.png?raw=true)\n\n# napari-spatialdata: interactive exploration and annotation of spatial omics data\n\n[![License](https://img.shields.io/pypi/l/napari-spatialdata.svg?color=green)](https://github.com/scverse/napari-spatialdata/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spatialdata.svg?color=green)](https://pypi.org/project/napari-spatialdata)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spatialdata.svg?color=green)](https://python.org)\n[![tests](https://github.com/scverse/napari-spatialdata/workflows/tests/badge.svg)](https://github.com/scverse/napari-spatialdata/actions)\n[![codecov](https://codecov.io/gh/scverse/napari-spatialdata/branch/main/graph/badge.svg?token=ASqlOKnOj7)](https://codecov.io/gh/scverse/napari-spatialdata)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/scverse/napari-spatialdata/main.svg)](https://results.pre-commit.ci/latest/github/scverse/napari-spatialdata/main)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatialdata)](https://napari-hub.org/plugins/napari-spatialdata)\n[![DOI](https://zenodo.org/badge/477021400.svg)](https://zenodo.org/badge/latestdoi/477021400)\n[![Documentation][badge-pypi]][link-pypi]\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-spatialdata/badges/version.svg)](https://anaconda.org/conda-forge/napari-spatialdata)\n\n[badge-pypi]: https://badge.fury.io/py/napari-spatialdata.svg\n[link-pypi]: https://pypi.org/project/napari-spatialdata/\n\nThis repository contains a napari plugin for interactively exploring and annotating\nSpatialData objects. Here you can find the [napari-spatialdata documentation](https://spatialdata.scverse.org/projects/napari/en/stable/notebooks/spatialdata.html). `napari-spatialdata` is part of the `SpatialData` ecosystem. To learn more about SpatialData, please see the [spatialdata documentation](https://spatialdata.scverse.org/).\n\n## Installation\n\nYou can install `napari-spatialdata` via [pip]:\n\n    pip install napari-spatialdata[all]\n\nThe `all` command will install the qt bindings `PyQt5`.\n\nNapari now also includes multiple triangulation backends. These improve the speed by which a napari 'Shapes' layer gets\nloaded (this does not improve the speed of editing large numbers of shapes yet!). See also the napari\n[documentation](https://napari.org/stable/guides/triangulation.html) and already slightly older [blog post](https://napari.org/island-dispatch/blog/triangles_speedup_beta.html). For installation via\npip:\n\n    pip install napari-spatialdata[all, bermuda]\n\nYou can find more details on this in the [installation instructions](https://spatialdata.scverse.org/en/stable/installation.html).\n\n## Using napari-spatialdata as default zarr reader\n\nIf you would like to use the plugin as the default zarr reader, in napari please go to `File` -> `Preferences`\n-> `Plugins` and follow the instructions under `File extension readers`.\n\n## Development Version\n\nYou can install `napari-spatialdata` from Github with:\n\n    pip install git+https://github.com/scverse/napari-spatialdata\n\nOr, you can also install in editable mode after cloning the repo by:\n\n    git clone https://github.com/scverse/napari-spatialdata\n    cd napari-spatialdata\n    pip install -e .\n\nNote: when performing an editable install of `napari-spatialdata`, `spatialdata`\nwill be reinstalled from `pip`. So, if you previously also made an editable install\nof `spatialdata`, you need to re-run `pip install -e .` on the `spatialdata`\nrepository. Please find more details on this in the [installation instructions](https://spatialdata.scverse.org/en/stable/installation.html).\n\n## Getting started\n\nTo learn how to use the `napari-spatialdata` plugin, please see the [documentation](https://spatialdata.scverse.org/projects/napari/en/stable/notebooks/spatialdata.html).\nTo learn how to integrate napari-spatialdata into your analysis workflows, please\nsee the [SpatialData tutorials](https://spatialdata.scverse.org/en/stable/tutorials/notebooks/notebooks.html). In particular:\n\n- [Annotating regions of interest with napari](https://spatialdata.scverse.org/en/stable/tutorials/notebooks/notebooks/examples/napari_rois.html)\n- [Use landmark annotations to align multiple -omics layers](https://spatialdata.scverse.org/en/stable/tutorials/notebooks/notebooks/examples/alignment_using_landmarks.html)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-spatialdata\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please file an [issue] along with a detailed description.\n\n## Citation\n\nMarconato, L., Palla, G., Yamauchi, K.A. et al. SpatialData: an open and universal data framework for spatial omics. Nat Methods (2024). https://doi.org/10.1038/s41592-024-02212-x\n\n[napari]: https://github.com/napari/napari\n[cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[mit]: http://opensource.org/licenses/MIT\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[pypi]: https://pypi.org/\n[issue]: https://github.com/scverse/napari-spatialdata/issues\n[//]: # \"numfocus-fiscal-sponsor-attribution\"\n\nnapari-spatialdata is part of the scverse¬Æ project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nIf you like scverse¬Æ and want to support our mission, please consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs.\n\n<div align=\"center\">\n<a href=\"https://numfocus.org/project/scverse\">\n  <img\n    src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\"\n    width=\"200\"\n  >\n</a>\n</div>\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Scatter",
      "View",
      "Annotation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "brillouin-imaging",
    "name": "brillouin-imaging",
    "display_name": "Brillouin Imaging",
    "version": "0.1.0",
    "created_at": "2025-08-13",
    "modified_at": "2025-08-13",
    "authors": [
      "Carlo Bevilacqua"
    ],
    "author_emails": [
      "carlo.bevilacqua@embl.de"
    ],
    "license": "GNU LESSER GENERAL PUBLIC LICE...",
    "home_pypi": "https://pypi.org/project/brillouin-imaging/",
    "home_github": "https://github.com/prevedel-lab/brillouin-imaging-napari",
    "home_other": null,
    "summary": "A plugin to work with brim files, which contain Brillouin imaging data",
    "categories": [
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "brimfile",
      "brimfile[remote-store]",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# brillouin-imaging Napari plugin \n\n## What is it?\n\n*brillouin-imaging* is a napari plugin to work with brim (**Br**illouin **im**aging) files, containing spectral data and metadata from Brillouin microscopy.\n\nMore information about the scope and definition of the brim file format can be found [here](https://github.com/prevedel-lab/Brillouin-standard-file).\n\nAt the current stage the brillouin-imaging plugin can only read from both local and remote brim files, by implementing the [reader contribution](https://napari.org/stable/plugins/building_a_plugin/guides.html#readers). It also provides some sample data through the [sample data contribution](https://napari.org/stable/plugins/building_a_plugin/guides.html#sample-data).\n\n## How to install it\n\n*brillouin-imaging* can be installed from the napari plugin manager, as any standard napari plugin. More info can be found [here](https://napari.org/stable/plugins/start_using_plugins/finding_and_installing_plugins.html).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.brim.zip",
      "*.brim.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "drosophila LSBM",
      "zebrafish eye confocal",
      "zebrafish notochord SBS",
      "oil beads FTBM"
    ]
  },
  {
    "normalized_name": "vollseg-napari-trackmate",
    "name": "vollseg-napari-trackmate",
    "display_name": "vollseg-napari-trackmate",
    "version": "2.6.7",
    "created_at": "2022-12-31",
    "modified_at": "2025-08-13",
    "authors": [
      "Varun Kapoor",
      "Mari Tolonen",
      "Jakub Sedzinski"
    ],
    "author_emails": [
      "randomaccessiblekapoor@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/vollseg-napari-trackmate/",
    "home_github": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate",
    "home_other": null,
    "summary": "Track analysis using TrackMate xml and csv generated tracks using NapaTrackMater as the base library",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "caped-ai",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# vollseg-napari-trackmate\n\n[![License BSD-3](https://img.shields.io/pypi/l/vollseg-napari-trackmate.svg?color=green)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/vollseg-napari-trackmate.svg?color=green)](https://pypi.org/project/vollseg-napari-trackmate)\n[![Python Version](https://img.shields.io/pypi/pyversions/vollseg-napari-trackmate.svg?color=green)](https://python.org)\n[![codecov](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-trackmate/branch/main/graph/badge.svg)](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-trackmate)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari-trackmate)](https://napari-hub.org/plugins/vollseg-napari-trackmate)\n\nTrack analysis using TrackMate xml and csv generated tracks using NapaTrackMater as the base library\n\n----------------------------------\n\nElaborate documentation for users of this repository at this [documentation]\n\n## Tutorial\n\n- A detailed tutorial can be found at [Demo](https://youtu.be/7Yjd-Z3zJtk?si=_AksSBUJuEXbvIFM)\n\nThis [napari] plugin was generated with [Cookiecutter] using [@caped]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `vollseg-napari-trackmate` via [pip]:\n\n    pip install vollseg-napari-trackmate\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-trackmate.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"vollseg-napari-trackmate\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[@caped]: https://github.com/Kapoorlabs-CAPED/\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/Kapoorlabs-CAPED/cookiecutter-kapoorlabs-napari-plugin\n[documentation]: https://kapoorlabs-caped.github.io/vollseg-napari-trackmate\n[file an issue]: https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "NapaTrackMater"
    ],
    "contributions_sample_data": [
      "Test Track Data"
    ]
  },
  {
    "normalized_name": "ilastik-napari",
    "name": "ilastik-napari",
    "display_name": "ilastik plugin for napari",
    "version": "0.3.1",
    "created_at": "2023-02-28",
    "modified_at": "2025-08-12",
    "authors": [
      "Emil Melnikov"
    ],
    "author_emails": [
      "Emil Melnikov <emilmelnikov@gmail.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/ilastik-napari/",
    "home_github": "https://github.com/ilastik/ilastik-napari",
    "home_other": null,
    "summary": "ilastik plugin for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari>=0.4.13",
      "numpy>=1.20",
      "qtpy",
      "scikit-learn",
      "sparse"
    ],
    "package_metadata_description": "# ilastik-napari\n\n[Napari][napari] plugin for interactive pixel classification.\nDesigned to be similar to the pixel classification workflow in [classic ilastik][ilastik].\n\n## Installation\n\nThis plugin requires you to use a _conda_ environment. The environment manager conda comes in a few different forms.\nIf you haven't used conda before, you can find more information in the [conda user guide][conda-user-guide].\nYou can use whichever variant you prefer, as the resulting environment should be the same, but we recommend the [_mambaforge_][mambaforge] variant as it is usually the fastest.\nWhen using mambaforge, the `mamba` command usually replaces the `conda` command one would otherwise use.\n\nOnce you have installed mambaforge, set up a conda environment with napari and the _fastfilters_ package, and then use pip to install _ilastik-napari_:\n```shell\nconda create -y -c ilastik-forge -c conda-forge -n my-napari-env napari pyqt=5.51 fastfilters sparse qtpy scikit-learn\nconda activate my-napari-env\npip install ilastik-napari\n```\n\nFinally, run napari:\n```shell\nnapari\n```\nThat's it! You should be able to find the ilastik-napari plugin in the Plugins menu.\n\nIf you prefer to __install napari using pip__ instead of conda:\nMake sure to install `napari[all]`.\nUnless you want to [choose a PyQt implementation other than _PyQt5_][napari-pyqt], in which case you should leave out the `[all]` extra.\n\n## Usage\n\nAs a prerequisite, make sure you understand the [napari basics][napari-quickstart].\n\n1. Open your image, or use a sample in _File - Open Sample_.\n\n   ![Use a sample image](https://ilastik.org/assets/ilastik-napari/image-sample.png \"Use a sample image\")\n\n2. Activate the plugin in the _Plugins_ menu.\n\n   ![Activate the plugin](https://ilastik.org/assets/ilastik-napari/activation.png \"Activate the plugin\")\n\n3. In _layer list_, create a new _Labels_ layer.\n\n   ![Labels layer](https://ilastik.org/assets/ilastik-napari/labels-layer.png \"Labels layer\")\n\n4. In _layers control_, switch to the _paint_ action.\n\n   ![Paint action](https://ilastik.org/assets/ilastik-napari/paint-action.png \"Paint action\")\n\n5. Draw your background labels.\n\n   ![Paint the background](https://ilastik.org/assets/ilastik-napari/draw-background.png \"Paint the background\")\n\n6. Switch to a new label.\n\n   ![Switch label](https://ilastik.org/assets/ilastik-napari/new-label.png \"Switch label\")\n\n7. Draw your foreground labels.\n\n   ![Paint cells](https://ilastik.org/assets/ilastik-napari/draw-cells.png \"Paint cells\")\n\n8. Select output types you need, and click _Run_.\n\n   ![Plugin interface](https://ilastik.org/assets/ilastik-napari/interface.png \"Plugin interface\")\n\n9. The plugin will create one layer for each output type, which you save as normal napari layers.\n\n   ![Example output](https://ilastik.org/assets/ilastik-napari/example.png \"Example output\")\n\n## Development\n\nCreate a development environment:\n```\nconda create -y -n ilastik-napari-dev -c ilastik-forge fastfilters pyqt=5.51 fastfilters sparse qtpy scikit-learn setuptools-scm conda-build anaconda-client\nconda activate napari-ilastik-dev\npip install -e .\n```\n\nBuild conda package:\n```\nconda activate napari-ilastik-dev\nconda build -c ilastik-forge conda-recipe\nanaconda upload /path/to/the/new/package.tar.bz2\n```\n\nBuild wheel and sdist packages:\n```\nconda activate napari-ilastik-dev\npip install build twine\npython -m build\npython -m twine upload --repository testpypi dist/*\n```\n\n[napari]: https://napari.org/\n[ilastik]: https://www.ilastik.org/\n[conda-user-guide]: https://docs.conda.io/projects/conda/en/latest/user-guide/index.html\n[miniconda]: https://docs.conda.io/en/latest/miniconda.html\n[mambaforge]: https://github.com/conda-forge/miniforge#mambaforge\n[napari-quickstart]: https://napari.org/tutorials/fundamentals/quick_start.html\n[napari-pyqt]: https://napari.org/stable/plugins/best_practices.html#don-t-include-pyside2-or-pyqt5-in-your-plugin-s-dependencies\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ilastik Pixel Classification"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-filaments",
    "name": "napari-filaments",
    "display_name": "napari filaments",
    "version": "0.4.0",
    "created_at": "2022-07-01",
    "modified_at": "2025-08-12",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "Hanjin Liu <liuhanjin.sc@gmail.com>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-filaments/",
    "home_github": "https://github.com/hanjinliu/napari-filaments",
    "home_other": null,
    "summary": "Plugin for filament analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "macro-kit",
      "magic-class>=0.7.6",
      "magicgui",
      "matplotlib",
      "numpy",
      "pandas",
      "psygnal",
      "scipy",
      "himena; extra == 'himena'",
      "napari; extra == 'napari'",
      "napari; extra == 'testing'",
      "pyqt5; extra == 'testing'",
      "pytest; extra == 'testing'",
      "pytest-cov; extra == 'testing'",
      "pytest-qt; extra == 'testing'",
      "roifile; extra == 'testing'",
      "tox; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-filaments\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-filaments.svg?color=green)](https://github.com/hanjinliu/napari-filaments/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-filaments.svg?color=green)](https://pypi.org/project/napari-filaments)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-filaments.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-filaments/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-filaments/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-filaments/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-filaments)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-filaments)](https://napari-hub.org/plugins/napari-filaments)\n\nA napari plugin for filament analysis.\n\nThis plugin helps you to manually track filaments using path shapes of `Shapes` layer.\n\n![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fit.gif)\n\nCurrently implemented functions\n\n- Fit paths to filaments in an image as a 2-D spline curve.\n- Clip/extend paths.\n- Measurement of filament length at sub-pixel precision.\n- Basic quantification (mean, std, etc.) along paths.\n- Import paths from ImageJ ROI file.\n\nBasic Usage\n-----------\n\nClick `Layers > open image` to open a tif file. You'll find the image you chose and a shapes layer are added to the layer list.\n\n![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fig-1.png)\n\n- The \"target filaments\" box shows the working shapes layer.\n- The \"target image\" box shows the image layer on which fitting and quantification will be conducted.\n- The \"filament\" box shows currently selected shape (initially this box is empty).\n\nAdd path shapes and push key `F1` to fit the shape to the filament in the target image layer.\n\n- In the \"Spline\" tab, you can cut/extend or re-fit splines.\n- In the \"Measure\" tab, click \"measure property\" to measure mean instensity etc along each spline.\n\nHow it works\n------------\n\nSpline fitting is done as following.\n\n![](https://github.com/hanjinliu/napari-filaments/raw/main/resources/fig-2.png)\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-filaments` via [pip]:\n\n    pip install napari-filaments\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-filaments.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-filaments\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hanjinliu/napari-filaments/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Open Filament Analyzer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cryofibsem-imgproc",
    "name": "napari-cryofibsem-imgproc",
    "display_name": "Cryo-FIB/SEM Image Stacks Post-Processing",
    "version": "0.0.7",
    "created_at": "2024-09-20",
    "modified_at": "2025-08-11",
    "authors": [
      "Shaina V. To"
    ],
    "author_emails": [
      "shaina.to@radboudumc.nl"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-cryofibsem-imgproc/",
    "home_github": "https://github.com/EMCRUMC/napari-cryofibsem-imgproc",
    "home_other": null,
    "summary": "A napari plugin for artifact removal, noise reduction, contrast enhancement and stack brightness correction of Cryo-FIB/SEM image stacks.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy==1.26.4",
      "magicgui",
      "scikit-image==0.24.0",
      "PyWavelets==1.6.0",
      "opencv-python==4.10.0.84",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-cryofibsem-imgproc\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-cryofibsem-imgproc.svg?color=green)](https://github.com/EMCRUMC/napari-cryofibsem-imgproc/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cryofibsem-imgproc.svg?color=green)](https://pypi.org/project/napari-cryofibsem-imgproc)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cryofibsem-imgproc.svg?color=green)](https://python.org)\n[![tests](https://github.com/EMCRUMC/napari-cryofibsem-imgproc/workflows/tests/badge.svg)](https://github.com/EMCRUMC/napari-cryofibsem-imgproc/actions)\n[![codecov](https://codecov.io/gh/EMCRUMC/napari-cryofibsem-imgproc/branch/main/graph/badge.svg)](https://codecov.io/gh/EMCRUMC/napari-cryofibsem-imgproc)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cryofibsem-imgproc)](https://napari-hub.org/plugins/napari-cryofibsem-imgproc)\n\nA napari plugin for artifact removal, noise reduction, contrast enhancement and stack brightness correction of Cryo-FIB/SEM image stacks.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nIt is recommended to create a fresh [conda] environment with Python 3.12 and napari:\n\n    conda create -n imgproc-env -c conda-forge python=3.12.0 napari pyqt\n\nYou can install `napari-cryofibsem-imgproc` via [pip]:\n\n    pip install napari-cryofibsem-imgproc\n\nTo install latest development version :\n\n    pip install git+https://github.com/EMCRUMC/napari-cryofibsem-imgproc.git\n\n## Usage \nCryo-FIB/SEM Image Processing offers 5 functions: curtaining artifact removal, charging artifact removal, noise reduction, contrast enhancement, and stack brightness correction. Using the widgets, the user can choose the input image or stack, set the function parameters, and call the function. These functions can be used in sequence with each other or individually, depending on the image processing requirements. \n\n![widget.png](widget.png)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-cryofibsem-imgproc\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/EMCRUMC/napari-cryofibsem-imgproc/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Decurtaining",
      "Decharging",
      "Denoising",
      "Median Blur",
      "Enhancing Contrast",
      "Correcting Brightness"
    ],
    "contributions_sample_data": [
      "Cryo-FIB/SEM Image Stacks Post-Processing"
    ]
  },
  {
    "normalized_name": "movement",
    "name": "movement",
    "display_name": "movement",
    "version": "0.9.0",
    "created_at": "2025-03-10",
    "modified_at": "2025-08-06",
    "authors": [
      "Nikoloz Sirmpilatze",
      "Chang Huan Lo",
      "Sof√≠a Mi√±ano"
    ],
    "author_emails": [
      "Nikoloz Sirmpilatze <niko.sirbiladze@gmail.com>",
      "Chang Huan Lo <changhuan.lo@ucl.ac.uk>",
      "Sof√≠a Mi√±ano <s.minano@ucl.ac.uk>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/movement/",
    "home_github": "https://github.com/neuroinformatics-unit/movement",
    "home_other": null,
    "summary": "Analysis of body movement",
    "categories": [],
    "package_metadata_requires_python": ">=3.11.0",
    "package_metadata_requires_dist": [
      "numpy<2.3.0,>=2.0.0",
      "pandas",
      "h5py",
      "attrs",
      "pooch",
      "tqdm",
      "shapely",
      "sleap-io",
      "xarray[accel,io,viz]",
      "PyYAML",
      "napari-video",
      "pyvideoreader>=0.5.3",
      "qt-niu",
      "loguru",
      "pynwb",
      "ndx-pose>=0.2.1",
      "napari[all]>=0.6.0; extra == \"napari\"",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "codespell; extra == \"dev\"",
      "setuptools_scm; extra == \"dev\"",
      "pandas-stubs; extra == \"dev\"",
      "types-attrs; extra == \"dev\"",
      "check-manifest; extra == \"dev\"",
      "types-PyYAML; extra == \"dev\"",
      "types-requests; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "movement[napari]; extra == \"dev\""
    ],
    "package_metadata_description": "[![Python Version](https://img.shields.io/pypi/pyversions/movement.svg)](https://pypi.org/project/movement)\n[![PyPI Version](https://img.shields.io/pypi/v/movement.svg)](https://pypi.org/project/movement)\n[![Conda Forge Version](https://anaconda.org/conda-forge/movement/badges/version.svg)](https://anaconda.org/conda-forge/movement)\n[![Downloads](https://pepy.tech/badge/movement)](https://pepy.tech/project/movement)\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-orange.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![CI](https://img.shields.io/github/actions/workflow/status/neuroinformatics-unit/movement/test_and_deploy.yml?label=CI)](https://github.com/neuroinformatics-unit/movement/actions)\n[![codecov](https://codecov.io/gh/neuroinformatics-unit/movement/branch/main/graph/badge.svg?token=P8CCH3TI8K)](https://codecov.io/gh/neuroinformatics-unit/movement)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/neuroinformatics-unit/movement/gh-pages?filepath=notebooks/examples)\n[![Code style: Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/format.json)](https://github.com/astral-sh/ruff)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![project chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement/topic/Welcome!)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12755724.svg)](https://zenodo.org/doi/10.5281/zenodo.12755724)\n\n# movement\n\nA Python toolbox for analysing animal body movements across space and time.\n\n\n![](docs/source/_static/movement_overview.png)\n\n## Quick install\n\nCreate and activate a conda environment with movement installed (including the GUI):\n```bash\nconda create -n movement-env -c conda-forge movement napari pyqt\nconda activate movement-env\n```\n\n\n> [!Note]\n> Read the [documentation](https://movement.neuroinformatics.dev) for more information, including [full installation instructions](https://movement.neuroinformatics.dev/user_guide/installation.html) and [examples](https://movement.neuroinformatics.dev/examples/index.html).\n\n## Overview\n\nDeep learning methods for motion tracking have revolutionised a range of\nscientific disciplines, from neuroscience and biomechanics, to conservation\nand ethology. Tools such as\n[DeepLabCut](https://www.mackenziemathislab.org/deeplabcut) and\n[SLEAP](https://sleap.ai/) now allow researchers to track animal movements\nin videos with remarkable accuracy, without requiring physical markers.\nHowever, there is still a need for standardised, easy-to-use methods\nto process the tracks generated by these tools.\n\n`movement` aims to provide a consistent, modular interface for analysing\nmotion tracks, enabling steps such as data cleaning, visualisation,\nand motion quantification. We aim to support all popular animal tracking\nframeworks and file formats.\n\nFind out more on our [mission and scope](https://movement.neuroinformatics.dev/community/mission-scope.html) statement and our [roadmap](https://movement.neuroinformatics.dev/community/roadmaps.html).\n\n<!-- Start Admonitions -->\n\n> [!Tip]\n> If you prefer analysing your data in R, we recommend checking out the\n> [animovement](https://roald-arboel.com/animovement/) toolbox, which is similar in scope.\n> We are working together with its developer\n> to gradually converge on common data standards and workflows.\n\n<!-- End Admonitions -->\n\n## Join the movement\n\nContributions to movement are absolutely encouraged, whether to fix a bug, develop a new feature, or improve the documentation.\nTo help you get started, we have prepared a detailed [contributing guide](https://movement.neuroinformatics.dev/community/contributing.html).\n\n- [Chat with the team on Zulip](https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement).\n- [Open an issue](https://github.com/neuroinformatics-unit/movement/issues) to report a bug or request a new feature.\n- [Follow this Zulip topic](https://neuroinformatics.zulipchat.com/#narrow/channel/406001-Movement/topic/Community.20Calls) to receive updates about upcoming Community Calls.\n\n## Citation\n\nIf you use movement in your work, please cite the following Zenodo DOI:\n\n> Nikoloz Sirmpilatze, Chang Huan Lo, Sof√≠a Mi√±ano, Brandon D. Peri, Dhruv Sharma, Laura Porta, Iv√°n Varela & Adam L. Tyson (2024). neuroinformatics-unit/movement. Zenodo. https://zenodo.org/doi/10.5281/zenodo.12755724\n\n## License\n‚öñÔ∏è [BSD 3-Clause](./LICENSE)\n\n## Package template\nThis package layout and configuration (including pre-commit hooks and GitHub actions) have been copied from the [python-cookiecutter](https://github.com/neuroinformatics-unit/python-cookiecutter) template.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "movement"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-kldeconv",
    "name": "napari-kldeconv",
    "display_name": "KLDeconv",
    "version": "0.1.0",
    "created_at": "2025-08-04",
    "modified_at": "2025-08-04",
    "authors": [
      "Qiqi Lu"
    ],
    "author_emails": [
      "136303971@qq.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-kldeconv/",
    "home_github": "https://github.com/qiqi-lu/napari-kldeconv",
    "home_other": null,
    "summary": "A napari plugin for using KLDeconv algorithm.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-kldeconv\n\n[![License MIT](https://img.shields.io/pypi/l/napari-kldeconv.svg?color=green)](https://github.com/qiqi-lu/napari-kldeconv/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-kldeconv.svg?color=green)](https://pypi.org/project/napari-kldeconv)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-kldeconv.svg?color=green)](https://python.org)\n[![tests](https://github.com/qiqi-lu/napari-kldeconv/workflows/tests/badge.svg)](https://github.com/qiqi-lu/napari-kldeconv/actions)\n[![codecov](https://codecov.io/gh/qiqi-lu/napari-kldeconv/branch/main/graph/badge.svg)](https://codecov.io/gh/qiqi-lu/napari-kldeconv)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-kldeconv)](https://napari-hub.org/plugins/napari-kldeconv)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA napari plugin for using KLDeconv algorithm.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-kldeconv` via [pip]:\n\n```\npip install napari-kldeconv\n```\n\nIf napari is not already installed, you can install `napari-kldeconv` with napari and Qt via:\n\n```\npip install \"napari-kldeconv[all]\"\n```\n\n\nTo install latest development version :\n\n```\npip install git+https://github.com/qiqi-lu/napari-kldeconv.git\n```\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-kldeconv\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/qiqi-lu/napari-kldeconv/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Container Threshold",
      "Magic Threshold",
      "Autogenerate Threshold",
      "Example QWidget"
    ],
    "contributions_sample_data": [
      "KLDeconv"
    ]
  },
  {
    "normalized_name": "napari-ndev",
    "name": "napari-ndev",
    "display_name": "nDev",
    "version": "0.11.8",
    "created_at": "2023-01-31",
    "modified_at": "2025-08-04",
    "authors": [
      "Tim Monko"
    ],
    "author_emails": [
      "Tim Monko <timmonko@gmail.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-ndev/",
    "home_github": "https://github.com/TimMonko/napari-ndev",
    "home_other": null,
    "summary": "napari widgets to (batch) process images from start to finish.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy<2.0,>=1.26",
      "siphash24>=1.6",
      "magicgui>=0.8.3",
      "magic-class",
      "napari[optional]>=0.4.19",
      "apoc",
      "pyclesperanto-prototype",
      "pyclesperanto",
      "dask",
      "napari-workflows",
      "napari-pyclesperanto-assistant",
      "napari-segment-blobs-and-things-with-membranes",
      "natsort",
      "seaborn",
      "stackview",
      "tifffile<2025.2.18,>=2023.3.15",
      "scikit-image>=0.18.0",
      "ngff-zarr>0.10.0",
      "zarr<3",
      "bioio>=1.1.0",
      "bioio-base==1.0.4",
      "bioio-imageio>=1",
      "bioio-tifffile>=1",
      "bioio-ome-tiff>=1",
      "bioio-ome-zarr>=1",
      "bioio-nd2>=1",
      "matplotlib-scalebar>=0.8.1",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "tox; extra == \"testing\"",
      "tox-uv; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[all]; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "bioio-czi>=1.0.1; extra == \"testing\"",
      "napari-ndev[extras]; extra == \"testing\"",
      "mkdocs; extra == \"docs\"",
      "mkdocs-autorefs; extra == \"docs\"",
      "mkdocs-material; extra == \"docs\"",
      "mkdocstrings; extra == \"docs\"",
      "mkdocstrings-python; extra == \"docs\"",
      "mkdocs-jupyter; extra == \"docs\"",
      "mkdocs-pdf; extra == \"docs\"",
      "mkdocs-spellcheck[all]; extra == \"docs\"",
      "mkdocs-literate-nav; extra == \"docs\"",
      "mkdocs-table-reader-plugin; extra == \"docs\"",
      "black; extra == \"docs\"",
      "ruff; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "napari-ndev[testing]; extra == \"dev\"",
      "napari-ndev[docs]; extra == \"dev\"",
      "napari-ndev[pyqt6]; extra == \"qtpy-backend\"",
      "napari[pyqt6]; extra == \"pyqt6\"",
      "napari[pyqt5]; extra == \"pyqt5\"",
      "napari[pyside]; extra == \"pyside\"",
      "napari-simpleitk-image-processing; extra == \"extras\"",
      "bioio-czi>=1.0.1; extra == \"gpl-extras\"",
      "bioio-lif>=1; extra == \"gpl-extras\"",
      "napari-ndev[qtpy-backend]; extra == \"all\"",
      "napari-ndev[extras]; extra == \"all\"",
      "napari-ndev[gpl_extras]; extra == \"all\""
    ],
    "package_metadata_description": "# napari-ndev\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-ndev.svg?color=green)](https://github.com/TimMonko/napari-ndev/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ndev.svg?color=green)](https://pypi.org/project/napari-ndev)\n[![Python package index download statistics](https://img.shields.io/pypi/dm/napari-ndev.svg)](https://pypistats.org/packages/napari-ndev)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ndev.svg?color=green)](https://python.org)\n[![tests](https://github.com/TimMonko/napari-ndev/workflows/tests/badge.svg)](https://github.com/TimMonko/napari-ndev/actions)\n[![codecov](https://codecov.io/gh/TimMonko/napari-ndev/branch/main/graph/badge.svg)](https://codecov.io/gh/TimMonko/napari-ndev)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ndev)](https://napari-hub.org/plugins/napari-ndev)\n![Static Badge](https://img.shields.io/badge/plugin-npe2-brightgreen?style=flat-square&label=plugin)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14787853.svg)](https://doi.org/10.5281/zenodo.14787853)\n\n<img src=\"./resources/nDev-logo-large.png\" alt=\"logo\" width=\"400\" style=\"display: block; margin: auto;\">\n\nA collection of widgets intended to serve any person seeking to process microscopy images from start to finish, *with no coding necessary*. `napari-ndev` was designed to address the **gap between the napari viewer and batch python scripting**.\n\n* Accepts **diverse image formats**, dimensionality, file size, and maintains key metadata.\n* Allows **advanced, arbitrary image processing** workflows to be used by novices.\n* **User-friendly** sparse annotation and batch training of **machine learning classifiers**.\n* Flexible label measurements, parsing of metadata, and summarization for **easily readable datasets**.\n* Designed for ease of use, modification, and reproducibility.\n\n## [Check out the Docs to learn more!](https://ndev-kit.github.io)\n\n### See the [poster presented at BINA 2024](https://ndev-kit.github.io/BINA_poster/) for an overview of the plugins in action\n\n### Try out the [Virtual I2K 2024 Workshop](https://ndev-kit.github.io/tutorial/00_setup/) for an interactive tutorial\n\n## Installation\n\n**napari-ndev** is a pure Python package, and can be installed with [pip]:\n\n```bash\npip install napari-ndev\n```\n\nIf napari is currently not installed in your environment, you will also need to include a QtPy backend:\n\n```bash\npip install napari-ndev[qtpy-backend]\n```\n\nThe easiest way to get started with **napari-ndev** is to install all the optional dependencies (see note below) with:\n\n```bash\npip install napari-ndev[all]\n```\n\n----------------------------------\n\n### Optional Libraries\n\n**napari-ndev** is most useful when interacting with some other napari plugins (e.g. napari-assistant) and can read additional filetypes. A few extra BSD3 compatible napari-plugins may be installed with [pip]:\n\n```bash\npip install napari-ndev[extras]\n```\n\n**napari-ndev** can optionally use GPL-3 licensed libraries to enhance its functionality, but are not required. If you choose to install and use these optional dependencies, you must comply with the GPL-3 license terms. The main functional improvement is from some `bioio` libraries to support extra image formats, including `czi` and `lif` files. These libraries can be installed with [pip]:\n\n```bash\npip install napari-ndev[gpl-extras]\n```\n\nIn addition, you may need to install specific [`bioio` readers](https://github.com/bioio-devs/bioio) to support your specific image, such as `bioio-czi` and `bioio-lif` (included in `[gpl-extras]`) or `bioio-bioformats`.\n\n### Development Libraries\n\nFor development use the `[dev]` optional libraries to verify your changes, which includes the `[docs]` and `[testing]` optional groups. However, the Github-CI will test pull requests with `[testing]` only.\n\n----------------------------------\n\nThe wide breadth of this plugin's scope is only made possible by the amazing libraries and plugins from the python and napari community, especially [Robert Haase](https://github.com/haesleinhuepf).\n\nThis [napari] plugin was generated with [Cookiecutter] using [napari]'s [cookiecutter-napari-plugin] template.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-ndev\" is free and open source software.\n\nSome optional libraries can be installed to add functionality to `napari-ndev`, including some that may be more restrictive than this package's BSD-3-Clause.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.lms",
      "*.oif",
      "*.pcoraw",
      "*.seq",
      "*.c01",
      "*.k25",
      "*.pr3",
      "*.fff",
      "*.rec",
      "*.spc",
      "*.labels",
      "*.rw2",
      "*.ids",
      "*.mod",
      "*.jng",
      "*.jif",
      "*.am",
      "*.dng",
      "*.ics",
      "*.tfr",
      "*.img",
      "*.cine",
      "*.pxr",
      "*.cr2",
      "*.cap",
      "*.g3",
      "*.sdt",
      "*.pcx",
      "*.dds",
      "*.pict",
      "*.bay",
      "*.nd2",
      "*.im",
      "*.nii.gz",
      "*.r3d",
      "*.vsi",
      "*.jp2",
      "*.mhd",
      "*.gdcm",
      "*.msr",
      "*.afi",
      "*.his",
      "*.nrw",
      "*.xdce",
      "*.lim",
      "*.xys",
      "*.msp",
      "*.pnl",
      "*.zvi",
      "*.sr2",
      "*.array-like",
      "*.nrrd",
      "*.al3d",
      "*.koa",
      "*.htm",
      "*.mov",
      "*.sld",
      "*.lei",
      "*.wap",
      "*.cut",
      "*.klb",
      "*.mic",
      "*.gbr",
      "*.html",
      "*.iff",
      "*.sxm",
      "*.jpe",
      "*.lsm",
      "*.wpi",
      "*.ftc",
      "*.avi",
      "*.xpm",
      "*.ffr",
      "*.sti",
      "*.cur",
      "*.wav",
      "*.2fl",
      "*.bmp",
      "*.mdb",
      "*.aim",
      "*.ipl",
      "*.acff",
      "*.hdr",
      "*.mvd2",
      "*.txt",
      "*.ct.img",
      "*.pfm",
      "*.arw",
      "*.pgm",
      "*.frm",
      "*.grey",
      "*.grib",
      "*.oib",
      "*.rdc",
      "*.j2c",
      "*.erf",
      "*.ch5",
      "*.flc",
      "*.mri",
      "*.dm3",
      "*.fit",
      "*.fake",
      "*.mnc2",
      "*.ome.tif",
      "*.ppm",
      "*.srf",
      "*.imggz",
      "*.mpeg",
      "*.xv",
      "*.ims",
      "*.scan",
      "*.jpk",
      "*.pic",
      "*.im3",
      "*.bif",
      "*.bmq",
      "*.gel",
      "*.rwl",
      "*.nd",
      "*.cs1",
      "*.psd",
      "*.zfp",
      "*.gif",
      "*.nii",
      "*.wdp",
      "*.ipm",
      "*.mp4",
      "*.htd",
      "*.par",
      "*.niigz",
      "*.fdf",
      "*.dcr",
      "*.kc2",
      "*.zpo",
      "*.rcpnl",
      "*.apl",
      "*.acqp",
      "*.bip",
      "*.bsdf",
      "*.ome.tiff",
      "*.gipl",
      "*.naf",
      "*.mos",
      "*.exp",
      "*.png",
      "*.exr",
      "*.wat",
      "*.hdp",
      "*.3fr",
      "*.mkv",
      "*.zfr",
      "*.vtk",
      "*.nia",
      "*.dv",
      "*.tnb",
      "*.j2k",
      "*.tim",
      "*.cxd",
      "*.ps",
      "*.ia",
      "*.qptiff",
      "*.fts",
      "*.lbm",
      "*.thm",
      "*.wbm",
      "*.ndpi",
      "*.jpeg",
      "*.ano",
      "*.nef",
      "*.raw",
      "*.kdc",
      "*.csv",
      "*.liff",
      "*.vff",
      "*.xbm",
      "*.jpx",
      "*.mrc",
      "*.orf",
      "*.sif",
      "*.tiff",
      "*.dsc",
      "*.bin",
      "*.dicom",
      "*.stp",
      "*.targa",
      "*.wmv",
      "*.1sc",
      "*.tif",
      "*.amiramesh",
      "*.nhdr",
      "*.pcd",
      "*.i2i",
      "*.ftu",
      "*.pct",
      "*.dcx",
      "*.pxn",
      "*.raf",
      "*.jfif",
      "*.vms",
      "*.xqd",
      "*.rgb",
      "*.lfr",
      "*.inf",
      "*.scn",
      "*.tga",
      "*.cfg",
      "*.iiq",
      "*.lif",
      "*.dti",
      "*.wlz",
      "*.l2d",
      "*.ome",
      "*.svs",
      "*.st",
      "*.cat",
      "*.webp",
      "*.flex",
      "*.wmf",
      "*.ptx",
      "*.df3",
      "*.srw",
      "*.fpx",
      "*.mgh",
      "*.hdf",
      "*.mtb",
      "*.mef",
      "*.arf",
      "*.sm2",
      "*.ico",
      "*.lfp",
      "*.hdf5",
      "*.ct",
      "*.inr",
      "*.spi",
      "*.db",
      "*.fits",
      "*.mpo",
      "*.afm",
      "*.cif",
      "*.mng",
      "*.zip",
      "*.ras",
      "*.jpc",
      "*.dat",
      "*.xqf",
      "*.epsi",
      "*.swf",
      "*.bufr",
      "*.fli",
      "*.dcm",
      "*.crw",
      "*.iim",
      "*.jpf",
      "*.oir",
      "*.dc2",
      "*.pef",
      "*.icns",
      "*.ipw",
      "*.vws",
      "*.dm2",
      "*.rwz",
      "*.sm3",
      "*.stk",
      "*.ali",
      "*.h5",
      "*.czi",
      "*.rgba",
      "*.wbmp",
      "*.xml",
      "*.ecw",
      "*.fz",
      "*.eps",
      "*.hed",
      "*.mha",
      "*.bw",
      "*.hx",
      "*.obf",
      "*.v",
      "*.pbm",
      "*.qtk",
      "*.fid",
      "*.top",
      "*.drf",
      "*.mpg",
      "*.ndpis",
      "*.jpg",
      "*.emf",
      "*.jxr",
      "*.spe",
      "*.mdc",
      "*.npz",
      "*.mnc",
      "*.mrw"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "nDev App",
      "Image Utilities",
      "Workflow Widget",
      "APOC Widget",
      "Measure Widget",
      "Settings Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-noise2vst",
    "name": "napari-noise2vst",
    "display_name": "Noise2VST",
    "version": "0.1.1",
    "created_at": "2025-08-03",
    "modified_at": "2025-08-04",
    "authors": [
      "Ibrahima Alain Gueye"
    ],
    "author_emails": [
      "Ibrahima Alain Gueye <gueyeibrahimaalain@gmail.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-noise2vst/",
    "home_github": "https://github.com/IbrahimaAlain/napari-noise2vst",
    "home_other": null,
    "summary": "A plugin for denoising microscopy images using Noise2VST",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "matplotlib",
      "torch",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari",
      "torchvision",
      "Noise2VST",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-noise2vst\n\n[![License MIT](https://img.shields.io/pypi/l/napari-noise2vst.svg?color=green)](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-noise2vst.svg?color=green)](https://pypi.org/project/napari-noise2vst)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-noise2vst.svg?color=green)](https://python.org)\n[![tests](https://github.com/IbrahimaAlain/napari-noise2vst/workflows/tests/badge.svg)](https://github.com/IbrahimaAlain/napari-noise2vst/actions)\n[![codecov](https://codecov.io/gh/IbrahimaAlain/napari-noise2vst/branch/main/graph/badge.svg)](https://codecov.io/gh/IbrahimaAlain/napari-noise2vst)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-noise2vst)](https://napari-hub.org/plugins/napari-noise2vst)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\n> A plugin for denoising microscopy images using Noise2VST  \n> Developed by **Ibrahima Alain GUEYE**\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#gett\nDependenciesing-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Dependencies\n\nThis plugin relies on the Noise2VST framework [S. Herbreteau and M. Unser, ICCV'25].\nThe source code is available at:\nhttps://github.com/sherbret/Noise2VST\n- ‚úÖ No manual installation is required ‚Äî this version is installed automatically when you install the plugin.\n\n## Installation\n\nTo install in an environment using conda:\n\n```\nconda create --name napari-env\nconda activate napari-env\nconda install pip\n```\n\nYou can install `napari-noise2vst` via [pip]:\n\n```\npip install napari-noise2vst\n```\n\nIf napari is not already installed, you can install `napari-noise2vst` with napari and Qt via:\n\n```\npip install \"napari-noise2vst[all]\"\n```\n\nIf you prefer installing napari separately:\n\n```\npip install \"napari[all]\"\n```\n\nTo install latest development version:\n\n```\npip install git+https://github.com/IbrahimaAlain/napari-noise2vst.git\n```\n\n## Usage\n\nAfter installation, you can launch the **Noise2VST Denoising** plugin directly from the napari interface.\nIn the napari top menu, go to:\n\n**`Plugins > Noise2VST Denoising (Denoising Noise2VST)`**\n\n![image_0.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_0.png)\n\nOpen your image by clicking:\n**`File ‚Üí Open File(s)...`**\nSelect the noisy image (e.g., .tif, .png, etc.) that you want to denoise. The image will appear in the napari viewer.\n\n![image_1.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_1.png)\n\nOnce the image is loaded, scroll to the plugin panel on the right.\nSet the number of training iterations using the slider (e.g., 2000).\nThen click the Fit button to train the denoising model on the image.\n\nThe region shown here highlights the relevant settings and the training button.\n\n![image_2.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_2.png)\n![image_3.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_3.png)\n\nA progress bar appears, indicating the training status in real time.\nYou can follow the advancement of model fitting visually.\n\n![image_4.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_4.png)\n\nOnce training is complete, the plugin automatically stores the model weights.\nClick the Run Denoising button to generate the denoised version of the input image.\n\n![image_5.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_5.png)\n\nThe denoised image appears as a new layer in the napari viewer, alongside the original one.\nYou can toggle visibility, adjust contrast, and compare both layers interactively.\n\n![image_6.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_6.png)\n\nClick the Visualize VST button to display the spline transformation (VST) learned during training.\nA matplotlib window pops up with a plot showing the input-output relationship.\n\n![image_7.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_7.png)\n\nTo save the spline transformation values, click the Save Spline Knots button.\nA dialog window opens to let you choose where to store the CSV file containing the knots.\n\n![image_8.png](https://github.com/IbrahimaAlain/napari-noise2vst/raw/main/docs/images/image_8.png)\n\n\n## Citation\n\n```BibTex\n@article{herbreteau2024noise2vst,\n  title={Self-Calibrated Variance-Stabilizing Transformations for Real-World Image Denoising},\n  author={Herbreteau, S{\\'e}bastien and Unser, Michael},\n  journal={arXiv preprint arXiv:2407.17399},\n  year={2024}\n}\n```\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/IbrahimaAlain/napari-noise2vst/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Denoising"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "set-calibration",
    "name": "set-calibration",
    "display_name": "Calibration Tool",
    "version": "0.0.4",
    "created_at": "2023-11-03",
    "modified_at": "2025-08-04",
    "authors": [
      "Clement H. Benedetti"
    ],
    "author_emails": [
      "clement.benedetti@mri.cnrs.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/set-calibration/",
    "home_github": "https://github.com/MontpellierRessourcesImagerie/set-calibration",
    "home_other": null,
    "summary": "A tool allowing to modify the calibration (physical units) of images",
    "categories": [
      "Measurement",
      "Transformations",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "qtpy; extra == \"testing\""
    ],
    "package_metadata_description": "# set-scale-tool\n\n[![License MIT](https://img.shields.io/pypi/l/set-calibration.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/set-calibration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/set-calibration.svg?color=green)](https://pypi.org/project/set-calibration)\n[![Python Version](https://img.shields.io/pypi/pyversions/set-calibration.svg?color=green)](https://python.org)\n[![tests](https://github.com/MontpellierRessourcesImagerie/set-calibration/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/set-calibration/actions)\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/set-calibration/branch/main/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/set-calibration)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/set-calibration)](https://napari-hub.org/plugins/set-calibration)\n\nThe tool allows to set the voxel size for x, y and z and and the physical unit of a layer.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `set-calibration` via [pip]:\n\n    pip install set-calibration\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MontpellierRessourcesImagerie/set-calibration.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"set-calibration\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MontpellierRessourcesImagerie/set-calibration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Scale Tool"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-roi-manager",
    "name": "napari-roi-manager",
    "display_name": "ROI Manager",
    "version": "0.0.6",
    "created_at": "2024-02-25",
    "modified_at": "2025-08-03",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "Hanjin Liu <liuhanjin.sc@gmail.com>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-roi-manager/",
    "home_github": "https://github.com/hanjinliu/napari-roi-manager",
    "home_other": null,
    "summary": "A ROI Manager Widget with an UI similar to ImageJ",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "roifile",
      "napari; extra == 'testing'",
      "pyqt5; extra == 'testing'",
      "pytest; extra == 'testing'",
      "pytest-cov; extra == 'testing'",
      "pytest-qt; extra == 'testing'",
      "tox; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-roi-manager\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-roi-manager.svg?color=green)](https://github.com/hanjinliu/napari-roi-manager/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-roi-manager.svg?color=green)](https://pypi.org/project/napari-roi-manager)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-roi-manager.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-roi-manager/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-roi-manager/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-roi-manager/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-roi-manager)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi-manager)](https://napari-hub.org/plugins/napari-roi-manager)\n\nA ROI Manager Widget with an UI similar to ImageJ.\n\n![](https://github.com/hanjinliu/napari-roi-manager/blob/main/images/demo.gif)\n\n&check; The layer is just a Shapes layer, so it is compatible with any other plugins.\n\n&check; Supports importing and exporting ImageJ ROI files.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-roi-manager` via [pip]:\n\n    pip install napari-roi-manager\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-roi-manager.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-roi-manager\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hanjinliu/napari-roi-manager/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ROI Manager"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-maicrobe",
    "name": "napari-mAIcrobe",
    "display_name": "mAIcrobe",
    "version": "0.0.1",
    "created_at": "2025-08-01",
    "modified_at": "2025-08-01",
    "authors": [
      "Ant√≥nio Brito"
    ],
    "author_emails": [
      "antmsbrito95@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-maicrobe/",
    "home_github": "https://github.com/HenriquesLab/napari-mAIcrobe",
    "home_other": null,
    "summary": "mAIcrobe",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy<2.0",
      "magicgui>=0.10.0",
      "napari[all]",
      "tensorflow<=2.15.0",
      "napari-skimage-regionprops",
      "stardist-napari==2022.12.6",
      "scikit-learn",
      "scikit-image==0.20.0",
      "pandas",
      "cellpose==3.1.1.1",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-mAIcrobe\n\nADD BADGES # TODO\n\nmAIcrobe napari plugin\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `mAIcrobe` via [pip]:\n\n    pip install napari-mAIcrobe\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/HenriquesLab/napari-mAIcrobe.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mAIcrobe\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/HenriquesLab/napari-mAIcrobe/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Compute label",
      "Compute cells",
      "Filter cells"
    ],
    "contributions_sample_data": [
      "Phase contrast S. aureus",
      "Membrane dye S.aureus",
      "DNA dye S.aureus"
    ]
  },
  {
    "normalized_name": "napari-phasors",
    "name": "napari-phasors",
    "display_name": "Napari Phasors",
    "version": "0.0.5",
    "created_at": "2024-11-22",
    "modified_at": "2025-08-01",
    "authors": [
      "Bruno Pannunzio",
      "Marcelo L. Zoccoler",
      "Bruno Schuty",
      "Leonel Malacrida"
    ],
    "author_emails": [
      "bpannunzio@pasteur.edu.uy",
      "marzoccoler@gmail.com",
      "schutyb@schutyb.com",
      "lmalacrida@pasteur.edu.uy"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-phasors/",
    "home_github": "https://github.com/napari-phasors/napari-phasors",
    "home_other": null,
    "summary": "A simple plugin to use phasor analysis",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "phasorpy==0.6",
      "qtpy",
      "scikit-image",
      "biaplotter>=0.4.2",
      "lfdfiles",
      "sdtfile",
      "ptufile",
      "tifffile",
      "pandas",
      "pyqt5",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "qtpy; extra == \"testing\"",
      "scikit-image; extra == \"testing\"",
      "biaplotter>=0.2.0; extra == \"testing\"",
      "PyQt5; extra == \"testing\"",
      "pandas; extra == \"testing\"",
      "black; extra == \"testing\"",
      "isort; extra == \"testing\"",
      "phasorpy==0.6; extra == \"testing\"",
      "tifffile; extra == \"testing\"",
      "lfdfiles; extra == \"testing\"",
      "sdtfile; extra == \"testing\"",
      "ptufile; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-phasors\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-phasors.svg?color=green)](https://github.com/napari-phasors/napari-phasors/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-phasors.svg?color=green)](https://pypi.org/project/napari-phasors)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-phasors.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari-phasors/napari-phasors/workflows/tests/badge.svg)](https://github.com/napari-phasors/napari-phasors/actions)\n[![codecov](https://codecov.io/gh/napari-phasors/napari-phasors/branch/main/graph/badge.svg)](https://codecov.io/gh/napari-phasors/napari-phasors)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-phasors)](https://napari-hub.org/plugins/napari-phasors)\n\nA simple plugin to do phasor analysis in napari. Based on the [phasorpy](https://www.phasorpy.org/) library.\n\n[Jump to Intallation](#installation)\n\n----------------------------------\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Usage\n\nnapari-phasors is composed of a few widgets that allow reading a few specific FLIM and hyperspectral file formats, perform phasor analysis, and display and export the results of manual phasor selections.\n\n### Sample Data\n\nTwo sample datasets for FLIM are provided, along with their corresponding calibration images. Additionally, a paramecium image is included as sample data for hyperspectral analysis.\n\n![sample_data](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/sample_data.gif)\n\n### Phasor Analysis\n\n#### Plot FLIM Data\n\nFLIM phasor data can be plotted as a 2D histogram or scatter plot. The colormap, the number of bins and the scale of the colors can be customized.\nFiltering and thresholding can also be applied to process phasor data and the mean intensity image. \n\n![phasors_flim](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/phasors_flim.gif)\n\n#### Plot Hyperspectral Data\n\nHyperspectral phasor data can also be plotted as a 2D histogram or scatter plot and visualized in the full universal circle.\n\n![phasors_hyperspectral](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/phasors_hyperspectral.gif)\n\n### Apparent Lifetime Display\n\nA FLIM image can be colormapped according to the phase or modulation apparent lifetime. A histogram is also created for visualization of the distribution of apparent lifetimes of the FLIM image.\n\n![lifetimes](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/lifetimes.gif)\n\n### Phasor Calibration\n\nFLIM images can be calibrated using a reference image acquired under the same experimental parameters. This reference image should consist of a homogeneous solution of a fluorophore with a known fluorescence lifetime and the laser frequency used in the experiment. This ensures accuracy and consistency in lifetime measurements.\n\n![calibration](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/calibration.gif)\n\n### Phasor Custom Import\n\nSupported file formats (`.tif`, `.ptu`, `.sdt`, `.fbd`, `.lsm`, `.ome.tif`) can be read and transformed to the phasor space. Additional options, such as the harmonics, channels and frames, can be specified depending on the file format to be read.\n\n![custom_import](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/custom_import.gif)\n\n### Phasor Export\n\nThe average intensity image and the phasor coordinates can be exported as OME-TIF files that can be read by napari-phasors and PhasorPy. Alternatively, the phasor coordinates, as well as the selections (cursors) can be exported as a CSV file.\n\n![export_phasors](https://github.com/napari-phasors/napari-phasors/raw/main/gifs/export_phasors.gif)\n\n## Installation\n\nYou can install `napari-phasors` via [pip]. Follow these steps from a terminal.\n\nWe recommend using `miniforge` whenever possible. Click [here](https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge) to choose the right download option for your OS.\n**If you do not use `miniforge`, but rather Anaconda or Miniconda, replace the `mamba` term whenever you see it below with `conda`.**\n\nCreate a conda environment with napari by typing :\n\n    mamba create -n napari-phasors-env napari pyqt python=3.10\n    \nActivate the environment :\n\n    mamba activate napari-phasors-env\n\nInstall `napari-phasors` via [pip] :\n\n    pip install napari-phasors\n\nAlternatively, install latest development version with :\n\n    pip install git+https://github.com/napari-phasors/napari-phasors.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-phasors\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/napari-phasors/napari-phasors/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*ome.tif",
      "*.tif",
      "*.fbd",
      "*.ptu",
      "*.lsm",
      "*.sdt"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Phasor Custom Import",
      "Calibration Widget",
      "Phasor Plot",
      "Lifetime Widget",
      "Export Phasor"
    ],
    "contributions_sample_data": [
      "Convallaria FLIM",
      "FLUTE's FLIM Embryo sample data FLIM",
      "Paramecium Hyperspectral Image"
    ]
  },
  {
    "normalized_name": "quantpunc",
    "name": "quantpunc",
    "display_name": "QuantPunc",
    "version": "0.0.1.post1",
    "created_at": "2025-07-30",
    "modified_at": "2025-07-31",
    "authors": [
      "tehahn"
    ],
    "author_emails": [],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/quantpunc/",
    "home_github": "https://github.com/tehahn/quantpunc/",
    "home_other": null,
    "summary": "A Napari plugin for puncta analysis and quantification in 2D microscopy images.",
    "categories": [
      "Annotation",
      "Image Processing",
      "Measurement",
      "Registration",
      "Segmentation",
      "Utilities",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy<2.3",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scikit-learn>=1.6.1",
      "pywavelets>=1.6.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# QuantPunc\n\n[![PyPI](https://img.shields.io/pypi/v/quantpunc.svg?color=green)](https://pypi.org/project/quantpunc)\n[![Python Version](https://img.shields.io/pypi/pyversions/quantpunc.svg?color=green)](https://python.org)\n[![License BSD-3](https://img.shields.io/pypi/l/quantpunc.svg?color=green)](https://github.com/tehahn/quantpunc/blob/main/LICENSE)\n\nQuantPunc is a Napari plugin for puncta analysis and quantification in 2D microscopy images. A brief overview of QuantPunc's workflow can be found below. \n\nA comprehensive guide to QuantPunc can be found at https://tehahn.github.io/quantpunc/.\n\nQuantPunc is currently in beta. Please report any problems to the [issues page].\n\n## Features\n1. Automated puncta labeling and counting\n2. Watershed segmentation\n3. Colocalization analysis \n4. Exportable counts and stats\n\n## Installation\nYou can install `quantpunc` via Napari's plugin manager:\n\n1. Click on \"Plugins\" in the toolbar.\n2. Click on \"Install/Uninstall Plugins...\" in the context menu.\n3. Type \"quantpunc\" in the searchbar.\n4. Click install.\n\nYou can also install `quantpunc` via [pip]:\n\n    pip install quantpunc\n\n## Puncta Quantification and Workflow\nHere‚Äôs an ideal, high-level workflow for puncta segmentation and quantification. This is the recommended way of using QuantPunc. However, QuantPunc‚Äôs widgets are modularly designed so that they can be used as standalone tools. If this is your first time working with these tools, you can access the more in-depth guide mentioned above [here].\n\n### 1. Preprocessing (optional but recommended)\nQuantPunc uses wavelet denoising and adaptive histogram equalization to enhance edge information and contrast in your image. If you feel that your image has sufficient contrast and minimal noise, feel free to skip this step.\n\n<img \nsrc=\"https://raw.githubusercontent.com/tehahn/quantpunc/refs/heads/main/demo_imgs/preprocessing_example.gif\" \nalt=\"\" \nwidth=\"1000\"\nstyle=\"border-radius: 10px; max-width: 100%;\">\n\n### 2. Automated labeling\nThere are two automated labeling methods. One of them uses [skimage‚Äôs blob detection] algorithms and the other uses a random forest classifier with features inspired by [ilastik]. You also have the option to only segment puncta within ROIs. You can provide your own ROIs or create them using a labels layer in Napari.\n\n### Random forest classifier (RFC)\nYou can use Napari to create annotations or provide your own. After annotating your puncta and specifying your integer labels, you can then train the RFC. Click on *Label puncta* to segment your puncta.\n\n<img \nsrc=\"https://raw.githubusercontent.com/tehahn/quantpunc/refs/heads/main/demo_imgs/rfc_example.gif\" \nalt=\"\" \nwidth=\"1000\"\nstyle=\"border-radius: 10px; max-width: 100%;\">\n\n### Skimage blob detection\nSelect your favorite blob detection algorithm from the *Method* dropdown menu. After parameterizing it, click on *Label puncta*.\n\n<img \nsrc=\"https://raw.githubusercontent.com/tehahn/quantpunc/refs/heads/main/demo_imgs/skimage_blob_example.gif\" \nalt=\"\" \nwidth=\"1000\"\nstyle=\"border-radius: 10px; max-width: 100%;\">\n\n### 3. Manual labeling\nYou can remove any puncta labels you don‚Äôt want from automated segmentation by using Napari‚Äôs layer control toolbar. You also have the option to do a full manual segmentation using a labels layer and quantify your annotations after.\n\n### 4. Watershed segmentation (optional)\nIf your puncta exhibits lots of clumping, you can use the watershed tool to perform instance segmentation on your puncta labels layer. You can choose either to use a distance transform or Sobel filter to generate the elevation map.\n\n<img \nsrc=\"https://raw.githubusercontent.com/tehahn/quantpunc/refs/heads/main/demo_imgs/watershed_example.gif\"\nalt=\"\" \nwidth=\"1000\"\nstyle=\"border-radius: 10px; max-width: 100%;\">\n\n### Seed point generation\nYou can either provide your own seed points or generate them using skimage‚Äôs blob counting algorithms with a low min and max sigma.\n\n### 5. Puncta counting\nAfter you have a segmentation you‚Äôre happy with, select the image you want to quantify and click *Count puncta*. Make sure that your puncta labels layer is named after the image you‚Äôre quantifying with ‚Äú_puncta‚Äù as its suffix, e.g., ‚Äúyour_img_puncta‚Äù.\n\n<img \nsrc=\"https://raw.githubusercontent.com/tehahn/quantpunc/refs/heads/main/demo_imgs/counts_example.gif\" \nalt=\"\" \nheight=\"750\"\nstyle=\"border-radius: 10px; max-width: 100%;\">\n\n### 6. Colocalization analysis (optional)\nColocalization is measured using the intersection over union (IoU), aka the Jaccard Index. Select two puncta labels layers and click on *Compute IoU*.\n\n<img \nsrc=\"https://raw.githubusercontent.com/tehahn/quantpunc/refs/heads/main/demo_imgs/coloc_example.gif\" \nalt=\"\" \nheight=\"750\"\nstyle=\"border-radius: 10px; max-width: 100%;\">\n\n### 7. Displaying stats and saving\nClick on any of the tabs in the table widget to view different summaries of the puncta you‚Äôve quantified. To export the tables as a csv for the layer you‚Äôve selected, click on *Save selected data*. If you want to save the data for all the images you‚Äôve quantified click on *Save all data*.\n\n## Contributing\nContributions are very, very welcome. QuantPunc allows you to implement your own automated puncta labeler. Look in the Github repo at [abstract_puncta_labeler] to see what methods need to be implemented and [default_puncta_labelers] for examples. If you're interested in making it available to everyone else or have any other improvements, feel free to send a pull request!\n\n## License\nDistributed under the terms of the [BSD-3] license,\nQuantPunc is free and open source software\n\n## Issues\nQuantPunc is still in beta, so bugs are to be expected. Please report any problems to the [issues page].\n\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[pip]: https://pypi.org/project/pip/\n\n[ilastik]: https://www.ilastik.org/\n[skimage‚Äôs blob detection]: https://scikit-image.org/docs/0.25.x/auto_examples/features_detection/plot_blob.html\n\n[abstract_puncta_labeler]: https://github.com/tehahn/quantpunc/blob/main/src/quantpunc/quantification/abstract_puncta_labeler.py\n[default_puncta_labelers]: https://github.com/tehahn/quantpunc/blob/main/src/quantpunc/quantification/default_puncta_labelers.py\n\n[Napari hub]: https://napari-hub.org/plugins/quantpunc.html\n[here]: https://tehahn.github.io/quantpunc/\n[issues page]: https://github.com/tehahn/quantpunc/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "QuantPunc"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-layer-divider",
    "name": "napari-layer-divider",
    "display_name": "Layer Divider",
    "version": "0.0.3",
    "created_at": "2025-07-29",
    "modified_at": "2025-07-30",
    "authors": [
      "LLLLAAAA2333"
    ],
    "author_emails": [
      "wjh19937458882@mail.ustc.edu.cn"
    ],
    "license": "Copyright (c) 2025, LLLLAAAA23...",
    "home_pypi": "https://pypi.org/project/napari-layer-divider/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin to divide an image layer into several parts",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pytest-xvfb; sys_platform == \"linux\" and extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-layer-divider\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-layer-divider.svg?color=green)](https://github.com/Wenlab/napari-layer-divider/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-layer-divider.svg?color=green)](https://pypi.org/project/napari-layer-divider)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-layer-divider.svg?color=green)](https://python.org)\n[![tests](https://github.com/Wenlab/napari-layer-divider/workflows/tests/badge.svg)](https://github.com/Wenlab/napari-layer-divider/actions)\n[![codecov](https://codecov.io/gh/Wenlab/napari-layer-divider/branch/main/graph/badge.svg)](https://codecov.io/gh/Wenlab/napari-layer-divider)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-divider)](https://napari-hub.org/plugins/napari-layer-divider)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA plugin to divide an image layer into several parts\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-layer-divider` via [pip]:\n\n```\npip install napari-layer-divider\n```\n\nIf napari is not already installed, you can install `napari-layer-divider` with napari and Qt via:\n\n```\npip install \"napari-layer-divider[all]\"\n```\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-layer-divider\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Layer Divider"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nninteractive",
    "name": "napari-nninteractive",
    "display_name": "nnInteractive",
    "version": "1.0.5",
    "created_at": "2025-03-11",
    "modified_at": "2025-07-30",
    "authors": [
      "Lars Kr√§mer",
      "Fabian Isensee",
      "Maximilian Rokuss"
    ],
    "author_emails": [
      "lars.kraemer@dkfz-heidelberg.de",
      "f.isensee@dkfz-heidelberg.de",
      "maximilian.rokuss@dkfz-heidelberg.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-nninteractive/",
    "home_github": "https://github.com/MIC-DKFZ/napari-nninteractive",
    "home_other": null,
    "summary": "nnInteractive plugin for Napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "torch",
      "numpy",
      "qtpy",
      "napari-nifti",
      "huggingface_hub",
      "hf_transfer",
      "nnInteractive>=1.0.0",
      "napari_toolkit",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "<img src=\"https://github.com/MIC-DKFZ/napari-nninteractive/raw/main/imgs/nnInteractive_header.png\"  width=\"1200\">\n\n# nnInteractive: Redefining 3D Promptable Segmentation\n\nThis repository contains the napari plugin for nnInteractive. Check out the\n[python backend](https://github.com/MIC-DKFZ/nnInteractive) and [MITK integration](https://www.mitk.org/MITK-nnInteractive) for more.\n\n## What is nnInteractive?\n\n> Isensee, F.\\*, Rokuss, M.\\*, Kr√§mer, L.\\*, Dinkelacker, S., Ravindran, A., Stritzke, F., Hamm, B., Wald, T., Langenberg, M., Ulrich, C., Deissler, J., Floca, R., & Maier-Hein, K. (2025). nnInteractive: Redefining 3D Promptable Segmentation. https://arxiv.org/abs/2503.08373 \\\n> \\*: equal contribution\n\nLink: [![arXiv](https://img.shields.io/badge/arXiv-2503.08373-b31b1b.svg)](https://arxiv.org/abs/2503.08373)\n\n##### Abstract:\n\nAccurate and efficient 3D segmentation is essential for both clinical and research applications.\nWhile foundation models like SAM have revolutionized interactive segmentation, their 2D design and domain shift limitations make them ill-suited for 3D medical images.\nCurrent adaptations address some of these challenges but remain limited, either lacking volumetric awareness, offering restricted interactivity, or supporting only a small set of structures and modalities.\nUsability also remains a challenge, as current tools are rarely integrated into established imaging platforms and often rely on cumbersome web-based interfaces with restricted functionality.\nWe introduce nnInteractive, the first comprehensive 3D interactive open-set segmentation method.\nIt supports diverse prompts‚Äîincluding points, scribbles, boxes, and a novel lasso prompt‚Äîwhile leveraging intuitive 2D interactions to generate full 3D segmentations.\nTrained on 120+ diverse volumetric 3D datasets (CT, MRI, PET, 3D Microscopy, etc.), nnInteractive sets a new state-of-the-art in accuracy, adaptability, and usability.\nCrucially, it is the first method integrated into widely used image viewers (e.g., Napari, MITK), ensuring broad accessibility for real-world clinical and research applications.\nExtensive benchmarking demonstrates that nnInteractive far surpasses existing methods, setting a new standard for AI-driven interactive 3D segmentation.\n\n<img src=\"https://github.com/MIC-DKFZ/napari-nninteractive/raw/main/imgs/figure1_method.png\" width=\"1200\">\n\n## Demo Videos\n\n<a href=\"https://www.youtube.com/watch?v=H_L6LL0FRoo\">\n    <img src=\"https://img.youtube.com/vi/H_L6LL0FRoo/0.jpg\" width=\"270\">\n</a>\n<a href=\"https://www.youtube.com/watch?v=YoMZ7Xv7gKI\">\n    <img src=\"https://img.youtube.com/vi/YoMZ7Xv7gKI/0.jpg\" width=\"270\">\n</a>\n<a href=\"https://www.youtube.com/watch?v=V0rqPYA3sjA\">\n    <img src=\"https://img.youtube.com/vi/V0rqPYA3sjA/0.jpg\" width=\"270\">\n</a>\n\n## Installation\n\n### Prerequisites\n\nYou need a Linux or Windows computer with a Nvidia GPU. 10GB of VRAM is recommended. Small objects should work with \\<6GB.\n\n##### 1. Create a virtual environment:\n\nnnInteractive supports Python 3.10+ and works with Conda, pip, or any other virtual environment. Here‚Äôs an example using Conda:\n\n```\nconda create -n nnInteractive python=3.12\nconda activate nnInteractive\n```\n\n##### 2. Install the correct PyTorch for your system\n\nGo to the [PyTorch homepage](https://pytorch.org/get-started/locally/) and pick the right configuration.\nNote that since recently PyTorch needs to be installed via pip. This is fine to do within your conda environment.\n\nFor Ubuntu with a Nvidia GPU, pick 'stable', 'Linux', 'Pip', 'Python', 'CUDA12.6' (if all drivers are up to date, otherwise use and older version):\n\n```\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n```\n\n##### 3. Install this repository + dependencies via\n\nInstall napari if necessary\n\n```bash\npip install napari[all]\n```\n\nInstall the plugin via pip:\n\n```bash\npip install napari-nninteractive\n```\n\nOr clone and install this repository:\n\n```bash\ngit clone https://github.com/MIC-DKFZ/napari-nninteractive\ncd napari-nninteractive\npip install -e .\n```\n\n**Note:** Model weights are automatically downloaded on first use. This can take up to a couple of minutes depending on your internet connection\n\n## Getting Started\n\nUse one of these three options to start napari and activate the plugin.\nAfterward, Drag and drop your images into napari.\n\n\\***Note if getting asked which plugin to use for opening .nii.gz files use napari-nifti.**\n\na) Start napari, then Plugins -> nnInteractive.\n\n```\nnapari\n```\n\nb) Run this to start napari with the plugin already started.\n\n```\nnapari -w napari-nninteractive\n```\n\nc) Run this to start napari with the plugin and open an image directly\n\n```\nnapari demo_data/liver_145_0000.nii.gz -w napari-nninteractive\n```\n\n# How to use\n\n**Note:** To open Nifti (.nii.gz, .nii) files we recommend to select napari-nifti.\n\n<img src=\"https://github.com/MIC-DKFZ/napari-nninteractive/raw/main/imgs/gui_instuctions.png\" width=\"1200\">\n\n## Citation\n\nWhen using nnInteractive, please cite the following paper:\n\n> Isensee, F.\\*, Rokuss, M.\\*, Kr√§mer, L.\\*, Dinkelacker, S., Ravindran, A., Stritzke, F., Hamm, B., Wald, T., Langenberg, M., Ulrich, C., Deissler, J., Floca, R., & Maier-Hein, K. (2025). nnInteractive: Redefining 3D Promptable Segmentation. https://arxiv.org/abs/2503.08373 \\\n> \\*: equal contribution\n\nLink: [![arXiv](https://img.shields.io/badge/arXiv-2503.08373-b31b1b.svg)](https://arxiv.org/abs/2503.08373)\n\n# License\n\nNote that while this repository is available under Apache-2.0 license (see [LICENSE](./LICENSE)), the [model checkpoint](https://huggingface.co/nnInteractive/nnInteractive) is `Creative Commons Attribution Non Commercial Share Alike 4.0`!\n\n______________________________________________________________________\n\n## Acknowledgments\n\n<p align=\"left\">\n  <img src=\"https://github.com/MIC-DKFZ/napari-nninteractive/raw/main/imgs/Logos/HI_Logo.png\" width=\"150\"> &nbsp;&nbsp;&nbsp;&nbsp;\n  <img src=\"https://github.com/MIC-DKFZ/napari-nninteractive/raw/main/imgs/Logos/DKFZ_Logo.png\" width=\"500\">\n</p>\n\nThis repository is developed and maintained by the Applied Computer Vision Lab (ACVL)\nof [Helmholtz Imaging](https://www.helmholtz-imaging.de/) and the\n[Division of Medical Image Computing](https://www.dkfz.de/en/medical-image-computing) at DKFZ.\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n[copier]: https://copier.readthedocs.io/en/stable/\n[napari]: https://github.com/napari/napari\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "nnInteractive"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "biaplotter",
    "name": "biaplotter",
    "display_name": "Canvas Widget from BiAPoL",
    "version": "0.4.2",
    "created_at": "2024-05-06",
    "modified_at": "2025-07-29",
    "authors": [
      "Marcelo Leomil Zoccoler"
    ],
    "author_emails": [
      "Marcelo Leomil Zoccoler <marzoccoler@gmail.com>"
    ],
    "license": "Copyright (c) 2024, DFG Cluste...",
    "home_pypi": "https://pypi.org/project/biaplotter/",
    "home_github": "https://github.com/BiAPoL/biaplotter",
    "home_other": null,
    "summary": "A base napari plotter widget for interactive plotting",
    "categories": [
      "Utilities",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy>=1.22.0",
      "magicgui",
      "qtpy",
      "napari-matplotlib",
      "nap-plot-tools>=0.1.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# biaplotter\n\n[![License BSD-3](https://img.shields.io/pypi/l/biaplotter.svg?color=green)](https://github.com/BiAPoL/biaplotter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/biaplotter.svg?color=green)](https://pypi.org/project/biaplotter)\n[![Python Version](https://img.shields.io/pypi/pyversions/biaplotter.svg?color=green)](https://python.org)\n[![tests](https://github.com/BiAPoL/biaplotter/workflows/tests/badge.svg)](https://github.com/BiAPoL/biaplotter/actions)\n[![codecov](https://codecov.io/gh/BiAPoL/biaplotter/branch/main/graph/badge.svg)](https://codecov.io/gh/BiAPoL/biaplotter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/biaplotter)](https://napari-hub.org/plugins/biaplotter)\n\nA base napari plotter widget for interactive plotting\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Documentation\n\nThe full documentation with API and examples can be found [here](https://biapol-biaplotter.readthedocs.io/en/stable).\n\n## Installation\n\n* Make sure you have Python in your computer, e.g. download [miniforge](https://github.com/conda-forge/miniforge?tab=readme-ov-file#download).\n\n* Create a new environment, for example, like this:\n\n```\nmamba create --name biaplotter-env python=3.11\n```\n\nIf you never used mamba/conda environments before, take a look at [this blog post](https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html).\n\n* **Activate** the new environment with `mamba`:\n\n```\nmamba activate biaplotter-env\n```\n\n* Install [napari](https://napari.org/stable/), e.g. via `mamba`:\n\n```\nmamba install -c conda-forge napari pyqt\n```\n\nAfterwards, install `biaplotter` via `pip`:\n\n```\npip install biaplotter\n```\n\nTo install latest development version :\n\n```\npip install git+https://github.com/BiAPoL/biaplotter.git\n```\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"biaplotter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/BiAPoL/biaplotter/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Interactive Canvas"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-fluoresfm",
    "name": "napari-fluoresfm",
    "display_name": "FluoResFM",
    "version": "0.2.3",
    "created_at": "2025-07-22",
    "modified_at": "2025-07-29",
    "authors": [
      "Qiqi Lu"
    ],
    "author_emails": [
      "136303971@qq.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-fluoresfm/",
    "home_github": "https://github.com/qiqi-lu/napari-fluoresfm",
    "home_other": null,
    "summary": "A plugin to use FluoResFM model in napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "scikit-image",
      "torch",
      "torchvision",
      "torchaudio",
      "tqdm",
      "scipy",
      "open_clip_torch",
      "pandas",
      "pytorch_msssim",
      "pydicom",
      "torchinfo",
      "tensorboard",
      "transformers",
      "openpyxl",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-fluoresfm\n\n[![License MIT](https://img.shields.io/pypi/l/napari-fluoresfm.svg?color=green)](https://github.com/qiqi-lu/napari-fluoresfm/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-fluoresfm.svg?color=green)](https://pypi.org/project/napari-fluoresfm)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-fluoresfm.svg?color=green)](https://python.org)\n[![tests](https://github.com/qiqi-lu/napari-fluoresfm/workflows/tests/badge.svg)](https://github.com/qiqi-lu/napari-fluoresfm/actions)\n[![codecov](https://codecov.io/gh/qiqi-lu/napari-fluoresfm/branch/main/graph/badge.svg)](https://codecov.io/gh/qiqi-lu/napari-fluoresfm)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-fluoresfm)](https://napari-hub.org/plugins/napari-fluoresfm)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nThis is a `napari` plugin developed for using FluoResFM model in napari.\nFluoresFM is a deep learning-based foundation model for multi-task cross-distribution restoration of fluorescence microscopic images.\n\nFluoResFM's `napari` plugin is in early satge, therefore I highly encourage any feedback and suggestions.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Before Installation\nAs FluoResFM is a deep learning-based model, it is recommended to use a GPU for inference and training on Linux system. So no choice to use CPU is provided in the plugin. Besides, as the code is depended on PyTorch and `triton` packages, you should install the plugin through command lines.\n\nI recommand you to install the plugin in a new envoroment created by `conda` .\n\n First, create a new environment with `conda` and activate it.\n```\nconda create -y --name napari-fluoresfm python=3.12\nconda activate napari-fluoresfm\n```\n\nThen, install `napari`.\n```\npip install -U \"napari[all]\"\n```\n\nTo use GPU for inference and training, you should install the GPU version of PyTorch. You can use `nvcc -V` to check the cuda version. Then install the corresponding version of PyTorch by check the [table](https://pytorch.org/get-started/previous-versions/) provided by PyTorch. For example, if you have `cuda 12.4`, you should install the following version of PyTorch.\n```\npip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n```\n\nWe recommend you using Linux for training and inference, as the `triton` support for Windows is not stable. And the with Linux system, the acceleration of `triton` is much better than Windows, which allow larger `batch size` and `patch size` to be used in training and inference.\n\nIf you are using Windows, you should install the `triton` package first according the PyTorch version you installed. Please check this [link](https://github.com/woct0rdho/triton-windows?tab=readme-ov-file#3-pytorch) for more details.\n```\npip install -U \"triton-windows<3.3\"\n```\n\n## Installation\n\nYou can install `napari-fluoresfm` via [pip]:\n\n```\npip install napari-fluoresfm\n```\n\nTo install latest development version :\n\n```\npip install git+https://github.com/qiqi-lu/napari-fluoresfm.git\n```\n\n## Functions\nThis plugin can be used for data preprocessing, model training, and model inference.\n\n![interface](src/napari_fluoresfm/images/interface.png)\n**Figure 1: The interface of the plugin.** **a** The label page uased for prediction of restored images. **b** The page for data preprocessing, including data patching and text embedding modules. **c** The page for model training.\n\n**Each page can be run independetly. You only need to input the data and model information as described below from top to bottom, and click `run` button to start. To train or fine-tune the mdoel, you need to preprocess the data firstly using the `Preprocess` page. The training and fine-tuning can both be done in `Train` page.**\n\n### Predict\nThis label page is used for prediction of restored images. You can select the pretrained model and the input image to predict the restored image.\n#### PATH box\nThis box is used to select the folder for data and models.\n- **Input Folder**: The folder containing the input images. The input images should be in `.tif` format with a shape of `(1, H, W)` or `(H, W)`. The model will restored the images one by one and save them into the **Output Folder**.\n- **Index File**: This should be a `.txt` file containing all the file names of the images to be restored in each line. The file name should be the same as the file name of the input images.\n- **Output Folder** (optional): The folder to save the restored images. If not specified, the restored images will be saved into the `#Input Folder#_fluoresfm`.\n- **Embedder**: The folder saved the text embedder model. You can download the text model from my [Google Drive](https://drive.google.com/drive/folders/1pfiCHtXrf5ne6fjKJQAvwQhgBO_yVpWy?usp=sharing).\n- **Checkpoint**: The pre-trained FluoResFM model checkpoint with a suffix `.pt`.\n\n#### PARAMETERS box\nThis box is used to set the parameters for prediction.\n- **Device**: The device to run the model. Only support `cuda`.\n- **Compile model**: Whether to compile the model. If checked, the model will be compiled with `triton` for faster inference and lower BPU memory usage. But the compile process will take a few minutes. if only a few image to be restored, you can uncheck this box.\n- **Input interpolation (nearest)**: Do nearest interpolation on the input image to implement super-resolution task, as the input and output image of FluoResFM have the save shape.\n- **Batch size**: The batch size used during inference. Larger batch size will use more memory and faster inference. If your GPU memory is not enough, you can reduce this value.\n- **Patch size**: The patch size used during inference. Larger patch size will use more memory. If your GPU memory is not enough, you can reduce this value. Different pacth size may lead to slightly different results due to the patch stiching process.\n#### TEXT box\nThis box is used to set the text prompt for the model.\n- **Task**: The task to be performed. For example, \"denoising\", \"deconvolution\", or \"super-resolution with a scale factor of 2\". When inputing \"super-resolution with a scale factor of 2\", the **Input interpolation (nearest)** should be also set as 2. Other tasks may result in unexpected results as the model is not trained for these tasks.\n- **Sample**: The image sample. For example, \"fixed COS-7 cell line\".\n- **Structure**: The imaging structure. For example, \"microtubules\".\n- **Fluorescence indicator**: The fluorescence indicator. For example, \"mEmerald (GFP)\".\n- **INPUT**: The imaging condition of image image.\n    - **Microscope**: The microscope used for imaging. Such as, \"wide-field microscope\".\n    - **Mircoscopy params**: The microscope parameters. For example, \"with excitation numrical aperture (NA) of 1.35, detection namerical aperture (NA) of 1.3\".\n    - **Pixel size**: The pixel size of the image. For example, \"62.6 x 62.6 nm\".\n\n- **OUTPUT**: The imaging condition of the target image.\n    - **Microscope**: The microscope used for imaging. Such as, \"linear structured illumination microscopy\".\n    - **Mircoscopy params**: The microscope parameters. For example, \"with excitation numrical aperture (NA) of 1.35, detection namerical aperture (NA) of 1.3\".\n    - **Pixel size**: The pixel size of the image. For example, \"62.6 x 62.6 nm\".\n\n#### RUN box\nThis box is used to start, stop, and watch the prediction process. Press the **run** button to start the prediction. Press the **stop** button to stop the prediction. The prediciton process will be shown in the progress bar.\n\n### Preprocess\nThis page is used for data preprocessing, including data patching and text embedding modules.\n#### IMAGE PATCHING box\n- **PATH**\n    - **Dataset Folder**: The folder containing the images to be patched. The images should be in `.tif` format with a shape of `(1, H, W)` or `(H, W)`. The model will patch the images one by one and save them into a folder named `#Dataset Folder#_p#patch size#_s#patch stride#_2d`.\n    - **Index File**: This should be a `.txt` file containing all the file names of the images to be patched in each line. The file name should be the same as that of images in the **Dataset Folder**.\n\n- **PARAMETERS**\n    - **Patch size**: The size of the patch. Deault is `64`, which is same as that used for FluoResFM pretraining.\n    - **Patch stride**: The stride of the patch. Deault is `64`, i.e., no overlap between patches, which is same as that used for FluoResFM pretraining.\n    - **Normalization (low)**: The lower bound of the percentile-based normalization. Deault is `0.03`.\n    - **Normalization (high)**: The upper bound of the percentile-based normalization. Deault is `0.995`.\n\n- **RUN**\n\n    This box is used to start, stop, and watch the preprocessing process. Same function as the **RUN box** in the **Predict** page.\n\n#### EMBEDDING box\n- **PATH**\n    - **Excel File**: The excel file containing all the information for the datasets used for training or fine-tuning. The excel file should be in `.xlsx` format. The excel file should contain the all the columns as shown in the example data.\n    - **Output Folder**: The folder to save the text embeddings. The generated text will be saved into a `.txt` file named as `dataset_text_#Text type#.txt`. The corresponding text embedding will be saved into a folder named `dataset_text_#Text type#_#Context length#`. Each `.npy` file is for each dataset. The id is corresponding to the order of the dataset in the excel file.\n    - **Embedder**: The folder saved the text embedder model.\n\n- **PARAMETERS**\n    - **Device**: The device to run the model. Only support `cuda`.\n    - **Context length**: The context length of the text embedding. Deault is `160`, which is same as that used for FluoResFM pretraining.\n    - **Text type**: The type of the text. [\"ALL\", \"T\", \"TS\"], where \"ALL\" means all the text informatio will be used, \"T\" means only the task informaiton will be used, and \"TS\" means only the task and structure informaiton will be used.\n\n- **RUN**: This box is used to start, stop, and watch the preprocessing process. Same function as the **RUN box** in the **Predict** page.\n\n### Train\nThis page is used for model training.\n#### PATH box\n- **Information Folder**: The folder containing the information for the datasets used for training or fine-tuning, includeingt the path of input and reference images and the path of their corresponding index files. Other information should be same as the provided example.\n- **Text Embedding**: The folder containing the text embeddings for the datasets used for training or fine-tuning, which should be generated first using the **EMBEDDING box** in the **Preprocess** page.\n- **Checkpoint (load from)**: The pre-trained FluoResFM model checkpoint with a suffix `.pt`. If not specified, the model will be trained from scratch.\n- **Finetune**: Whether to fine-tune the model. If checked, **Checkpoint (load from)** must be specified and will be fine-tuned (only the first and last convolution layers will be trainable). If not checked, all the parameters in the model wil be setted as trainable.\n- **Checkpoint (save to)**: The folder  to save the trained model checkpoint. The checkpoint will be saved into a folder named `unet_sd_c_mae_bs#bactch size#_lr_#learning rate#-160-res1-att0123`. It `finetune` is checked, the folder will be added a suffix of `-ft-in-out`.\n#### PARAMETERS box\n- **Device**: : The device to run the model. Only support `cuda`.\n- **Compile**: Whether to compile the model. The compiling of model will take a few minutes, but will accelerate the training/fine-tuning process and save the GPU memory. On Linux system, the compiling of model will be more efficient than on Windows system.\n- **Batch size**: The batch size used during training.\n- **Epochs**: The number of epochs used during training.\n- **Learning rate**: The start learning rate.\n- **Decay (every iter)**: The learning rate will decay every `#Decay (every iter)#` iterations. The decay rate is 0.5.\n- **Validation (every iter)**: The validation will be performed every `#Validation (every iter)#` iterations.\n- **Validation (fraction)**: The fraction of the dataset used for validation. If it is set as 0, the validation will not be performed. (0,1)*100% dataset will be used for validation.\n- **Save Model (every iter)**: The model will be saved every `#Save Model (every iter)#` iterations.\n\n#### RUN box\nThis box is used to start, stop, and watch the training process. Same function as the **RUN box** in the **Predict** page.\n\n### Log\nThis page is used to show the working log.\nPress the **CLEAR** button to clear the log.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-fluoresfm\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/qiqi-lu/napari-fluoresfm/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "FluoResFM"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "featureforest",
    "name": "featureforest",
    "display_name": "Feature Forest",
    "version": "0.1.2",
    "created_at": "2024-10-10",
    "modified_at": "2025-07-28",
    "authors": [
      "Mehdi Seifi",
      "Vera Galinova"
    ],
    "author_emails": [
      "Mehdi Seifi <mehdi.seifi@fht.org>",
      "Vera Galinova <vera.galinova@fht.org>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/featureforest/",
    "home_github": "https://github.com/juglab/featureforest",
    "home_other": "https://featureforest.github.io/",
    "summary": "A napari plugin for segmentation using vision transformer features",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "h5py",
      "iopath>=0.1.10",
      "magicgui",
      "matplotlib",
      "napari[all]",
      "numpy<2.2",
      "opencv-python",
      "pims",
      "pooch",
      "pynrrd",
      "qtpy",
      "scikit-image",
      "scikit-learn",
      "scipy",
      "tifffile",
      "timm",
      "torch>=2.5.1",
      "torchvision>=0.20.1",
      "tqdm>=4.66.1",
      "mkdocs-material; extra == 'dev'",
      "pre-commit; extra == 'dev'",
      "pytest; extra == 'dev'",
      "pytest-cov; extra == 'dev'",
      "sybil; extra == 'dev'",
      "tox; extra == 'dev'",
      "tox-gh-actions; extra == 'dev'"
    ],
    "package_metadata_description": "# Feature Forest\n\n[![License BSD-3](https://img.shields.io/pypi/l/featureforest.svg?color=green)](https://github.com/juglab/featureforest/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/featureforest.svg?color=green)](https://pypi.org/project/featureforest)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/featureforest)\n[![Python Version](https://img.shields.io/pypi/pyversions/featureforest.svg?color=green)](https://python.org)\n[![tests](https://github.com/juglab/featureforest/workflows/tests/badge.svg)](https://github.com/juglab/featureforest/actions)\n[![codecov](https://codecov.io/gh/juglab/featureforest/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/featureforest)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/featureforest)](https://napari-hub.org/plugins/featureforest)\n<!--[![Downloads](https://pepy.tech/badge/featureforest)](https://pepy.tech/project/featureforest)-->\n\n**A napari plugin for making image annotation using feature space of vision transformers and random forest classifier.**  \nWe developed a *napari* plugin to train a *Random Forest* model using extracted features of vision foundation models and just a few scribble labels provided by the user as input. This approach can do the segmentation of desired objects almost as well as manual segmentations but in a much shorter time with less manual effort.  \n\n----------------------------------\n\n## Documentation\nYou can check the documentation [here](https://juglab.github.io/featureforest/) (‚ö†Ô∏è work in progress!).  \n\n## Installation\nWe provided `install.sh` for Linux & Mac OS users, and `install.bat` for Windows users.  \nFirst you need to clone the repo:  \n```bash\ngit clone https://github.com/juglab/featureforest\ncd ./featureforest\n```\nNow run the installation script:  \n```bash\n# Linux or Mac OS\nsh ./install.sh\n```\n```bash\n# Windows\n./install.bat\n```\n\nFor developers that want to contribute to FeatureForest, you need to use this command to install the `dev` dependencies:  \n```bash\npip install -U \"featureforest[dev]\"\n```\nAnd make sure you have `pre-commit` installed in your environment, before committing changes:  \n```bash\npre-commit install\n```\n\nFor more detailed installation guide, check out [here](https://juglab.github.io/featureforest/install/).\n\n\n## Cite us\n\nSeifi, Mehdi, Damian Dalle Nogare, Juan Battagliotti, Vera Galinova, Ananya Kediga Rao, AI4Life Horizon Europe Programme Consortium, Johan Decelle, Florian Jug, and Joran Deschamps. \"FeatureForest: the power of foundation models, the usability of random forests.\" bioRxiv (2024): 2024-12. [DOI: 10.1101/2024.12.12.628025](https://www.biorxiv.org/content/10.1101/2024.12.12.628025v1.full)\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"featureforest\" is free and open source software.  \n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/juglab/featureforest/issues/new) along with a detailed description.  \n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[conda]: https://conda.io/projects/conda/en/latest/index.html\n[mamba]: https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Feature Extractor",
      "Segmentation Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-copick",
    "name": "napari-copick",
    "display_name": "copick",
    "version": "1.0.1",
    "created_at": "2025-07-27",
    "modified_at": "2025-07-27",
    "authors": [
      "Kyle Harrington",
      "Utz H. Ermel"
    ],
    "author_emails": [
      "Kyle Harrington <czi@kyleharrington.com>",
      "\"Utz H. Ermel\" <utz.ermel@czii.org>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-copick/",
    "home_github": "https://github.com/kephale/napari-copick",
    "home_other": null,
    "summary": "A plugin for collaborative annotation in cryoET using copick",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "click",
      "copick-shared-ui==0.2.0",
      "copick>=1.6.0",
      "fsspec",
      "magicgui",
      "napari",
      "napari-ome-zarr",
      "numpy",
      "pydantic>=2",
      "qtpy",
      "scikit-image",
      "trimesh",
      "zarr",
      "black>=25.1.0; extra == 'dev'",
      "hatch-vcs>=0.4.0; extra == 'dev'",
      "hatchling>=1.25.0; extra == 'dev'",
      "pre-commit>=4.2.0; extra == 'dev'",
      "ruff>=0.12.0; extra == 'dev'",
      "napari; extra == 'testing'",
      "pyqt6; extra == 'testing'",
      "pytest; extra == 'testing'",
      "pytest-cov; extra == 'testing'",
      "pytest-qt; extra == 'testing'",
      "tox; extra == 'testing'",
      "tox-gh-actions; extra == 'testing'",
      "tox-uv; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-copick\n\n[![License MIT](https://img.shields.io/pypi/l/napari-copick.svg?color=green)](https://github.com/kephale/napari-copick/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-copick.svg?color=green)](https://pypi.org/project/napari-copick)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-copick.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-copick/workflows/tests/badge.svg)](https://github.com/kephale/napari-copick/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-copick/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-copick)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-copick)](https://napari-hub.org/plugins/napari-copick)\n\nA plugin for collaborative annotation in cryoET using copick\n\n![interface.png](https://github.com/copick/napari-copick/raw/main/docs/assets/interface.png)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-copick` via [pip]:\n\n    pip install napari-copick\n\nTo install latest development version:\n\n    pip install git+https://github.com/copick/napari-copick.git\n\n## Usage\n\n### Using a copick config file\n\n```bash\nnapari-copick run --config path/to/copick_config.json\n```\n\n### Using dataset IDs from CZ cryoET Data Portal\n\n```bash\nnapari-copick run --dataset-ids 10440 10441 --overlay-root /path/to/overlay_root\n```\n\nYou can specify multiple dataset IDs separated by spaces.\n\n### GUI Usage\n\nThe plugin provides an intuitive interface with two loading options:\n\n1. **Load Config File**: Opens a file dialog to select a copick configuration JSON file\n2. **Load from Dataset IDs**: Opens a dialog to enter CZ cryoET Data Portal dataset IDs and overlay root path\n\nAfter loading, you'll see a hierarchical tree of the project structure that you can navigate to access tomograms, segmentations, and picks.\n\n### Tomogram Handling\n\nnapari-copick now handles multiscale zarr arrays directly:\n\n- Automatically detects and loads all available resolution levels\n- Creates a proper multiscale image stack using napari's native multiscale API\n- Uses dask for efficient lazy loading of large tomogram data\n- Applies appropriate scaling factors based on the voxel size metadata\n\nThis direct zarr handling provides better performance and more flexibility compared to relying on external plugins.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-copick\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-copick/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n## Code of Conduct\n\nThis project adheres to the Contributor Covenant [code of conduct](https://github.com/chanzuckerberg/.github/blob/main/CODE_OF_CONDUCT.md). By participating, you are expected to uphold this code. Please report unacceptable behavior to [opensource@chanzuckerberg.com](mailto:opensource@chanzuckerberg.com).\n\n## Reporting Security Issues\n\nIf you believe you have found a security issue, please responsibly disclose by contacting us at [security@chanzuckerberg.com](mailto:security@chanzuckerberg.com).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari copick"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-label-manager",
    "name": "napari-label-manager",
    "display_name": "Label Manager",
    "version": "0.1.4",
    "created_at": "2025-07-10",
    "modified_at": "2025-07-26",
    "authors": [
      "JH Wang"
    ],
    "author_emails": [
      "wjh19937458882@mail.ustc.edu.cn"
    ],
    "license": "Copyright (c) 2025, JH Wang\nAl...",
    "home_pypi": "https://pypi.org/project/napari-label-manager/",
    "home_github": "https://github.com/Wenlab/napari-label-manager",
    "home_other": null,
    "summary": "A plugin for management of label colormap generation and opacity control",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-label-manager\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-label-manager.svg?color=green)](https://github.com/Wenlab/napari-label-manager/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-label-manager.svg?color=green)](https://pypi.org/project/napari-label-manager)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-label-manager.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-label-manager)](https://napari-hub.org/plugins/napari-label-manager)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n## Description\nThis is a plugin for management of label colormap generation and opacity control.\n- Select your label layer from the dropdown\n- Generate a new colormap or use existing colors\n- Specify target label IDs (e.g., \"1-5,10,15-20\")\n- Adjust opacity for selected labels and background\n- Apply changes to visualize your selection\n\n## Features\n\n### Label Management\n- Batch management of label colors and opacity\n- Random colormap generation with customizable seeds\n- Support for label ID ranges and individual selections\n- Quick presets for common label selections (first 10, even/odd IDs, all current)\n\n### Label Annotation\n- **NEW**: Excel-like annotation table for labeling digital IDs\n- Fill ranges of label IDs automatically\n- Load current layer's labels into annotation table\n- Add custom annotations/descriptions for each label\n- Export annotations to Excel format (.xlsx)\n\n### Performance Optimizations\n- Memory-efficient processing for large datasets\n- Time-series optimization (processes current slice only)\n- Smart sampling strategies for extremely large arrays\n- Background computation to maintain UI responsiveness\n\n## Installation\n\nYou can install `napari-label-manager` via [pip]:\n\n```\npip install napari-label-manager\n```\n\nIf napari is not already installed, you can install `napari-label-manager` with napari and Qt via:\n\n```\npip install \"napari-label-manager[all]\"\n```\n\n### For Excel Export and Load Functionality\n\nTo enable Excel export features for label annotations, install the optional and pandas dependency:\n\n```\npip install openpyxl pandas\n```\n\nOr install everything together:\n\n```\npip install napari-label-manager openpyxl pandas\n```\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-label-manager\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "LabelManager"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "seu-3d",
    "name": "seu-3d",
    "display_name": "seu-3d",
    "version": "1.1.11",
    "created_at": "2025-05-27",
    "modified_at": "2025-07-26",
    "authors": [
      "pxieLab"
    ],
    "author_emails": [
      "pxieLab <220242543@seu.edu.cn>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/seu-3d/",
    "home_github": "https://github.com/DingAnZhong/SEU-3D",
    "home_other": null,
    "summary": "3d spatial visualization napari plugin",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": null,
    "package_metadata_description": "# SEU-3D\n\n## description\n\n3d visualization and analysis plugin for spatial transcription embryo base on napari\n\n## updata log\n\n[1.1.11] fix buges\n\n[1.1.10] Fix buges and optimize SelectXYZ\n\n[1.1.9] Multi-gene analysis adjusts image contrast by channel\n\n[1.1.8] 1.A Z-axis control slider has been added\n        2.A color bar was added to a single gene analysis\n        3.Contrast adjustment has been added in moran's I\n        4.Convert all contrast adjustment logic to quantiles\n        5.Select tissue by legend tab available\n\n[1.1.7] fix bug of one gene adjust contrast darker,expand adjust contrast function to 1/2/3gene analyse\n\n[1.1.6] Expand select XY to select XYZ\n\n[1.1.5] 1.Fixed the bug where visible cells decreased with the number of operations\n        2.When creating a flatten layer, the visibility inherits from the original visibility\n        3.Unify the length of the mask variable when calculating and plotting\n        4.The original situation where the threshold constraints of each gene were mutually inherited has been changed. Now, they are all inherited from tissue_filter\n\n[1.1.4] 1.Fixed the bug related to multi-gene screening \n        2.The calculation and plotting of moran's I have been optimized\n\n[1.1.3] fix buges\n\n[1.1.2] enrich function\n\n[1.1.1] change color_map\n\n[1.1.0] rebuild whole package\n\n## acknowledge\n\nhttps://github.com/GuignardLab/sc3D\nhttps://github.com/GuignardLab/napari-sc3D-viewer\n\n## environment\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Load spatial transcriptomics data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-animation",
    "name": "napari-animation",
    "display_name": "napari-animation",
    "version": "0.0.9",
    "created_at": "2021-04-23",
    "modified_at": "2025-07-23",
    "authors": [
      "Nicholas Sofroniew",
      "Alister Burt",
      "Guillaume Witz",
      "Faris Abouakil",
      "Talley Lambert",
      "napari"
    ],
    "author_emails": [],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-animation/",
    "home_github": "https://github.com/napari/napari-animation",
    "home_other": null,
    "summary": "A plugin for making animations in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "imageio",
      "imageio-ffmpeg",
      "napari>=0.5",
      "npe2",
      "numpy",
      "qtpy",
      "scipy",
      "tqdm>=4.56.0",
      "superqt",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "tox; extra == \"testing\"",
      "sphinx>6; extra == \"doc\"",
      "sphinx-autobuild; extra == \"doc\"",
      "sphinx-external-toc; extra == \"doc\"",
      "sphinx-copybutton; extra == \"doc\"",
      "sphinx-gallery; extra == \"doc\"",
      "sphinx-favicon; extra == \"doc\"",
      "sphinxcontrib-video; extra == \"doc\"",
      "matplotlib; extra == \"doc\"",
      "myst-nb; extra == \"doc\"",
      "napari-sphinx-theme>=0.3.0; extra == \"doc\"",
      "pre-commit; extra == \"dev\"",
      "black; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "check-manifest; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "sphinx>6; extra == \"dev\"",
      "sphinx-autobuild; extra == \"dev\"",
      "sphinx-external-toc; extra == \"dev\"",
      "sphinx-copybutton; extra == \"dev\"",
      "sphinx-gallery; extra == \"dev\"",
      "sphinx-favicon; extra == \"dev\"",
      "sphinxcontrib-video; extra == \"dev\"",
      "matplotlib; extra == \"dev\"",
      "myst-nb; extra == \"dev\"",
      "napari-sphinx-theme>=0.3.0; extra == \"dev\""
    ],
    "package_metadata_description": "# napari-animation\n\n[![License](https://img.shields.io/pypi/l/napari-animation.svg?color=green)](https://github.com/napari/napari-animation/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-animation.svg?color=green)](https://pypi.org/project/napari-animation)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-animation.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari/napari-animation/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/napari/napari-animation/actions)\n[![codecov](https://codecov.io/gh/napari/napari-animation/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/napari-animation)\n\n**napari-animation** is a plugin for making animations in [napari](https://napari.org).\n\n<p align=\"center\">\n  <img width=\"500\" src=\"https://user-images.githubusercontent.com/7307488/196110138-6c4663b1-67b2-4c79-97b7-57b706d1d49c.gif\">\n</p>\n\n----------------------------------\n\n[Merlin Lange](https://twitter.com/Merlin_Lange) used *napari-animation* to create one of [Nature's best science images for September 2022](https://www.nature.com/immersive/d41586-022-03051-6/index.html)\n\n----------------------------------\n\nThis plugin is built on [`naparimovie`](https://github.com/guiwitz/naparimovie) from [@guiwitz](https://github.com/guiwitz). `naparimovie` was submitted to napari in [PR#851](https://github.com/napari/napari/pull/780) before napari plugin infrastructure existed.\n\n----------------------------------\n\n## Overview\n\n**napari-animation** provides a framework for the creation of animations in napari. The plugin contains:\n\n- an easy to use GUI for creating animations interactively;\n- a Python package for the programmatic creation of animations.\n\nThis plugin remains under development and contributions are very welcome, please open an issue to discuss potential improvements.\n\nYou can read the documentation at [https://napari.org/napari-animation](https://napari.org/napari-animation)\n\n## Installation\n\n### PyPI\n`napari-animation` is available through the Python package index and can be installed using `pip`.\n\n```sh\npip install napari-animation\n```\n\n```{warning}\n`napari-animation` uses `ffmpeg` to export animations. If you are using a macOS arm64 computer (Apple Silicon e.g. M1, M2 processor)\nthe PyPI package does not include the needed binary for your platform. You will need to install `ffmpeg` using\n`conda` from the [conda-forge channel](https://conda-forge.org/docs/#what-is-conda-forge) (`conda install -c conda-forge ffmpeg`)\nor using [`homebrew`](https://brew.sh) (`brew install ffmpeg`).\n```\n\n### Conda\n`napari-animation` is also available for install using `conda` through the [conda-forge channel](https://conda-forge.org/docs/#what-is-conda-forge).\n\n```sh\nconda install -c conda-forge napari-animation\n```\n\n### Local\nYou can clone this repository and install locally with\n\n    pip install -e .\n\n### Interactive use\n**napari-animation** can be used interactively.\n\nAn animation is created by capturing [keyframes](https://en.wikipedia.org/wiki/Key_frame) containing the current viewer state.\n\n<p align=\"center\">\n  <img width=\"600\" src=\"https://user-images.githubusercontent.com/7307488/196113682-96ce0da3-fa5c-411e-8fb1-52dc3a8f96b6.png\">\n</p>\n\nTo activate the GUI, select **napari-animation: wizard** from the *plugins menu*\n\n<p align=\"center\">\n  <img width=\"200\" src=\"https://user-images.githubusercontent.com/7307488/196114466-56cb5985-0d79-4cfa-96f1-38cf3ccfbc48.png\">\n</p>\n\n### Scripting\n\n**napari-animation** can also be run programatically, allowing for reproducible, scripted creation of animations.\n\n```python\nfrom napari_animation import Animation\n\nanimation = Animation(viewer)\n\nviewer.dims.ndisplay = 3\nviewer.camera.angles = (0.0, 0.0, 90.0)\nanimation.capture_keyframe()\nviewer.camera.zoom = 2.4\nanimation.capture_keyframe()\nviewer.camera.angles = (-7.0, 15.7, 62.4)\nanimation.capture_keyframe(steps=60)\nviewer.camera.angles = (2.0, -24.4, -36.7)\nanimation.capture_keyframe(steps=60)\nviewer.reset_view()\nviewer.camera.angles = (0.0, 0.0, 90.0)\nanimation.capture_keyframe()\nanimation.animate('demo.mov', canvas_only=False)\n```\n\n## Examples\n\nExamples can be found in our [Examples gallery](https://napari.org/napari-animation/gallery), generated from [our example scripts](https://github.com/napari/napari-animation/tree/main/examples). Simple examples for both interactive and headless\nuse of the plugin follow.\n\n## Contributing\n\nContributions are very welcome and a detailed contributing guide is coming soon.\nIn the meantime, clone this repository and install it in editable mode using `pip`:\n\n```\npip install -e .\n```\nWe recommend using a virtual environment, for example `conda`.\n\n\n```{important}\nEnsure you have a suitable Qt backend for napari! We recommend `PyQt5`.\nFor more information, see the napari [Qt backend installation guide](https://napari.org/stable/tutorials/fundamentals/installation.html#choosing-a-different-qt-backend)\n```\n\nTo set up your development installation, clone this repository, navigate to the clone folder, and install napari-animation in editable mode using `pip`.\n\n```sh\nconda create -n nap-anim python=3.10\nconda activate nap-anim\npip install -e \".[dev]\" PyQt5\n\n```\n\nTests are run with `pytest`.\nYou can make sure your `[dev]` installation is working properly by running\n`pytest .` from within the repository.\n\n```{note}\nWe use [`pre-commit`](https://pre-commit.com) to sort imports and lint with\n[`ruff`](https://github.com/astral-sh/ruff) and format code with\n[`black`](https://github.com/psf/black) automatically prior to each commit.\nTo minmize test errors when submitting pull requests, please install `pre-commit`\nin your environment as follows:\n\n`pre-commit install`\n```\n\n## Documentation\n\nThe documentation is available at [https://napari.org/napari-animation](https://napari.org/napari-animation)\n\nThe documentation for napari-animation is built with [Sphinx](https://www.spinx-doc.org) and the [napari Sphinx Theme](https://github.com/napari/napari-sphinx-theme).\n\n### Building docs locally\n\nAfter installing the documentation dependencies with\n\n```sh\npip install \".[doc]\"\n```\n\nyou can see a local version of the documentation by running\n\n```sh\nmake docs\n```\n\nOpen up the `docs/_build/index.html` file in your browser, and you'll see the home page of the docs being displayed.\n\n### Deploying docs\n\nThe napari-animation documentation is automatically built and deployed to the website\nwhenever the main branch is updated, or a new release is tagged.\nThis is controlled by the [deploy_docs.yml](https://github.com/napari/napari-animation/blob/main/.github/workflows/deploy_docs.yml) github actions script.\n\nYou can also manually trigger a documenation re-build and deployment [from the github actions tab](https://github.com/napari/napari-animation/actions/workflows/deploy_docs.yml).\n\n## License\n\nDistributed under the terms of the [BSD-3 license](http://opensource.org/licenses/BSD-3-Clause),\n`napari-animation` is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/napari/napari-animation/issues) along with a detailed description.\n\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/napari/napari-animation/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "wizard"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bigfish",
    "name": "napari-bigfish",
    "display_name": "BigFISH smFISH Analysis",
    "version": "0.8",
    "created_at": "2023-04-07",
    "modified_at": "2025-07-23",
    "authors": [
      "Volker Baecker"
    ],
    "author_emails": [
      "volker.baecker@mri.cnrs.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-bigfish/",
    "home_github": "https://github.com/MontpellierRessourcesImagerie/napari-bigfish",
    "home_other": null,
    "summary": "A napari-plugin providing an alternative GUI for Big-FISH. Big-FISH is a python package for the analysis of smFISH images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pyperclip",
      "big-fish",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": " # napari-bigfish\n\n[![License MIT](https://img.shields.io/pypi/l/napari-bigfish.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/napari-bigfish/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bigfish.svg?color=green)](https://pypi.org/project/napari-bigfish)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bigfish.svg?color=green)](https://python.org)\n[![tests](https://github.com/MontpellierRessourcesImagerie/napari-bigfish/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/napari-bigfish/actions)\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-bigfish/branch/main/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-bigfish)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bigfish)](https://napari-hub.org/plugins/napari-bigfish)\n\nA napari-plugin providing an alternative GUI for [Big-FISH](https://github.com/fish-quant/big-fish). Big-FISH is a python package for the analysis of smFISH images.\n\nThe plugin provides a graphical user interface for some of the functionality in Big-FISH. Currently implemented are:\n\n * Gaussian-background subtraction\n * FISH-spot detection with \n\t* Elimination of duplicates\n\t* Auto-detection of threshold\n* Dense-region decomposition\n\nThe plugin further implements by itself:\n\n* Counting of spots per cell, inside and outside of the nucleus\n* Batch processing on a list of images\n\n\nYou can find the user and the api-documentation of napari-bigfish [here](https://montpellierressourcesimagerie.github.io/napari-bigfish/).\n \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-bigfish` via [pip]:\n\n    pip install napari-bigfish\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-bigfish\" is free and open source software\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Detect FISH spots"
    ],
    "contributions_sample_data": [
      "BigFISH smFISH Analysis"
    ]
  },
  {
    "normalized_name": "napari-geff",
    "name": "napari-geff",
    "display_name": "Geff IO",
    "version": "0.0.1",
    "created_at": "2025-07-23",
    "modified_at": "2025-07-23",
    "authors": [
      "Live Image Tracking Tools (LITT) team"
    ],
    "author_emails": [],
    "license": "Copyright (c) 2025, Live Image...",
    "home_pypi": "https://pypi.org/project/napari-geff/",
    "home_github": null,
    "home_other": "None",
    "summary": "A reader and writer for the graph exchange file format (geff)",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "geff==0.4.0",
      "pandas",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-geff\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-geff.svg?color=green)](https://github.com/live-image-tracking-tools/napari-geff/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-geff.svg?color=green)](https://pypi.org/project/napari-geff)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-geff.svg?color=green)](https://python.org)\n[![tests](https://github.com/live-image-tracking-tools/napari-geff/workflows/tests/badge.svg)](https://github.com/live-image-tracking-tools/napari-geff/actions)\n[![codecov](https://codecov.io/gh/live-image-tracking-tools/napari-geff/branch/main/graph/badge.svg)](https://codecov.io/gh/live-image-tracking-tools/napari-geff)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-geff)](https://napari-hub.org/plugins/napari-geff)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA reader and writer for the graph exchange file format (geff)\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n![geff-read](https://github.com/user-attachments/assets/bd3d510e-a9c8-490a-b499-47093f15105d)\n\n\n## Installation\n\nYou can install `napari-geff` via [pip]:\n\n```\npip install napari-geff\n```\n\nIf napari is not already installed, you can install `napari-geff` with napari and Qt via:\n\n```\npip install \"napari-geff[all]\"\n```\n\n## Usage\n\n`napari-geff` supports loading directed graphs stored as [GEFF](https://live-image-tracking-tools.github.io/geff/latest/) files into\n`napari` as `Tracks` layers, and saving them back out to GEFF format.\n\nTo use `napari-geff` after installation, simply drag a GEFF file into the viewer and select `GEFF IO` from the\nplugin selection dialog, if required. The file will be loaded as a `Tracks` layer.\n\nAny node properties defined on your graph will be stored as features on your tracks layer. Edge properties\nwill be available under `layer.metadata['edge_properties']` as a dictionary, but cannot currently be displayed\nor used for visualization in `napari`.\n\nIf your file contains `image` or `labels` related objects as per the GEFF\n[spec](https://live-image-tracking-tools.github.io/geff/v0.4.0/specification/#geff_related_objects),\nthese will also be loaded alongside your `Tracks` layer.\n\nIf you wish to open your geff file into layers **programmatically**, you can do so using the `viewer.open` method:\n\n```python\nimport napari\npath = 'path/to/top_level_zarr.zarr/my-geff-group/\n\nviewer = napari.Viewer()\nlayers = viewer.open(path, plugin='napari-geff')\n```\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-geff\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "Sample tracks data, w masks and images",
      "Sample tracks data, 3D"
    ]
  },
  {
    "normalized_name": "napari-sam4is",
    "name": "napari-SAM4IS",
    "display_name": "napari-SAM4IS",
    "version": "0.1.1",
    "created_at": "2023-04-28",
    "modified_at": "2025-07-23",
    "authors": [
      "Hiroki Kawai"
    ],
    "author_emails": [
      "Hiroki Kawai <h.kawai888@gmail.com>"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-sam4is/",
    "home_github": "https://github.com/hiroalchem/napari-SAM4IS",
    "home_other": null,
    "summary": "Create annotations for instance segmentation using Segment Anything models",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy>=1.20.0",
      "magicgui>=0.7.0",
      "qtpy>=2.0.0",
      "torch>=1.12.0",
      "torchvision>=0.13.0",
      "scikit-image>=0.19.0",
      "napari[all]>=0.4.17",
      "requests>=2.25.0",
      "urllib3>=1.26.0",
      "Pillow>=8.0.0",
      "tox; extra == \"testing\"",
      "pytest>=6.0; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "black>=22.0; extra == \"dev\"",
      "ruff>=0.1.0; extra == \"dev\"",
      "pre-commit; extra == \"dev\""
    ],
    "package_metadata_description": "# napari-SAM4IS\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-SAM4IS.svg?color=green)](https://github.com/hiroalchem/napari-SAM4IS/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-SAM4IS.svg?color=green)](https://pypi.org/project/napari-SAM4IS)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-SAM4IS.svg?color=green)](https://python.org)\n[![tests](https://github.com/hiroalchem/napari-SAM4IS/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-SAM4IS/actions)\n[![codecov](https://codecov.io/gh/hiroalchem/napari-SAM4IS/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-SAM4IS)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-SAM4IS)](https://napari-hub.org/plugins/napari-SAM4IS)\n\n\n### napari plugin for instance and semantic segmentation annotation using Segment Anything Model (SAM)\n\nThis is a plugin for [napari](https://napari.org/), a multi-dimensional image viewer for Python, that allows for instance and semantic segmentation annotation. This plugin provides an easy-to-use interface for annotating images with the option to output annotations as COCO format.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\n### Step 1: Install napari-SAM4IS\n\nYou can install `napari-SAM4IS` via [pip]:\n\n```bash\npip install napari-SAM4IS\n```\n\nOr via conda\n\n```bash\nconda install -c conda-forge napari-SAM4IS\n```\n\n\n### Step 2: Install Segment Anything Model\n\n**IMPORTANT**: You must install the Segment Anything Model separately to use this plugin:\n\n```bash\npip install git+https://github.com/facebookresearch/segment-anything.git\n```\n\n### Development Installation\n\nTo install the latest development version:\n\n```\npip install git+https://github.com/facebookresearch/segment-anything.git\n```\n\nOr you can install from source by cloning the repository:\n\n```\ngit clone https://github.com/facebookresearch/segment-anything.git\ncd segment-anything\npip install -e .\n```\n\nFor more detailed instructions, please refer to the [SAM installation guide](https://github.com/facebookresearch/segment-anything#installation).\n\n### napari-SAM4IS Installation\n\nYou can install `napari-SAM4IS` via [pip]:\n\n    pip install napari-SAM4IS\n\n\nOr via conda\n\n    conda install -c conda-forge napari-SAM4IS\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hiroalchem/napari-SAM4IS.git\n\n## Usage\n### Preparation\n1. Open an image in napari and launch the plugin. (Opening an image after launching the plugin is also possible.)\n2. Upon launching the plugin, three layers will be automatically created: SAM-Box, SAM-Predict, and Accepted. The usage of these layers will be explained later.\n3. In the widget that appears, select the model you want to use and click the load button. (The default option is recommended.)\n4. Next, select the image layer you want to annotate.\n5. Then, select whether you want to do instance segmentation or semantic segmentation. (Note that for 3D images, semantic segmentation should be chosen in the current version.)\n6. Finally, select the output layer as \"shapes\" for instance segmentation or \"labels\" for semantic segmentation. (For instance segmentation, the \"Accept\" layer can also be used.)\n\n### Annotation\n1. Select the SAM-Box layer and use the rectangle tool to enclose the object you want to segment.\n2. An automatic segmentation mask will be created and output to the SAM-Predict layer.\n3. If you want to make adjustments, do so in the SAM-Predict layer.\n4. To accept or reject the annotation, press \"a\" or \"r\" on the keyboard, respectively.\n5. If you accept the annotation, it will be output as label 1 for semantic segmentation or converted to a polygon and output to the designated layer for instance segmentation.\n6. If you reject the annotation, the segmentation mask in the SAM-Predict layer will be discarded.\n7. After accepting or rejecting the annotation, the SAM-Predict layer will automatically reset to blank and return to the SAM-Box layer.\n\n### Saving\n1. If you have output to the labels layer, use napari's standard functionality to save the mask.\n2. If you have output to the shapes layer, you can save the shapes layer using napari's standard functionality, or you can click the \"save\" button to output a JSON file in COCO format for each image in the folder. (The JSON file will have the same name as the image.)\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-SAM4IS\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hiroalchem/napari-SAM4IS/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SAM Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-simpleannotate",
    "name": "napari-simpleannotate",
    "display_name": "SimpleAnnotate",
    "version": "0.1.2",
    "created_at": "2023-10-29",
    "modified_at": "2025-07-23",
    "authors": [
      "Hiroki Kawai"
    ],
    "author_emails": [
      "Hiroki Kawai <h.kawai888@gmail.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-simpleannotate/",
    "home_github": "https://github.com/hiroalchem/napari-simpleannotate#README.md",
    "home_other": null,
    "summary": "A napari plugin for simple image and video annotation",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "pyyaml",
      "qtpy",
      "scikit-image",
      "pandas",
      "napari_video",
      "zarr",
      "numcodecs",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-simpleannotate\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-simpleannotate.svg?color=green)](https://github.com/hiroalchem/napari-simpleannotate/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-simpleannotate.svg?color=green)](https://pypi.org/project/napari-simpleannotate)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-simpleannotate.svg?color=green)](https://python.org)\n[![tests](https://github.com/hiroalchem/napari-simpleannotate/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-simpleannotate/actions)\n[![codecov](https://codecov.io/gh/hiroalchem/napari-simpleannotate/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-simpleannotate)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-simpleannotate)](https://napari-hub.org/plugins/napari-simpleannotate)\n\nA napari plugin for simple image and video annotation that provides three main annotation workflows:\n\n1. **Bounding Box Annotation (YOLO format)**: For object detection training data on images\n2. **Video Bounding Box Annotation**: For object detection training data on video files\n3. **Image Classification Labeling**: For image classification training data\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n![overview](https://github.com/hiroalchem/napari-simpleannotate/raw/main/images/dog_and_cat.jpg)\n\n\n## Installation\n\nYou can install `napari-simpleannotate` via [pip]:\n\n    pip install napari-simpleannotate\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hiroalchem/napari-simpleannotate.git\n\n\n## How to use\n\n### Getting Started\n\nAfter installing napari-simpleannotate, launch napari and navigate to `Plugins > Add dock widget` to find three annotation widgets:\n\n- **Bbox annotation**: For bounding box annotation on images\n- **Bbox video annotation**: For bounding box annotation on video files  \n- **Label image classification**: For image classification labeling\n\n### Bounding Box Annotation (Images)\n\n**Prerequisites**: None required\n\n1. **Opening Files**:\n   - Single file: Click `Open File` to select an image file\n   - Directory: Click `Open Directory` to select a folder containing images\n   - If a `class.yaml` file exists in the directory, you'll be prompted to load existing classes\n\n2. **Class Management**:\n   - Enter class names in the text box and click `Add class`\n   - Classes are automatically assigned sequential IDs (0, 1, 2, ...)\n   - Select a class and click `Delete selected class` to remove it\n   - Classes are saved to `class.yaml` alongside annotations\n\n3. **Creating Annotations**:\n   - Select a class from the list (becomes your active class)\n   - Use napari's rectangle tool (shortcut: R) to draw bounding boxes\n   - New rectangles automatically inherit the selected class\n   - Change existing rectangles: select the shape, then click a different class\n\n4. **Saving Work**:\n   - Click `Save Annotations` to export in YOLO format\n   - Files saved: `image_name.txt` (YOLO coordinates) + `class.yaml` (class definitions)\n   - YOLO format: `class_id x_center y_center width height` (normalized 0-1)\n\n### Video Bounding Box Annotation\n\n**Prerequisites**: Install PyAV for video support: `pip install av`\n\n1. **Opening Videos**:\n   - Click `Open Video` to select video files\n   - Supported formats: MP4, AVI, MOV, MKV, WMV, FLV, WebM\n   - Video loads with frame-by-frame navigation\n\n2. **Navigation**:\n   - Use napari's time slider to navigate frames\n   - Frame counter shows current position: \"Frame: X/Y\"\n   - **Keyboard shortcuts**: Q (previous annotation), W (next annotation)\n   - Click navigation buttons to jump to nearest annotations\n   - Video performance optimized with LRU cache and parallel prefetching\n\n3. **Frame-Aware Annotation**:\n   - Navigate to target frame before annotating\n   - Create bounding boxes with napari's rectangle tool\n   - Each annotation automatically records the current frame number\n   - Annotations only visible on their respective frames\n\n4. **Class and Export**:\n   - Class management identical to image annotation\n   - Extended YOLO format: `class_id frame x_center y_center width height`\n   - Saves to `video_name.txt` + `class.yaml` in video directory\n\n### Image Classification Labeling\n\n**Prerequisites**: None required\n\n1. **Opening Directory**:\n   - Click `Open Directory` to select image folder\n   - Recursively finds all images (PNG, TIF, JPG, JPEG, TIFF)\n   - Automatically loads existing `labels.csv` and `class.txt` if present\n\n2. **Display Options**:\n   - **Split Channels**: Check to display multi-channel images as separate layers\n   - Contrast settings preserved when switching between images\n   - Navigate images using the file list on the left\n\n3. **Labeling Workflow**:\n   - Add classes: Type in text box and press Enter (or click `Add class`)\n   - Remove classes: Type existing class name and press Enter\n   - Assign labels: Select image ‚Üí Click class name to label it\n   - Real-time auto-save to `labels.csv` and `class.txt`\n\n4. **Resume Sessions**:\n   - Previous work automatically loaded when reopening directories\n   - Continue labeling from where you left off\n\n## Performance Notes\n\n- **Video annotation**: Optimized with frame caching and parallel prefetching for smooth playback\n- **Large datasets**: Classification widget handles thousands of images efficiently  \n- **Memory management**: LRU cache prevents memory overflow during long annotation sessions\n\n## Output Formats\n\n| Widget | Annotation File | Class File | Format |\n|--------|----------------|------------|---------|\n| Bbox (Images) | `image.txt` | `class.yaml` | YOLO standard |\n| Bbox (Video) | `video.txt` | `class.yaml` | Extended YOLO with frame |\n| Classification | `labels.csv` | `class.txt` | CSV with image-label pairs |\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-simpleannotate\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hiroalchem/napari-simpleannotate/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Bbox annotation",
      "Label image classification",
      "Bbox video annotation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-serialcellpose",
    "name": "napari-serialcellpose",
    "display_name": "serialcellpose",
    "version": "0.3.0",
    "created_at": "2022-07-28",
    "modified_at": "2025-07-22",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-serialcellpose/",
    "home_github": "https://github.com/guiwitz/napari-serialcellpose",
    "home_other": null,
    "summary": "A simple plugin to batch segment cells with cellpose",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "cellpose<4",
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "napari-skimage-regionprops",
      "bioio",
      "bioio-tifffile",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-serialcellpose\n\n[![License](https://img.shields.io/pypi/l/napari-serialcellpose.svg?color=green)](https://github.com/guiwitz/napari-serialcellpose/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-serialcellpose.svg?color=green)](https://pypi.org/project/napari-serialcellpose)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-serialcellpose.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-serialcellpose/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-serialcellpose/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-serialcellpose/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-serialcellpose)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-serialcellpose)](https://napari-hub.org/plugins/napari-serialcellpose)\n\nThis napari plugin allows you to segment single images or series of images using built-in or custom Cellpose models as well as to analyze the properties of these segmented regions (\"region properties\"). Properties can be visualized for a single image or a complete experiment in the form of histograms that can also be filtered (e.g. based on area size, mean intensity etc.) Thanks to the [napari-skimage-regionprops](https://github.com/haesleinhuepf/napari-skimage-regionprops) plugin, properties of segmented objects can be interactively explored at a single object level.\n\n## Main goal\n\nThe main goal of this plugin is to simplify the classical image processing pipeline of image segmentation followed by region analysis via Cellpose. It allows to quickly get a quantification of a set of images without the need for any scripting.\n\n## Installation\n\nIn order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:\n\n    conda create -n serialcellpose python=3.8.5 napari -c conda-forge\n\nThen activate it and install the plugin:\n    \n    conda activate serialcellpose\n    pip install napari-serialcellpose\n\n### Potential issue with PyTorch\n\nCellpose and therefore the plugin and napari can crash without warning in some cases with ```torch==1.12.0```. This can be fixed by reverting to an earlier version using:\n    \n    pip install torch==1.11.0\n\n### GPU\n\nIn order to use a GPU:\n\n1. Uninstall the PyTorch version that gets installed by default with Cellpose:\n\n        pip uninstall torch\n\n2. Make sure your have up-to-date drivers for your NVIDIA card installed.\n\n3. Re-install a GPU version of PyTorch via conda using a command that you can find [here](https://pytorch.org/get-started/locally/) (this takes care of the cuda toolkit, cudnn etc. so **no need to install manually anything more than the driver**). The command will look like this:\n\n        conda install pytorch torchvision cudatoolkit=11.3 -c pytorch\n\n### Plugin Updates\n\nTo update the plugin, you only need to activate the existing environment and install the new version:\n\n    conda activate serialcellpose\n    pip install napari-serialcellpose -U\n\n## Usage: segmentation\n\nThe main interface is shown below. The sequence of events should be the following:\n\n1. Select a folder containing images. The list of files within that folder will appear in the area above. You can also just drag and drop a folder or an image in that area. When selecting an image, it gets displayed in the viewer. Images are opened via [aicsimageio](https://allencellmodeling.github.io/aicsimageio/). You can use grayscale images, RGB images or multi-channel images. In the latter case, **make sure each channel opens as a separate layer when you open them using the napari-aicsimagio importer**.\n2. If you want to save the segmentation and tables with properties, select a folder that will contain the output.\n3. Select the type of cellpose model.\n4. If you use a custom model, select its location.\n5. Run the analysis on the currently selected image or on all files in the folder.\n### Options\n\n6. Select if you want to use a GPU or not.\n7. If you are using multi-channel images, you can specify which channel to segment and optionally which to use as \"nuclei\" channel to help cell segmentation.\n8. In case you are using one of the built-in models, you can set the estimated diameter of your objects.\n9. In the Options tab you will find a few more options for segmentation, including the two thresholds ```flow_threshold``` and ```cellprob_threshold```. You can also decide to discard objects touching the border. Using the ```Select options yml file``` you can select a ```.yml``` file which contains a list of additional options to pass to the ```eval``` method of the Cellpose model. **Note that options specified in the yml file will override options set in the GUI**. The file [my_options.yml](https://raw.githubusercontent.com/guiwitz/napari-serialcellpose/main/src/napari_serialcellpose/_tests/my_options.yml) is an example of such a file where for example the ```diameter``` (also available in the GUI) and ```resample``` (not available in the GUI) options are set. \n\n<img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui1.png\" alt=\"image\" width=\"500\">\n<img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui1b.png\" alt=\"image\" width=\"500\">\n\n### Properties\n\n10. After segmentation, properties of the objects can automatically be computed. You can select which properties should be computed in the Options tab. As defined in ```napari-skimage-regionprops``` properties are grouped by types. If you want to measure intensity properties such as mean intensity, you have to specify which channel (```Analysis channel```) you want to perform the measurement on.\n\n### Output\n\nThe results of the analysis are saved in the folder chosen in #2. The segmentation mask is saved with the same name as the original image with the suffix ```_mask.tif```. A table with properties is saved in the subfolder ```tables``` also with the same name as the image with the suffix ```props.csv```. If you run the plugin on multiple files in a folder, a ```summary.csv``` file is also generated which compiles all the data.\n## Usage: post-processing\n\nAfter the analysis is done, when you select an image, the corresponding segmentation mask is shown on top of the image as shown below. This also works for saved segmentations: in that case you just select a folder with data and the corresponding output folder.\n\n<img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui2.png\" alt=\"image\" width=\"500\">\n\n### Properties\n\nIf you head to the **Properties** tab, you will find there two histograms showing the distribution of two properties that you can choose from a list at the top of the window. Below the plot you find the table containing information for each cell (each line is a cell).\n\nAs shown below, if you select the box ```show selected```, you can select items in the properties table and it will highlight the corresponding cell in the viewer. If you select the pipet tool, you can also select a cell and see the corresponding line in the table highlighted.\n\n<img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui3.png\" alt=\"image\" width=\"500\">\n\n### Summary\n\nFinally if you select the **Summary** tab, and click on ```Load summary```, it will load all data of the current output folder and create histograms of two properties that can be selected. An additional property can be used for filtering the data. Using the sliders, one can set a minimum and maximum threshold on the \"filtering property\", which will create a sub-selection of the data.\n\n<img src=\"https://github.com/guiwitz/napari-serialcellpose/raw/main/illustrations/napari_serialcellpose_gui4.png\" alt=\"image\" width=\"500\">\n\n## Data\n\nSample data were acquired by Fabian Blank at the DBMR, University of Bern.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-serialcellpose\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-serialcellpose/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Serial Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "mobiofox",
    "name": "mobiofox",
    "display_name": "MOBIOFOX",
    "version": "0.1.0",
    "created_at": "2025-07-21",
    "modified_at": "2025-07-21",
    "authors": [
      "Adrian Surojit M√ºller"
    ],
    "author_emails": [
      "a.s.mueller@student.vu.nl"
    ],
    "license": "Copyright (c) 2025, Adrian Sur...",
    "home_pypi": "https://pypi.org/project/mobiofox/",
    "home_github": "https://github.com/OwlSurojit/mobiofox",
    "home_other": null,
    "summary": "A napari plugin implementing a pipeline for MOrphometric BIOgenicity analysis of purported microFOssil (P)XCT scans.",
    "categories": [
      "Segmentation",
      "Measurement",
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "napari",
      "magicgui",
      "matplotlib",
      "qtpy",
      "scikit-image",
      "scipy",
      "pandas",
      "scikit-learn",
      "sip",
      "meshio",
      "seaborn",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# MOBIOFOX\n[![License BSD-3](https://img.shields.io/pypi/l/mobiofox.svg?color=green)](https://github.com/OwlSurojit/mobiofox/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/mobiofox.svg?color=green)](https://pypi.org/project/mobiofox)\n[![Python Version](https://img.shields.io/pypi/pyversions/mobiofox.svg?color=green)](https://python.org)\n[![tests](https://github.com/OwlSurojit/mobiofox/workflows/tests/badge.svg)](https://github.com/OwlSurojit/mobiofox/actions)\n[![codecov](https://codecov.io/gh/OwlSurojit/mobiofox/branch/main/graph/badge.svg)](https://codecov.io/gh/OwlSurojit/mobiofox)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/mobiofox)](https://napari-hub.org/plugins/mobiofox)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA napari plugin implementing a pipeline for MOrphometric BIOgenicity analysis of purported microFOssil (P)XCT scans.\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `mobiofox` via [pip]:\n\n    pip install mobiofox\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/OwlSurojit/mobiofox.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"mobiofox\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/OwlSurojit/mobiofox/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Full Pipeline",
      "1. Cropping",
      "2. Filtering",
      "3. Segmentation",
      "4. Morphometry",
      "Edit Metadata of selected layer",
      "Histogram"
    ],
    "contributions_sample_data": [
      "HG2 delta scan",
      "HG2 beta scan",
      "HG1 delta scan",
      "KB1 delta scan",
      "KB2 delta scan",
      "KB2 beta scan"
    ]
  },
  {
    "normalized_name": "affinder",
    "name": "affinder",
    "display_name": "affinder",
    "version": "0.5.0",
    "created_at": "2021-02-04",
    "modified_at": "2025-07-20",
    "authors": [
      "Juan Nunez-Iglesias"
    ],
    "author_emails": [
      "juan.nunez-iglesias@monash.edu"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/affinder/",
    "home_github": "https://github.com/jni/affinder",
    "home_other": null,
    "summary": "Quickly find the affine matrix mapping one image to another using manual correspondence points annotation",
    "categories": [
      "Transformations"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari>=0.4.17",
      "npe2>=0.1.2",
      "numpy",
      "scikit-image>=0.19.2",
      "magicgui>=0.3.7",
      "toolz",
      "coverage; extra == \"testing\"",
      "pydantic<2; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "scikit-image[data]; extra == \"testing\"",
      "napari[pyqt5]!=0.4.18; extra == \"testing\"",
      "pygments!=2.16; extra == \"testing\"",
      "zarr; extra == \"testing\"",
      "furo; extra == \"docs\"",
      "myst-parser; extra == \"docs\""
    ],
    "package_metadata_description": "# Description\n\nThis GUI plugin allows you to quickly find the affine matrix mapping\none image to another using manual correspondence points annotation.\n\nMore simply, this plugin allows you to select corresponding points\non an image, and a second image you wish to transform. It computes \nthe requisite transformation matrix using Affine Transform, Euclidean Transform, \nor Similarity Transform, and performs this transformation on the\nmoving image, aligning it to the reference image.\n\nhttps://user-images.githubusercontent.com/17995243/120086403-f1d0b300-c121-11eb-8000-a44a2ac54339.mp4\n\n\n# Who is This For?\n\nThis is a simple plugin which can be used on any 2D images, provided\nthey can be loaded as layers into napari. The images need not be the same\nfile format and this plugin also works with labels layers.\n\nNo prior understanding of the transformation methods is required, as\nthey perform in the background based on the reference points selected.\n\n# How to Guide\n\nYou will need a combination of two or more 2D image and/or labels layers \nloaded into napari. Once you have installed affinder, you can find it in\nthe dock widgets menu.\n\n![Affinder widget in the Plugins->Add Dock Widget menu](https://i.imgur.com/w7MCXQy.png)\n\nThe first two dropdown boxes will be populated with the layers currently\nloaded into napari. Select a layer to use as reference, and another to\ntransform.\n\n![Dropdowns allow you to select the reference and moving layers](https://i.imgur.com/Tdbm1sX.png)\n\nNext, you can select the transformation model to use (affine is selected by default\nand is the least rigid transformation of those available). See [below](#transformation-models) for a\ndescription of the different models.\n\nFinally, you can optionally select a path to a text file for saving out the\nresulting transformation matrix.\n\nWhen you click Start, affinder will add two points layers to napari. \nThe plugin will also bring your reference image in focus, and its associated points\nlayer. You can then start adding reference points by clicking on your image.\n\n![Adding reference points to layer](https://i.imgur.com/WPzNtyy.png)\n\nOnce three points are added, affinder will switch focus to the moving image,\nand you should then proceed to select the corresponding three points.\n\n![Adding corresponding points to newly focused layer](https://i.imgur.com/JVZCvmp.png)\n\naffinder will immediately transform the moving image to align the points you've\nselected when you add your third corresponding point to your moving image.\n\n![The moving image is transformed once three points are added](https://i.imgur.com/NTne9fj.png)\n\nFrom there, you can continue iteratively adding points until you \nare happy with the alignment. Affinder will switch focus between\nreference and moving image with each point.\n\nClick Finish to exit affinder.\n\n## Transformation Models\n\nThere are three transformation models available for use with affinder.\nThey are listed here in order of increasing rigidity in the types of\ntransforms they will allow. The eponymous Affine Transform is the \nleast rigid and is the default choice.\n\n- [**Affine Transform**](https://en.wikipedia.org/wiki/Affine_transformation): \nthe least rigid transformation, it preserves\nlines and parallelism, but not necessarily distance and angles. Translation,\nscaling, similarity, reflection, rotation and shearing are all valid\naffine transformations.\n\n- [**Similarity Transform**](https://en.wikipedia.org/wiki/Similarity_(geometry)): \nthis is a \"shape preserving\" transformation, producing objects which are \ngeometrically similar. Translation, rotation, reflection and uniform scaling are \nvalid similarity transforms. Shearing is not.\n\n- [**Euclidean Transform**](https://en.wikipedia.org/wiki/Rigid_transformation):\nAlso known as a rigid transformation, this transform preserves the Euclidean\ndistance between each pair of points on the image. This includes rotation,\ntranslation and reflection but not scaling or shearing.\n\n# Getting Help\n\nIf you find a bug with affinder, or would like support with using it, please raise an\nissue on the [GitHub repository](https://github.com/jni/affinder).\n\n# How to Cite\n\nMany plugins may be used in the course of published (or publishable) research, as well as\nduring conference talks and other public facing events. If you'd like to be cited in\na particular format, or have a DOI you'd like used, you should provide that information here.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Start affinder",
      "Copy affine",
      "Apply affine",
      "Load affine"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "alveoleye",
    "name": "AlveolEye",
    "display_name": "AlveolEye",
    "version": "0.1.7",
    "created_at": "2024-05-22",
    "modified_at": "2025-07-18",
    "authors": [
      "Joseph Hirsh"
    ],
    "author_emails": [
      "josephhirsh9@gmail.com"
    ],
    "license": "BSD",
    "home_pypi": "https://pypi.org/project/alveoleye/",
    "home_github": null,
    "home_other": "None",
    "summary": "Reads lung slides with AI-driven and classical methods",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "typeguard",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "qtpy; extra == \"testing\""
    ],
    "package_metadata_description": "# AlveolEye: Automated Lung Morphometry Made Easy\n\n[![Napari Plugin](https://img.shields.io/badge/Napari-Plugin-1157c4?logo=napari)](https://www.napari-hub.org/plugins/AlveolEye)\n![Python Version](https://img.shields.io/badge/Python-3.9%20|%203.10%20|%203.11-blue)\n![OS Support](https://img.shields.io/badge/Platform-Windows%20%7C%20macOS%20%7C%20Linux-blue)\n![GitHub Release](https://img.shields.io/github/v/release/SucreLab/AlveolEye?display_name=tag)\n![License](https://img.shields.io/github/license/SucreLab/AlveolEye)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/AlveolEye)](https://pypi.org/project/AlveolEye/)\n![Maintenance](https://img.shields.io/maintenance/yes/2025)\n![Last Commit](https://img.shields.io/github/last-commit/SucreLab/AlveolEye)\n![Issues](https://img.shields.io/github/issues/SucreLab/AlveolEye)\n\nThis repository contains the beta version of AlveolEye, created by the [Sucre lab](https://www.sucrelab.org/).  \nThe code is authored by Samuel Hirsh, Joseph Hirsh, Nick Negretti, and Shawyon Shirazi.\n\nAlveolEye is a Napari plugin that uses computer vision and classical image processing  \nto calculate mean linear intercept (MLI) and airspace volume density (ASVD) of histologic images.\n\nThe goal of this tool is to aid researchers, not provide a complete automated annotation solution.\n\nWe welcome GitHub issues and feedback!\n\n## Installation\n\nThe goal of this process is to create a conda environment containing Napari and all AlveolEye requirements.\n\n*If you already have conda set up, you can skip step 1.*\n\n1. **Install Miniconda** by downloading the appropriate version from [here](https://www.anaconda.com/docs/getting-started/miniconda/install):  \n   - Choose the version that matches your processor.  \n   - Download the `.pkg` version for easy installation.\n\n2. **Clone the repository** (by opening a terminal or Miniconda prompt and running the following)\n   ```\n   git clone https://github.com/SucreLab/AlveolEye\n   ```\n\n3. **Navigate to the directory**:\n   ```\n   cd AlveolEye\n   ```\n\n4. **Create the conda environment**:\n   ```\n   conda env create -f ./environment.yml\n   ```\n\n5. **Activate the environment**:\n   ```\n   conda activate AlveolEye\n   ```\n\n6. **Install the plugin**:\n   ```\n   pip install .\n   ```\n\n7. **Launch Napari** and locate the plugin in the plugin menu:\n   ```\n   napari\n   ```\n\n## Running Post-Installation\n\n1. **Activate the environment** in the terminal or Miniconda prompt:\n   ```\n   conda activate AlveolEye\n   ```\n\n2. **Run Napari** in the terminal:\n   ```\n   napari\n   ```\n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n\n## Usage\n\n### Processing: Identify and Segment Vessel Endothelium and Airway Epithelium with Computer Vision\n\n![processing diagram](https://raw.githubusercontent.com/SucreLab/AlveolEye/main/docs/PROCESSING_FINAL.svg)\n\n1. **Import image**  \n   - Click the \"Import Image\" button.  \n   - Use the file dialog to select an image (`.jpg`, `.png`, or `.tiff`).  \n   - Verify that the image correctly loaded. The file name should appear to the right of the button.\n\n2. **Toggle processing with computer vision**  \n   - Keep the checkbox selected to process the image with computer vision (continue to step 3).  \n   - Deselect to skip computer vision processing (skip to step 5).\n\n3. **Import weights**  \n   - To use the default model, proceed to step 4.  \n   - To use a custom model:  \n     - Click the \"Import Weights\" button.  \n     - Select a model file (`.pth`).  \n     - Verify that the weights correctly loaded. The file name should appear to the right of the button.\n\n4. **Set minimum confidence**  \n   - Adjust the minimum confidence using the input box or the \"-/+\" buttons.  \n   - Predictions from the computer vision model with lower confidence than this threshold will not appear.\n\n5. **Run processing**  \n   - Click the \"Run Processing\" button.  \n   - Once completed, manually edit the prediction as needed using Napari's built-in tools.\n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n\n### Postprocessing: Segment Alveolar Tissue and Find Vessel and Aireway Lumens\n\n![postprocessing diagram](https://raw.githubusercontent.com/SucreLab/AlveolEye/main/docs/POSTPROCESSING_FINAL.svg)\n\n1. **Configure thresholding**  \n   - For manual thresholding: Select the \"Manual threshold\" checkbox and use the spinbox to set the threshold level.  \n   - For automatic thresholding ([Otsu's method](https://learnopencv.com/otsu-thresholding-with-opencv/)): Leave the box unchecked.\n\n2. **Remove small particles**  \n   - Set the minimum size cutoff.\n   - Particles with fewer pixels than this value will be removed.\n\n3. **Remove small holes**  \n   - Set the minimum size cutoff.  \n   - Holes with fewer pixels than this value will be removed.\n\n4. **Run postprocessing**  \n   - Click the \"Run Postprocessing\" button.  \n   - Once completed, manually edit the results as needed using Napari's built-in tools.\n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n\n### Assessments: Calculate Morphometry Measurements\n\n![assessments diagram](https://raw.githubusercontent.com/SucreLab/AlveolEye/main/docs/ASSESSMENTS_FINAL.svg)\n\n1. **Airspace Volume Density (ASVD)**\n   - Select the checkbox to run ASVD calculation.\n   - Deselect the checkbox to exclude data from export and increase processing speed.\n\n2. **Mean Linear Intercept (MLI)**\n   - Select the checkbox to run MLI calculation.\n   - Deselect the checkbox to exclude data from export and increase processing speed.\n\n3. **Number of lines**\n   - Set the number of lines used for MLI calculation.\n\n5. **Minimum length**\n   - Set the minimum chord length for inclusion in MLI calculations.\n   - Note: Chords are the line segments that span across an airspace between two alveolar tissue boundaries during MLI calculation.\n\n7. **Scale**\n   - Set the pixel-to-physical space multiplier.\n\n9. **Run assessments**\n   - Click the \"Run Assessments\" button.\n   - View results displayed beside assessment checkboxes and in the export box.\n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n\n### Export Results: Save Assessment Results as a CSV or JSON File\n\n![export diagram](https://raw.githubusercontent.com/SucreLab/AlveolEye/main/docs/EXPORT_FINAL.svg)\n\n1. **Add results**\n   - Click \"Add\" to include current assessment data in the export file.\n\n3. **Remove last result**\n   - Click \"Remove\" to delete the last added results from the export file.\n\n5. **Clear export data**\n   - Click \"Clear\" to empty the export file.\n\n7. **Export results**\n   - Click \"Export Results\" to save the data (`.csv` or `.json` format).\n\n**Results Key**\n\n- **MLI**: Mean Linear Intercept for the tissue image\n \n- **Standard deviation**: Standard deviation of chord lengths used in MLI calculation\n  \n- **Number of chords**: Number of chords used in MLI calculation\n\n- **ASVD**: Airspace Volume Density for the image\n \n- **Airspace pixels**: Total number of airspace pixels\n   \n- **Non-airspace pixels**: Total number of non-airspace pixels\n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n\n## Manual Annotation Guide\n\n### Label Reference\n\n| Structure          | Label Number |\n|--------------------|--------------|\n| Blocker            | 1            |\n| Airway Epithelium  | 2            |\n| Vessel Endothelium | 3            |\n| Airway Lumen       | 4            |\n| Vessel Lumen       | 5            |\n| Parenchyma         | 6            |\n| Alveoli            | 7            |\n\n### Annotation Tips\n\n- **Eyedropper tool**: Click the eyedropper tool, then click a pixel in the image to set your active label (for drawing and editing) to that pixel's label.  \n- **Layer selection**: Ensure you're working on the correct layer before annotating.  \n- **Visibility control**: Hide unnecessary layers using the eye icon on the layer boxes (to the left of the image viewer) for clearer viewing.\n- **Blocking**: Encircle airways and vessels in the blocking label, and everything within that closed shape will be discounted from assessments calculation. \n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n\n## Additional Information\n\n### Theme Settings\n\nToggle between dark and light mode using:\n\n- **Windows/Linux**: `Ctrl + Shift + T`  \n- **macOS**: `Cmd + Shift + T`\n\nOr through Napari preferences:\n\n1. Select \"napari\" in the menu bar.\n   \n2. Choose \"Preferences.\"\n   \n3. Click \"Appearance\" in the left menu.\n     \n4. Select \"dark,\" \"light,\" or \"system\" in the theme dropdown.\n\n<div align=\"right\">\n  <a href=\"#alveoleye-automated-lung-morphometry-made-easy\">Back to Top</a>\n</div>\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      ".jpeg",
      ".tif",
      ".png",
      ".tiff",
      ".jpg"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "AlveolEye"
    ],
    "contributions_sample_data": [
      "AlveolEye"
    ]
  },
  {
    "normalized_name": "fspi-analysis",
    "name": "fspi_analysis",
    "display_name": "Controls",
    "version": "0.1.2",
    "created_at": "2025-05-18",
    "modified_at": "2025-07-17",
    "authors": [
      "Olivier Cahn"
    ],
    "author_emails": [
      "oc124@ic.ac.uk"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/fspi-analysis/",
    "home_github": "https://github.com/engpol/fspi_analysis",
    "home_other": null,
    "summary": "A simple plugin to perform fspi analysis within napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "pandas",
      "napari",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# fspi_analysis\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/fspi_analysis.svg?color=green)](https://github.com/engpol/fspi_analysis/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/fspi_analysis.svg?color=green)](https://pypi.org/project/fspi_analysis)\n[![Python Version](https://img.shields.io/pypi/pyversions/fspi_analysis.svg?color=green)](https://python.org)\n[![tests](https://github.com/engpol/fspi_analysis/workflows/tests/badge.svg)](https://github.com/engpol/fspi_analysis/actions)\n[![codecov](https://codecov.io/gh/engpol/fspi_analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/engpol/fspi_analysis)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/fspi_analysis)](https://napari-hub.org/plugins/fspi_analysis)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA simple plugin to analyse fluorescent sediment profile images (FSPI) within napari.\n\n[Here](https://vimeo.com/1090902747) is a tutorial on installing, and using, the plugin from within the napari standalone application.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `fspi_analysis` via [pip]:\n\n    pip install fspi_analysis\n\nTo install latest development version :\n\n    pip install git+https://github.com/engpol/fspi_analysis.git\n\nAlternatively, install directly from within napari by searching for `fspi_analysis` in the 'Plugins' Tab.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"fspi_analysis\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/engpol/fspi_analysis/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "FSPI Image Analysis"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-automatic-range",
    "name": "napari-automatic-range",
    "display_name": "Automatic Range",
    "version": "0.1.4",
    "created_at": "2025-07-17",
    "modified_at": "2025-07-17",
    "authors": [
      "Pac√¥me Prompsy"
    ],
    "author_emails": [
      "pacome.prompsy@unil.ch"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/napari-automatic-range/",
    "home_github": "https://github.com/pacomito/napari-automatic-range",
    "home_other": null,
    "summary": "An open‚Äësource, reproducible intensity‚Äëscaling method that leverages the nuclear signal as an anchor to calibrate per‚Äëchannel contrast. ",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "AutomaticRange",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-automatic-range\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-automatic-range.svg?color=green)](https://github.com/pacomito/napari-automatic-range/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-automatic-range.svg?color=green)](https://pypi.org/project/napari-automatic-range)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-automatic-range.svg?color=green)](https://python.org)\n[![tests](https://github.com/pacomito/napari-automatic-range/workflows/tests/badge.svg)](https://github.com/pacomito/napari-automatic-range/actions)\n[![codecov](https://codecov.io/gh/pacomito/napari-automatic-range/branch/main/graph/badge.svg)](https://codecov.io/gh/pacomito/napari-automatic-range)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-automatic-range)](https://napari-hub.org/plugins/napari-automatic-range)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nAn open‚Äësource, reproducible intensity‚Äëscaling method that leverages the nuclear signal as an anchor to calibrate per‚Äëchannel contrast. \n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-automatic-range` via [pip]:\n\n```\npip install napari-automatic-range\n```\n\nIf napari is not already installed, you can install `napari-automatic-range` with napari and Qt via:\n\n```\npip install \"napari-automatic-range[all]\"\n```\n\n\nTo install latest development version :\n\n```\npip install git+https://github.com/pacomito/napari-automatic-range.git\n```\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-automatic-range\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/pacomito/napari-automatic-range/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Automatic Range"
    ],
    "contributions_sample_data": [
      "Automatic Range"
    ]
  },
  {
    "normalized_name": "skan",
    "name": "skan",
    "display_name": "skan",
    "version": "0.13.0",
    "created_at": "2023-01-31",
    "modified_at": "2025-07-17",
    "authors": [
      "Juan Nunez-Iglesias"
    ],
    "author_emails": [
      "Juan Nunez-Iglesias <juan.nunez-iglesias@monash.edu>"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/skan/",
    "home_github": "https://github.com/jni/skan",
    "home_other": null,
    "summary": "Skeleton analysis in Python",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "imageio>=2.10.1",
      "magicgui>=0.7.3",
      "matplotlib>=3.4",
      "networkx>=2.7",
      "numba>=0.58",
      "numpy>=1.25",
      "pandas>=2.0.2",
      "openpyxl>=2.6",
      "scikit-image>=0.17.1",
      "scipy>=1.7",
      "toolz>=0.10.0",
      "tqdm>=4.57.0",
      "scikit-image[data]; extra == \"all\"",
      "coverage; extra == \"testing\"",
      "hypothesis; extra == \"testing\"",
      "napari[pyqt5]>=0.4.19rc1; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "seaborn<1.0; extra == \"testing\"",
      "tifffile; extra == \"testing\"",
      "napari[all]>=0.4.19rc1; extra == \"docs\"",
      "sphinx; extra == \"docs\"",
      "jupyter; extra == \"docs\"",
      "notebook; extra == \"docs\"",
      "seaborn<1.0,>=0.13; extra == \"docs\"",
      "sphinx-toggleprompt; extra == \"docs\"",
      "sphinx-copybutton; extra == \"docs\"",
      "sphinxcontrib-bibtex; extra == \"docs\"",
      "myst-nb; extra == \"docs\"",
      "zarr; extra == \"docs\"",
      "pydata-sphinx-theme<1.0; extra == \"docs\""
    ],
    "package_metadata_description": "# skan: skeleton analysis in Python\nPython module to analyse skeleton (thin object) images\n\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/jni/skan/main?filepath=doc%2Fgetting_started.ipynb)\n[![Coverage Status](https://coveralls.io/repos/github/jni/skan/badge.svg?branch=main)](https://coveralls.io/github/jni/skan?branch=master)\n\nSee the documentation at [https://skeleton-analysis.org](https://skeleton-analysis.org).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Skeleton Widget",
      "Color Skeleton Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-clusters-plotter",
    "name": "napari-clusters-plotter",
    "display_name": "napari clusters plotter",
    "version": "0.10.0",
    "created_at": "2021-11-15",
    "modified_at": "2025-07-16",
    "authors": [
      "Laura Zigutyte",
      "Ryan Savill",
      "Marcelo Zoccoler",
      "Thorsten Wagner",
      "Robert Haase"
    ],
    "author_emails": [
      "Johannes Soltwedel <johannes_richard.soltwedel@tu-dresden.de>"
    ],
    "license": "Copyright (c) 2022, DFG Cluste...",
    "home_pypi": "https://pypi.org/project/napari-clusters-plotter/",
    "home_github": "https://github.com/BiAPoL/napari-clusters-plotter",
    "home_other": null,
    "summary": "A plugin to use with napari for clustering objects according to their properties",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "npe2",
      "scikit-learn",
      "pandas",
      "umap-learn",
      "scikit-image",
      "scipy",
      "biaplotter>=0.3.1",
      "imagecodecs",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-clusters-plotter\n\n[![License](https://img.shields.io/pypi/l/napari-clusters-plotter.svg?color=green)](https://github.com/lazigu/napari-clusters-plotter/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-clusters-plotter.svg?color=green)](https://pypi.org/project/napari-clusters-plotter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-clusters-plotter.svg?color=green)](https://python.org)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-clusters-plotter/badges/version.svg)](https://anaconda.org/conda-forge/napari-clusters-plotter)\n[![tests](https://github.com/BiAPoL/napari-clusters-plotter/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/BiAPoL/napari-clusters-plotter/actions/workflows/test_and_deploy.yml)\n[![codecov](https://codecov.io/gh/BiAPoL/napari-clusters-plotter/branch/main/graph/badge.svg?token=R6W2KO1NJ8)](https://codecov.io/gh/BiAPoL/napari-clusters-plotter)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-clusters-plotter/badges/downloads.svg)](https://anaconda.org/conda-forge/napari-clusters-plotter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-clusters-plotter)](https://www.napari-hub.org/plugins/napari-clusters-plotter)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15670497.svg)](https://doi.org/10.5281/zenodo.15670497)\n\nA napari-plugin for clustering objects according to their properties.\n\n## [Documentation](https://biapol.github.io/napari-clusters-plotter/)\n\nThe documentation for the napari-clusters-plotter is available under the above link.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-clusters-plotter\" is free and open source software\n\n## Acknowledgements\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany‚Äôs Excellence Strategy ‚Äì EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.\nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/BiAPoL/napari-clusters-plotter/issues) along\nwith a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[pytest]: https://docs.pytest.org/en/7.0.x/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[conda]: https://docs.conda.io/projects/conda/en/latest/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Plot & select features",
      "Dimensionality reduction (features)",
      "Clustering (features)"
    ],
    "contributions_sample_data": [
      "BBBC007 v1 dataset (images & labels)",
      "TGMM mini dataset (tracks and segmentations)",
      "Cells3D mitotic nucleus surface curvatures",
      "Skan skeleton dataset (labels and paths)"
    ]
  },
  {
    "normalized_name": "napari-data-inspection",
    "name": "napari-data-inspection",
    "display_name": "Data Inspection",
    "version": "0.0.4",
    "created_at": "2025-06-16",
    "modified_at": "2025-07-16",
    "authors": [
      "Lars Kr√§mer"
    ],
    "author_emails": [
      "lars.kraemer@dkfz-heidelberg.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-data-inspection/",
    "home_github": "https://github.com/MIC-DKFZ/napari-data-inspection",
    "home_other": null,
    "summary": "Data Inspection Plugin, designed to streamline file navigation and enhance the efficiency of data inspection.",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "napari_toolkit",
      "blosc2",
      "SimpleITK",
      "tifffile",
      "scikit-image",
      "natsort",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-data-inspection\n\nA data inspection plugin for loading image tiles from multiple folders.\nWith data loading and prefetching handled automatically, file navigation is streamlined to enable fast and efficient data inspection.\nAny number of folders for images and labels can be specified, and files are automatically paired based on their order ‚Äî manual file selection is eliminated.\nPerfect for high-throughput inspection workflows and rapid dataset review, especially in semantic segmentation tasks.\n\n## Installation\n\n```bash\n# 1. Install napari if necessary\npip install napari[all]\n# 2. Install the plugin\npip install napari-data-inspection\n```\n\n## Prerequisites\n\n### Supported File Types\nThe following file types are supported: `.nii.gz`, `.png`, `.b2nd`, `.nrrd`, `.mha`, `.tif`, `.tiff`.\nIf you want to add custom ones add a loader to `src/napari_data_inspection/utils/data_loading.py`.\n\n### Data Organization Requirements\nYour data should be organized so that different images and different labels can be clearly distinguished‚Äîeither by placing them in separate folders or by using consistent filename patterns (e.g., *_img for images and *_seg for labels).\n**The number of files must match across all folders, as they are paired by order.**\n\n## How to\n\n```\nnapari -w napari-data-inspection\n```\n\n<img src=\"https://github.com/MIC-DKFZ/napari-data-inspection/raw/main/imgs/GUI.png\">\n\n# Acknowledgments\n\n<p align=\"left\">\n  <img src=\"https://github.com/MIC-DKFZ/napari-data-inspection/raw/main/imgs/Logos/HI_Logo.png\" width=\"150\"> &nbsp;&nbsp;&nbsp;&nbsp;\n  <img src=\"https://github.com/MIC-DKFZ/napari-data-inspection/raw/main/imgs/Logos/DKFZ_Logo.png\" width=\"500\">\n</p>\n\nThis repository is developed and maintained by the Applied Computer Vision Lab (ACVL)\nof [Helmholtz Imaging](https://www.helmholtz-imaging.de/) and the\n[Division of Medical Image Computing](https://www.dkfz.de/en/medical-image-computing) at DKFZ.\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n[copier]: https://copier.readthedocs.io/en/stable/\n[napari]: https://github.com/napari/napari\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Data Inspection Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-event-monitor",
    "name": "napari-event-monitor",
    "display_name": "Napari Event Monitor",
    "version": "0.1.3",
    "created_at": "2025-07-16",
    "modified_at": "2025-07-16",
    "authors": [
      "Ian Coccimiglio"
    ],
    "author_emails": [
      "Ian Coccimiglio <icoccimi@gmail.com>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-event-monitor/",
    "home_github": "https://github.com/ian-coccimiglio/napari-event-monitor",
    "home_other": null,
    "summary": "Dynamic event monitoring and diagnostics in napari",
    "categories": [],
    "package_metadata_requires_python": ">3.10",
    "package_metadata_requires_dist": null,
    "package_metadata_description": "# napari-event-monitor\nTesting and Documenting the napari Event Loop\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Make Event Monitor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-samv2",
    "name": "napari-samv2",
    "display_name": "napari-SAMV2",
    "version": "0.1.2",
    "created_at": "2024-10-08",
    "modified_at": "2025-07-16",
    "authors": [
      "Krishnan Venkataraman"
    ],
    "author_emails": [
      "krishvraman95@gmail.com"
    ],
    "license": "Copyright (c) 2024, Krishnan V...",
    "home_pypi": "https://pypi.org/project/napari-samv2/",
    "home_github": "https://github.com/Krishvraman/napari-SAMV2",
    "home_other": null,
    "summary": "Napari plugin for segment anything version 2 model from meta. Plugin primarily useful for segmenting 3d volumetric data or 3d time series data. ",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "numpy; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-SAMV2\n\nNapari plugin to use segment anything version 2.1 models from Meta.\n\nPlugin made for segmenting 3d volumetric data or 3d time series data.\n\n----------------------------------\n\n## Installation\n\nThe plugin requires the following pre-requisite to be installed :\n\n1. Python and pytorch versions\n\npython>=3.10,torch>=2.5.1 and torchvision>=0.20.1 required\n\nTo install pytorch with your respective OS please visit - https://pytorch.org/get-started/locally/\n\n2. SAM v2 installation from meta\n\nPlease refer https://github.com/facebookresearch/sam2\n\n3. Install napari\n\npython -m pip install \"napari[all]\"\n\nFollowing is a sample conda environment installation with the above pre-req \n\n    conda create -n samv2_env python=3.10\n    conda activate samv2_env\n    pip3 install torch torchvision\n\n    git clone https://github.com/facebookresearch/sam2.git && cd sam2\n    pip install -e .\n\n    python -m pip install \"napari[all]\"\n\n    pip install napari-SAMV2\n\n\n## Usage\n\nMiddle mouse click - positive point or keyboard shortcut \"a\"\n\nCtrl + Middle mouse click - negative point or keyboard shortcut \"n\"\n\nTime Series Segmentation :\n\n![samv2_time_series_demo](https://github.com/user-attachments/assets/078ca2bb-3016-4257-ac7c-c3cde8f9d125)\n\n\n\nVolume Segmentation :\n\n![samv2_volume_segmentation](https://github.com/user-attachments/assets/af05fcc4-a60d-44e8-ae05-70764d96e828)\n\n\n\nReference :\n\nExample Data in the demo videos are from,\n\nCell tracking challenge - https://celltrackingchallenge.net/ \n\nand\n\nFlyEM project - https://www.janelia.org/project-team/flyem/hemibrain\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-SAMV2\" is free and open source software\n\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Krishvraman/napari-SAMV2/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SAMV2"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "vollseg-napari",
    "name": "vollseg-napari",
    "display_name": "vollseg-napari",
    "version": "2.4.9",
    "created_at": "2021-12-10",
    "modified_at": "2025-07-16",
    "authors": [
      "Varun Kapoor"
    ],
    "author_emails": [
      "varun.kapoor@kapoorlabs.org"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/vollseg-napari/",
    "home_github": "https://github.com/kapoorlab/vollseg-napari",
    "home_other": null,
    "summary": "Irregular cell shape segmentation using VollSeg",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "vollseg",
      "tensorflow; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "napari>=0.4.13",
      "magicgui>=0.4.0",
      "pyqt6",
      "pynvml",
      "pytest; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "napari[pyqt]>=0.4.13; extra == \"test\""
    ],
    "package_metadata_description": "# VollSeg Napari Plugin\n\n# Developed by KapoorLabs\n\n\n<img src=\"images/mtrack.png\" alt=\"Logo1\" width=\"150\"/>\n<img src=\"images/kapoorlablogo.png\" alt=\"Logo2\" width=\"150\"/>\n\nThis product is a testament to our expertise at KapoorLabs, where we specialize in creating cutting-edge solutions. We offer bespoke pipeline development services, transforming your developmental biology questions into publishable figures with our advanced computer vision and AI tools. Leverage our expertise and resources to achieve end-to-end solutions that make your research stand out.\n\n**Note:** The tools and pipelines showcased here represent only a fraction of what we can achieve. For tailored and comprehensive solutions beyond what was done in the referenced publication, engage with us directly. Our team is ready to provide the expertise and custom development you need to take your research to the next level. Visit us at [KapoorLabs](https://www.kapoorlabs.org/).\n\n\n[![PyPI version](https://img.shields.io/pypi/v/vollseg-napari.svg)](https://pypi.org/project/vollseg-napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari)](https://napari-hub.org/plugins/vollseg-napari)\n[![License](https://img.shields.io/pypi/l/napari-metroid.svg?color=green)](https://github.com/kapoorlab/napari-vollseg/raw/main/LICENSE)\n[![codecov](https://codecov.io/gh/kapoorlab/napari-vollseg/branch/main/graph/badge.svg)](https://codecov.io/gh/kapoorlab/napari-vollseg)\n[![Twitter Badge](https://badgen.net/badge/icon/twitter?icon=twitter&label)](https://twitter.com/entracod)\n\n## Segmentation Algorithm\n\nVollSeg is more than just a single segmentation algorithm; it is a meticulously designed modular segmentation tool tailored to diverse model organisms and imaging methods. While a U-Net might suffice for certain image samples, others might benefit from utilizing StarDist, and some could require a blend of both, potentially coupled with denoising or region of interest models. The pivotal decision left to make is how to select the most appropriate VollSeg configuration for your dataset, a question we comprehensively address in our [documentation website](https://kapoorlabs-caped.github.io/vollseg-napari/).\n\nThis project provides the [napari](https://napari.org/) plugin for [VollSeg](https://github.com/kapoorlab/vollseg), a deep learning based 2D and 3D segmentation tool for irregular shaped cells. VollSeg has originally been developed (see [papers](http://conference.scipy.org/proceedings/scipy2021/varun_kapoor.html)) for the segmentation of densely packed membrane labelled cells in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\nFor detailed demo of the plugin see these [videos](https://www.youtube.com/watch?v=W_gKrLWKNpQ) and a short video about the [parameter selection](https://www.youtube.com/watch?v=7tQMn_u8_7s&t=1s) \n\n\n## Installation & Usage\n\nInstall the plugin with `pip install vollseg-napari` or from within napari via `Plugins > Install/Uninstall Package(s)‚Ä¶`. \n\nYou can activate the plugin in napari via `Plugins > VollSeg: VollSeg`. Example images for testing are provided via `File > Open Sample > VollSeg`.\n\nIf you use this plugin for your research, please [cite us](http://conference.scipy.org/proceedings/scipy2021/varun_kapoor.html).\n\n\n## Examples\n\nVollSeg comes with different options to combine CARE based denoising with UNET, StarDist and segmentation in a region of interest (ROI). We present some examples which are represent optimal combination of these different modes for segmenting different cell types. We summarize this in the table below:\n| Example Image | Description | Training Data | Trained Model |\n| --- | --- |--- | --- |\n| ![Raw Ascadian Embryo](images/Ascadian_raw.png)| Light sheet fused from four angles 3D single channel| [Training Data ~320 GB](https://figshare.com/articles/dataset/Astec-half-Pm1_Cut_at_2-cell_stage_half_Phallusia_mammillata_embryo_live_SPIM_imaging_stages_6-16_/11309570?backTo=/s/765d4361d1b073beedd5)| [UNET model](https://zenodo.org/record/6337699) |\n| ![Raw Carcinoma](images/Carcinoma_raw.png)| Confocal microscopy 3D single channel 8 bit| [Training Data](https://zenodo.org/record/5904082#.Yi8-BnrMJD8)| [Denoising Model](https://zenodo.org/record/5910645/) and [StarDist Model](https://zenodo.org/record/6354077/) |\n| ![Raw Xenopus Tissue](images/Xenopus_tissue_raw.png)| LaserScanningConfocalMicroscopy 2D single channel| [Dataset](https://zenodo.org/record/6076614#.YjBaNnrMJD8)| [UNET Model](https://zenodo.org/record/6060378/)  |\n\n\n\n## Troubleshooting & Support\n\n- The [image.sc forum](https://forum.image.sc/tag/vollseg) is the best place to start getting help and support. Make sure to use the tag `vollseg`, since we are monitoring all questions with this tag.\n- If you have technical questions or found a bug, feel free to [open an issue](https://github.com/kapoorlab/vollseg-napari/issues).\n\n## Authors\n\n- Varun Kapoor <randomaccessiblekapoor@gmail.com>\n- Mari Tolonen\n- Jakub Sedzinski\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "VollSeg"
    ],
    "contributions_sample_data": [
      "Embryo Cells (3D)",
      "Arabidopsis (3D)",
      "Breast Cancer Cells (3DT)"
    ]
  },
  {
    "normalized_name": "napari-dinosim",
    "name": "napari-dinosim",
    "display_name": "DINOSim Segmentation",
    "version": "0.1.3",
    "created_at": "2025-03-09",
    "modified_at": "2025-07-15",
    "authors": [
      "Aitor Gonzalez-Marfil"
    ],
    "author_emails": [
      "aitorgacad@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-dinosim/",
    "home_github": "https://github.com/AAitorG/napari-DINOSim",
    "home_other": null,
    "summary": "A simple plugin to use DINOSim in napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": "<3.13,>=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "torch",
      "torchvision",
      "tqdm",
      "pillow",
      "matplotlib",
      "opencv-python",
      "tifffile",
      "napari[all]",
      "tox; extra == \"testing\"",
      "pytest>=8.3.5; extra == \"testing\"",
      "pytest-qt>=4.4.0; extra == \"testing\"",
      "pytest-xvfb>=3.0.0; extra == \"testing\"",
      "pytest-cov>=6.0; extra == \"testing\"",
      "pyqt5>=5.15.11; extra == \"testing\"",
      "napari>=0.5.6; extra == \"testing\"",
      "magicgui>=0.10.0; extra == \"testing\""
    ],
    "package_metadata_description": "# DINOSim\n\n[![License: MIT](https://img.shields.io/pypi/l/napari-dinosim.svg?color=blue)](https://github.com/AAitorG/napari-DINOSim/raw/main/LICENSE)\n[![biorxiv](https://img.shields.io/badge/bioRxiv-Paper-bd2635.svg)](https://doi.org/10.1101/2025.03.09.642092)\n[![PyPI](https://img.shields.io/pypi/v/napari-DINOSim.svg?color=green)](https://pypi.org/project/napari-DINOSim)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-DINOSim.svg?color=green)](https://python.org)\n[![tests](https://github.com/AAitorG/napari-DINOSim/workflows/tests/badge.svg)](https://github.com/AAitorG/napari-DINOSim/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-dinosim)](https://napari-hub.org/plugins/napari-dinosim)\n\n![DINOSim-simple](docs/DINOSim-simplest.png)\n\nA [napari] plugin for zero-shot image segmentation using DINOv2 vision transformers.\n\n----------------------------------\n\n## Overview\n\n`napari-DINOSim` enables zero-shot image segmentation by selecting reference points on an image. The plugin leverages DINOv2's powerful feature extraction capabilities to compute similarity maps and generate segmentation masks.\n\nFor detailed information about the widget's functionality, UI elements, and usage instructions, please refer to the [Plugin Documentation](./docs/plugin_documentation.md). A simple [example notebook](./src/DINOSim_example.ipynb) demonstrating how to use DINOSim programmatically is also available.\n\n## Installation\n\nYou can install `napari-DINOSim` via [pip]:\n\n```sh\npip install napari-dinosim\n```\n\nor from source using [conda]:\n\n```bash\n# Clone the repository\ngit clone https://github.com/AAitorG/napari-DINOSim.git\ncd napari-DINOSim\n\n# Create and activate the conda environment\nconda env create -f environment.yml\nconda activate napari-dinosim\n```\n\n## Usage\n\nTo launch napari, run the following command in your terminal:\n\n```sh\nnapari\n```\n\nWithin the napari interface, locate and click the `DINOSim segmentation` plugin in the Plugins section of the top bar. You can then:\n1. Drag and drop your image into the napari viewer\n2. Select points on the objects you want to segment\n3. The plugin will automatically generate segmentation masks based on your selections\n\nFor more detailed instructions and examples, please refer to our [Plugin Documentation](./docs/plugin_documentation.md).\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-DINOSim\" is free and open source software.\n\n## Citation\n\nPlease note that DINOSim is based on a [publication](https://doi.org/10.1101/2025.03.09.642092). If you use DINOSim in your research, please be so kind to cite our work:\n\n```bibtex\n@article {Gonzalez-Marfil2025dinosim,\n    title = {DINOSim: Zero-Shot Object Detection and Semantic Segmentation on Electron Microscopy Images},\n    author = {Gonz{\\'a}lez-Marfil, Aitor and G{\\'o}mez-de-Mariscal, Estibaliz and Arganda-Carreras, Ignacio},\n    journal = {bioRxiv},\n    publisher = {Cold Spring Harbor Laboratory},\n    url = {https://www.biorxiv.org/content/early/2025/03/13/2025.03.09.642092},\n    doi = {10.1101/2025.03.09.642092},\n    year = {2025}\n}\n```\n\n## Contributing\n\nContributions are very welcome! Tests can be run with [tox]. Please ensure the test coverage at least stays the same before submitting a pull request.\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/AAitorG/napari-DINOSim/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[MIT]: http://opensource.org/licenses/MIT\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[conda]: https://docs.conda.io/en/latest/miniconda.html\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "DINOSim Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-moltrack",
    "name": "napari-moltrack",
    "display_name": "MolTrack",
    "version": "0.1.9",
    "created_at": "2024-05-30",
    "modified_at": "2025-07-15",
    "authors": [
      "Piers Turner"
    ],
    "author_emails": [
      "piers.turner@physics.ox.ac.uk"
    ],
    "license": "Copyright (c) 2024, Piers Turn...",
    "home_pypi": "https://pypi.org/project/napari-moltrack/",
    "home_github": "https://github.com/piedrro/napari-moltrack",
    "home_other": null,
    "summary": "A user-friendly SMLM analysis platfrom for napari, which includes single molecule localisation, tracking, and analysis features.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari[all]==0.5.0",
      "bactfit>=0.1.6",
      "numpy",
      "magicgui",
      "qtpy",
      "scipy",
      "pyqtgraph",
      "picassosr==0.8.0",
      "pandas",
      "matplotlib>=3.7.0",
      "scipy",
      "opencv-python",
      "tqdm",
      "originpro",
      "pyqt5-tools",
      "torch",
      "torchvision",
      "cellpose==3.0.1",
      "omnipose",
      "trackpy",
      "shapely",
      "astropy",
      "mat4py",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-moltrack\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-moltrack.svg?color=green)](https://github.com/piedrro/napari-moltrack/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-moltrack.svg?color=green)](https://pypi.org/project/napari-moltrack)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-moltrack.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-moltrack)](https://napari-hub.org/plugins/napari-moltrack)\n\nA user-friendly SMLM analysis platfrom for napari, which includes single molecule localisation, tracking, and analysis features. \nBased on established python packages such as **Picasso**, **GPUfit** and **Trackpy**.\nThis plugin was designed to detect/track single molecules inside cells, but can be used for any other SMLM/tracking application.\n\nAll functions are parallelised/GPU accelerated where possible to increase performance.\nMultiple datasets can be loaded and processed in parallel.\n\nSingle molecule localisations can be filtered by their properties (e.g. photons, width, etc.) and can be rendered as a super resolution image.\n\nNapari-moltrack is also compatible with **FRET** and **ALEX FRET** image data, can be used to calculate FRET efficiencies of single molecules/tracks.\n\nSegmentations can be used to exclude regions from single molecule localisation and tracking.\nSegmentations can be added automatically using Cellpose or can be added manually. Includes tools for editing/modifying segmentations at a sub-pixel resolution.\n\nCompatible with both single and multi-channel .tif and .fits files.\n\nnapari-moltrack was written by Piers Turner, Kapanidis Group, University of Oxford.\n\nhttps://www.physics.ox.ac.uk/research/group/gene-machines\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-moltrack` via [pip]:\n\n    pip install napari-moltrack\n\nTo update `napari-moltrack` to the latest version, use:\n\n    pip install napari-moltrack --upgrade\n\nTo install latest development version :\n\n    pip install git+https://github.com/piedrro/napari-moltrack.git\n\n\n## BactFit\n\nnapari-moltrack integrates BactFit, a package for fitting the shape of rod shaped bacterial cells to an ideal cell model.\nBactFit allows cell renders and heatmaps to be generated through the transformation of SMLM localisations to an ideal cell model.\n\n### BactFit Heatmap\n\n![Feature Image](resources/heatmap.png)\n\n### BactFit Cell Render\n\n![Feature Image](resources/render.png)\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-moltrack\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/piedrro/napari-moltrack/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MolTrack"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-unispac",
    "name": "napari-UniSPAC",
    "display_name": "napari-UniSPAC",
    "version": "1.0.6",
    "created_at": "2025-02-07",
    "modified_at": "2025-07-11",
    "authors": [],
    "author_emails": [],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-unispac/",
    "home_github": "https://github.com/ddd9898/UniSPAC",
    "home_other": null,
    "summary": "A Unified Segmentation framework for Proofreading and Annotation in Connectomics (UniSPAC)!",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "napari"
    ],
    "package_metadata_description": "# napari-UniSPAC\nThe napari plugin for UniSPAC [A Unified Segmentation framework for Proofreading and Annotation in Connectomics]. UniSPAC provides interactive 3D neuron segmentation. Neuron segmentation, proofreading and tracking can be done with just mouse clicks, which is much more efficient than existing tools.\n\n## Requirements\n\nA system with enough GPU memory and pytorch installed. The size of the GPU memory is related to the size of the vEM image that can be processed. For  `test_roi1_sub_z0-100.tiff` with a shape of 800x800x100, the recommended minimum GPU memory is 12GB.\n\n## Installation\n\nStep 1: install napari via pip:\n\n```shell\npip install -U 'napari[all]'\n```\n\nStep 2: install `napari-UniSPAC`\n\n```shell\ngit clone https://github.com/ddd9898/napari-UniSPAC.git\ncd napari-UniSPAC\npip install -e .\n```\n\nStep 3: run napari:\n\n```shell\nnapari\n```\n\nYou can familiarise yourself with how UniSPAC's napari plugin operates by labeling  `test_roi1_sub_z0-100.tiff`, which is an example of Drosophila vEM images.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "UniSPAC"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "partseg",
    "name": "PartSeg",
    "display_name": "PartSeg",
    "version": "0.16.3",
    "created_at": "2020-10-27",
    "modified_at": "2025-07-10",
    "authors": [
      "Grzegorz Bokota"
    ],
    "author_emails": [
      "Grzegorz Bokota <g.bokota@uw.edu.pl>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/partseg/",
    "home_github": "https://github.com/4DNucleome/PartSeg",
    "home_other": null,
    "summary": "PartSeg is python GUI and set of napari plugins for bio imaging analysis especially nucleus analysis,",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "IPython>=7.7.0",
      "PartSegCore-compiled-backend<0.16.0,>=0.13.11",
      "PartSegData==0.10.0",
      "QtAwesome!=1.2.0,>=1.0.3",
      "QtPy>=1.10.0",
      "SimpleITK>=2.1.0",
      "appdirs>=1.4.4",
      "czifile>=2019.5.22",
      "defusedxml>=0.6.0",
      "fonticon-fontawesome6>=6.1.1",
      "h5py>=3.3.0",
      "imagecodecs>=2020.5.30",
      "imageio>=2.20.0",
      "ipykernel>=5.2.0",
      "local-migrator>=0.1.7",
      "magicgui!=0.5.0,>=0.4.0",
      "mahotas>=1.4.12",
      "napari>=0.4.19",
      "nme>=0.1.7",
      "numpy>=1.22.2; python_version >= \"3.10\"",
      "numpy<2,>=1.22.2; python_version < \"3.10\"",
      "oiffile>=2020.1.18",
      "openpyxl>=3.0.7",
      "packaging>=22.0",
      "pandas>=1.3.0",
      "psygnal>=0.3.4",
      "pydantic<3,>=1.9.1",
      "pygments>=2.12.0",
      "qtconsole>=4.7.7",
      "requests>=2.25.0",
      "scipy>=1.5.4",
      "sentry-sdk>=2.4.0",
      "six>=1.11.0",
      "superqt>=0.4.1",
      "sympy>=1.10",
      "tifffile>=2020.9.30",
      "traceback-with-variables>=2.0.4",
      "vispy>=0.14.1",
      "xlrd>=1.1.0",
      "xlsxwriter>=2.0.0",
      "PartSeg[accelerate,pyqt5]; extra == \"all\"",
      "autodoc-pydantic; extra == \"docs\"",
      "sphinx!=3.0.0,!=3.5.0; extra == \"docs\"",
      "sphinx-autodoc-typehints; extra == \"docs\"",
      "sphinx-qt-documentation; extra == \"docs\"",
      "PartSeg[pyinstaller_base,pyqt5]; extra == \"pyinstaller\"",
      "PartSeg[accelerate]; extra == \"pyinstaller-base\"",
      "PyInstaller; extra == \"pyinstaller-base\"",
      "pydantic; extra == \"pyinstaller-base\"",
      "PartSeg[pyqt5]; extra == \"pyqt\"",
      "PyQt5!=5.15.0,>=5.12.3; extra == \"pyqt5\"",
      "napari[pyqt5]; extra == \"pyqt5\"",
      "PyQt6; extra == \"pyqt6\"",
      "napari[pyqt6]>=0.5.0; extra == \"pyqt6\"",
      "PartSeg[pyside2]; extra == \"pyside\"",
      "PySide2!=5.15.0,>=5.12.3; extra == \"pyside2\"",
      "napari[pyside]; extra == \"pyside2\"",
      "PySide6; extra == \"pyside6\"",
      "napari[pyside6_experimental]>=0.5.0; extra == \"pyside6\"",
      "coverage; extra == \"test\"",
      "lxml[html_clean]; extra == \"test\"",
      "pytest>=7.0.0; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "pytest-timeout; extra == \"test\"",
      "scikit-image; extra == \"test\"",
      "pytest; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "lxml; extra == \"testing\""
    ],
    "package_metadata_description": "# PartSeg\n\n![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)\n![Tests](https://github.com/4DNucleome/PartSeg/workflows/Tests/badge.svg?branch=develop)\n[![PyPI version](https://badge.fury.io/py/PartSeg.svg)](https://badge.fury.io/py/PartSeg)\n[![Anaconda version](https://anaconda.org/conda-forge/partseg/badges/version.svg)](https://anaconda.org/conda-forge/partseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/partseg.svg)](https://pypi.org/project/partseg)\n[![Documentation Status](https://readthedocs.org/projects/partseg/badge/?version=latest)](https://partseg.readthedocs.io/en/latest/?badge=latest)\n[![Azure Pipelines Build Status](https://dev.azure.com/PartSeg/PartSeg/_apis/build/status/4DNucleome.PartSeg?branchName=develop)](https://dev.azure.com/PartSeg/PartSeg/_build/latest?definitionId=1&branchName=develop)\n[![DOI](https://zenodo.org/badge/166421141.svg)](https://zenodo.org/badge/latestdoi/166421141)\n[![Publication DOI](https://img.shields.io/badge/Publication%20DOI-10.1186%2Fs12859--021--03984--1-blue)](https://doi.org/10.1186/s12859-021-03984-1)\n[![Licence: BSD3](https://img.shields.io/github/license/4DNucleome/PartSeg)](https://github.com/4DNucleome/PartSeg/blob/master/License.txt)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)\n[![CodeQL](https://github.com/4DNucleome/PartSeg/actions/workflows/codeql-analysis.yml/badge.svg?branch=develop)](https://github.com/4DNucleome/PartSeg/actions/workflows/codeql-analysis.yml)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/f9b0f1eb2c92486d9efd99ed5b2ef326)](https://www.codacy.com/gh/4DNucleome/PartSeg/dashboard?utm_source=github.com&utm_medium=referral&utm_content=4DNucleome/PartSeg&utm_campaign=Badge_Grade)\n[![codecov](https://codecov.io/gh/4DNucleome/PartSeg/branch/develop/graph/badge.svg?token=nbAbkOAe1C)](https://codecov.io/gh/4DNucleome/PartSeg)\n[![DeepSource](https://deepsource.io/gh/4DNucleome/PartSeg.svg/?label=active+issues&show_trend=true&token=RuuHPIzqyqGaU-bKtOKPFWTg)](https://deepsource.io/gh/4DNucleome/PartSeg/?ref=repository-badge)\n\nPartSeg is a GUI and a library for segmentation algorithms. PartSeg also provide napari plugins for IO and labels measurement.\n\nThis application is designed to help biologist with segmentation based on threshold and connected components.\n\n![interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_analysis.png)\n\n## Tutorials\n\n- Tutorial: **Chromosome 1 (as gui)** [link](https://github.com/4DNucleome/PartSeg/blob/master/tutorials/tutorial-chromosome-1/tutorial-chromosome1_16.md)\n- Data for chromosome 1 tutorial [link](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/PartSeg_samples.zip)\n- Tutorial: **Different neuron types (as library)** [link](https://github.com/4DNucleome/PartSeg/blob/master/tutorials/tutorial_neuron_types/Neuron_types_example.ipynb)\n\n## Installation\n\n- From binaries:\n\n  - [Windows](https://github.com/4DNucleome/PartSeg/releases/download/v0.16.3/PartSeg-0.16.3-windows.zip) (build on Windows 10)\n  - [Linux](https://github.com/4DNucleome/PartSeg/releases/download/v0.16.3/PartSeg-0.16.3-linux.zip) (build on Ubuntu 20.04)\n  - [macOS](https://github.com/4DNucleome/PartSeg/releases/download/v0.16.3/PartSeg-0.16.3-macos.zip) (build on macOS 13)\n  - [macOS arm](https://github.com/4DNucleome/PartSeg/releases/download/v0.16.3/PartSeg-0.16.3-macos-arm64.zip) (build on macOS 14)\n    There are reported problems with permissions systems on macOS. If you have a problem with starting the application, please try to run it from the terminal.\n\n- With pip:\n\n  - From pypi: `pip install PartSeg[all]`\n  - From repository: `pip install git+https://github.com/4DNucleome/PartSeg.git`\n\n- With conda:\n\n  - `conda install -c conda-forge partseg`\n  - `mamba install -c conda-forge partseg` - As mamba is faster than conda\n\n- With napari:\n\n  If you do not know how to setup python environment on your system you may use [napari](https://napari.org/) to run PartSeg.\n  It is a GUI for scientific image analysis. PartSeg is also a plugin for napari so could be installed from plugin dialog.\n  To install napari bundle please download it [napari bundle](https://github.com/napari/napari/releases/latest)\n  and follow [installation instructions](https://napari.org/stable/tutorials/fundamentals/installation.html#install-as-a-bundled-app).\n\nInstallation troubleshooting information could be found in wiki: [wiki](https://github.com/4DNucleome/PartSeg/wiki/Instalation-troubleshoot).\nIf this information does not solve problem you can open [issue](https://github.com/4DNucleome/PartSeg/issues).\n\n### Qt 6 support\n\nPartSeg development branch support (and stable since 0.15.0) has experimental Qt6 support. Test are passing but not whole GUI code is covered by tests. Inf you Find any problem please report it.\n\n## Running\n\nIf you downloaded binaries, run the `PartSeg` (or `PartSeg.exe` for Windows) file inside the `PartSeg` folder\n\nIf you installed from repository or from pip, you can run it with `PartSeg` command or `python -m PartSeg`.\nFirst option does not work on Windows.\n\nPartSeg export few commandline options:\n\n- `--no_report` - disable error reporting\n- `--no_dialog` - disable error reporting and error dialog. Use only when running from terminal.\n- `roi` - skip launcher and start *ROI analysis* gui\n- `mask`- skip launcher and start *ROI mask* gui\n\n## napari plugin\n\nPartSeg provides napari plugins for io to allow reading projects format in napari viewer.\n\n## Save Format\n\nSaved projects are tar files compressed with gzip or bz2.\n\nMetadata is saved in data.json file (in json format).\nImages/masks are saved as \\*.npy (numpy array format).\n\n## Interface\n\nLauncher. Choose the program that you will launch:\n\n![launcher](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/launcher.png)\n\nMain window of Segmentation Analysis:\n\n![interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_analysis.png)\n\nMain window of Segmentation Analysis with view on measurement result:\n\n![interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_analysis2.png)\n\nWindow for creating a set of measurements:\n\n![statistics](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/measurement.png)\n\nMain window of Mask Segmentation:\n\n![mask interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_mask.png)\n\n## Laboratory\n\nLaboratory of Functional and Structural Genomics\n[http://4dnucleome.cent.uw.edu.pl/](http://4dnucleome.cent.uw.edu.pl/)\n\n## Cite as\n\nBokota, G., Sroka, J., Basu, S. et al. PartSeg: a tool for quantitative feature extraction\nfrom 3D microscopy images for dummies. BMC Bioinformatics 22, 72 (2021).\n[https://doi.org/10.1186/s12859-021-03984-1](https://doi.org/10.1186/s12859-021-03984-1)\n\n\n## Changelog\n\nAll notable changes to this project will be documented in this file.\n\n### 0.16.3 - 2025-07-10\n\n#### üêõ Bug Fixes\n\n- Do not use default parameters in default leaf ([#1283](https://github.com/4DNucleome/PartSeg/pull/1283))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `napari`, `numpy`, `pydantic`, `sentry-sdk`, `tifffile`, `vispy` ([#1273](https://github.com/4DNucleome/PartSeg/pull/1273))\n- [Automatic] Constraints upgrades: `h5py`, `ipython`, `magicgui`, `numpy`, `pandas`, `pydantic`, `requests`, `sentry-sdk`, `superqt`, `tifffile`, `xlrd` ([#1276](https://github.com/4DNucleome/PartSeg/pull/1276))\n- [Automatic] Constraints upgrades: `numpy`, `pygments`, `scipy`, `simpleitk`, `superqt`, `xlsxwriter` ([#1278](https://github.com/4DNucleome/PartSeg/pull/1278))\n- [Automatic] Constraints upgrades: `ipython`, `napari`, `partsegcore-compiled-backend`, `psygnal`, `sentry-sdk` ([#1281](https://github.com/4DNucleome/PartSeg/pull/1281))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- Block `sentry==3.0.0a1` for pre-tests ([#1272](https://github.com/4DNucleome/PartSeg/pull/1272))\n- Add fallback version for `setuptools_scm` ([#1271](https://github.com/4DNucleome/PartSeg/pull/1271))\n- [pre-commit.ci] pre-commit autoupdate ([#1270](https://github.com/4DNucleome/PartSeg/pull/1270))\n- Use ubuntu 22.04 when building pyinstaller bundle ([#1275](https://github.com/4DNucleome/PartSeg/pull/1275))\n- [pre-commit.ci] pre-commit autoupdate ([#1274](https://github.com/4DNucleome/PartSeg/pull/1274))\n- [pre-commit.ci] pre-commit autoupdate ([#1277](https://github.com/4DNucleome/PartSeg/pull/1277))\n- Block sentry alpha 3.0.0a1 and 3.0.0a2 ([#1279](https://github.com/4DNucleome/PartSeg/pull/1279))\n- Block `pytest-qt==4.5.0` (for `pyside2` compatibility) ([#1282](https://github.com/4DNucleome/PartSeg/pull/1282))\n- [pre-commit.ci] pre-commit autoupdate ([#1280](https://github.com/4DNucleome/PartSeg/pull/1280))\n- Migrate Windows images from 2019 to 2022 in Azure Pipelines and GitHub Actions ([#1284](https://github.com/4DNucleome/PartSeg/pull/1284))\n\n### 0.16.2 - 2025-05-12\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `numpy`, `partsegcore-compiled-backend`, `tifffile` ([#1253](https://github.com/4DNucleome/PartSeg/pull/1253))\n- [Automatic] Constraints upgrades: `imagecodecs`, `ipython`, `pydantic`, `sentry-sdk`, `superqt`, `tifffile` ([#1255](https://github.com/4DNucleome/PartSeg/pull/1255))\n- [Automatic] Constraints upgrades: `ipython`, `napari`, `numpy`, `packaging`, `pydantic`, `sentry-sdk`, `simpleitk`, `sympy`, `xlsxwriter` ([#1266](https://github.com/4DNucleome/PartSeg/pull/1266))\n- [Automatic] Constraints upgrades: `oiffile`, `psygnal`, `scipy`, `sentry-sdk`, `tifffile` ([#1267](https://github.com/4DNucleome/PartSeg/pull/1267))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- [pre-commit.ci] pre-commit autoupdate ([#1251](https://github.com/4DNucleome/PartSeg/pull/1251))\n- Prepare for napari 0.6.0 deprecations ([#1256](https://github.com/4DNucleome/PartSeg/pull/1256))\n- [pre-commit.ci] pre-commit autoupdate ([#1254](https://github.com/4DNucleome/PartSeg/pull/1254))\n- [pre-commit.ci] pre-commit autoupdate ([#1259](https://github.com/4DNucleome/PartSeg/pull/1259))\n- Update workflow to use Ubuntu 22.04 instead of 20.04 ([#1261](https://github.com/4DNucleome/PartSeg/pull/1261))\n- [pre-commit.ci] pre-commit autoupdate ([#1260](https://github.com/4DNucleome/PartSeg/pull/1260))\n- Try to fix Upgrade dependencies workflow part 2 ([#1265](https://github.com/4DNucleome/PartSeg/pull/1265))\n- [pre-commit.ci] pre-commit autoupdate ([#1268](https://github.com/4DNucleome/PartSeg/pull/1268))\n\n#### üõ°Ô∏è Security\n\n- *(deps)* Bump peter-evans/create-pull-request from 6 to 7 ([#1262](https://github.com/4DNucleome/PartSeg/pull/1262))\n- *(deps)* Bump codecov/codecov-action from 4 to 5 ([#1263](https://github.com/4DNucleome/PartSeg/pull/1263))\n\n### 0.16.1 - 2025-03-10\n\n#### üêõ Bug Fixes\n\n- Fix rendering of alternative representation if there are more components in the alternative representation than in ROI ([#1240](https://github.com/4DNucleome/PartSeg/pull/1240))\n- Enable czifile workaround for 2019.7.2.1 ([#1246](https://github.com/4DNucleome/PartSeg/pull/1246))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `ipython`, `magicgui`, `numpy`, `partsegcore-compiled-backend`, `pydantic` ([#1231](https://github.com/4DNucleome/PartSeg/pull/1231))\n- [Automatic] Constraints upgrades: `imagecodecs` ([#1233](https://github.com/4DNucleome/PartSeg/pull/1233))\n- [Automatic] Constraints upgrades: `oiffile`, `pygments`, `scipy`, `superqt` ([#1234](https://github.com/4DNucleome/PartSeg/pull/1234))\n- [Automatic] Constraints upgrades: `imageio`, `napari`, `numpy`, `partsegcore-compiled-backend`, `pydantic`, `pygments`, `scipy`, `sentry-sdk`, `simpleitk`, `tifffile`, `xlsxwriter` ([#1236](https://github.com/4DNucleome/PartSeg/pull/1236))\n- [Automatic] Constraints upgrades: `ipython`, `xlsxwriter` ([#1239](https://github.com/4DNucleome/PartSeg/pull/1239))\n- [Automatic] Constraints upgrades: `numpy`, `partsegcore-compiled-backend`, `psygnal`, `qtpy`, `scipy`, `sentry-sdk` ([#1241](https://github.com/4DNucleome/PartSeg/pull/1241))\n- [Automatic] Constraints upgrades: `czifile`, `h5py`, `ipython`, `qtawesome`, `sentry-sdk`, `tifffile`, `traceback-with-variables` ([#1247](https://github.com/4DNucleome/PartSeg/pull/1247))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- [pre-commit.ci] pre-commit autoupdate ([#1232](https://github.com/4DNucleome/PartSeg/pull/1232))\n- [pre-commit.ci] pre-commit autoupdate ([#1235](https://github.com/4DNucleome/PartSeg/pull/1235))\n- [pre-commit.ci] pre-commit autoupdate ([#1237](https://github.com/4DNucleome/PartSeg/pull/1237))\n- Drop python 3.9 tests for napari repository ([#1244](https://github.com/4DNucleome/PartSeg/pull/1244))\n- Block ipykernel 7.0.0a1 ([#1248](https://github.com/4DNucleome/PartSeg/pull/1248))\n- [pre-commit.ci] pre-commit autoupdate ([#1242](https://github.com/4DNucleome/PartSeg/pull/1242))\n- Switch to `get_qapp` from `get_app` to handle napari deprecation ([#1249](https://github.com/4DNucleome/PartSeg/pull/1249))\n\n### 0.16.0 - 2024-12-21\n\n#### üöÄ Features\n\n- Allow set units that will be used for load/save data using PartSeg as napari plugin ([#1228](https://github.com/4DNucleome/PartSeg/pull/1228))\n- Show filename when importing with PartSeg in napari ([#1226](https://github.com/4DNucleome/PartSeg/pull/1226))\n\n#### üêõ Bug Fixes\n\n- Fix error when image changed during segmentation ([#1218](https://github.com/4DNucleome/PartSeg/pull/1218))\n- Fix pre release tests ([#1219](https://github.com/4DNucleome/PartSeg/pull/1219))\n- Drop imagej LUTs of size `24*x` ([#1227](https://github.com/4DNucleome/PartSeg/pull/1227))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `h5py`, `imageio`, `ipython`, `napari`, `numpy`, `sentry-sdk` ([#1201](https://github.com/4DNucleome/PartSeg/pull/1201))\n- [Automatic] Constraints upgrades: `sentry-sdk` ([#1214](https://github.com/4DNucleome/PartSeg/pull/1214))\n- [Automatic] Constraints upgrades: `ipython`, `traceback-with-variables` ([#1221](https://github.com/4DNucleome/PartSeg/pull/1221))\n- [Automatic] Constraints upgrades: `numpy`, `packaging`, `pydantic`, `qtconsole`, `qtpy`, `sentry-sdk`, `traceback-with-variables` ([#1223](https://github.com/4DNucleome/PartSeg/pull/1223))\n- [Automatic] Constraints upgrades: `imageio`, `ipython`, `pydantic` ([#1225](https://github.com/4DNucleome/PartSeg/pull/1225))\n- [Automatic] Constraints upgrades: `napari`, `numpy`, `pydantic`, `sentry-sdk`, `six`, `superqt`, `tifffile` ([#1229](https://github.com/4DNucleome/PartSeg/pull/1229))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- Stop using mambaforge in tests ([#1203](https://github.com/4DNucleome/PartSeg/pull/1203))\n- [pre-commit.ci] pre-commit autoupdate ([#1202](https://github.com/4DNucleome/PartSeg/pull/1202))\n- [pre-commit.ci] pre-commit autoupdate ([#1204](https://github.com/4DNucleome/PartSeg/pull/1204))\n- Remove defining default version of language in pre-commit configuration ([#1208](https://github.com/4DNucleome/PartSeg/pull/1208))\n- [pre-commit.ci] pre-commit autoupdate ([#1209](https://github.com/4DNucleome/PartSeg/pull/1209))\n- Drop python 3.8 ([#1206](https://github.com/4DNucleome/PartSeg/pull/1206))\n- Use `PublishPipelineArtifact` in place of `PublishBuildArtifacts` to reduce CI fragility ([#1213](https://github.com/4DNucleome/PartSeg/pull/1213))\n- Update CI configuration to use more modern OS and python versions ([#1207](https://github.com/4DNucleome/PartSeg/pull/1207))\n- Fix `upgrade-dependencies.yaml`docs constraints ([#1215](https://github.com/4DNucleome/PartSeg/pull/1215))\n- [pre-commit.ci] pre-commit autoupdate ([#1216](https://github.com/4DNucleome/PartSeg/pull/1216))\n- Add changelog 0.16.0a1\n- [pre-commit.ci] pre-commit autoupdate ([#1222](https://github.com/4DNucleome/PartSeg/pull/1222))\n- [pre-commit.ci] pre-commit autoupdate ([#1230](https://github.com/4DNucleome/PartSeg/pull/1230))\n\n### 0.15.4 - 2024-09-27\n\n#### üöÄ Features\n\n- Add preview of image metadata ([#1154](https://github.com/4DNucleome/PartSeg/pull/1154))\n- Add option to combine channels using sum and max ([#1159](https://github.com/4DNucleome/PartSeg/pull/1159))\n- Add metadata viewer as napari widget ([#1195](https://github.com/4DNucleome/PartSeg/pull/1195))\n- Read channel colors from `*.czi` metadata ([#1198](https://github.com/4DNucleome/PartSeg/pull/1198))\n- Use image color when add layer to napari ([#1200](https://github.com/4DNucleome/PartSeg/pull/1200))\n\n#### üêõ Bug Fixes\n\n- Fix selection of custom label colors for napari 0.5.0 ([#1138](https://github.com/4DNucleome/PartSeg/pull/1138))\n- Add pint call to enforce initialization of unit registry ([#1146](https://github.com/4DNucleome/PartSeg/pull/1146))\n- Workaround for lack of zsd support in czifile ([#1142](https://github.com/4DNucleome/PartSeg/pull/1142))\n- Fix preparing data for `mahotas.haralick` to avoid overflow problem ([#1150](https://github.com/4DNucleome/PartSeg/pull/1150))\n- Fix `use_convex` type from `int` to `bool` for segmentation algorithms ([#1152](https://github.com/4DNucleome/PartSeg/pull/1152))\n- Prevent propagation of decreasing contrast limits set by user ([#1166](https://github.com/4DNucleome/PartSeg/pull/1166))\n- Prevent error on searching component if there is no component ([#1167](https://github.com/4DNucleome/PartSeg/pull/1167))\n- Fix checking if channel requested by MeasurementProfile exists ([#1165](https://github.com/4DNucleome/PartSeg/pull/1165))\n- Fix trying to access to just deleted measurement profile from edit window. ([#1168](https://github.com/4DNucleome/PartSeg/pull/1168))\n- Fix bug in code for checking for survey file ([#1174](https://github.com/4DNucleome/PartSeg/pull/1174))\n- Fix plugin discovery in bundle to register them in napari viewer ([#1175](https://github.com/4DNucleome/PartSeg/pull/1175))\n- Fix problem with setting range of auto-generated widget ([#1187](https://github.com/4DNucleome/PartSeg/pull/1187))\n- Fix reading channel names from single channel czi files ([#1194](https://github.com/4DNucleome/PartSeg/pull/1194))\n\n#### üöú Refactor\n\n- Make warnings error in tests ([#1192](https://github.com/4DNucleome/PartSeg/pull/1192))\n- Merge all channel-specific attributes of the Image class ([#1191](https://github.com/4DNucleome/PartSeg/pull/1191))\n\n#### üìö Documentation\n\n- Change homepage URL ([#1139](https://github.com/4DNucleome/PartSeg/pull/1139))\n- Add link for download macOS arm bundle ([#1140](https://github.com/4DNucleome/PartSeg/pull/1140))\n- Add changelog for 0.15.4 release\n- Update changelog ([#1176](https://github.com/4DNucleome/PartSeg/pull/1176))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `napari`, `sentry-sdk`, `sympy` ([#1128](https://github.com/4DNucleome/PartSeg/pull/1128))\n- [Automatic] Constraints upgrades: `mahotas`, `numpy`, `sentry-sdk`, `sympy` ([#1145](https://github.com/4DNucleome/PartSeg/pull/1145))\n- [Automatic] Constraints upgrades: `numpy`, `tifffile` ([#1163](https://github.com/4DNucleome/PartSeg/pull/1163))\n- [Automatic] Constraints upgrades: `napari`, `sentry-sdk`, `tifffile` ([#1169](https://github.com/4DNucleome/PartSeg/pull/1169))\n- [Automatic] Constraints upgrades: `magicgui`, `sentry-sdk` ([#1172](https://github.com/4DNucleome/PartSeg/pull/1172))\n- [Automatic] Constraints upgrades: `sympy`, `tifffile` ([#1177](https://github.com/4DNucleome/PartSeg/pull/1177))\n- [Automatic] Constraints upgrades: `imageio`, `napari`, `numpy` ([#1180](https://github.com/4DNucleome/PartSeg/pull/1180))\n- Constraints upgrades: `sentry-sdk` and fix tests ([#1182](https://github.com/4DNucleome/PartSeg/pull/1182))\n- `napari==0.5.3` related fixes, Constraints upgrades: `imageio`, `ipython`, `numpy`, `qtconsole`, `scipy`, `simpleitk`, `tifffile` ([#1183](https://github.com/4DNucleome/PartSeg/pull/1183))\n- [Automatic] Constraints upgrades: `numpy`, `pydantic` ([#1188](https://github.com/4DNucleome/PartSeg/pull/1188))\n- [Automatic] Constraints upgrades: `imagecodecs`, `pandas`, `pydantic`, `sentry-sdk`, `sympy`, `tifffile` ([#1190](https://github.com/4DNucleome/PartSeg/pull/1190))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- Speedup tests by use `tox-uv` ([#1141](https://github.com/4DNucleome/PartSeg/pull/1141))\n- Get additional dict from PR branch for checking PR title ([#1144](https://github.com/4DNucleome/PartSeg/pull/1144))\n- Relax numpy constraint ([#1143](https://github.com/4DNucleome/PartSeg/pull/1143))\n- Allow to skip spellchecking PR title ([#1147](https://github.com/4DNucleome/PartSeg/pull/1147))\n- [pre-commit.ci] pre-commit autoupdate ([#1149](https://github.com/4DNucleome/PartSeg/pull/1149))\n- Create only archive with version in name on azures pipeline ([#1151](https://github.com/4DNucleome/PartSeg/pull/1151))\n- Fix tests for napari from repository ([#1148](https://github.com/4DNucleome/PartSeg/pull/1148))\n- Use python 3.11 to determine updated packages in PR description ([#1160](https://github.com/4DNucleome/PartSeg/pull/1160))\n- [pre-commit.ci] pre-commit autoupdate ([#1164](https://github.com/4DNucleome/PartSeg/pull/1164))\n- [pre-commit.ci] pre-commit autoupdate ([#1170](https://github.com/4DNucleome/PartSeg/pull/1170))\n- Disable thumbnail generation in napari layer as it is fragile and not used ([#1171](https://github.com/4DNucleome/PartSeg/pull/1171))\n- [pre-commit.ci] pre-commit autoupdate ([#1173](https://github.com/4DNucleome/PartSeg/pull/1173))\n- [pre-commit.ci] pre-commit autoupdate ([#1178](https://github.com/4DNucleome/PartSeg/pull/1178))\n- Fix call of logger to properly pass arguments to messages ([#1179](https://github.com/4DNucleome/PartSeg/pull/1179))\n- Fix coverage files upload by enable hidden files upload ([#1186](https://github.com/4DNucleome/PartSeg/pull/1186))\n- [pre-commit.ci] pre-commit autoupdate ([#1184](https://github.com/4DNucleome/PartSeg/pull/1184))\n- Use PyQt6 in pre-tests ([#1196](https://github.com/4DNucleome/PartSeg/pull/1196))\n- Add missed code from #1191 ([#1197](https://github.com/4DNucleome/PartSeg/pull/1197))\n- [pre-commit.ci] pre-commit autoupdate ([#1189](https://github.com/4DNucleome/PartSeg/pull/1189))\n- Auto add \"skip check PR title\" label in update dependencies PR ([#1199](https://github.com/4DNucleome/PartSeg/pull/1199))\n\n#### Build\n\n- Remove PyOpenGL-accelerate from dependencies because of numpy incompatibility ([#1155](https://github.com/4DNucleome/PartSeg/pull/1155))\n- Update install constraints on numpy and qt packages ([#1157](https://github.com/4DNucleome/PartSeg/pull/1157))\n- Enforce napari 0.5.0 for Qt6 bindings ([#1161](https://github.com/4DNucleome/PartSeg/pull/1161))\n- Require napari>=0.5.0 only for python 3.9+ ([#1162](https://github.com/4DNucleome/PartSeg/pull/1162))\n\n### 0.15.3 - 2024-07-08\n\n#### üöÄ Features\n\n- Pydantic 2 compatibility ([#1084](https://github.com/4DNucleome/PartSeg/pull/1084))\n\n#### üêõ Bug Fixes\n\n- Fix rendering icons in colormap preview ([#1040](https://github.com/4DNucleome/PartSeg/pull/1040))\n- Fix test for validation length of message for sentry-sdk 2.0 release ([#1098](https://github.com/4DNucleome/PartSeg/pull/1098))\n- When fix reader check lowercase extension for validate compatibility ([#1097](https://github.com/4DNucleome/PartSeg/pull/1097))\n- Fix napari 0.5.0 compatibility ([#1116](https://github.com/4DNucleome/PartSeg/pull/1116))\n\n#### üöú Refactor\n\n- Fix Qt flags ([#1041](https://github.com/4DNucleome/PartSeg/pull/1041))\n- Fix qt flags in roi mask code ([#1042](https://github.com/4DNucleome/PartSeg/pull/1042))\n- Fix qt flags in roi analysis ([#1043](https://github.com/4DNucleome/PartSeg/pull/1043))\n- Migrate from setup.cfg to `pyproject.toml` ([#1070](https://github.com/4DNucleome/PartSeg/pull/1070))\n\n#### üìö Documentation\n\n- Allow to use newer release of build docs dependencies ([#1057](https://github.com/4DNucleome/PartSeg/pull/1057))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `imagecodecs`, `imageio`, `ipykernel`, `ipython`, `numpy`, `oiffile`, `pandas`, `psygnal`, `pyinstaller`, `qtconsole`, `qtpy`, `sentry-sdk`, `simpleitk`, `superqt`, `tifffile`, `xlsxwriter` ([#1020](https://github.com/4DNucleome/PartSeg/pull/1020))\n- [Automatic] Constraints upgrades: `h5py`, `imageio`, `ipython`, `numpy`, `packaging`, `pydantic`, `pyinstaller`, `pyqt5`, `scipy`, `sentry-sdk`, `superqt`, `tifffile`, `xlsxwriter` ([#1027](https://github.com/4DNucleome/PartSeg/pull/1027))\n- [Automatic] Constraints upgrades: `imageio`, `magicgui`, `xlsxwriter` ([#1030](https://github.com/4DNucleome/PartSeg/pull/1030))\n- [Automatic] Constraints upgrades: `ipykernel`, `pandas`, `qtpy` ([#1032](https://github.com/4DNucleome/PartSeg/pull/1032))\n- [Automatic] Constraints upgrades: `imageio`, `ipykernel`, `ipython`, `numpy`, `pandas`, `psygnal`, `pygments`, `pyinstaller`, `qtconsole`, `scipy`, `sentry-sdk`, `simpleitk` ([#1035](https://github.com/4DNucleome/PartSeg/pull/1035))\n- [Automatic] Constraints upgrades: `imagecodecs`, `imageio`, `ipykernel`, `magicgui`, `pandas`, `pyinstaller`, `qtawesome`, `sentry-sdk`, `tifffile` ([#1048](https://github.com/4DNucleome/PartSeg/pull/1048))\n- [Automatic] Constraints upgrades: `ipykernel`, `numpy`, `pandas`, `partsegcore-compiled-backend`, `pydantic`, `scipy`, `sentry-sdk` ([#1058](https://github.com/4DNucleome/PartSeg/pull/1058))\n- Improve test of PartSegImage ([#1072](https://github.com/4DNucleome/PartSeg/pull/1072))\n- Improve test suite for `PartSegCore` ([#1077](https://github.com/4DNucleome/PartSeg/pull/1077))\n- [Automatic] Constraints upgrades: `imageio`, `ipykernel`, `local-migrator`, `napari`, `numpy`, `pandas`, `partsegcore-compiled-backend`, `pyinstaller`, `sentry-sdk`, `tifffile`, `vispy`, `xlsxwriter` ([#1063](https://github.com/4DNucleome/PartSeg/pull/1063))\n- [Automatic] Constraints upgrades: `magicgui`, `packaging`, `psygnal`, `pyinstaller`, `sentry-sdk`, `superqt` ([#1086](https://github.com/4DNucleome/PartSeg/pull/1086))\n- [Automatic] Constraints upgrades: `psygnal`, `pydantic`, `sentry-sdk`, `vispy` ([#1090](https://github.com/4DNucleome/PartSeg/pull/1090))\n- [Automatic] Constraints upgrades: `h5py`, `ipykernel`, `mahotas`, `pandas`, `psygnal`, `pydantic`, `pyinstaller`, `qtawesome`, `scipy`, `sentry-sdk`, `superqt` ([#1092](https://github.com/4DNucleome/PartSeg/pull/1092))\n- [Automatic] Constraints upgrades: `imageio`, `tifffile` ([#1100](https://github.com/4DNucleome/PartSeg/pull/1100))\n- [Automatic] Constraints upgrades: `pydantic`, `sentry-sdk`, `superqt`, `tifffile` ([#1102](https://github.com/4DNucleome/PartSeg/pull/1102))\n- [Automatic] Constraints upgrades: `psygnal`, `pygments`, `qtconsole`, `sentry-sdk`, `superqt`, `tifffile` ([#1105](https://github.com/4DNucleome/PartSeg/pull/1105))\n- [Automatic] Constraints upgrades: `imagecodecs`, `magicgui`, `oiffile`, `openpyxl`, `packaging`, `pydantic`, `pyinstaller`, `requests`, `scipy`, `sentry-sdk`, `superqt`, `sympy`, `tifffile`, `vispy` ([#1107](https://github.com/4DNucleome/PartSeg/pull/1107))\n- [Automatic] Constraints upgrades: `pydantic` ([#1112](https://github.com/4DNucleome/PartSeg/pull/1112))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- [pre-commit.ci] pre-commit autoupdate ([#1019](https://github.com/4DNucleome/PartSeg/pull/1019))\n- Remove plugin page preview as it is no longer maintained ([#1021](https://github.com/4DNucleome/PartSeg/pull/1021))\n- [pre-commit.ci] pre-commit autoupdate ([#1022](https://github.com/4DNucleome/PartSeg/pull/1022))\n- [pre-commit.ci] pre-commit autoupdate ([#1026](https://github.com/4DNucleome/PartSeg/pull/1026))\n- [pre-commit.ci] pre-commit autoupdate ([#1031](https://github.com/4DNucleome/PartSeg/pull/1031))\n- [pre-commit.ci] pre-commit autoupdate ([#1034](https://github.com/4DNucleome/PartSeg/pull/1034))\n- Use new semgrep configuration ([#1039](https://github.com/4DNucleome/PartSeg/pull/1039))\n- Upload raw coverage information ([#1044](https://github.com/4DNucleome/PartSeg/pull/1044))\n- [pre-commit.ci] pre-commit autoupdate ([#1036](https://github.com/4DNucleome/PartSeg/pull/1036))\n- Run coverage upload in separate steep ([#1053](https://github.com/4DNucleome/PartSeg/pull/1053))\n- Generate local report in `Tests` workflow and use proper script for fetch report ([#1054](https://github.com/4DNucleome/PartSeg/pull/1054))\n- Move coverage back to main workflow ([#1055](https://github.com/4DNucleome/PartSeg/pull/1055))\n- [pre-commit.ci] pre-commit autoupdate ([#1056](https://github.com/4DNucleome/PartSeg/pull/1056))\n- [pre-commit.ci] pre-commit autoupdate ([#1059](https://github.com/4DNucleome/PartSeg/pull/1059))\n- Update `actions/upload-artifact` and `actions/download-artifact` from 3 to 4 ([#1062](https://github.com/4DNucleome/PartSeg/pull/1062))\n- [pre-commit.ci] pre-commit autoupdate ([#1064](https://github.com/4DNucleome/PartSeg/pull/1064))\n- Group actions update ([#1065](https://github.com/4DNucleome/PartSeg/pull/1065))\n- [pre-commit.ci] pre-commit autoupdate ([#1068](https://github.com/4DNucleome/PartSeg/pull/1068))\n- Remove requirement of 2 builds upload to codecov.io ([#1073](https://github.com/4DNucleome/PartSeg/pull/1073))\n- Re add tests to coverage report ([#1074](https://github.com/4DNucleome/PartSeg/pull/1074))\n- Switch from setup.cfg to pyproject.toml in workflows ([#1076](https://github.com/4DNucleome/PartSeg/pull/1076))\n- Fix compiling pyinstaller pre-deps ([#1075](https://github.com/4DNucleome/PartSeg/pull/1075))\n- Add codespell to pre-commit and fix pointed bugs ([#1078](https://github.com/4DNucleome/PartSeg/pull/1078))\n- Add new ruff rules and apply them ([#1079](https://github.com/4DNucleome/PartSeg/pull/1079))\n- [pre-commit.ci] pre-commit autoupdate ([#1080](https://github.com/4DNucleome/PartSeg/pull/1080))\n- [pre-commit.ci] pre-commit autoupdate ([#1081](https://github.com/4DNucleome/PartSeg/pull/1081))\n- Fix upgrade depenecies workflow ([#1083](https://github.com/4DNucleome/PartSeg/pull/1083))\n- Block using `mpmath==1.4.0a0` and `sentry-sdk` 2.0.0a1/a2 in pre-test ([#1085](https://github.com/4DNucleome/PartSeg/pull/1085))\n- [pre-commit.ci] pre-commit autoupdate ([#1089](https://github.com/4DNucleome/PartSeg/pull/1089))\n- Fix jupyter failing test by using constraints ([#1093](https://github.com/4DNucleome/PartSeg/pull/1093))\n- [pre-commit.ci] pre-commit autoupdate ([#1091](https://github.com/4DNucleome/PartSeg/pull/1091))\n- [pre-commit.ci] pre-commit autoupdate ([#1096](https://github.com/4DNucleome/PartSeg/pull/1096))\n- Add python 3.12 testing ([#1087](https://github.com/4DNucleome/PartSeg/pull/1087))\n- Exclude pyside2 on python 3.11 and 3.12 from testing ([#1099](https://github.com/4DNucleome/PartSeg/pull/1099))\n- [pre-commit.ci] pre-commit autoupdate ([#1101](https://github.com/4DNucleome/PartSeg/pull/1101))\n- [pre-commit.ci] pre-commit autoupdate ([#1103](https://github.com/4DNucleome/PartSeg/pull/1103))\n- Bump macos runners to macos-13 (both azure and GHA) ([#1113](https://github.com/4DNucleome/PartSeg/pull/1113))\n- [pre-commit.ci] pre-commit autoupdate ([#1108](https://github.com/4DNucleome/PartSeg/pull/1108))\n- Remove pyqt5 from constraints ([#1118](https://github.com/4DNucleome/PartSeg/pull/1118))\n- Add workflow for releases from GHA ([#1117](https://github.com/4DNucleome/PartSeg/pull/1117))\n- Add actionlint to CI to early prevent bug in github workflows ([#1119](https://github.com/4DNucleome/PartSeg/pull/1119))\n- Fix release workflow, by update permissions\n- Check if release notes are properly created ([#1122](https://github.com/4DNucleome/PartSeg/pull/1122))\n- Proper use enum in checking new version ([#1123](https://github.com/4DNucleome/PartSeg/pull/1123))\n- Refactor and simplify menu bar creation, add workaround for macOS numpy problem ([#1124](https://github.com/4DNucleome/PartSeg/pull/1124))\n- Simplify release workflow ([#1126](https://github.com/4DNucleome/PartSeg/pull/1126))\n- Fix `make_release.yml` to proper detect release, attempt 3 ([#1127](https://github.com/4DNucleome/PartSeg/pull/1127))\n\n#### üõ°Ô∏è Security\n\n- *(deps)* Bump actions/checkout from 3 to 4 ([#1029](https://github.com/4DNucleome/PartSeg/pull/1029))\n- *(deps)* Bump conda-incubator/setup-miniconda from 2 to 3 ([#1038](https://github.com/4DNucleome/PartSeg/pull/1038))\n- *(deps)* Bump aganders3/headless-gui from 1 to 2 ([#1047](https://github.com/4DNucleome/PartSeg/pull/1047))\n- *(deps)* Bump actions/checkout from 3 to 4 ([#1045](https://github.com/4DNucleome/PartSeg/pull/1045))\n- *(deps)* Bump hynek/build-and-inspect-python-package from 1 to 2 ([#1050](https://github.com/4DNucleome/PartSeg/pull/1050))\n- *(deps)* Bump actions/setup-python from 4 to 5 ([#1046](https://github.com/4DNucleome/PartSeg/pull/1046))\n- *(deps)* Bump github/codeql-action from 2 to 3 ([#1051](https://github.com/4DNucleome/PartSeg/pull/1051))\n- *(deps)* Bump peter-evans/create-pull-request from 5 to 6 ([#1067](https://github.com/4DNucleome/PartSeg/pull/1067))\n- *(deps)* Bump codecov/codecov-action from 3 to 4 ([#1066](https://github.com/4DNucleome/PartSeg/pull/1066))\n\n#### Build\n\n- Fix not bundling `Font Awesome 6 Free-Solid-900.otf` file to executable ([#1114](https://github.com/4DNucleome/PartSeg/pull/1114))\n- Update readme and release to point to GitHub releases ([#1115](https://github.com/4DNucleome/PartSeg/pull/1115))\n- Do not create archive twice when create bundle ([#1120](https://github.com/4DNucleome/PartSeg/pull/1120))\n- Enable macOS-arm bundle builds ([#1121](https://github.com/4DNucleome/PartSeg/pull/1121))\n\n### 0.15.2 - 2023-08-28\n\n#### üêõ Bug Fixes\n\n- Fix range threshold selection of algorithms ([#1009](https://github.com/4DNucleome/PartSeg/pull/1009))\n- When run batch check if file extension is supported by loader ([#1016](https://github.com/4DNucleome/PartSeg/pull/1016))\n- Do not allow to select and render corrupted batch plans ([#1015](https://github.com/4DNucleome/PartSeg/pull/1015))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `imagecodecs`, `ipykernel`, `magicgui`, `psygnal`, `scipy`, `superqt`, `tifffile` ([#1011](https://github.com/4DNucleome/PartSeg/pull/1011))\n- [Automatic] Constraints upgrades: `imageio`, `pyinstaller`, `tifffile` ([#1018](https://github.com/4DNucleome/PartSeg/pull/1018))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- Use faster version of black ([#1010](https://github.com/4DNucleome/PartSeg/pull/1010))\n- [pre-commit.ci] pre-commit autoupdate ([#1013](https://github.com/4DNucleome/PartSeg/pull/1013))\n\n### 0.15.1 - 2023-08-08\n\n#### üöÄ Features\n\n- Allow to save multiple napari image layers to single tiff file ([#1000](https://github.com/4DNucleome/PartSeg/pull/1000))\n- Add option to export batch project with data ([#996](https://github.com/4DNucleome/PartSeg/pull/996))\n\n#### üêõ Bug Fixes\n\n- Fix possible problem of double registration napari plugin in PartSeg bundle ([#974](https://github.com/4DNucleome/PartSeg/pull/974))\n- Bump OS versions for part of testing workflows. ([#977](https://github.com/4DNucleome/PartSeg/pull/977))\n- Bump os version for main tests workflow. ([#979](https://github.com/4DNucleome/PartSeg/pull/979))\n- Ensure that the module `PartSegCore.channel_class` is present in bundle ([#980](https://github.com/4DNucleome/PartSeg/pull/980))\n- Lower npe2 schema version to work with older napari version ([#981](https://github.com/4DNucleome/PartSeg/pull/981))\n- Generate test report per platform ([#978](https://github.com/4DNucleome/PartSeg/pull/978))\n- Importing plugins in bundle keeping proper module names ([#983](https://github.com/4DNucleome/PartSeg/pull/983))\n- Fix napari repo workflow ([#985](https://github.com/4DNucleome/PartSeg/pull/985))\n- Fix bug in read tiff files with double `Q` in axes but one related to dummy dimension ([#992](https://github.com/4DNucleome/PartSeg/pull/992))\n- Fix bug that lead to corrupted state when saving calculation plan to excel file ([#995](https://github.com/4DNucleome/PartSeg/pull/995))\n- Enable python 3.11 test on CI, fix minor errors ([#869](https://github.com/4DNucleome/PartSeg/pull/869))\n\n#### üß™ Testing\n\n- [Automatic] Constraints upgrades: `imageio`, `ipython`, `psygnal`, `scipy`, `sentry-sdk` ([#975](https://github.com/4DNucleome/PartSeg/pull/975))\n- [Automatic] Constraints upgrades: `h5py`, `imagecodecs`, `imageio`, `ipykernel`, `napari`, `numpy`, `pandas`, `pydantic`, `pyinstaller`, `scipy`, `sentry-sdk`, `tifffile`, `vispy` ([#986](https://github.com/4DNucleome/PartSeg/pull/986))\n- [Automatic] Constraints upgrades: `imagecodecs`, `sentry-sdk`, `tifffile` ([#997](https://github.com/4DNucleome/PartSeg/pull/997))\n- [Automatic] Constraints upgrades: `ipykernel`, `pydantic` ([#1002](https://github.com/4DNucleome/PartSeg/pull/1002))\n- [Automatic] Constraints upgrades: `numpy`, `pygments`, `sentry-sdk`, `superqt` ([#1007](https://github.com/4DNucleome/PartSeg/pull/1007))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- [pre-commit.ci] pre-commit autoupdate ([#973](https://github.com/4DNucleome/PartSeg/pull/973))\n- [pre-commit.ci] pre-commit autoupdate ([#982](https://github.com/4DNucleome/PartSeg/pull/982))\n- [pre-commit.ci] pre-commit autoupdate ([#987](https://github.com/4DNucleome/PartSeg/pull/987))\n- [pre-commit.ci] pre-commit autoupdate ([#988](https://github.com/4DNucleome/PartSeg/pull/988))\n- [pre-commit.ci] pre-commit autoupdate ([#991](https://github.com/4DNucleome/PartSeg/pull/991))\n- [pre-commit.ci] pre-commit autoupdate ([#998](https://github.com/4DNucleome/PartSeg/pull/998))\n- [pre-commit.ci] pre-commit autoupdate ([#1004](https://github.com/4DNucleome/PartSeg/pull/1004))\n- Change markdown linter from pre-commit to mdformat ([#1006](https://github.com/4DNucleome/PartSeg/pull/1006))\n- [pre-commit.ci] pre-commit autoupdate ([#1008](https://github.com/4DNucleome/PartSeg/pull/1008))\n\n### 0.15.0 - 2023-05-30\n\n#### üöÄ Features\n\n- Add `PARTSEG_SENTRY_URL` env variable support and basic documentation about error reporting ([#802](https://github.com/4DNucleome/PartSeg/pull/802))\n- Allow to see underlying exception when show warning caused by exception ([#829](https://github.com/4DNucleome/PartSeg/pull/829))\n- Add voxel size measurement and allow to overwrite voxel size in batch ([#853](https://github.com/4DNucleome/PartSeg/pull/853))\n- Add alpha support for Qt6 ([#866](https://github.com/4DNucleome/PartSeg/pull/866))\n- Add option to create projection alongside z-axis ([#919](https://github.com/4DNucleome/PartSeg/pull/919))\n- Add napari image custom representation for better error report via sentry ([#861](https://github.com/4DNucleome/PartSeg/pull/861))\n- Add import and export operation for labels and colormaps ([#936](https://github.com/4DNucleome/PartSeg/pull/936))\n- Implement napari widgets for colormap and labels control ([#935](https://github.com/4DNucleome/PartSeg/pull/935))\n- Add forget all button to multiple files widget ([#942](https://github.com/4DNucleome/PartSeg/pull/942))\n- Do not abort processing whole mask segmentation project during exception on single component ([#943](https://github.com/4DNucleome/PartSeg/pull/943))\n- Add distance based watersheed to flow methods ([#915](https://github.com/4DNucleome/PartSeg/pull/915))\n- Add napari widgets for all group of algorithms ([#958](https://github.com/4DNucleome/PartSeg/pull/958))\n- Add napari widget to copy labels along z-axis ([#968](https://github.com/4DNucleome/PartSeg/pull/968))\n\n#### üêõ Bug Fixes\n\n- Print all exceptions instead of the latest one in exception dialog ([#799](https://github.com/4DNucleome/PartSeg/pull/799))\n- Fix ROIExtractionResult `__str__`and `__repr__` to use `ROIExtractionResult` not `SegmentationResult` ([#810](https://github.com/4DNucleome/PartSeg/pull/810))\n- Fix code to address changes in napari repository ([#817](https://github.com/4DNucleome/PartSeg/pull/817))\n- Fix problem with resize of multiline widgets ([#832](https://github.com/4DNucleome/PartSeg/pull/832))\n- Fix tox configuration to run all required tests ([#840](https://github.com/4DNucleome/PartSeg/pull/840))\n- Fix MSO `step_limit` description in GUI ([#843](https://github.com/4DNucleome/PartSeg/pull/843))\n- Fix `redefined-while-unused`import code for python 3.9.7 ([#844](https://github.com/4DNucleome/PartSeg/pull/844))\n- Fix warnings reported by Deepsource ([#846](https://github.com/4DNucleome/PartSeg/pull/846))\n- Ensure that \"ROI\" layer is in proper place for proper visualization ([#856](https://github.com/4DNucleome/PartSeg/pull/856))\n- Fix tests of napari widgets ([#862](https://github.com/4DNucleome/PartSeg/pull/862))\n- Fix build of bundle for a new psygnal release ([#863](https://github.com/4DNucleome/PartSeg/pull/863))\n- Fix minimal requirements pipeline ([#877](https://github.com/4DNucleome/PartSeg/pull/877))\n- Update pyinstaller configuration ([#926](https://github.com/4DNucleome/PartSeg/pull/926))\n- Use text icon, not pixmap icon in colormap and labels list ([#938](https://github.com/4DNucleome/PartSeg/pull/938))\n- Resolve warnings when testing custom save dialog. ([#941](https://github.com/4DNucleome/PartSeg/pull/941))\n- Add padding zeros for component num when load Mask seg file to ROI GUI ([#944](https://github.com/4DNucleome/PartSeg/pull/944))\n- Proper calculate bounds for watershed napari widget ([#969](https://github.com/4DNucleome/PartSeg/pull/969))\n- Fix bug in the wrong order of axis saved in napari contribution ([#972](https://github.com/4DNucleome/PartSeg/pull/972))\n\n#### üöú Refactor\n\n- Simplify and refactor github workflows. ([#864](https://github.com/4DNucleome/PartSeg/pull/864))\n- Better load Mask project in Roi Analysis ([#921](https://github.com/4DNucleome/PartSeg/pull/921))\n- Use more descriptive names in `pylint: disable` ([#922](https://github.com/4DNucleome/PartSeg/pull/922))\n- Remove `pkg_resources` usage as it is deprecated ([#967](https://github.com/4DNucleome/PartSeg/pull/967))\n- Convert napari plugin to npe2 ([#966](https://github.com/4DNucleome/PartSeg/pull/966))\n\n#### üìö Documentation\n\n- Update README and project metadata ([#805](https://github.com/4DNucleome/PartSeg/pull/805))\n- Create release notes for PartSeg 0.15.0 ([#971](https://github.com/4DNucleome/PartSeg/pull/971))\n\n#### üé® Styling\n\n- Change default theme to dark, remove blinking windows on startup. ([#809](https://github.com/4DNucleome/PartSeg/pull/809))\n\n#### üß™ Testing\n\n- [Automatic] Dependency upgrades: `packaging`, `pyinstaller`, `pyopengl-accelerate`, `tifffile`, `xlsxwriter` ([#932](https://github.com/4DNucleome/PartSeg/pull/932))\n- [Automatic] Constraints upgrades: `fonticon-fontawesome6`, `imageio`, `numpy`, `partsegcore-compiled-backend`, `pygments`, `sentry-sdk` ([#937](https://github.com/4DNucleome/PartSeg/pull/937))\n- [Automatic] Constraints upgrades: `imageio`, `ipython`, `pandas`, `requests`, `sentry-sdk` ([#948](https://github.com/4DNucleome/PartSeg/pull/948))\n- [Automatic] Constraints upgrades: `ipython`, `nme`, `qtconsole`, `requests`, `sentry-sdk` ([#955](https://github.com/4DNucleome/PartSeg/pull/955))\n- [Automatic] Constraints upgrades: `ipykernel`, `local-migrator`, `pyinstaller`, `sentry-sdk`, `sympy` ([#957](https://github.com/4DNucleome/PartSeg/pull/957))\n- [Automatic] Constraints upgrades: `sentry-sdk`, `xlsxwriter` ([#959](https://github.com/4DNucleome/PartSeg/pull/959))\n- [Automatic] Constraints upgrades: `requests` ([#961](https://github.com/4DNucleome/PartSeg/pull/961))\n- [Automatic] Constraints upgrades: `imageio`, `pandas`, `pydantic`, `pyopengl-accelerate`, `sentry-sdk`, `xlsxwriter` ([#970](https://github.com/4DNucleome/PartSeg/pull/970))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- Improve ruff configuration, remove isort ([#815](https://github.com/4DNucleome/PartSeg/pull/815))\n- Use `fail_on_no_env` feature from `tox-gh-actions` ([#842](https://github.com/4DNucleome/PartSeg/pull/842))\n- Add python 3.11 to list of supported versions ([#867](https://github.com/4DNucleome/PartSeg/pull/867))\n- Disable python 3.11 test because of timeout ([#870](https://github.com/4DNucleome/PartSeg/pull/870))\n- Bump ruff to 0.0.218, remove flake8 from pre-commit ([#880](https://github.com/4DNucleome/PartSeg/pull/880))\n- Replace GabrielBB/xvfb-action@v1 by aganders3/headless-gui, part 2 ([#887](https://github.com/4DNucleome/PartSeg/pull/887))\n- Better minimal requirements test ([#888](https://github.com/4DNucleome/PartSeg/pull/888))\n- Improve regexp for proper generate list of packages in update report ([#894](https://github.com/4DNucleome/PartSeg/pull/894))\n- Add check for PR title ([#933](https://github.com/4DNucleome/PartSeg/pull/933))\n- Update codecov configuration to wait on two reports before post information ([#934](https://github.com/4DNucleome/PartSeg/pull/934))\n- [pre-commit.ci] pre-commit autoupdate ([#945](https://github.com/4DNucleome/PartSeg/pull/945))\n- Migrate from `nme` to `local_migrator` ([#951](https://github.com/4DNucleome/PartSeg/pull/951))\n- [pre-commit.ci] pre-commit autoupdate ([#956](https://github.com/4DNucleome/PartSeg/pull/956))\n- [pre-commit.ci] pre-commit autoupdate ([#964](https://github.com/4DNucleome/PartSeg/pull/964))\n\n#### üõ°Ô∏è Security\n\n- *(deps)* Bump peter-evans/create-pull-request from 4 to 5 ([#928](https://github.com/4DNucleome/PartSeg/pull/928))\n\n#### Bugfix\n\n- Fix bug with generation of form for model with hidden field ([#920](https://github.com/4DNucleome/PartSeg/pull/920))\n\n#### Dep\n\n- [Automatic] Dependency upgrades ([#824](https://github.com/4DNucleome/PartSeg/pull/824))\n- [Automatic] Dependency upgrades ([#828](https://github.com/4DNucleome/PartSeg/pull/828))\n- [Automatic] Dependency upgrades: `ipykernel`, `packaging` ([#838](https://github.com/4DNucleome/PartSeg/pull/838))\n- [Automatic] Dependency upgrades: `imageio`, `ipykernel`, `napari`, `numpy`, `sentry` ([#850](https://github.com/4DNucleome/PartSeg/pull/850))\n- [Automatic] Dependency upgrades: `imagecodecs`, `ipykernel`, `numpy`, `psygnal` ([#859](https://github.com/4DNucleome/PartSeg/pull/859))\n- [Automatic] Dependency upgrades: `pydantic`, `pygments`, `xlsxwriter` ([#874](https://github.com/4DNucleome/PartSeg/pull/874))\n- [Automatic] Dependency upgrades: `imageio`, `packaging`, `scipy`, `xlsxwriter` ([#878](https://github.com/4DNucleome/PartSeg/pull/878))\n- [Automatic] Dependency upgrades: `ipykernel`, `requests`, `sentry`, `xlsxwriter` ([#884](https://github.com/4DNucleome/PartSeg/pull/884))\n- [Automatic] Dependency upgrades: `h5py`, `imagecodecs`, `imageio`, `ipykernel`, `pandas`, `sentry`, `tifffile` ([#889](https://github.com/4DNucleome/PartSeg/pull/889))\n- [Automatic] Dependency upgrades: `ipython`, `pyqt5` ([#893](https://github.com/4DNucleome/PartSeg/pull/893))\n- [Automatic] Dependency upgrades: `imageio`, `ipykernel`, `ipython`, `numpy`, `openpyxl`, `psygnal`, `pydantic`, `pyinstaller`, `pyqt5`, `scipy`, `sentry-sdk`, `tifffile`, `xlsxwriter` ([#897](https://github.com/4DNucleome/PartSeg/pull/897))\n- [Automatic] Dependency upgrades: `imageio`, `psygnal` ([#905](https://github.com/4DNucleome/PartSeg/pull/905))\n- [Automatic] Dependency upgrades: `ipython`, `magicgui`, `scipy`, `sentry-sdk`, `tifffile` ([#906](https://github.com/4DNucleome/PartSeg/pull/906))\n- [Automatic] Dependency upgrades: `imagecodecs`, `imageio`, `ipykernel`, `openpyxl`, `pydantic`, `pyinstaller`, `qtawesome`, `qtconsole`, `sentry-sdk`, `tifffile`, `xlsxwriter` ([#908](https://github.com/4DNucleome/PartSeg/pull/908))\n- [Automatic] Dependency upgrades: `imageio`, `ipykernel`, `ipython`, `pandas`, `psygnal`, `pydantic`, `pygments`, `pyinstaller`, `qtpy`, `sentry-sdk`, `tifffile` ([#917](https://github.com/4DNucleome/PartSeg/pull/917))\n\n### 0.14.6 - 2022-11-13\n\n#### üêõ Bug Fixes\n\n- Fix bug when loading already created project causing hide of ROI layer ([#787](https://github.com/4DNucleome/PartSeg/pull/787))\n\n### 0.14.5 - 2022-11-09\n\n#### üöÄ Features\n\n- Add option for ensure type in EventedDict and use it to validate profiles structures ([#776](https://github.com/4DNucleome/PartSeg/pull/776))\n- Add option to create issue from error report dialog ([#782](https://github.com/4DNucleome/PartSeg/pull/782))\n- Add option for multiline field in algorithm parameters ([#766](https://github.com/4DNucleome/PartSeg/pull/766))\n\n#### üêõ Bug Fixes\n\n- Fix scalebar color ([#774](https://github.com/4DNucleome/PartSeg/pull/774))\n- Fix bug when saving segmentation parameters in mask analysis ([#781](https://github.com/4DNucleome/PartSeg/pull/781))\n- Fix multiple error related to loading new file in interactive mode ([#784](https://github.com/4DNucleome/PartSeg/pull/784))\n\n#### üöú Refactor\n\n- Optimize CLI actions ([#772](https://github.com/4DNucleome/PartSeg/pull/772))\n- Clean warnings about threshold methods ([#783](https://github.com/4DNucleome/PartSeg/pull/783))\n\n#### Build\n\n- *(deps)* Bump chanzuckerberg/napari-hub-preview-action from 0.1.5 to 0.1.6 ([#775](https://github.com/4DNucleome/PartSeg/pull/775))\n\n### 0.14.4 - 2022-10-24\n\n#### üöÄ Features\n\n- Load alternatives labeling when open PartSeg projects in napari ([#731](https://github.com/4DNucleome/PartSeg/pull/731))\n- Add option to toggle scale bar ([#733](https://github.com/4DNucleome/PartSeg/pull/733))\n- Allow customize settings directory using the `PARTSEG_SETTINGS_DIR` environment variable ([#751](https://github.com/4DNucleome/PartSeg/pull/751))\n- Separate recent algorithms from general application settings ([#752](https://github.com/4DNucleome/PartSeg/pull/752))\n- Add multiple otsu as threshold method with selection range of components ([#710](https://github.com/4DNucleome/PartSeg/pull/710))\n- Add function to load components from Mask Segmentation with background in ROI Analysis ([#768](https://github.com/4DNucleome/PartSeg/pull/768))\n\n#### üêõ Bug Fixes\n\n- Fix typos\n- Fix `get_theme` calls to prepare for napari 0.4.17 ([#729](https://github.com/4DNucleome/PartSeg/pull/729))\n- Fix saving pipeline from GUI ([#756](https://github.com/4DNucleome/PartSeg/pull/756))\n- Fix profile export/import dialogs ([#761](https://github.com/4DNucleome/PartSeg/pull/761))\n- Enable compare button if ROI is available ([#765](https://github.com/4DNucleome/PartSeg/pull/765))\n- Fix bug in cut with roi to do not make black artifacts ([#767](https://github.com/4DNucleome/PartSeg/pull/767))\n\n#### üß™ Testing\n\n- Add new build and inspect wheel action ([#747](https://github.com/4DNucleome/PartSeg/pull/747))\n\n#### ‚öôÔ∏è Miscellaneous Tasks\n\n- Prepare pyinstaller configuration for napari 0.4.17 ([#748](https://github.com/4DNucleome/PartSeg/pull/748))\n- Add ruff linter ([#754](https://github.com/4DNucleome/PartSeg/pull/754))\n\n#### Bugfix\n\n- Fix sentry tests ([#742](https://github.com/4DNucleome/PartSeg/pull/742))\n- Fix reporting error in load settings from drive ([#725](https://github.com/4DNucleome/PartSeg/pull/725))\n\n#### Build\n\n- *(deps)* Bump actions/checkout from 2 to 3 ([#716](https://github.com/4DNucleome/PartSeg/pull/716))\n- *(deps)* Bump actions/download-artifact from 1 to 3 ([#709](https://github.com/4DNucleome/PartSeg/pull/709))\n\n### 0.14.3 - 2022-08-18\n\n#### üêõ Bug Fixes\n\n- Delay setting image if an algorithm is still running ([#627](https://github.com/4DNucleome/PartSeg/pull/627))\n- Wrong error report when no component is found in restartable segmentation algorithm. ([#633](https://github.com/4DNucleome/PartSeg/pull/633))\n- Fix process of build documentation ([#653](https://github.com/4DNucleome/PartSeg/pull/653))\n\n#### üöú Refactor\n\n- Clean potential vulnerabilities ([#630](https://github.com/4DNucleome/PartSeg/pull/630))\n\n#### üß™ Testing\n\n- Add more tests for common GUI elements ([#622](https://github.com/4DNucleome/PartSeg/pull/622))\n- Report coverage per package. ([#639](https://github.com/4DNucleome/PartSeg/pull/639))\n- Update conda environment to not use PyQt5 in test ([#646](https://github.com/4DNucleome/PartSeg/pull/646))\n- Add tests files to calculate coverage ([#655](https://github.com/4DNucleome/PartSeg/pull/655))\n\n#### Build\n\n- *(deps)* Bump qtpy from 2.0.1 to 2.1.0 in /requirements ([#613](https://github.com/4DNucleome/PartSeg/pull/613))\n- *(deps)* Bump pyinstaller from 5.0.1 to 5.1 in /requirements ([#629](https://github.com/4DNucleome/PartSeg/pull/629))\n- *(deps)* Bump tifffile from 2022.4.28 to 2022.5.4 in /requirements ([#619](https://github.com/4DNucleome/PartSeg/pull/619))\n- *(deps)* Bump codecov/codecov-action from 1 to 3 ([#637](https://github.com/4DNucleome/PartSeg/pull/637))\n- *(deps)* Bump requests from 2.27.1 to 2.28.0 in /requirements ([#647](https://github.com/4DNucleome/PartSeg/pull/647))\n- *(deps)* Bump actions/setup-python from 3 to 4 ([#648](https://github.com/4DNucleome/PartSeg/pull/648))\n- *(deps)* Bump pyqt5 from 5.15.6 to 5.15.7 in /requirements ([#652](https://github.com/4DNucleome/PartSeg/pull/652))\n- *(deps)* Bump sentry-sdk from 1.5.12 to 1.6.0 in /requirements ([#659](https://github.com/4DNucleome/PartSeg/pull/659))\n- *(deps)* Bump numpy from 1.22.4 to 1.23.0 in /requirements ([#660](https://github.com/4DNucleome/PartSeg/pull/660))\n- *(deps)* Bump lxml from 4.9.0 to 4.9.1 in /requirements ([#665](https://github.com/4DNucleome/PartSeg/pull/665))\n- *(deps)* Bump mahotas from 1.4.12 to 1.4.13 in /requirements ([#662](https://github.com/4DNucleome/PartSeg/pull/662))\n- *(deps)* Bump pyinstaller from 5.1 to 5.2 in /requirements ([#667](https://github.com/4DNucleome/PartSeg/pull/667))\n\n### 0.14.2 - 2022-05-05\n\n#### üêõ Bug Fixes\n\n- Fix bug in save label colors between sessions ([#610](https://github.com/4DNucleome/PartSeg/pull/610))\n- Register PartSeg plugins before start napari widgets. ([#611](https://github.com/4DNucleome/PartSeg/pull/611))\n- Mouse interaction with components work again after highlight. ([#620](https://github.com/4DNucleome/PartSeg/pull/620))\n\n#### üöú Refactor\n\n- Limit test run ([#603](https://github.com/4DNucleome/PartSeg/pull/603))\n- Filter and solve warnings in tests ([#607](https://github.com/4DNucleome/PartSeg/pull/607))\n- Use QAbstractSpinBox.AdaptiveDecimalStepType in SpinBox instead of hardcoded bounds ([#616](https://github.com/4DNucleome/PartSeg/pull/616))\n- Clean and test `PartSeg.common_gui.universal_gui_part` ([#617](https://github.com/4DNucleome/PartSeg/pull/617))\n\n#### üìö Documentation\n\n- Update changelog ([#621](https://github.com/4DNucleome/PartSeg/pull/621))\n\n#### üß™ Testing\n\n- Speedup test by setup cache for pip ([#604](https://github.com/4DNucleome/PartSeg/pull/604))\n- Setup cache for azure pipelines workflows ([#606](https://github.com/4DNucleome/PartSeg/pull/606))\n\n#### Build\n\n- *(deps)* Bump sentry-sdk from 1.5.10 to 1.5.11 in /requirements ([#615](https://github.com/4DNucleome/PartSeg/pull/615))\n\n### 0.14.1 - 2022-04-27\n\n#### üöÄ Features\n\n- Use pygments for coloring code in exception window ([#591](https://github.com/4DNucleome/PartSeg/pull/591))\n- Add option to calculate Measurement per Mask component ([#590](https://github.com/4DNucleome/PartSeg/pull/590))\n\n#### üêõ Bug Fixes\n\n- Update build wheels and sdist to have proper version tag ([#583](https://github.com/4DNucleome/PartSeg/pull/583))\n- Fix removing the first measurement entry in the napari Measurement widget ([#584](https://github.com/4DNucleome/PartSeg/pull/584))\n- Fix compatybility bug for conda Pyside2 version ([#595](https://github.com/4DNucleome/PartSeg/pull/595))\n- Error when synchronization is loaded and new iloaded image has different dimensionality than currently loaded. ([#598](https://github.com/4DNucleome/PartSeg/pull/598))\n\n#### üöú Refactor\n\n- Refactor the create batch plan widgets and add test for it ([#587](https://github.com/4DNucleome/PartSeg/pull/587))\n- Drop napari below 0.4.12 ([#592](https://github.com/4DNucleome/PartSeg/pull/592))\n- Update the order of ROI Mask algorithms to be the same as in older PartSeg versions ([#600](https://github.com/4DNucleome/PartSeg/pull/600))\n\n#### Build\n\n- *(deps)* Bump partsegcore-compiled-backend from 0.13.11 to 0.14.0 in /requirements ([#582](https://github.com/4DNucleome/PartSeg/pull/582))\n- *(deps)* Bump simpleitk from 2.1.1 to 2.1.1.2 in /requirements ([#589](https://github.com/4DNucleome/PartSeg/pull/589))\n- *(deps)* Bump pyinstaller from 4.10 to 5.0 in /requirements ([#586](https://github.com/4DNucleome/PartSeg/pull/586))\n\n### 0.14.0 - 2022-04-14\n\n#### üöÄ Features\n\n- Allow to set zoom factor from interface in Search Label napari plugin ([#538](https://github.com/4DNucleome/PartSeg/pull/538))\n- Add controlling of zoom factor of search ROI in main GUI ([#540](https://github.com/4DNucleome/PartSeg/pull/540))\n- Better serialization mechanism allow for declaration data structure migration locally ([#462](https://github.com/4DNucleome/PartSeg/pull/462))\n- Make \\`\\*.obsep\" file possible to load in PartSeg Analysis ([#564](https://github.com/4DNucleome/PartSeg/pull/564))\n- Add option to extract measurement profile or roi extraction profile from batch plan ([#568](https://github.com/4DNucleome/PartSeg/pull/568))\n- Allow import calculation plan from batch result excel file ([#567](https://github.com/4DNucleome/PartSeg/pull/567))\n- Improve error reporting when fail to deserialize data ([#574](https://github.com/4DNucleome/PartSeg/pull/574))\n- Launch PartSeg GUI from napari ([#581](https://github.com/4DNucleome/PartSeg/pull/581))\n\n#### üêõ Bug Fixes\n\n- Fix \"Show selected\" rendering mode in PartSeg ROI Mask ([#565](https://github.com/4DNucleome/PartSeg/pull/565))\n\n#### üöú Refactor\n\n- Store PartSegImage.Image channels as separated arrays ([#554](https://github.com/4DNucleome/PartSeg/pull/554))\n- Remove deprecated modules. ([#429](https://github.com/4DNucleome/PartSeg/pull/429))\n- Switch serialization backen to `nme` ([#569](https://github.com/4DNucleome/PartSeg/pull/569))\n\n#### üìö Documentation\n\n- Update changelog and add new badges to readme ([#580](https://github.com/4DNucleome/PartSeg/pull/580))\n\n#### üß™ Testing\n\n- Add test of creating AboutDialog ([#539](https://github.com/4DNucleome/PartSeg/pull/539))\n- Setup test for python 3.10. Disable class_generator test for this python ([#570](https://github.com/4DNucleome/PartSeg/pull/570))\n\n#### Bugfix\n\n- Add access by operator [] to pydantic.BaseModel base structures for keep backward compatybility ([#579](https://github.com/4DNucleome/PartSeg/pull/579))\n\n#### Build\n\n- *(deps)* Bump sentry-sdk from 1.5.2 to 1.5.3 in /requirements ([#512](https://github.com/4DNucleome/PartSeg/pull/512))\n- *(deps)* Bump ipython from 8.0.0 to 8.0.1 in /requirements ([#513](https://github.com/4DNucleome/PartSeg/pull/513))\n- *(deps)* Bump pandas from 1.3.5 to 1.4.0 in /requirements ([#514](https://github.com/4DNucleome/PartSeg/pull/514))\n- *(deps)* Bump oiffile from 2021.6.6 to 2022.2.2 in /requirements ([#521](https://github.com/4DNucleome/PartSeg/pull/521))\n- *(deps)* Bump numpy from 1.22.1 to 1.22.2 in /requirements ([#524](https://github.com/4DNucleome/PartSeg/pull/524))\n- *(deps)* Bump tifffile from 2021.11.2 to 2022.2.2 in /requirements ([#523](https://github.com/4DNucleome/PartSeg/pull/523))\n- *(deps)* Bump qtpy from 2.0.0 to 2.0.1 in /requirements ([#522](https://github.com/4DNucleome/PartSeg/pull/522))\n- *(deps)* Bump sentry-sdk from 1.5.3 to 1.5.4 in /requirements ([#515](https://github.com/4DNucleome/PartSeg/pull/515))\n- *(deps)* Bump pyinstaller from 4.8 to 4.10 in /requirements ([#545](https://github.com/4DNucleome/PartSeg/pull/545))\n- *(deps)* Bump pillow from 9.0.0 to 9.0.1 in /requirements ([#549](https://github.com/4DNucleome/PartSeg/pull/549))\n- *(deps)* Bump sphinx from 4.4.0 to 4.5.0 in /requirements ([#561](https://github.com/4DNucleome/PartSeg/pull/561))\n- *(deps)* Bump tifffile from 2022.2.9 to 2022.3.25 in /requirements ([#562](https://github.com/4DNucleome/PartSeg/pull/562))\n- *(deps)* Bump sympy from 1.10 to 1.10.1 in /requirements ([#556](https://github.com/4DNucleome/PartSeg/pull/556))\n- *(deps)* Bump sentry-sdk from 1.5.7 to 1.5.8 in /requirements ([#557](https://github.com/4DNucleome/PartSeg/pull/557))\n\n### 0.13.15\n\n#### Bug Fixes\n\n- Using `translation` instead of `translation_grid` for shifting layers. (#474)\n- Bugs in napari plugins (#478)\n- Missing mask when using roi extraction from napari (#479)\n- Fix segmentation fault on macos machines (#487)\n- Fixes for napari 0.4.13 (#506)\n\n#### Documentation\n\n- Create 0.13.15 release (#511)\n- Add categories and preview page workflow for the napari hub (#489)\n\n#### Features\n\n- Assign properties to mask layer in napari measurement widget (#480)\n\n#### Build\n\n- Bump qtpy from 1.11.3 to 2.0.0 in /requirements (#498)\n- Bump pydantic from 1.8.2 to 1.9.0 in /requirements (#496)\n- Bump sentry-sdk from 1.5.1 to 1.5.2 in /requirements (#497)\n- Bump sphinx from 4.3.1 to 4.3.2 in /requirements (#500)\n- Bump pyinstaller from 4.7 to 4.8 in /requirements (#502)\n- Bump pillow from 8.4.0 to 9.0.0 in /requirements (#501)\n- Bump requests from 2.26.0 to 2.27.1 in /requirements (#495)\n- Bump numpy from 1.21.4 to 1.22.0 in /requirements (#499)\n- Bump numpy from 1.22.0 to 1.22.1 in /requirements (#509)\n- Bump sphinx from 4.3.2 to 4.4.0 in /requirements (#510)\n\n### 0.13.14\n\n#### Bug Fixes\n\n- ROI alternative representation (#471)\n- Change additive to translucent in rendering ROI and Mask (#472)\n\n#### Features\n\n- Add morphological watershed segmentation (#469)\n- Add Bilateral image filter (#470)\n\n### 0.13.13\n\n#### Bug Fixes\n\n- Fix bugs in the generation process of the changelog for release. (#428)\n- Restoring ROI on home button click in compare viewer (#443)\n- Fix Measurement name prefix in bundled PartSeg. (#458)\n- Napari widgets registration in pyinstaller bundle (#465)\n- Hide points button if no points are loaded, hide Mask checkbox if no mask is set (#463)\n- Replace Label data instead of adding/removing layers - fix blending layers (#464)\n\n#### Features\n\n- Add threshold information in layer annotation in the Multiple Otsu ROI extraction method (#430)\n- Add option to select rendering method for ROI (#431)\n- Add callback mechanism to ProfileDict, live update of ROI render parameters (#432)\n- Move the info bar on the bottom of the viewer (#442)\n- Add options to load recent files in multiple files widget (#444)\n- Add ROI annotations as properties to napari labels layer created by ROI Extraction widgets (#445)\n- Add signals to ProfileDict, remove redundant synchronization mechanisms (#449)\n- Allow ignoring updates for 21 days (#453)\n- Save all components if no components selected in mask segmentation (#456)\n- Add modal dialog for search ROI components (#459)\n- Add full measurement support as napari widget (#460)\n- Add search labels as napari widget (#467)\n\n#### Refactor\n\n- Export common code for load/save dialog to one place (#437)\n- Change most of call QFileDialog to more generic code (#440)\n\n#### Testing\n\n- Add test for `PartSeg.common_backend` module (#433)\n\n### 0.13.12\n\n#### Bug Fixes\n\n- Importing the previous version of settings (#406)\n- Cutting without masking data (#407)\n- Save in subdirectory in batch plan (#414)\n- Loading plugins for batch processing (#423)\n\n#### Features\n\n- Add randomization option for correlation calculation (#421)\n- Add Imagej TIFF writer for image. (#405)\n- Mask create widget for napari (#395)\n- In napari roi extraction method show information from roi extraction method (#408)\n- Add `*[0-9].tif` button in batch processing window (#412)\n- Better label representation in 3d view (#418)\n\n#### Refactor\n\n- Use Font Awesome instead of custom symbols (#424)\n\n### 0.13.11\n\n#### Bug Fixes\n\n- Adding mask in Prepare Plan for batch (#383)\n- Set proper completion mode in SearchComboBox (#384)\n- Showing warnings on the error with ROI load (#385)\n\n#### Features\n\n- Add CellFromNucleusFlow \"Cell from nucleus flow\" cell segmentation method (#367)\n- When cutting components in PartSeg ROI mask allow not masking outer data (#379)\n- Theme selection in GUI (#381)\n- Allow return points from ROI extraction algorithm (#382)\n- Add measurement to get ROI annotation by name. (#386)\n- PartSeg ROI extraction algorithms as napari plugins (#387)\n- Add Pearson, Mander's, Intensity, Spearman colocalization measurements (#392)\n- Separate standalone napari settings from PartSeg embedded napari settings (#397)\n\n#### Performance\n\n- Use faster calc bound function (#375)\n\n#### Refactor\n\n- Remove CustomApplication (#389)\n\n### 0.13.10\n\n- change tiff save backend to ome-tiff\n- add `DistanceROIROI` and `ROINeighbourhoodROI` measurements\n\n### 0.13.9\n\n- annotation show bugfix\n\n### 0.13.8\n\n- napari deprecation fixes\n- speedup simple measurement\n- bundle plugins initial support\n\n### 0.13.7\n\n- add measurements widget for napari\n- fix bug in pipeline usage\n\n### 0.13.6\n\n- Hotfix release\n- Prepare for a new napari version\n\n### 0.13.5\n\n- Small fixes for error reporting\n- Fix mask segmentation\n\n### 0.13.4\n\n- Bugfix for outdated profile/pipeline preview\n\n### 0.13.3\n\n- Fix saving roi_info in multiple files and history\n\n### 0.13.2\n\n- Fix showing label in select label tab\n\n### 0.13.1\n\n- Add Haralick measurements\n- Add obsep file support\n\n### 0.13.0\n\n- Add possibility of custom input widgets for algorithms\n- Switch to napari Colormaps instead of custom one\n- Add points visualization\n- Synchronization widget for builtin (View menu) napari viewer\n- Drop Python 3.6\n\n### 0.12.7\n\n- Fixes for napari 0.4.6\n\n### 0.12.6\n\n- Fix prev_mask_get\n- Fix cache mechanism on mask change\n- Update PyInstaller build\n\n### 0.12.5\n\n- Fix bug in pipeline execute\n\n### 0.12.4\n\n- Fix ROI Mask windows related build (signal not properly connected)\n\n### 0.12.3\n\n- Fix ROI Mask\n\n### 0.12.2\n\n- Fix windows bundle\n\n### 0.12.1\n\n- History of last opened files\n- Add ROI annotation and ROI alternatives\n- Minor bugfix\n\n### 0.12.0\n\n- Toggle multiple files widget in View menu\n- Toggle Left panel in ROI Analysis in View Menu\n- Rename Mask Segmentation to ROI Mask\n- Add documentation for interface\n- Add Batch processing tutorial\n- Add information about errors to batch processing output file\n- Load image from the batch prepare window\n- Add search option in part of list and combo boxes\n- Add drag and drop mechanism to load list of files to batch window.\n\n### 0.11.5\n\n- add side view to viewer\n- fix horizontal view for Measurements result table\n\n### 0.11.4\n\n- bump to napari 0.3.8 in bundle\n- fix bug with not presented segmentation loaded from project\n- add frame (1 pix) to image cat from base one based on segmentation\n- pin to Qt version to 5.14\n\n### 0.11.3\n\n- prepare for napari 0.3.7\n- split napari io plugin on multiple part\n- better reporting for numpy array via sentry\n- fix setting color for mask marking\n\n### 0.11.2\n\n- Speedup image set in viewer using async calls\n- Fix bug in long name of sheet with parameters\n\n### 0.11.1\n\n- Add screenshot option in View menu\n- Add Voxels measurements\n\n### 0.11.0\n\n- Make sprawl algorithm name shorter\n- Unify capitalisation of measurement names\n- Add simple measurements to mask segmentation\n- Use napari as viewer\n- Add possibility to preview additional output of algorithms (In View menu)\n- Update names of available Algorithm and Measurement to be more descriptive.\n\n### 0.10.8\n\n- fix synchronisation between viewers in Segmentation Analysis\n- fix batch crash on error during batch run, add information about file on which calculation fails\n- add changelog preview in Help > About\n\n### 0.10.7\n\n- in measurements, on empty list of components mean will return 0\n\n### 0.10.6\n\n- fix border rim preview\n- fix problem with size of image preview\n- zoom with scroll and moving if rectangle zoom is not marked\n\n### 0.10.5\n\n- make PartSeg PEP517 compatible.\n- fix multiple files widget on Windows (path normalisation)\n\n### 0.10.4\n\n- fix slow zoom\n\n### 0.10.3\n\n- deterministic order of elements in batch processing.\n\n### 0.10.2\n\n- bugfixes\n\n### 0.10.1\n\n- bugfixes\n\n### 0.10.0\n\n- Add creating custom label coloring.\n- Change execs interpreter to python 3.7.\n- Add masking operation in Segmentation Mask.\n- Change license to BSD.\n- Allow select root type in batch processing.\n- Add median filter in preview.\n\n### 0.9.7\n\n- fix bug in compare mask\n\n### 0.9.6\n\n- fix bug in loading project with mask\n- upgrade PyInstaller version (bug GHSA-7fcj-pq9j-wh2r)\n\n### 0.9.5\n\n- fix bug in loading project in \"Segmentation analysis\"\n\n### 0.9.4\n\n- read mask segmentation projects\n- choose source type in batch\n- add initial support to OIF and CZI file format\n- extract utils to PartSegCore module\n- add automated tests of example notebook\n- reversed mask\n- load segmentation parameters in mask segmentation\n- allow use sprawl in segmentation tool\n- add radial split of mask for measurement\n- add all measurement results in batch, per component sheet\n\n### 0.9.3\n\n- start automated build documentation\n- change color map backend and allow for user to create custom color map.\n- segmentation compare\n- update test engines\n- support of PySide2\n\n### 0.9.2.3\n\n- refactor code to make easier create plugin for mask segmentation\n- create class base updater for update outdated algorithm description\n- fix save functions\n- fix different bugs\n\n### 0.9.2.2\n\n- extract static data to separated package\n- update marker of fix range and add mark of gauss in channel control\n\n### 0.9.2.1\n\n- add VoteSmooth and add choosing of smooth algorithm\n\n### 0.9.2\n\n- add pypi base check for update\n\n- remove resetting image state when change state in same image\n\n- in stack segmentation add options to picking components from segmentation's\n\n- in mask segmentation add:\n\n  - preview of segmentation parameters per component,\n  - save segmentation parameters in save file\n  - new implementation of batch mode.\n\n### 0.9.1\n\n- Add multiple files widget\n\n- Add Calculating distances between segmented object and mask\n\n- Batch processing plan fixes:\n\n  - Fix adding pipelines to plan\n  - Redesign mask widget\n\n- modify measurement backend to allow calculate multi channel measurements.\n\n### 0.9\n\nBegin of changelog\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.gz",
      "*.bz2",
      "*.oif",
      "*.tbz2",
      "*.czi",
      "*.tif",
      "*.obsep",
      "*.seg",
      "*.lsm",
      "*.oib",
      "*.tgz"
    ],
    "contributions_writers_filename_extensions": [
      ".seg",
      ".tif",
      ".tgz",
      ".tiff"
    ],
    "contributions_widgets": [
      "Search Label",
      "Measurement",
      "Simple Measurement",
      "ROI Analysis Extraction",
      "ROI Mask Extraction",
      "Mask Create",
      "PartSeg GUI Launcher",
      "Image Colormap",
      "Label Selector",
      "Copy Labels",
      "Border Smooth",
      "Connected Components",
      "Noise Filter",
      "Double Threshold",
      "Split Core Objects",
      "Threshold",
      "Watershed",
      "Layer Metadata",
      "Settings Editor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nanopyx",
    "name": "napari-nanopyx",
    "display_name": "napari NanoPyx",
    "version": "0.2.4",
    "created_at": "2023-06-14",
    "modified_at": "2025-07-09",
    "authors": [
      "Ricardo Henriques",
      "Bruno Saraiva",
      "In√™s Cunha",
      "Ant√≥nio Brito"
    ],
    "author_emails": [
      "bruno.msaraiva2@gmail.com"
    ],
    "license": "LGPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-nanopyx/",
    "home_github": "https://github.com/HenriquesLab/napari-NanoPyx",
    "home_other": null,
    "summary": "napari plugin of Nanoscopy Python library (NanoPyx, the successor to NanoJ) - focused on light microscopy and super-resolution imaging",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari",
      "nanopyx>=1.2",
      "scikit-image",
      "magicgui",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-nanopyx\n\n<img src=\"https://github.com/HenriquesLab/NanoPyx/blob/main/.github/logo.png\" align=\"right\" width=\"230\"/>\n\n[![License](https://img.shields.io/github/license/HenriquesLab/NanoPyx?color=Green)](https://github.com/HenriquesLab/NanoPyx/blob/main/LICENSE.txt)\n[![PyPI](https://img.shields.io/pypi/v/napari-nanopyx.svg?color=green)](https://pypi.org/project/napari-nanopyx)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nanopyx.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nanopyx)](https://napari-hub.org/plugins/napari-nanopyx)\n[![Docs](https://img.shields.io/badge/documentation-link-blueviolet)](https://github.com/HenriquesLab/napari-NanoPyx/wiki/3.-Methods)\n[![Wiki](https://img.shields.io/badge/wiki-click_me-blue)](https://github.com/HenriquesLab/napari-NanoPyx/wiki)\n\nnapari plugin of [NanoPyx](https://github.com/HenriquesLab/NanoPyx) (the successor to NanoJ) - focused on light microscopy and super-resolution imaging.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## What is the NanoPyx üî¨ Library?\n\nNanoPyx is a library specialized in the analysis of light microscopy and super-resolution data.\nIt is a successor to [NanoJ](https://github.com/HenriquesLab/NanoJ-Core), which is a Java library for the analysis of super-resolution microscopy data.\n\nNanoPyx focuses on performance, by heavily exploiting cython aided multiprocessing and simplicity. It implements methods for the bioimage analysis field, with a special emphasis on those developed by the [Henriques Laboratory](https://henriqueslab.github.io/).\nIt will be distributed as a Python Library and also as [Codeless Jupyter Notebooks](https://github.com/HenriquesLab/NanoPyx#codeless-jupyter-notebooks-available), that can be run locally or on Google Colab, and as a [napari plugin](https://github.com/HenriquesLab/napari-NanoPyx).\n\nYou can read more about NanoPyx in our [publication].\n\nCurrently it implements the following approaches:\n- A reimplementation of the NanoJ image registration, SRRF and Super Resolution metrics\n- eSRRF\n- Non-local means denoising\n- More to come soon‚Ñ¢\n\n\n## Installation\n\nYou can install `napari-nanopyx` via [pip]:\n\n    pip install napari-nanopyx\n\n## User Documentation\n\nYou can find installation and usage instructions in the [wiki](https://github.com/HenriquesLab/napari-NanoPyx/wiki).\n\n## Contributing\n\nContributions are very welcome.\nPlease read our [Contribution Guidelines](https://github.com/HenriquesLab/NanoPyx/blob/main/CONTRIBUTING.md) to know how to proceed.\n\n## License\n\nDistributed under the terms of the [CC-By v4.0] license,\n\"napari-nanopyx\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[CC-By v4.0]: https://creativecommons.org/licenses/by/4.0/\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[publication]: https://doi.org/10.1038/s41592-024-02562-6\n\n## Citing\n\nIf you found this work useful, please cite our [publication].\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Benchmark Liquid Engine methods",
      "Estimate Drift Correction",
      "Apply Drift Correction",
      "Estimate Channel Alignment",
      "Apply Channel Alignment",
      "Generate SRRF Image",
      "Calculate FRC",
      "Calculate Decorrelation Analysis",
      "Calculate Error Map",
      "Generate eSRRF 2D Image",
      "Run eSRRF 2D Parameter Sweep",
      "Generate eSRRF 3D Image",
      "Non-local Means Denoising"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-3d-registration",
    "name": "napari-3D-registration",
    "display_name": "napari 3D registration",
    "version": "0.2.1",
    "created_at": "2025-07-08",
    "modified_at": "2025-07-08",
    "authors": [
      "Leo Guignard"
    ],
    "author_emails": [
      "leo.guignard@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-3d-registration/",
    "home_github": "https://github.com/GuignardLab/napari-3D-registration",
    "home_other": null,
    "summary": "A plugin to help registering fluorescent movies in space and time",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "3D-registration",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-3D-registration\n\n[![License MIT](https://img.shields.io/pypi/l/napari-3D-registration.svg?color=green)](https://github.com/GuignardLab/napari-3D-registration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-3D-registration.svg?color=green)](https://pypi.org/project/napari-3D-registration)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-3D-registration.svg?color=green)](https://python.org)\n[![tests](https://github.com/GuignardLab/napari-3D-registration/workflows/tests/badge.svg)](https://github.com/GuignardLab/napari-3D-registration/actions)\n[![codecov](https://codecov.io/gh/GuignardLab/napari-3D-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/GuignardLab/napari-3D-registration)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-3D-registration)](https://napari-hub.org/plugins/napari-3D-registration)\n\nA plugin to help registering fluorescent movies in space and time\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-3D-registration` via [pip]:\n\n    pip install napari-3D-registration\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/GuignardLab/napari-3D-registration.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-3D-registration\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/GuignardLab/napari-3D-registration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Time registration",
      "Spatial registration"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-laber-manager",
    "name": "napari-laber-manager",
    "display_name": "Laber Manager",
    "version": "0.0.8",
    "created_at": "2025-07-07",
    "modified_at": "2025-07-08",
    "authors": [
      "JH Wang"
    ],
    "author_emails": [
      "wjh19937458882@mail.ustc.edu.cn"
    ],
    "license": "Copyright (c) 2025, JH Wang\nAl...",
    "home_pypi": "https://pypi.org/project/napari-laber-manager/",
    "home_github": "https://github.com/Wenlab/napari-label-manager",
    "home_other": null,
    "summary": "A plugin for management of label colormap generation and opacity control",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari[all]; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[qt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-laber-manager\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-laber-manager.svg?color=green)](https://github.com/Wenlab/napari-laber-manager/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-laber-manager.svg?color=green)](https://pypi.org/project/napari-laber-manager)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-laber-manager.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-laber-manager)](https://napari-hub.org/plugins/napari-laber-manager)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n## Description\nThis is a plugin for management of label colormap generation and opacity control.\n- Select your label layer from the dropdown\n- Generate a new colormap or use existing colors\n- Specify target label IDs (e.g., \"1-5,10,15-20\")\n- Adjust opacity for selected labels and background\n- Apply changes to visualize your selection\n\n## Installation\n\nYou can install `napari-laber-manager` via [pip]:\n\n```\npip install napari-laber-manager\n```\n\nIf napari is not already installed, you can install `napari-laber-manager` with napari and Qt via:\n\n```\npip install \"napari-laber-manager[all]\"\n```\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-laber-manager\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "LabelManager"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-threedee",
    "name": "napari-threedee",
    "display_name": "napari-threedee",
    "version": "0.0.29",
    "created_at": "2022-06-10",
    "modified_at": "2025-07-08",
    "authors": [
      "napari team"
    ],
    "author_emails": [
      "napari team <napari-steering-council@googlegroups.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-threedee/",
    "home_github": "https://github.com/napari-threedee/napari-threedee",
    "home_other": null,
    "summary": "A suite of useful tools based on 3D interactivity in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "einops",
      "imageio!=2.11.0,!=2.22.1,>=2.5.0",
      "libigl",
      "magicgui",
      "morphosamplers",
      "mrcfile",
      "napari>=0.5.0",
      "numpy",
      "pandas",
      "pooch",
      "psygnal",
      "pydantic",
      "qtpy",
      "scipy",
      "superqt",
      "vispy",
      "zarr<3",
      "lxml[html-clean]>5; extra == 'dev'",
      "mkdocs; extra == 'dev'",
      "mkdocs-gallery>0.7.6; extra == 'dev'",
      "mkdocs-material; extra == 'dev'",
      "mkdocs-video; extra == 'dev'",
      "mkdocstrings[python]; extra == 'dev'",
      "pytest; extra == 'dev'",
      "pytest-qt; extra == 'dev'",
      "qtgallery; extra == 'dev'",
      "scikit-image[data]; extra == 'dev'"
    ],
    "package_metadata_description": "# napari-threedee\n\n[![License](https://img.shields.io/pypi/l/napari-threedee.svg?color=green)](https://github.com/alisterburt/napari-threedee/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-threedee.svg?color=green)](https://pypi.org/project/napari-threedee)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-threedee.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari-threedee/napari-threedee/workflows/tests/badge.svg)](https://github.com/napari-threedee/napari-threedee/actions)\n[![codecov](https://codecov.io/gh/napari-threedee/napari-threedee/branch/main/graph/badge.svg)](https://codecov.io/gh/napari-threedee/napari-threedee)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-threedee)](https://napari-hub.org/plugins/napari-threedee)\n\nA suite of useful tools based on 3D interactivity in napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-threedee` via [pip]:\n\n    pip install napari-threedee\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/alisterburt/napari-threedee.git\n\n## Example applications\n\nSee the full list of [example gallery scripts here on our website](https://napari-threedee.github.io/generated/gallery/).\n\n<table border=\"0\">\n<tr><td>\n\n\n<img src=\"https://user-images.githubusercontent.com/1120672/173021751-9206de7d-5675-4aac-aa9e-8585457a7799.gif\"\nwidth=\"300\"/>\n\n</td><td>\n\n[mesh lighting control](https://github.com/napari-threedee/napari-threedee/blob/main/docs/examples/plugin/mesh_headlight_plugin.py)\n\n</td></tr><tr><td>\n\n<img src=\"https://user-images.githubusercontent.com/1120672/173022286-2473b6b2-a20e-4514-88a4-8295e001f099.gif\"\nwidth=\"300\"/>\n\n</td><td>\n\n[annotate points on planes](https://github.com/napari-threedee/napari-threedee/blob/main/docs/examples/plugin/point_annotator_plugin.py)\n\n</td></tr><tr><td>\n\n<img src=\"https://user-images.githubusercontent.com/1120672/173023185-b6936d1d-590c-4b9b-816a-3779dfe774da.gif\"\nwidth=\"300\"/>\n\n</td><td>\n\n[render plane manipulator](https://github.com/napari-threedee/napari-threedee/blob/main/docs/examples/plugin/render_plane_manipulator_plugin.py)\n\n</td></tr><tr><td>\n\n<img src=\"https://user-images.githubusercontent.com/1120672/173023795-7150d3c2-d3d1-4913-981d-1092c1b59f21.gif\"\nwidth=\"300\"/>\n\n</td><td>\n\n[layer manipulator](https://github.com/napari-threedee/napari-threedee/blob/main/docs/examples/plugin/layer_manipulator_plugin.py)\n\n</td></tr><tr><td>\n\n<img src=\"https://user-images.githubusercontent.com/1120672/173024361-2f05c68b-e94d-4734-9f5e-1606391e6463.gif\"\nwidth=\"300\"/>\n\n</td><td>\n\n[point manipulator](https://github.com/napari-threedee/napari-threedee/blob/main/docs/examples/plugin/points_manipulator_plugin.py)\n\n\n</td></tr></table>\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-threedee\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/alisterburt/napari-threedee/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "point annotator",
      "path annotator",
      "sphere annotator",
      "surface annotator",
      "label annotator",
      "render plane manipulator",
      "point manipulator",
      "layer manipulator",
      "mesh lighting controls",
      "ambient occlusion controls",
      "camera spline control"
    ],
    "contributions_sample_data": [
      "HIV virus-like particles tomogram"
    ]
  },
  {
    "normalized_name": "ndev-sampledata",
    "name": "ndev-sampledata",
    "display_name": "ndev Sampledata",
    "version": "0.0.3",
    "created_at": "2025-07-05",
    "modified_at": "2025-07-07",
    "authors": [
      "Tim Monko"
    ],
    "author_emails": [
      "timmonko@gmail.com"
    ],
    "license": "Copyright (c) 2025, Tim Monko\n...",
    "home_pypi": "https://pypi.org/project/ndev-sampledata/",
    "home_github": "https://github.com/ndev-kit/ndev-sampledata",
    "home_other": null,
    "summary": "Sample data for the ndev kit",
    "categories": [
      "Dataset"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "bioio",
      "bioio-imageio",
      "bioio-ome-tiff",
      "napari[all]; extra == \"all\"",
      "napari; extra == \"testing\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# ndev-sampledata\n\n[![License BSD-3](https://img.shields.io/pypi/l/ndev-sampledata.svg?color=green)](https://github.com/ndev-kit/ndev-sampledata/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/ndev-sampledata.svg?color=green)](https://pypi.org/project/ndev-sampledata)\n[![Python Version](https://img.shields.io/pypi/pyversions/ndev-sampledata.svg?color=green)](https://python.org)\n[![tests](https://github.com/ndev-kit/ndev-sampledata/workflows/tests/badge.svg)](https://github.com/ndev-kit/ndev-sampledata/actions)\n[![codecov](https://codecov.io/gh/ndev-kit/ndev-sampledata/branch/main/graph/badge.svg)](https://codecov.io/gh/ndev-kit/ndev-sampledata)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/ndev-sampledata)](https://napari-hub.org/plugins/ndev-sampledata)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nSample data for the ndev kit\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `ndev-sampledata` via [pip]:\n\n```\npip install ndev-sampledata\n```\n\nIf napari is not already installed, you can install `ndev-sampledata` with napari and Qt via:\n\n```\npip install \"ndev-sampledata[all]\"\n```\n\n\nTo install latest development version :\n\n```\npip install git+https://github.com/ndev-kit/ndev-sampledata.git\n```\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"ndev-sampledata\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/ndev-kit/ndev-sampledata/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "ndev logo",
      "Neuron (2D+4Ch)",
      "Scratch Assay Labeled (10T+2Ch)",
      "Neocortex (3Ch)"
    ]
  },
  {
    "normalized_name": "napari-molseeq",
    "name": "napari-molseeq",
    "display_name": "molSEEQ",
    "version": "1.0.8",
    "created_at": "2024-07-25",
    "modified_at": "2025-07-06",
    "authors": [
      "Piers Turner"
    ],
    "author_emails": [
      "piers.turner@physics.ox.ac.uk"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-molseeq/",
    "home_github": "https://github.com/piedrro/napari-molseeq",
    "home_other": null,
    "summary": "A Napari plugin for extracting single molecule sequences from single/multi-channel SMLM microscopy data.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari[all]==0.5.0",
      "numpy",
      "magicgui",
      "qtpy",
      "scipy",
      "pyqtgraph",
      "picassosr==0.7.3",
      "pandas",
      "matplotlib>=3.7.0",
      "scipy",
      "opencv-python",
      "tqdm",
      "originpro",
      "pyqt5-tools",
      "trackpy",
      "shapely",
      "astropy",
      "mat4py",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-molSEEQ\n\n[![License MIT](https://img.shields.io/pypi/l/napari-GapSeq2.svg?color=green)](https://github.com/piedrro/napari-molseeq/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-GapSeq2.svg?color=green)](https://pypi.org/project/napari-molseeq/)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-GapSeq2.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-GapSeq2)](https://napari-hub.org/plugins/napari-molseeq)\n\nA **Napari** plugin for extracting single molecule sequences from single/multi-channel SMLM microscopy data. \n\nCompatible with both ALEX and FRET data. All functions are parallelised/GPU accelerated where possible to increase performance.\nMultiple datasets can be loaded and processed in parallel.\n\nnapari-molseeq uses **Picasso** (picassosr) as a backend and includes features for **aligning** image channels/datasets, **undrifting** images, **detecting/fitting** localisations and extracting **traces**, and supports both **ALEX** and **FRET** data. Traces can be exported in different formats for downstream analysis.\n\nnapari-molseeq traces can be analysed with TraceAnalyser: https://github.com/piedrro/TraceAnalyser\n\nThis is still undergoing development, so some features may not work as expected.\n\nThis was built by Dr Piers Turner from the Kapanidis Lab, University of Oxford.\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-molseeq` via [pip]:\n\n    pip install napari-molseeq\n\nYou can install `napari-molseeq` via [GitHub]:\n\n    conda create ‚Äì-name napari-molseeq python==3.9\n    conda activate napari-molseeq\n    conda install -c anaconda git\n    conda update --all\n\n    pip install git+https://github.com/piedrro/napari-molseeq.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-molseeq\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/piedrro/napari-GapSeq2/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari-molseeq"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bioio-reader",
    "name": "napari-bioio-reader",
    "display_name": "Bioio Reader",
    "version": "0.2.0",
    "created_at": "2025-07-04",
    "modified_at": "2025-07-04",
    "authors": [
      "Laurent Guerard"
    ],
    "author_emails": [
      "l.guerard42@gmail.com"
    ],
    "license": "Copyright (c) 2025, Laurent Gu...",
    "home_pypi": "https://pypi.org/project/napari-bioio-reader/",
    "home_github": "https://github.com/lguerard/napari-bioio-reader",
    "home_other": null,
    "summary": "A simple plugin to use the Bioio reader in napari",
    "categories": [
      "IO"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "bioio",
      "bioio-bioformats",
      "scyjava",
      "napari[all]<0.7,>=0.6.2",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-bioio-reader\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bioio-reader.svg?color=green)](https://github.com/lguerard/napari-bioio-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bioio-reader.svg?color=green)](https://pypi.org/project/napari-bioio-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bioio-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/lguerard/napari-bioio-reader/workflows/tests/badge.svg)](https://github.com/lguerard/napari-bioio-reader/actions)\n[![codecov](https://codecov.io/gh/lguerard/napari-bioio-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/lguerard/napari-bioio-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bioio-reader)](https://napari-hub.org/plugins/napari-bioio-reader)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA simple plugin to use the Bioio reader in napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-bioio-reader` via [pip]:\n\n    pip install napari-bioio-reader\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bioio-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.lms",
      "*.oif",
      "*.pcoraw",
      "*.seq",
      "*.c01",
      "*.k25",
      "*.pr3",
      "*.fff",
      "*.rec",
      "*.spc",
      "*.labels",
      "*.rw2",
      "*.ids",
      "*.mod",
      "*.jng",
      "*.jif",
      "*.am",
      "*.dng",
      "*.ics",
      "*.tfr",
      "*.img",
      "*.cine",
      "*.pxr",
      "*.cr2",
      "*.cap",
      "*.g3",
      "*.sdt",
      "*.pcx",
      "*.dds",
      "*.pict",
      "*.bay",
      "*.nd2",
      "*.im",
      "*.nii.gz",
      "*.r3d",
      "*.vsi",
      "*.jp2",
      "*.mhd",
      "*.gdcm",
      "*.msr",
      "*.afi",
      "*.his",
      "*.nrw",
      "*.xdce",
      "*.lim",
      "*.xys",
      "*.msp",
      "*.pnl",
      "*.zvi",
      "*.sr2",
      "*.array-like",
      "*.nrrd",
      "*.al3d",
      "*.koa",
      "*.htm",
      "*.mov",
      "*.sld",
      "*.lei",
      "*.wap",
      "*.cut",
      "*.klb",
      "*.mic",
      "*.gbr",
      "*.html",
      "*.iff",
      "*.sxm",
      "*.jpe",
      "*.lsm",
      "*.wpi",
      "*.ftc",
      "*.avi",
      "*.xpm",
      "*.ffr",
      "*.sti",
      "*.cur",
      "*.wav",
      "*.2fl",
      "*.bmp",
      "*.mdb",
      "*.aim",
      "*.ipl",
      "*.acff",
      "*.hdr",
      "*.mvd2",
      "*.txt",
      "*.ct.img",
      "*.pfm",
      "*.arw",
      "*.pgm",
      "*.frm",
      "*.grey",
      "*.grib",
      "*.oib",
      "*.rdc",
      "*.j2c",
      "*.erf",
      "*.ch5",
      "*.flc",
      "*.mri",
      "*.dm3",
      "*.fit",
      "*.fake",
      "*.mnc2",
      "*.ome.tif",
      "*.ppm",
      "*.srf",
      "*.imggz",
      "*.mpeg",
      "*.xv",
      "*.ims",
      "*.scan",
      "*.jpk",
      "*.pic",
      "*.im3",
      "*.bif",
      "*.bmq",
      "*.gel",
      "*.rwl",
      "*.nd",
      "*.cs1",
      "*.psd",
      "*.zfp",
      "*.gif",
      "*.nii",
      "*.wdp",
      "*.ipm",
      "*.mp4",
      "*.htd",
      "*.par",
      "*.niigz",
      "*.fdf",
      "*.dcr",
      "*.kc2",
      "*.zpo",
      "*.rcpnl",
      "*.apl",
      "*.acqp",
      "*.bip",
      "*.bsdf",
      "*.ome.tiff",
      "*.gipl",
      "*.naf",
      "*.mos",
      "*.exp",
      "*.png",
      "*.exr",
      "*.wat",
      "*.hdp",
      "*.3fr",
      "*.mkv",
      "*.zfr",
      "*.vtk",
      "*.nia",
      "*.dv",
      "*.tnb",
      "*.j2k",
      "*.tim",
      "*.cxd",
      "*.ps",
      "*.ia",
      "*.qptiff",
      "*.fts",
      "*.lbm",
      "*.thm",
      "*.wbm",
      "*.ndpi",
      "*.jpeg",
      "*.ano",
      "*.nef",
      "*.raw",
      "*.kdc",
      "*.csv",
      "*.liff",
      "*.vff",
      "*.xbm",
      "*.jpx",
      "*.mrc",
      "*.orf",
      "*.sif",
      "*.tiff",
      "*.dsc",
      "*.bin",
      "*.dicom",
      "*.stp",
      "*.targa",
      "*.wmv",
      "*.1sc",
      "*.tif",
      "*.amiramesh",
      "*.nhdr",
      "*.pcd",
      "*.i2i",
      "*.ftu",
      "*.pct",
      "*.dcx",
      "*.pxn",
      "*.raf",
      "*.jfif",
      "*.vms",
      "*.xqd",
      "*.rgb",
      "*.lfr",
      "*.inf",
      "*.scn",
      "*.tga",
      "*.cfg",
      "*.iiq",
      "*.lif",
      "*.dti",
      "*.wlz",
      "*.l2d",
      "*.ome",
      "*.svs",
      "*.st",
      "*.cat",
      "*.webp",
      "*.flex",
      "*.wmf",
      "*.ptx",
      "*.df3",
      "*.srw",
      "*.fpx",
      "*.mgh",
      "*.hdf",
      "*.mtb",
      "*.mef",
      "*.arf",
      "*.sm2",
      "*.ico",
      "*.lfp",
      "*.hdf5",
      "*.ct",
      "*.inr",
      "*.spi",
      "*.db",
      "*.fits",
      "*.mpo",
      "*.afm",
      "*.cif",
      "*.mng",
      "*.zip",
      "*.ras",
      "*.jpc",
      "*.dat",
      "*.xqf",
      "*.epsi",
      "*.swf",
      "*.bufr",
      "*.fli",
      "*.dcm",
      "*.crw",
      "*.iim",
      "*.jpf",
      "*.oir",
      "*.dc2",
      "*.pef",
      "*.icns",
      "*.ipw",
      "*.vws",
      "*.dm2",
      "*.rwz",
      "*.sm3",
      "*.stk",
      "*.ali",
      "*.h5",
      "*.czi",
      "*.rgba",
      "*.wbmp",
      "*.xml",
      "*.ecw",
      "*.fz",
      "*.eps",
      "*.hed",
      "*.mha",
      "*.bw",
      "*.hx",
      "*.obf",
      "*.v",
      "*.pbm",
      "*.qtk",
      "*.fid",
      "*.top",
      "*.drf",
      "*.mpg",
      "*.ndpis",
      "*.jpg",
      "*.emf",
      "*.jxr",
      "*.spe",
      "*.mdc",
      "*.npz",
      "*.mnc",
      "*.mrw"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "psf-analysis-cfim",
    "name": "psf-analysis-CFIM",
    "display_name": "psf-analysis-CFIM",
    "version": "1.7.6",
    "created_at": "2025-02-11",
    "modified_at": "2025-07-04",
    "authors": [
      "Markus L. Bille"
    ],
    "author_emails": [
      "github+Markus@bille.dk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/psf-analysis-cfim/",
    "home_github": "https://github.com/MaxusTheOne/napari-psf-analysis-CFIM-edition",
    "home_other": null,
    "summary": "A continuation of napari_psf_analysis, developed for CFIM - KU",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "aicsimageio",
      "aicspylibczi",
      "matplotlib<3.9",
      "matplotlib-inline",
      "matplotlib-scalebar",
      "napari[all]<0.6",
      "numpy<2.0,>=1.26",
      "pandas",
      "pydantic",
      "PyYAML",
      "QtPy",
      "scipy",
      "reportlab",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "tox; extra == \"testing\""
    ],
    "package_metadata_description": "# psf-analysis-CFIM\n\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/psf-analysis-CFIM)](https://napari-hub.org/plugins/psf-analysis-CFIM)\n\n---\n![application_screenshot](figs/PSF_CFIM_demo_v170.gif)\n\n## Installation\n\nYou can install this package using one of the following options:\n```bash\n  pip install psf-analysis-CFIM -U\n```\nFor the latest stable version (recommended)\n\n---\n\nor\n\n```bash\n  pip install git+https://github.com/MaxusTheOne/napari-psf-analysis-CFIM-edition\n```\nTo install the latest version (not guaranteed to be stable)\n\n---\nAdditionally, it can be installed via napari plugin manager under the name **psf-analysis-CFIM**.\n\n## About\n\nThis is a **fork** of the [napari-psf-analysis](https://github.com/fmi-faim/napari-psf-analysis) project.\n\nThe features from this edition are made as requested by the staff at CFIM.\n\nPlease contact me through Github or mail for any issues or help with development\n---\n\n## Extra Features\n\nThis edition includes the following additional features:\n\n- **Bulk Analysis for channels**: Allows for bulk analysis of multiple channels.\n  - **Combined Summary**: Adds a combined summary of all channels.\n  - **Channel Offset Table**: Adds a table with the relative offset of the channels.\n- **Bead Averaging**: Adds an image of an averaged bead from all selected.\n- **Visualisation**: Improves visualisation of the psf. Most notable color by wavelength.\n  - **Range indicator** Button to mark the min and max values of the image.\n- **PSF Report**: Adds a graded report on the quality of the PSF. <- WIP\n- **Bead Detection**: Detects beads in the image.\n- **Auto-Filling of Plugin Parameters**: Automatically populates parameters for the plugin.\n  - At least 1 input also looks better\n- **Auto Analysis of Image for PSF**: Performs automatic image analysis to ascertain the quality.\n- **CZI Reader**: Adds support for reading CZI image files.\n- **Debugging**: Adds a debug class to the IPython console. Small, but hey, we can show the psf box\n- **Error Handling**: Less likely to crash. errors points can be seen in viewer | Error UI.\n- **Bug fixes**: Fixes bugs involving zyx boxes, loading bar and other issues.\n\n## Known Issues\n\n- for autofilling, only .czi files are supported. Unlikely to work with other formats.\n- Installing plugin under paths including non-ASCII characters, like \"√¶√∏√•\" cause unintended behavior.\n- The output.csv file is comma seperated with dot as decimal seperator, this might require a legacy workaround in programs like Excel.\n- Intensity for bead finder is hardcoded for now.\n- Some images might still crash in the analysis.\n- Might have missed some requirements\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"psf-analysis-CFIM\" is free and open source software\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.czi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PSF Analysis - CFIM"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "lsfm-destripe-napari",
    "name": "LSFM_destripe_napari",
    "display_name": "Leonardo-DeStripe",
    "version": "0.3.0",
    "created_at": "2024-12-12",
    "modified_at": "2025-07-02",
    "authors": [
      "lennart kowitz"
    ],
    "author_emails": [
      "lennart.kowitz@isas.de"
    ],
    "license": "Copyright (c) 2024, Lennart Ko...",
    "home_pypi": "https://pypi.org/project/lsfm-destripe-napari/",
    "home_github": "https://github.com/peng-lab/lsfm_destripe_napari",
    "home_other": null,
    "summary": "A simple plugin to destripe microscopy images in napari",
    "categories": [
      "Visualization",
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "scikit-image",
      "bioio",
      "leonardo_toolset",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# lsfm_destripe_napari\n\n[![License BSD-3](https://img.shields.io/pypi/l/lsfm_destripe_napari.svg?color=green)](https://github.com/peng-Lab/lsfm_destripe_napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/lsfm_destripe_napari.svg?color=green)](https://pypi.org/project/lsfm_destripe_napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/lsfm_destripe_napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/peng-Lab/lsfm_destripe_napari/workflows/tests/badge.svg)](https://github.com/peng-Lab/lsfm_destripe_napari/actions)\n[![codecov](https://codecov.io/gh/peng-Lab/lsfm_destripe_napari/branch/main/graph/badge.svg)](https://codecov.io/gh/peng-Lab/lsfm_destripe_napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/lsfm_destripe_napari)](https://napari-hub.org/plugins/lsfm_destripe_napari)\n\nA simple plugin to destripe microscopy images\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `lsfm_destripe_napari` via [pip]:\n\n    pip install lsfm_destripe_napari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/peng-Lab/lsfm_destripe_napari.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"lsfm_destripe_napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/peng-Lab/lsfm_destripe_napari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [
      ".tiff"
    ],
    "contributions_widgets": [
      "LSFM Destripe Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "lsfm-fusion-napari",
    "name": "LSFM-fusion-napari",
    "display_name": "Leonardo-Fuse",
    "version": "0.2.0",
    "created_at": "2024-12-13",
    "modified_at": "2025-07-02",
    "authors": [
      "lennart kowitz"
    ],
    "author_emails": [
      "lennart.kowitz@isas.de"
    ],
    "license": "Copyright (c) 2024, lennart ko...",
    "home_pypi": "https://pypi.org/project/lsfm-fusion-napari/",
    "home_github": "https://github.com/peng-lab/lsfm_fusion_napari",
    "home_other": null,
    "summary": "A simple plugin to fuse microscopy images in napari",
    "categories": [
      "Visualization",
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "bioio",
      "leonardo_toolset",
      "napari[all]",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# lsfm_fusion_napari\n\n[![License BSD-3](https://img.shields.io/pypi/l/lsfm_fusion_napari.svg?color=green)](https://github.com/peng-lab/lsfm_fusion_napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/lsfm_fusion_napari.svg?color=green)](https://pypi.org/project/lsfm_fusion_napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/lsfm_fusion_napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/peng-lab/lsfm_fusion_napari/workflows/tests/badge.svg)](https://github.com/peng-lab/lsfm_fusion_napari/actions)\n[![codecov](https://codecov.io/gh/peng-lab/lsfm_fusion_napari/branch/main/graph/badge.svg)](https://codecov.io/gh/peng-lab/lsfm_fusion_napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/lsfm_fusion_napari)](https://napari-hub.org/plugins/lsfm_fusion_napari)\n\nA simple plugin to fuse LSFM images\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `lsfm_fusion_napari` via [pip]:\n\n    pip install lsfm_fusion_napari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/peng-lab/lsfm_fusion_napari.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"lsfm_fusion_napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/peng-lab/lsfm_fusion_napari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".tiff"
    ],
    "contributions_widgets": [
      "LSFM Fusion Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mass",
    "name": "napari-mass",
    "display_name": "Microscopy Array Section Setup",
    "version": "0.6.7",
    "created_at": "2025-06-30",
    "modified_at": "2025-06-30",
    "authors": [
      "Joost de Folter"
    ],
    "author_emails": [
      "folterj@gmail.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-mass/",
    "home_github": "https://github.com/FrancisCrickInstitute/napari-mass",
    "home_other": null,
    "summary": "Microscopy Array Section Setup napari plugin",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "napari-ome-zarr",
      "dask",
      "scikit-learn",
      "scikit-image",
      "opencv-contrib-python-headless",
      "tifffile",
      "matplotlib",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-mass\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-mass.svg?color=green)](https://github.com/FrancisCrickInstitute/napari-mass/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mass.svg?color=green)](https://pypi.org/project/napari-mass)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mass.svg?color=green)](https://python.org)\n[![tests](https://github.com/FrancisCrickInstitute/napari-mass/workflows/tests/badge.svg)](https://github.com/FrancisCrickInstitute/napari-mass/actions)\n[![codecov](https://codecov.io/gh/FrancisCrickInstitute/napari-mass/branch/main/graph/badge.svg)](https://codecov.io/gh/FrancisCrickInstitute/napari-mass)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mass)](https://napari-hub.org/plugins/napari-mass)\n\nMicroscopy Array Section Setup napari plugin: A napari plugin for annotation and guided acquisition working together\nwith [SBEMimage](https://github.com/SBEMimage/SBEMimage).\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nThis plugin can be installed via the plugin manager inside napari. For more information on how to install plugins, please refer to the [napari plugin documentation](https://napari.org/stable/plugins/start_using_plugins/finding_and_installing_plugins.html).\n\nAlternatively, you can install `napari-mass` via [pip]:\n\n    pip install napari-mass\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/FrancisCrickInstitute/napari-mass.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0](LICENSE.md) license,\n\"napari-mass\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please file an issue along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Microscopy Array Section Setup"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nifti-viewer",
    "name": "napari-nifti-viewer",
    "display_name": "NIfTI Viewer",
    "version": "0.1.2",
    "created_at": "2025-06-28",
    "modified_at": "2025-06-28",
    "authors": [
      "Yohanchiu"
    ],
    "author_emails": [
      "Yohanchiu <qyhohh@163.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-nifti-viewer/",
    "home_github": "https://github.com/yohanchiu/napari-nifti-viewer",
    "home_other": null,
    "summary": "A comprehensive napari plugin for NIfTI file analysis and visualization",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari>=0.4.18",
      "numpy>=1.21.0",
      "nibabel>=5.2.1",
      "qtpy>=2.0.0",
      "magicgui>=0.7.0",
      "pytest>=7.0; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "black; extra == \"dev\"",
      "isort; extra == \"dev\"",
      "flake8; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pytest>=7.0; extra == \"test\"",
      "pytest-cov; extra == \"test\""
    ],
    "package_metadata_description": "# napari-nifti-viewer\n\nA powerful napari plugin for comprehensive NIfTI file analysis and visualization.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nifti-viewer)](https://napari-hub.org/plugins/napari-nifti-viewer)\n\n## Overview\n\nnapari-nifti-viewer is a comprehensive napari plugin specifically designed for reading, analyzing, and visualizing NIfTI (.nii/.nii.gz) files. It provides detailed metadata extraction, intelligent label detection, and seamless integration with napari's visualization capabilities.\n\n## Features\n\n### üîç **Complete NIfTI Support**\n- Read .nii and .nii.gz format files\n- Support for NIfTI-1 standard\n- Compatible with both image and label data\n\n### üìä **Comprehensive Metadata Analysis**\n- Extract complete NIfTI header information (40+ fields)\n- Display affine transformation matrices\n- Show coordinate system information\n- Analyze voxel spacing and orientation\n\n### üè∑Ô∏è **Intelligent Label Detection**\n- Automatic label image detection\n- Statistical analysis of label distributions\n- Label value counting and percentage calculations\n\n### üìà **Data Statistics**\n- Complete data shape and type information\n- Statistical measures (min, max, mean, std)\n- Non-zero voxel counting\n- Unique value analysis\n\n### üíæ **Export Capabilities**\n- Export complete metadata as JSON\n- Preserve all numerical precision\n- Human-readable format\n\n### üé® **User-Friendly Interface**\n- Clean, organized tabbed interface\n- Real-time data loading\n- Seamless napari integration\n\n## Interface\n\nThe plugin provides a clean, organized interface with three main tabs:\n\n### üìã File Overview Tab\nDisplays basic file information and data statistics including file size, format, data shape, and statistical measures.\n\n### üìä Detailed Information Tab  \nShows complete NIfTI header fields and metadata in an organized table format, alongside full JSON metadata export.\n\n### üè∑Ô∏è Label Analysis Tab\nProvides intelligent label detection and statistical analysis with automatic identification of label images and distribution analysis.\n\n## Installation\n\n### From PyPI (Recommended)\n```bash\npip install napari-nifti-viewer\n```\n\n### From Source\n```bash\ngit clone https://github.com/yohanchiu/napari-nifti-viewer.git\ncd napari-nifti-viewer\npip install -e .\n```\n\n## Quick Start\n\n1. **Launch napari** with the plugin installed\n2. **Open the plugin** from the Plugins menu ‚Üí napari-nifti-viewer\n3. **Load a file** by clicking \"Browse...\" and selecting a .nii/.nii.gz file\n4. **Explore the data** across three informative tabs:\n   - **File Overview**: Basic information and statistics\n   - **Detailed Info**: Complete NIfTI headers and metadata\n   - **Label Analysis**: Label detection and analysis\n5. **Visualize in napari** by clicking \"Load to Napari\"\n\n## Usage Examples\n\n### Loading a Medical Image\n```python\nimport napari\nfrom napari_nifti_viewer import NiftiViewerWidget\n\n# Create napari viewer\nviewer = napari.Viewer()\n\n# The plugin will be available in the Plugins menu\n# Or you can add it programmatically:\nwidget = NiftiViewerWidget(viewer)\nviewer.window.add_dock_widget(widget, name=\"NIfTI Viewer\")\n```\n\n### Exporting Metadata\nThe plugin allows you to export complete metadata including:\n- File information (size, format, version)\n- NIfTI header fields (all 40+ standard fields)\n- Data statistics (shape, type, value ranges)\n- Coordinate system information\n- Affine transformation matrices\n\n## Requirements\n\n- **napari** >= 0.4.18\n- **nibabel** >= 5.2.1\n- **numpy** >= 1.21.0\n- **qtpy** >= 2.0.0\n- **magicgui** >= 0.7.0\n- **Python** >= 3.8\n\n## Supported File Formats\n\n- `.nii` - Uncompressed NIfTI files\n- `.nii.gz` - Compressed NIfTI files\n- Compatible with NIfTI-1 standard\n- Support for both neuroimaging and medical imaging data\n\n## Development\n\n### Setting up Development Environment\n\n```bash\n# Clone the repository\ngit clone https://github.com/yohanchiu/napari-nifti-viewer.git\ncd napari-nifti-viewer\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Run tests\npython -m pytest\n```\n\n### Running Tests\n\n```bash\n# Basic functionality test\npython test_plugin.py\n\n# Test with napari interface\npython test_plugin.py --napari\n```\n\n## Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.\n\n### Ways to Contribute\n- üêõ Report bugs\n- üí° Suggest new features\n- üìù Improve documentation\n- üîß Submit pull requests\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Citation\n\nIf you use this plugin in your research, please consider citing:\n\n```bibtex\n@software{napari_nifti_viewer,\n  title={napari-nifti-viewer: Comprehensive NIfTI Analysis for napari},\n  author={Qiu Yuheng},\n  year={2024},\n  url={https://github.com/yohanchiu/napari-nifti-viewer}\n}\n```\n\n## Acknowledgments\n\n- Built with [napari](https://napari.org/) - a fast, interactive, multi-dimensional image viewer\n- Uses [nibabel](https://nipy.org/nibabel/) for NIfTI file handling\n- Inspired by the neuroimaging and medical imaging communities\n\n## Support\n\n- üìñ [Documentation](https://github.com/yohanchiu/napari-nifti-viewer/wiki)\n- üêõ [Issue Tracker](https://github.com/yohanchiu/napari-nifti-viewer/issues)\n- üí¨ [Discussions](https://github.com/yohanchiu/napari-nifti-viewer/discussions)\n\n---\n\nMade with ‚ù§Ô∏è for the napari and neuroimaging communities \n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "NIfTI Viewer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-feature-classifier",
    "name": "napari-feature-classifier",
    "display_name": "napari feature classifier",
    "version": "0.3.2",
    "created_at": "2022-02-12",
    "modified_at": "2025-06-27",
    "authors": [
      "Joel Luethi and Max Hess"
    ],
    "author_emails": [
      "joel.luethi@uzh.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-feature-classifier/",
    "home_github": "https://github.com/fractal-napari-plugins-collection/napari-feature-classifier",
    "home_other": null,
    "summary": "An interactive classifier plugin to use with label images and feature measurements",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "napari",
      "matplotlib",
      "magicgui",
      "pandas>=2.2.0",
      "scikit-learn>=1.2.2",
      "pandera",
      "xxhash",
      "hypothesis"
    ],
    "package_metadata_description": "# napari-feature-classifier\n\n[![License](https://img.shields.io/pypi/l/napari-feature-classifier.svg?color=green)](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-feature-classifier.svg?color=green)](https://pypi.org/project/napari-feature-classifier)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-feature-classifier.svg?color=green)](https://python.org)\n[![tests](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/workflows/tests/badge.svg)](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/actions)\n[![codecov](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-classifier/branch/main/graph/badge.svg)](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-classifier)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-feature-classifier)](https://napari-hub.org/plugins/napari-feature-classifier)\n\nAn interactive classifier plugin that allows the user to assign objects in a label image to multiple classes and train a classifier to learn those classes based on a feature dataframe.\n\n## Usage\n<p align=\"center\"><img src=\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/assets/18033446/1ebf0890-1a7b-4e4b-a21c-88ca8f1dd800\" /></p>\n\nTo use the napari-feature-classifier, you need to have a label image and corresponding measurements: as a csv file, loaded to layer.features or in an [OME-Zarr Anndata table loaded with another plugin](https://github.com/fractal-napari-plugins-collection/napari-ome-zarr-navigator). Your feature measurements need to contain a `label` column that matches the label objects in the label image.\nThese interactive classification workflows are well suited to visually define cell types, find mitotic cells in images, do quality control by automatically detecting missegmented cells and other tasks where a user can easily assign objects to groups.\n\n#### Prepare the label layer:\n- Load your label layer into napari and add the features measurements to layer.features of the corresponding label layer. You can have multiple label layers with their features open at the same time\n    - To load features from a CSV file: `Plugins -> napari-feature-classifier -> CSV Feature Loader`, then load the features for the correct label image.\n    - To load features from an OME-Zarr file: Get both the label layer into memory as a normal label layer (not a pyramidal label layer, currently untested) and the corresponding features. If your OME-Zarr file is created by [Fractal](https://fractal-analytics-platform.github.io/), you can use [this ROI loader plugin](https://github.com/jluethi/napari-ome-zarr-roi-loader).\n    - To load features from anywhere else, load them manually to your label_layer.features\n- Your feature table should have 2 columns used for indexing (but stored as normal columns in layer.features):\n    - The `label` column to match the object in the label image\n    - The `roi_id` column to identify the image you're currently classifying (used when a classifier is trained on multiple label images)\n\n\n#### Initialize a classifier:\n- Start the classifier in napari by going to `Plugins -> napari-feature-classifier -> Initialize a Classifier`  \n- Select the features you want to use for the classifier (you need to do the feature selection before initializing. The feature selection can't be changed after initialization anymore). Hold the command key to select multiple features. Feature options are always shown for the features available in the last selected label layer, based on layer.features available features.\n- (Optional) Give your classes recognizable names (e.g. Mitotic & Interphase, Cell Type a, b and c etc.)\n<img width=\"1606\" alt=\"Screenshot 2023-05-09 at 11 46 35\" src=\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/assets/18033446/452c0d6a-98a3-4e2d-9233-33bfd5bcad19\">\n\n\n\n\n#### Classify objects:\n<img width=\"1802\" alt=\"Classifier_annotation\" src=\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/assets/18033446/556739b8-972b-4570-9da4-637738fc6a75\">\n\n- Make sure you have the label layer selected on which you want to classify\n- Select the current class with the radio buttons or by pressing 0, 1, 2, etc.\n- Click on label objects in the viewer to assign them to the currently selected class\n- Once you have trained enough examples, click \"Run Classifier\" to run the classifier and have it make a prediction for all objects. Aim for at least a dozen annotations per class, as the classifier divides your annotations 80/20 in training and test sets. \n- Once you get predictions, correct mistakes the classifier made and retrain it to improve its performance.\n- You can save the classifier under a different name or in a different location. Define the new output location and then click `Save Classifier` (you need to click the Save Classifier button. Just defining the new output path does not save it yet. But every run of the classifier triggers an autosave)\n<img width=\"1802\" alt=\"Classifier_prediction\" src=\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/assets/18033446/69cff600-4585-4a66-9274-d2e7caeb335f\">\n\n\n\n#### Apply the classifier to additional images:\n- You can apply a classifier trained on one image to additional label images. Use `Plugins -> napari-feature-classifier -> Load Classifier`  \n- Select the classifier (.clf file with the name you gave above) while already having the label images ready (see `Prepare the label layer` above).\n- Click Load Classifier, proceed as above.\n<img width=\"1606\" alt=\"Screenshot 2023-05-09 at 12 01 00\" src=\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/assets/18033446/e1143f9f-9729-4f8e-979c-2ab195e0aaca\">\n\n\n\n#### Export classifier results\n- To export the training data and the results of the classifier, define an Export Name (full path to an output file or just a filename ending in .csv) where the results of the classifier shall be saved. It defaults to the layer name for the selected layer in the last directory you chose (or the current working directory if none was chosen so far)\n- Click `Export Classifier Result` (Just selecting a filename is not enough, you need to click the export button). This will export the predictions for the currently selected layer.\n- The results of the classifier are save in a csv file. The label is an integer of the label object within that image. The prediction column contains predictions of the classifier for all objects (except those that contained NaNs in their feature data) and the annotation column contains the annotations you made (NaN for unclassified objects, -1 for objects you deselected, 1 - 9 for the classes)\n<img width=\"1802\" alt=\"Classifier_prediction\" src=\"https://github.com/fractal-napari-plugins-collection/napari-feature-classifier/assets/18033446/e8f6f7b7-d88b-44f8-b43e-8a2fa81e18d4\">\n\n\n#### Batch mode result export\n(To be updated: Create a new notebook to run batch processing, this is for the older version of the classifier)\nThere is a simple workflow for the classifier in the examples folder:\n- Install jupyter-lab (`pip install jupyterlab`)\n- Open the notebook in jupyter lab (Type `jupyter-lab` in the terminal when you are in the examples folder)\n- Follow the instructions to generate an example dataframe and an example label image\n- Use the classifier in napari with this simplified data\n\n\n#### Initializing the Annotator\nYou can use the annotation functionality also independently from the classifier\nStart the annotator widget by going to `Plugins -> napari-feature-classifier -> Annotator`\nSelect names for your classes. You can name up to 9 classes. Only classes that you give a name will be created upon initialization.\nThen click `Initialize`.\n\n<img width=\"1411\" alt=\"Screenshot 2023-02-16 at 14 49 38\" src=\"https://user-images.githubusercontent.com/18033446/219384524-9873bd66-270b-4cdd-b913-60d390f6c77a.png\">\n\nA annotator widget opens. Use the Radio-Buttons to select what class you're annotating (or keybindings for 1-9 for classes, 0 for deselect).\nThe annotator will always work on the currently selected label layer. While the annotator is open, you can't edit the labels. Restart napari to allow editing of labels again.\n\n<img width=\"1411\" alt=\"Screenshot 2023-02-16 at 14 50 00\" src=\"https://user-images.githubusercontent.com/18033446/219384925-b20e4c1a-2eca-4070-8269-902493c5d5ef.png\">\n\nThe annotations are saved in the `layer.features` table of the corresponding label layer as an `annotations` column.\n<img width=\"1411\" alt=\"Screenshot 2023-02-16 at 15 01 01\" src=\"https://user-images.githubusercontent.com/18033446/219385788-f61bd0a5-fbb6-42d7-81e5-f77ee4d1b4ff.png\">\n\n\n## Installation\n\nThis plugin is written for the new napari npe2 plugin engine. Thus, it requires napari >= 0.4.13.\nActivate your environment where you have napari installed (or install napari using `pip install \"napari[all]\"`), then install the classifier plugin:\n\n    pip install napari-feature-classifier\n\nThe layer.features dataframes have some issues in napari 0.4.17 (see [here](https://github.com/napari/napari/issues/5617)). They seem to be working again in the nighlty builds. To set up a nightly builds napari env, do the following:\n\n```\nconda create -n classifier-dev-napari-main -c \"napari/label/nightly\" -c conda-forge napari python=3.10 -y\n```\n    \n## Similar napari plugins\nIf you're looking for other classification approaches, [apoc](https://github.com/haesleinhuepf/apoc) by [Robert Haase](https://github.com/haesleinhuepf) has a pixel classifier in napari and an object classification workflow:  \n[napari-accelerated-pixel-and-object-classification (APOC)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification)  \nAlternatively, Cl√©ment Cazorla has built [napari-svetlana, a deep learning based classifier](https://www.napari-hub.org/plugins/napari-svetlana)\n\n## Release process\n1. Update the version number in src/napari-feature-classifier/__init__.py\n2. Update the version in setup.cfg\n3. Add a Github release with a new version tag (matching the version set above)\n4. Once tests pass, this should automatically be deployed to pypi\n5. Wait for conda automation to make a PR for an updated conda release (see https://github.com/conda-forge/napari-feature-classifier-feedstock). This can take 1-2 days. Make sure that PR gets merged.\n\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-feature-classifier\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Contributors\n[Joel L√ºthi](https://github.com/jluethi) & [Max Hess](https://github.com/MaksHess)\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotator",
      "Initialize a Classifier",
      "Load Classifier",
      "CSV Feature loader"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-feature-visualization",
    "name": "napari-feature-visualization",
    "display_name": "Napari Feature Visualization",
    "version": "0.0.2",
    "created_at": "2024-07-25",
    "modified_at": "2025-06-27",
    "authors": [
      "Joel Luethi",
      "Adrian Tschan"
    ],
    "author_emails": [
      "joel.luethi@uzh.ch"
    ],
    "license": "Copyright (c) 2024, Joel Lueth...",
    "home_pypi": "https://pypi.org/project/napari-feature-visualization/",
    "home_github": "https://github.com/fractal-napari-plugins-collection/napari-feature-visualization",
    "home_other": null,
    "summary": "Visualizing feature measurements on label images in napari",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "matplotlib",
      "pandas",
      "packaging",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-feature-visualization\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-feature-visualization.svg?color=green)](https://github.com/fractal-napari-plugins-collection/napari-feature-visualization/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-feature-visualization.svg?color=green)](https://pypi.org/project/napari-feature-visualization)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-feature-visualization.svg?color=green)](https://python.org)\n[![tests](https://github.com/fractal-napari-plugins-collection/napari-feature-visualization/workflows/tests/badge.svg)](https://github.com/fractal-napari-plugins-collection/napari-feature-visualization/actions)\n[![codecov](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-visualization/branch/main/graph/badge.svg)](https://codecov.io/gh/fractal-napari-plugins-collection/napari-feature-visualization)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-feature-visualization)](https://napari-hub.org/plugins/napari-feature-visualization)\n\nVisualizing feature measurements on label images in napari\n\n<img src=\"https://github.com/user-attachments/assets/d2c83d70-d122-4e08-812f-12c5e6006488\" alt=\"feature_vis_demo\" style=\"width: 100%;\"/>\n\nSupports both loading features from a CSV file or visualizing features saved in the label_layer.features dataframe. Through plugins like the [napari OME-Zarr navigator](https://github.com/fractal-napari-plugins-collection/napari-ome-zarr-navigator), this enables visualizing feature measurements stored in OME-Zarrs.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-feature-visualization` via [pip]:\n\n    pip install napari-feature-visualization\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/fractal-napari-plugins-collection/napari-feature-visualization.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-feature-visualization\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/fractal-napari-plugins-collection/napari-feature-visualization/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Feature Visualization"
    ],
    "contributions_sample_data": [
      "Napari Feature Visualization"
    ]
  },
  {
    "normalized_name": "napari-imagegrains",
    "name": "napari-imagegrains",
    "display_name": "ImageGrains",
    "version": "0.1.0",
    "created_at": "2025-06-26",
    "modified_at": "2025-06-26",
    "authors": [
      "Guillaume Witz",
      "Michael Horn"
    ],
    "author_emails": [
      "Guillaume Witz <guillaume.witz@unibe.ch>",
      "Michael Horn <michael.horn@unibe.ch}>"
    ],
    "license": "Copyright (c) 2025,  Universit...",
    "home_pypi": "https://pypi.org/project/napari-imagegrains/",
    "home_github": "https://github.com/guiwitz/napari-imagegrains",
    "home_other": null,
    "summary": "An interactive napari plugin for the ImageGrains software.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "superqt",
      "napari_matplotlib",
      "scikit-image",
      "seaborn",
      "pandas",
      "imagegrains",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "nbformat; extra == \"testing\"",
      "nbconvert; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-imagegrains\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-imagegrains.svg?color=green)](https://github.com/guiwitz/napari-imagegrains/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-imagegrains.svg?color=green)](https://pypi.org/project/napari-imagegrains)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-imagegrains.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-imagegrains/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-imagegrains/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-imagegrains/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-imagegrains)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-imagegrains)](https://napari-hub.org/plugins/napari-imagegrains)\n\nAn interactive napari plugin for the [ImageGrains](https://github.com/dmair1989/imagegrains) software.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nWe recommend to install the plugin in an isolated environment as provided by conda. For conda create an appropriate environment with (do not use Python more recent than 3.11):\n\n    conda create -n napari-imagegrains -c conda-forge python=3.11 napari pyqt\n    conda activate napari-imagegrains\n\n> :warning: \n> This is a work in progress and the plugin is available neither on PyPi nor in the napari plugin manager.\n>You can install `napari-imagegrains` via [pip]:\n>\n>    pip install napari-imagegrains\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/guiwitz/napari-imagegrains.git\n\nOr if you want to contribute to the plugin, fork the repository, clone it locally and install it in editable mode:\n\n    pip install -e .\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-imagegrains\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Authors\n\nThe original software ImageGrain was developed by David Mair, Institute of Geological Sciences, University of Bern. The current plugin, a user-interface for the ImageGrains software, was developed by Guillaume Witz and Michael Horn, Data Science Lab, University of Bern in collaboration with David Mair.\n\n## Citation\n\nIf you use this software, please cite the following publication: Mair, D., Witz, G., Do Prado, A.H., Garefalakis, P. & Schlunegger, F. (2023) Automated detecting, segmenting and measuring of grains in images of fluvial sediments: The potential for large and precise data from specialist deep learning models and transfer learning. Earth Surface Processes and Landforms, 1‚Äì18. <https://doi.org/10.1002/esp.5755>.\n\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/guiwitz/napari-imagegrains/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ImageGrain Processing Widget",
      "ImageGrain Stats Widget",
      "ImageGrain Demo Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-hsi-analysis",
    "name": "napari-hsi-analysis",
    "display_name": "Hyperspectral Imaging Analysis Plugin",
    "version": "0.3.1",
    "created_at": "2025-03-12",
    "modified_at": "2025-06-23",
    "authors": [
      "Alessia Di Benedetto"
    ],
    "author_emails": [
      "alessiadibenedetto.97@gmail.com"
    ],
    "license": "Copyright (c) 2025, Alessia Di...",
    "home_pypi": "https://pypi.org/project/napari-hsi-analysis/",
    "home_github": "https://github.com/alessiadb/napari-hsi-analysis",
    "home_other": null,
    "summary": "Napari plugin to perform analysis on Hyperspectral Imaging datasets.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scikit-learn",
      "h5py",
      "bokeh",
      "plotly",
      "PyWavelets",
      "scipy",
      "pyqtgraph",
      "qtawesome",
      "matplotlib",
      "umap-learn",
      "spectral",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "numpy; extra == \"testing\"",
      "magicgui; extra == \"testing\"",
      "qtpy; extra == \"testing\"",
      "scikit-image; extra == \"testing\"",
      "scikit-learn; extra == \"testing\"",
      "h5py; extra == \"testing\"",
      "bokeh; extra == \"testing\"",
      "plotly; extra == \"testing\"",
      "PyWavelets; extra == \"testing\"",
      "scipy; extra == \"testing\"",
      "pyqtgraph; extra == \"testing\"",
      "qtawesome; extra == \"testing\"",
      "matplotlib; extra == \"testing\"",
      "umap-learn; extra == \"testing\"",
      "spectral; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-hsi-analysis\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-hsi-analysis.svg?color=green)](https://github.com/alessiadb/napari-hsi-analysis/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-hsi-analysis.svg?color=green)](https://pypi.org/project/napari-hsi-analysis)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-hsi-analysis.svg?color=green)](https://python.org)\n[![tests](https://github.com/alessiadb/napari-hsi-analysis/workflows/tests/badge.svg)](https://github.com/alessiadb/napari-hsi-analysis/actions)\n[![codecov](https://codecov.io/gh/alessiadb/napari-hsi-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/alessiadb/napari-hsi-analysis)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-hsi-analysis)](https://napari-hub.org/plugins/napari-hsi-analysis)\n\nA Napari plugin to perform analysis on Hyperspectral Imaging datasets.\n\nThe 'Data Manager' widget loads, opens and visualize the datasets.\nThe 'Fusion' widget fused two or three opened datasets.\nThe 'UMAP' widget perform and visualize the Uniform Manifold Approximation and Projection analysis.\n\n----------------------------------\n\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n\n## Installation\n\nYou can install `napari-hsi-analysis` via [pip]:\n\n    pip install napari-hsi-analysis\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/alessiadb/napari-hsi-analysis.git\n\n## Usage\nA detailed guide which shows how to use the plugin and how to properly choose the parameters can be found [here].\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-hsi-analysis\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/alessiadb/napari-hsi-analysis/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[here]:  https://github.com/alessiadb/napari-hsi-analysis/blob/main/docs/guide.md\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "HSI Analysis"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-proofread-brainbow",
    "name": "napari-proofread-brainbow",
    "display_name": "Proofread Brainbow",
    "version": "0.3.1",
    "created_at": "2022-08-30",
    "modified_at": "2025-06-23",
    "authors": [
      "Seongbin Lim"
    ],
    "author_emails": [
      "sungbin246@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-proofread-brainbow/",
    "home_github": null,
    "home_other": "None",
    "summary": "Proofreading Brainbow images with napari",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "setuptools-scm; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-proofread-brainbow\n\n[![License MIT](https://img.shields.io/pypi/l/napari-proofread-brainbow.svg?color=green)](https://github.com/sbinnee/napari-proofread-brainbow/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-proofread-brainbow.svg?color=green)](https://pypi.org/project/napari-proofread-brainbow)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-proofread-brainbow.svg?color=green)](https://python.org)\n[![tests](https://github.com/sbinnee/napari-proofread-brainbow/workflows/tests/badge.svg)](https://github.com/sbinnee/napari-proofread-brainbow/actions)\n[![codecov](https://codecov.io/gh/sbinnee/napari-proofread-brainbow/branch/main/graph/badge.svg)](https://codecov.io/gh/sbinnee/napari-proofread-brainbow)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-proofread-brainbow)](https://napari-hub.org/plugins/napari-proofread-brainbow)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nProofreading Brainbow images with napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-proofread-brainbow` via [pip]:\n\n    pip install napari-proofread-brainbow\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-proofread-brainbow\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Proofread Brainbow",
      "Threshold probability (csv)"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pyav",
    "name": "napari-pyav",
    "display_name": "pyav video plugin",
    "version": "0.0.10",
    "created_at": "2024-10-05",
    "modified_at": "2025-06-23",
    "authors": [
      "jlab.berlin"
    ],
    "author_emails": [
      "yourname@example.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-pyav/",
    "home_github": "https://github.com/danionella/napari-pyav",
    "home_other": null,
    "summary": "Napari plugin for reading videos using PyAV",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "av<14",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-pyav\n\n[![License MIT](https://img.shields.io/pypi/l/napari-pyav.svg?color=green)](https://github.com/danionella/napari-pyav/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pyav.svg?color=green)](https://python.org)\n[![PyPI](https://img.shields.io/pypi/v/napari-pyav.svg?color=green)](https://pypi.org/project/napari-pyav)\n[![Conda Version](https://img.shields.io/conda/v/danionella/napari_pyav)](https://anaconda.org/danionella/napari_pyav)\n[![tests](https://github.com/danionella/napari-pyav/workflows/tests/badge.svg)](https://github.com/danionella/napari-pyav/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pyav)](https://napari-hub.org/plugins/napari-pyav)\n\nNapari plugin for reading videos using [PyAV](https://github.com/PyAV-Org/PyAV). Inspired by the [napari-video](https://github.com/janclemenslab/napari-video) project, which served us very well for many years. For some long videos, however, its dependency on opencv caused seek glitches, so we implemented this alternative plugin based on PyAV.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-pyav` via [pip]:\n\n    pip install napari-pyav\n\n\nOr via [conda](https://github.com/conda-forge/miniforge?tab=readme-ov-file#miniforge): \n\n    conda install danionella::napari_pyav\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/danionella/napari-pyav.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-pyav\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/danionella/napari-pyav/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mp4"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-roxas-ai",
    "name": "napari-roxas-ai",
    "display_name": "ROXAS AI",
    "version": "0.1.2",
    "created_at": "2025-05-17",
    "modified_at": "2025-06-19",
    "authors": [
      "Nicola Antonio Santacroce",
      "Marc Katzenmaier",
      "Triyan Bhardwaj",
      "Georg von Arx"
    ],
    "author_emails": [
      "The WSL Dendrosciences Group <roxas@wsl.ch>"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/napari-roxas-ai/",
    "home_github": "https://github.com/roxas-ai/napari-roxas-ai",
    "home_other": null,
    "summary": "A plugin that integrates the ROXAS AI analysis methods for quantitative wood anatomy in the napari platform",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "magicgui==0.10.0",
      "napari[all]==0.5.6",
      "numpy<=2.1.3,>=2.0.2",
      "opencv-contrib-python-headless==4.11.0.86",
      "qtpy==2.4.3",
      "rasterio==1.4.3",
      "scikit-image<=0.25.2,>=0.24.0",
      "matplotlib<=3.10.1,>=3.9.4",
      "torch<2.7.0,>=2.2.0; sys_platform == \"linux\" and platform_machine == \"x86_64\"",
      "torchvision==0.21.0; sys_platform == \"linux\" and platform_machine == \"x86_64\"",
      "torch<2.7.0,>=2.2.0; sys_platform == \"win32\" and platform_machine == \"AMD64\"",
      "torchvision==0.21.0; sys_platform == \"win32\" and platform_machine == \"AMD64\"",
      "torch<2.7.0,>=2.4.0; sys_platform == \"darwin\" or platform_machine == \"arm64\"",
      "torchvision==0.21.0; sys_platform == \"darwin\" or platform_machine == \"arm64\"",
      "pytorch-lightning==2.5.1",
      "segmentation-models-pytorch==0.4.0",
      "albumentations==2.0.5",
      "hydra-core==1.3.2",
      "tox==4.25.0; extra == \"testing\"",
      "pytest==8.3.5; extra == \"testing\"",
      "pytest-cov==6.0.0; extra == \"testing\"",
      "pytest-qt==4.4.0; extra == \"testing\"",
      "napari==0.5.6; extra == \"testing\"",
      "pyqt5==5.15.11; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-roxas-ai\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-roxas-ai.svg?color=green)](https://github.com/roxas-ai/napari-roxas-ai/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-roxas-ai.svg?color=green)](https://pypi.org/project/napari-roxas-ai)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-roxas-ai.svg?color=green)](https://python.org)\n[![tests](https://github.com/roxas-ai/napari-roxas-ai/workflows/tests/badge.svg)](https://github.com/roxas-ai/napari-roxas-ai/actions)\n[![codecov](https://codecov.io/gh/roxas-ai/napari-roxas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/roxas-ai/napari-roxas-ai)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roxas-ai)](https://napari-hub.org/plugins/napari-roxas-ai)\n\nA plugin that integrates the ROXAS AI analysis methods for quantitative wood anatomy in the napari platform\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\n### Environment setup\nIt's recommended to create a dedicated Python environment for napari-roxas-ai:\n\n1. Install Miniconda if you don't have it already: [Miniconda Installation Guide](https://docs.conda.io/en/latest/miniconda.html)\n\n2. Create a new environment:\n```bash\nconda create -n roxas-ai python=3.12\nconda activate roxas-ai\n```\n\n### Installation\nInstall `napari-roxas-ai` via [pip]:\n\n```bash\npip install napari-roxas-ai\n```\n\n### Launching the plugin\nOnce installed, you can launch napari with the roxas-ai plugin:\n\n```bash\nnapari\n```\n\n### Verifying installation\nTo check if the plugin is working correctly:\n1. Go to `File > Open Sample > ROXAS AI` in the napari interface.\n2. The first time you open a sample, it may take some time as sample data and model weights are being downloaded. Progress will be logged in the terminal.\n3. After the downloads, a sample made of three layers should open in the viewer\n\n### GPU Support\nIf you want to use GPU acceleration for model inference:\n\n1. Ensure you have the proper GPU drivers and CUDA installed for your system:\n   - [NVIDIA CUDA Installation Guide Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)\n   - [NVIDIA CUDA Installation Guide Windows](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)\n\n2. Enable GPU support in the napari-roxas-ai settings within the napari interface.\n\n3. You may need to reinstall PyTorch with CUDA support for your specific hardware:\n   Visit the [PyTorch Installation Guide](https://pytorch.org/get-started/locally/) to find the appropriate installation command for your setup.\n\n## Contributing\n\nContributions are very welcome. Tests are automatically run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n### Contributor Installation\n\nIn order to contribute to the development of the plugin the installation can be done as follows:\n1. Create an environment\n```bash\nconda create -n roxas-ai python=3.12\nconda activate roxas-ai\n```\n2. In the cloned / forked plugin directory, install the plugin dependencies\n```bash\npip install -e .\n```\n\n3. Install the testing dependencies, as well as the napari plugin engine\n```bash\npip install -e \".[testing]\"\npip install npe2\n```\n\n4. Install pre-commit for quality checks\n```bash\npip install pre-commit\npre-commit install\n```\n\n### Documentation: Plugin Template and Development\nYou can find more information on the plugin template on the [napari-plugin-template repository](https://github.com/napari/napari-plugin-template).\nYou can find more information on plugin contributions and how to create plugins on the [plugins section of the napari documentation](https://napari.org/dev/plugins/index.html).\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-roxas-ai\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/roxas-ai/napari-roxas-ai/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.cells*",
      "*.rings*",
      "*.scan*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "0 - Define project directory",
      "1 - Prepare project images for analysis",
      "2/4 - Load image(s)",
      "3A - Detect cells & rings (individual image)",
      "3B - Batch detect cells & rings",
      "5 - Edit cells",
      "6 - Edit rings",
      "7 - Visual cross-dating",
      "8 - Save cells & rings editing",
      "9A - Measure cells & rings (individual image)",
      "9B - Batch measure cells & rings",
      "ZZ - Settings"
    ],
    "contributions_sample_data": [
      "ROXAS AI"
    ]
  },
  {
    "normalized_name": "napari-psfgenerator",
    "name": "napari-psfgenerator",
    "display_name": "PSF Generator",
    "version": "0.3.2",
    "created_at": "2025-02-05",
    "modified_at": "2025-06-17",
    "authors": [
      "Vasiliki Stergiopoulou",
      "Jonathan Dong",
      "Yan Liu",
      "Daniel Sage"
    ],
    "author_emails": [
      "Vasiliki Stergiopoulou <vasiliki.stergiopoulou@epfl.ch>",
      "Jonathan Dong <jonathan.dong@epfl.ch>",
      "Yan Liu <yan.liu@epfl.ch>",
      "Daniel Sage <daniel.sage@epfl.ch>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-psfgenerator/",
    "home_github": "https://github.com/VStergiop/napari-psfgenerator",
    "home_other": null,
    "summary": "Plugin to compute the focal electric field",
    "categories": [
      "Simulation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "magicgui",
      "qtpy",
      "psf-generator",
      "pyqt5",
      "napari",
      "napari; extra == \"napari\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-psfgenerator\n\n[![MIT License](https://img.shields.io/github/license/Biomedical-Imaging-Group/napari-psfgenerator)](https://github.com/Biomedical-Imaging-Group/napari-psfgenerator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-psfgenerator.svg?color=green)](https://pypi.org/project/napari-psfgenerator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-psfgenerator.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psfgenerator)](https://napari-hub.org/plugins/napari-psfgenerator)\n\n## PSF Generator Napari Plugin\n\nThe **PSF Generator Napari Plugin** provides an intuitive, interactive platform for simulating **Point Spread Functions (PSFs)** directly within the Napari ecosystem. Built on **PyTorch**, this plugin supports both **CPU and GPU-accelerated** computations, ensuring fast and efficient simulations for fundamental and advanced optical modeling.\n\n### Key Features:\n\n- **Flexible Propagation Models:** Scalar and vectorial propagators in Cartesian and spherical coordinates.\n- **Customizable Parameters:** Configure **physical** (e.g., numerical aperture, wavelength), **numerical** (e.g., pixel size, Z-stacks), and **optical settings** (e.g., Gibson-Lanni corrections, Zernike aberrations).\n- **Real-Time Visualization:** Seamless integration with Napari for immediate visual feedback.\n- **Versatile API:** Access propagators programmatically for custom workflows.\n- **Image Export:** Save computed PSFs in **TIFF format**.\n\nThis plugin is a powerful tool for researchers in **optics, computational microscopy, and imaging science**, bridging user-friendly interactivity with the computational capabilities of our Python library.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nSet up a Python virtual environment and install napari following this [guide].\n\nYou can install `napari-psfgenerator` via [pip]:\n\n    pip install napari-psfgenerator\n\nTo install latest development version :\n\n    pip install git+https://github.com/Biomedical-Imaging-Group/napari-psfgenerator.git\n\n\nNow you can try the plugin out! Open napari, click on the menu \"Plugins\" and select \"Propagators (PSF Generator)\".\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-psfgenerator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n[guide]: https://napari.org/dev/tutorials/fundamentals/installation.html\n[file an issue]: https://github.com/VStergiop/napari-psfgenerator/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[psf_generator library]: https://github.com/Biomedical-Imaging-Group/psf_generator\n[plugin]: https://github.com/Biomedical-Imaging-Group/napari-psfgenerator\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Propagators"
    ],
    "contributions_sample_data": [
      "PSF Generator"
    ]
  },
  {
    "normalized_name": "napari-sc3d-viewer",
    "name": "napari-sc3D-viewer",
    "display_name": "sc3D Viewer",
    "version": "1.1.1",
    "created_at": "2022-06-21",
    "modified_at": "2025-06-17",
    "authors": [
      "Leo Guignard"
    ],
    "author_emails": [
      "leo.guignard@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-sc3d-viewer/",
    "home_github": "https://github.com/GuignardLab/napari-sc3D-viewer",
    "home_other": null,
    "summary": "A plugin to visualize 3D single cell omics",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "sc-3D",
      "matplotlib",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pyvista; extra == \"pyvista\""
    ],
    "package_metadata_description": "# napari-sc3D-viewer\n\n[![License](https://img.shields.io/pypi/l/napari-sc3D-viewer.svg?color=green)](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sc3D-viewer.svg?color=green)](https://python.org)\n[![PyPI](https://img.shields.io/pypi/v/napari-sc3D-viewer.svg?color=green)](https://pypi.org/project/napari-sc3D-viewer)\n[![tests](https://github.com/GuignardLab/napari-sc3D-viewer/workflows/tests/badge.svg)](https://github.com/GuignardLab/napari-sc3D-viewer/actions)\n[![codecov](https://codecov.io/gh/GuignardLab/napari-sc3D-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/GuignardLab/napari-sc3D-viewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sc3D-viewer)](https://napari-hub.org/plugins/napari-sc3D-viewer)\n\nA plugin to visualise 3D spatial single cell omics\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Test and atlas datasets\n\nBecause the datasets representing the mouse embryo at stages E8.5 and E9.0 are rather large, it is not possible to host them on GitHub. They are instead hosted on figshare at the following links:\n\n- [E8.5 replicate 1](https://figshare.com/s/1c29d867bc8b90d754d2)\n- [E8.5 replicate 2](https://doi.org/10.6084/m9.figshare.21695849.v1)\n- [E9.0 replicate 1](https://doi.org/10.6084/m9.figshare.21695879.v1)\n\nOnce downloaded, one can open them in the viewer as explained below (note that the files for the tissue names are stored in the json file there: `napari-sc3D-viewer/test_data/corresptissues.json`). It can be downloaded by right-clicking on the following [link](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/test_data/corresptissues.json) and then clicking on \"Save link as\".\n\n## Installation\n\n----------------------------------\n\n__Disclaimer:__\nWhile we tried to make the installation and usage as easy as possible, please keep in mind that [napari-sc3d-viewer] is still under development, it has been and is being developed by a single person. We will be happy to answer any question and help in any way.\n\n----------------------------------\n\nThere are many ways to install our viewer, but the global idea is that it works in two steps:\n\n- first installing [napari]\n- then installing the [napari-sc3d-viewer] plugin.\n\nInstalling [napari] and the [napari-sc3d-viewer] plugin can be done either through command line or using an interface.\n\nIf you have decided to use command line, as [napari] developers do, we strongly recommend to install the viewer in an environement such as a conda environment `conda` for example:\n\n```shell\nconda create -n sc3D python=3.10\nconda activate sc3D\n```\n\n### Installing napari\n\nThe first step is to [install napari](https://napari.org/stable/tutorials/fundamentals/installation.html) on your computer. The previous link should explain how to do so. There you can find either the installation via terminal or directly by [downloading the binary](https://napari.org/stable/tutorials/fundamentals/installation.html#install-as-a-bundled-app).\n\n#### Quick trouble shooting\n\nInstalling [napari] can sometimes be difficult. If you try to install [napari] via the command line and it gets stuck \"resolving the environment\" you can try to install it the following way:\n\n```shell\nconda create -n sc3D python=3.10\nconda activate sc3D\nconda install pyqt pip\npip install napari\n```\n\n### Installing napari-sc3D-viewer\n\nOnce [napari] is installed, you can install `napari-sc3D-viewer`.\nAs for [napari], [napari-sc3D-viewer] can be installed either through an interface or via the terminal.\n\n#### Installation via graphical interface\n\nTo install [napari-sc3D-viewer] with a visual interface, you should use the [napari's plugin manager](https://napari.org/stable/plugins/find_and_install_plugin.html) look for the plugin there and install it as explained in the previous link.\n\n#### Installation via the terminal\n\nAnother way is to install `napari-sc3D-viewer` via [pip] or via [conda]:\n\n```shell\nconda install napari-sc3d-viewer\n```\n\nor\n\n```shell\npip install napari-sc3d-viewer\n```\n\nFinally, to install latest development version :\n\n```shell\npip install git+https://github.com/GuignardLab/napari-sc3D-viewer.git\n```\n\n#### Installation of the surface computation module\n\nTo install the surface computation enabled version it is necessary to use Python 3.9 (until [VTK] is ported to Python 3.10) and you can run one of the following commands:\n\n```shell\npip install '.[pyvista]'\n```\n\nfrom the correct folder or\n\n```shell\npip install 'napari-sc3D-viewer[pyvista]'\n```\n\nor\n\n```shell\nconda install 'napari-sc3D-viewer[pyvista]'\n```\n\nto install directly from pip or\n\n```shell\npip install 'napari-sc3D-viewer[pyvista] @ git+https://github.com/GuignardLab/napari-sc3D-viewer.git'\n```\n\nto install the latest version\n\n## Usage\n\n`napari-sc3D-viewer` allows users to easily visualise and navigate 3D spatial single-cell transcriptomics using napari.\n\n### Starting the plugin\n\nFirst, you need to start [napari], for example, one can start it from a terminal just by typing:\n\n```shell\nnapari\n```\n\nin the correct environment.\n\nThen, one can follow the following steps to browse the dataset.\n\nTo open the plugin you can click on the \"Load spatial single cell\" from the `Plugins -> napari-sc3d-viewer` menu:\n![loading image](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/0.openplugin.png)\n\nOnce opened you should have an interface poping similar to the one showed in the image below (note that it might not be exactly the same depending on the version of the viewer you are using).\n\n### Loading and opening a dataset\n\nThe expected dataset is a [scanpy]/[anndata] h5ad file together with an optional json file that maps cluster id numbers to actual tissue/cluster name.\n\nThe json file should look like that:\n\n```json\n{\n    \"1\": \"Endoderm\",\n    \"2\": \"Heart\",\n    \"10\": \"Anterior neuroectoderm\"\n}\n```\n\nIf no json file or a wrong json file is given, the original cluster id numbers are used.\n\nThe h5ad file should be informed in (1) and the json file in (2).\n![loading image](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/1.loading.png)\n\nLet `data` be your h5ad data structure. To work properly, the viewer is expecting 4 different columns to be present in the h5ad file:\n\n- the cluster id column (by default named 'predicted.id' that can be accessed as `data.obs['predicted.id']`)\n- the 3D position column (by default named 'X_spatial_registered' that can be accessed as `data.obsm['X_spatial_registered']`)\n- the gene names if not already in the column name (by default named 'feature_name' that can be accessed as `data.var['feature_name']`)\n- umap coordinates (by default named 'X_umap' that can be accessed as `data.obsm['X_umap']`)\n\nIf the default column names are not consistent with your dataset, they can be changed in the tab `Parameters` (3) next to the tab `Loading files`\n\nOnce all the data paths and fields are correctly informed pressing the `Load Atlas` button (4) will load the dataset.\n\n### Exploring a dataset\n\nOnce the dataset is loaded there are few options to explore it.\n\nThe viewer should look like to the following:\n![viewer](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/2.viewer.png)\n\nIt is divided in two main parts, the Tissue visualisation (1) part and the Metric visualisation (2) one.\nBoth of them are themselves split in two and three tabs respectively. All these tabs allow you to visualise and explore the dataset in different fashions.\n\nThe Tissues tab (1.1) allows to select the tissues to display, to show the legend and to colour the cells according to their tissue types.\n\nThe Surfaces tab (1.2) allows to construct coarse surfaces of tissues and to display them.\n\nThe Single metric tab (2.1) allows to display a metric, whether it is a gene intensity or a numerical metric that is embedded in the visualised dataset. This tab also allows to threshold cells according to the viewed metric, to change the contrast and the colour map.\n\nThe 2 Genes (2.2) tab allows to display gene coexpression.\n\nThe umap tab (2.3) allows to display the umap of the selected cells and to manually select subcategories of cells to be displayed.\n\n![viewer](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/3.description.png)\n\n#### Explanatory \"videos\"\n\nThe plugin is meant to be easy to use. That means that you should be able to play with it and figure things out by yourself.\n\nThat being said, it is not always that easy. You can find below a series of videos showing how to perform some of the main features.\n\n#### Loading data\n\n![Loading data video](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/loading.gif)\n\n#### Selecting tissues\n\n![Selecting tissues video](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/tissue-select.gif)\n\n#### Displaying one gene\n\n![Displaying one gene video](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/gene1.gif)\n\n#### Displaying two genes co-expression\n\n![Displaying genes video](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/gene2.gif)\n\n#### Playing with the umap\n\n![Playing with the umap video](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/umap.gif)\n\n#### Computing and processing the surface\n\n![Computing and processing the surface video](https://raw.githubusercontent.com/GuignardLab/napari-sc3D-viewer/main/images/surfaces.gif)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-sc3D-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/GuignardLab/napari-sc3D-viewer/issues\n[napari-sc3d-viewer]: https://github.com/GuignardLab/napari-sc3D-viewer\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[VTK]: https://vtk.org/\n[scanpy]: https://scanpy.readthedocs.io/en/latest/index.html\n[anndata]: https://anndata.readthedocs.io/en/latest/\n[conda]: https://conda.io\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Load spatial single cell",
      "Register spatial single cell"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cci-annotator",
    "name": "napari-cci-annotator",
    "display_name": "CCI Annotator",
    "version": "0.6.0",
    "created_at": "2025-04-28",
    "modified_at": "2025-06-16",
    "authors": [
      "Anders Folkesson"
    ],
    "author_emails": [
      "anders.folkesson@gu.se"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-cci-annotator/",
    "home_github": null,
    "home_other": "None",
    "summary": "Plugin to simplify annotations of datasets",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "ultralytics",
      "shapely",
      "dask",
      "xlsxwriter",
      "openvino",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-cci-annotator\n\n[![License MIT](https://img.shields.io/pypi/l/napari-cci-annotator.svg?color=green)](https://github.com/xfolka/napari-cci-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cci-annotator.svg?color=green)](https://pypi.org/project/napari-cci-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cci-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/xfolka/napari-cci-annotator/workflows/tests/badge.svg)](https://github.com/xfolka/napari-cci-annotator/actions)\n[![codecov](https://codecov.io/gh/xfolka/napari-cci-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/xfolka/napari-cci-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cci-annotator)](https://napari-hub.org/plugins/napari-cci-annotator)\n\nPlugin to simplify annotations of datasets\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-cci-annotator` via [pip]:\n\n    pip install napari-cci-annotator\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-cci-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "CCI Annotator Plugin"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spotiflow",
    "name": "napari-spotiflow",
    "display_name": "napari-spotiflow",
    "version": "0.4.3",
    "created_at": "2024-02-02",
    "modified_at": "2025-06-16",
    "authors": [
      "Albert Dominguez Mantes",
      "Martin Weigert"
    ],
    "author_emails": [
      "Albert Dominguez Mantes <albert.dominguezmantes@epfl.ch>",
      "Martin Weigert <martin.weigert@epfl.ch>"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-spotiflow/",
    "home_github": "https://github.com/weigertlab/napari-spotiflow",
    "home_other": null,
    "summary": "Napari plugin for Spotiflow",
    "categories": [],
    "package_metadata_requires_python": "<3.13,>=3.9",
    "package_metadata_requires_dist": [
      "spotiflow",
      "npe2",
      "napari>=0.5"
    ],
    "package_metadata_description": "[![License: BSD-3](https://img.shields.io/badge/License-BSD3-blue.svg)](https://www.gnu.org/licenses/bsd3)\n[![PyPI](https://img.shields.io/pypi/v/napari-spotiflow.svg?color=green)](https://pypi.org/project/napari-spotiflow)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spotiflow.svg?color=green)](https://python.org)\n[![tests](https://github.com/weigertlab/napari-spotiflow/workflows/tests/badge.svg)](https://github.com/weigertlab/napari-spotiflow/actions)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-spotiflow)](https://pypistats.org/packages/napari-spotiflow)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spotiflow)](https://napari-hub.org/plugins/napari-spotiflow)\n\n![Logo](https://github.com/weigertlab/napari-spotiflow/raw/main/artwork/spotiflow_logo.png)\n---\n\n# Spotiflow: napari plugin\n\nNapari plugin for *Spotiflow*, a deep learning-based, threshold-agnostic, and subpixel-accurate spot detection method for 2D and 3D fluorescence microscopy images. The plugin allows using several pre-trained models as well as user-trained ones. For the main repository, see [here](https://github.com/weigertlab/spotiflow). \n\nhttps://github.com/weigertlab/napari-spotiflow/assets/11042162/02940480-daa9-4a21-8cf5-ad73c26c9838\n\nIf you use this plugin for your research, please [cite us](https://github.com/weigertlab/spotiflow#how-to-cite) as well as [napari](https://github.com/napari/napari?tab=readme-ov-file#citing-napari).\n\n----------------------------------\n\n## Installation\n\nThe plugin can be installed directly from PyPi (make sure you use a conda environment with `napari` and `spotiflow` installed):\n\n```\npip install napari-spotiflow\n```\n\n## Usage \n\n1. Open the image (or open one of our samples, _e.g._ `File > Open Sample > napari-spotiflow > HybISS`)\n2. Start the plugin `Plugins > napari-spotiflow`\n3. Select model (pre-trained or custom trained) and optionally adjust any other parameters\n4. Click `Detect spots`\n\n## Supported input formats\n- 2D (YX, YXC or CYX)\n- 2D+t (TYX, TYXC or TCYX)\n- 3D (ZYX, ZYXC or CZYX)\n- 3D+t (TZYX, TZYXC or TCZYX)\n\n## How to cite\nSee the [main repository's _How to cite_ section](https://github.com/weigertlab/spotiflow?tab=readme-ov-file#how-to-cite) as well as napari's [_citing napari_ section](https://github.com/napari/napari?tab=readme-ov-file#citing-napari).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Spotiflow widget"
    ],
    "contributions_sample_data": [
      "HybISS",
      "Terra",
      "Synthetic (3D)",
      "Telomeres (2D+t)"
    ]
  },
  {
    "normalized_name": "napari-timelapse-processor",
    "name": "napari-timelapse-processor",
    "display_name": "Timelapse Processor",
    "version": "0.1.1",
    "created_at": "2024-07-15",
    "modified_at": "2025-06-16",
    "authors": [
      "Johannes Soltwedel"
    ],
    "author_emails": [
      "johannes_richard.soltwedel@tu-dresden.de"
    ],
    "license": "Copyright (c) 2024, Johannes S...",
    "home_pypi": "https://pypi.org/project/napari-timelapse-processor/",
    "home_github": "https://github.com/jo-mueller/napari-timelapse-processor.git",
    "home_other": null,
    "summary": "meta plugin to ease processing timelapse image data",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "tqdm",
      "napari",
      "dask",
      "distributed",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-timelapse-processor\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-timelapse-processor.svg?color=green)](https://github.com/jo-mueller/napari-timelapse-processor/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-timelapse-processor.svg?color=green)](https://pypi.org/project/napari-timelapse-processor)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-timelapse-processor.svg?color=green)](https://python.org)\n[![tests](https://github.com/jo-mueller/napari-timelapse-processor/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-timelapse-processor/actions)\n[![codecov](https://codecov.io/gh/jo-mueller/napari-timelapse-processor/branch/main/graph/badge.svg)](https://codecov.io/gh/jo-mueller/napari-timelapse-processor)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-timelapse-processor)](https://napari-hub.org/plugins/napari-timelapse-processor)\n\nmeta plugin to ease processing timelapse image data\n\n## API\n\nThis plugin exposes two principal funcionalities:\n\n### TimelapseConverter\n\nThe `TimelapseConverter` class allows you to stack or unstack any of the supported napari layers from 4D data into a list of 3D layers or vice versa. Currently supported layers are:\n\n- `napari.layers.Image`\n- `napari.layers.Labels`\n- `napari.layers.Points`\n- `napari.layers.Vectors`\n- `napari.layers.Surface`\n\n`napari.layers.Tracks` are intrinsically 4D and thus not supported.\n\n**Unstacking example usage:**\n\n```python\nfrom napari_timelapse_processor import TimelapseConverter\nimport numpy as np\n\nimage_4d = np.random.rand(10, 32, 32, 32)  # 10 timepoints of 32x32x32 data\nconverter = TimelapseConverter()\nlist_of_images = converter.unstack(image_4d, layertype='napari.types.ImageData')\n```\n\n**Stacking example usage:**\n\n```python\nfrom napari_timelapse_processor import TimelapseConverter\nimport numpy as np\n\nrandom_points = [np.random.rand(10, 3)  for _ in range(10)]  # 10 timepoints of 10 random 3D points\nconverter = TimelapseConverter()\n\n# stack the points into a single 4D layer\nstacked_points = converter.stack(random_points, layertype='napari.types.PointsData')\n```\n\nThe `TimeLapseConverter` class also supports (un)stacking the `napari.layers.Layer` type (and its above-listed subclasses). Importantly, `features` that are associated with the respective layer are also (un)stacked.\n\n**Layer example usage**\n\n```python\nfrom napari_timelapse_processor import TimelapseConverter\nimport numpy as np\nfrom napari.layers import Points\nimport pandas as pd\n\nrandom_points = [np.random.rand(10, 3)  for _ in range(10)]  # 10 timepoints of 10 random 3D points\nrandom_features = [pd.DataFrame(np.random.rand(10)) for _ in range(10)]  # 10 timepoints of 10 random feature values\n\n# create a list of 10 Points layers\npoints = [Points(random_points[i], properties=random_features[i]) for i in range(10)]\n\nconverter = TimelapseConverter()\nstacked_points = converter.stack(points, layertype='napari.layers.Points')\n```\n\n## frame_by_frame\n\nThe frame-by-frame functionality provides a decorator that will inspect the decorated function for `TimelapseConverter`-compatible arguments and, if a 4D value is passed as argument, will automatically (un)stack the data before and after the function call. This allows for a more intuitive API when working with timelapse data. Currently supported type annotations are:\n\n- `napari.types.ImageData`\n- `napari.types.LabelsData`\n- `napari.types.PointsData`\n- `napari.types.VectorsData`\n- `napari.types.SurfaceData`\n- `napari.layers.Layer`\n- `napari.layers.Image`\n- `napari.layers.Labels`\n- `napari.layers.Points`\n- `napari.layers.Vectors`\n- `napari.layers.Surface`\n\nAdditionally, the `frame_by_frame` supports parallelization with [dask.distributed](https://distributed.dask.org/en/latest/). To use it, simply pass the `use_dask=True` argument to the decorated function, even if the function itself does not require this argument. The decorater will then automatically parallelize the function call over the time-axis and remove the `use_dask` argument when calling the function.\n\n**Example interactive code usage:** If you want to use the `frame_by_frame` functionality in, say, a Jupyter notebook, use it like this:\n\n```python\n\nfrom napari_timelapse_processor import frame_by_frame\nimport numpy as np\n\ndef my_function(image: 'napari.types.ImageData') -> 'napari.types.ImageData':\n    return 2 * image\n\nimage_4d = np.random.rand(10, 32, 32, 32)  # 10 timepoints of 32x32x32 data\n\nimage_4d_processed = frame_by_frame(my_function)(image_4d)  # without dask\nimage_4d_processed = frame_by_frame(my_function)(image_4d, use_dask=True)  # with dask\n```\n\n**Example napari code** If you want to use the `frame_by_frame` functionality in a napari plugin, use it like this:\n\n```python\nfrom napari_timelapse_processor import frame_by_frame\n\n@frame_by_frame\ndef my_function(image: 'napari.types.ImageData') -> 'napari.types.ImageData':\n    return 2 * image\n```\n\n**Hint:** The `frame_by_frame` functionality runs under the assumption that input napari-data (e.g., an Image, a Surface, Points, etc) are *always* arguments and any other parameters are *always* keyword arguments. If this is not the case, the decorator will not work as intended.\n\n```python\n\n# This works\nframe_by_frame(my_function)(image_4d, some_parameter=2, use_dask=True)\n\n# This does not work\nframe_by_frame(my_function)(image=image_4d, some_parameter=2, use_dask=True)\n```\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-timelapse-processor` via [pip]:\n\n    pip install napari-timelapse-processor\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-timelapse-processor\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "arcospx-napari",
    "name": "arcosPx-napari",
    "display_name": "arcosPx",
    "version": "0.1.3",
    "created_at": "2025-03-27",
    "modified_at": "2025-06-14",
    "authors": [
      "Benjamin Gr√§del"
    ],
    "author_emails": [
      "benjamin.graedel@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/arcospx-napari/",
    "home_github": "https://github.com/pertzlab/arcosPx-napari",
    "home_other": null,
    "summary": "A plugin to track spatio-temporal correlations in images",
    "categories": [
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "arcos4py>=0.3.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# arcosPx-napari\n\n[![License BSD-3](https://img.shields.io/pypi/l/arcosPx-napari.svg?color=green)](https://github.com/pertzlab/arcosPx-napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/arcosPx-napari.svg?color=green)](https://pypi.org/project/arcosPx-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/arcosPx-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/pertzlab/arcosPx-napari/workflows/tests/badge.svg)](https://github.com/pertzlab/arcosPx-napari/actions)\n[![codecov](https://codecov.io/gh/pertzlab/arcosPx-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/pertzlab/arcosPx-napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/arcosPx-napari)](https://napari-hub.org/plugins/arcosPx-napari)\n\n\n## Introduction\n\nThis repository contains a dedicated ARCOS.px plugin for the [napari](https://napari.org/stable/) image viewer. It tracks spatio-temporal correlations in images as described in a publication of Gr√§del et al. _Tracking Coordinated Cellular Dynamics in Time-Lapse Microscopy with ARCOS.px_ ([link](https://doi.org/10.1101/2025.03.14.643386)).\n\n<p align=\"center\">\n  <img alt=\"ARCOS.px logo\" src=\"misc/ARCOS-px-logo.png\" width=\"45%\">\n&nbsp; &nbsp; &nbsp; &nbsp;\n  <img alt=\"CDL logo\" src=\"misc/cellular-dynamics-lab-logo2.png\" width=\"45%\"> \n</p>\n\nARCOS.px is a computational method to identify and track clusters of correlated cell signaling in time-lapse microscopy images. \nIt is the latest addition to the [ARCOS ecosystem](https://arcos.gitbook.io/home) developed in the [Cellular Dynamics Lab](https://www.pertzlab.net) at the University of Bern.\n\n![ARCOS.px napari plugin screenshot](misc/napari-plugin.png)\n\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Example tracking\n\nActin polymerization waves in REF52 fibroblasts treated with 50 ng/mL PDGF, 24h before imaging.\n\n![Polymerisation wave in REF52 cells](misc/tracked_waves_rgb_wLabels_F1-181.gif)\n\n\n## Installation\n\nYou can install `arcosPx-napari` via [pip]:\n\n    pip install arcosPx-napari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/pertzlab/arcosPx-napari.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"arcosPx-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/pertzlab/arcosPx-napari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Remove Background",
      "Thresholding",
      "Track Events"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "brendan-beads",
    "name": "brendan-beads",
    "display_name": "Brendan Beads",
    "version": "0.2",
    "created_at": "2025-05-20",
    "modified_at": "2025-06-13",
    "authors": [
      "Anders Folkesson"
    ],
    "author_emails": [
      "anders.folkesson@gu.se"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/brendan-beads/",
    "home_github": "https://github.com/xfolka/brendan-beads",
    "home_other": null,
    "summary": "Calculating and visualizing distances of beads to surface",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "pyometiff",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "numpy; extra == \"testing\"",
      "pandas; extra == \"testing\"",
      "matplotlib; extra == \"testing\"",
      "pyometiff; extra == \"testing\""
    ],
    "package_metadata_description": "# brendan-beads\n\n[![License MIT](https://img.shields.io/pypi/l/brendan-beads.svg?color=green)](https://github.com/xfolka/brendan-beads/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/brendan-beads.svg?color=green)](https://pypi.org/project/brendan-beads)\n[![Python Version](https://img.shields.io/pypi/pyversions/brendan-beads.svg?color=green)](https://python.org)\n[![tests](https://github.com/xfolka/brendan-beads/workflows/tests/badge.svg)](https://github.com/xfolka/brendan-beads/actions)\n[![codecov](https://codecov.io/gh/xfolka/brendan-beads/branch/main/graph/badge.svg)](https://codecov.io/gh/xfolka/brendan-beads)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/brendan-beads)](https://napari-hub.org/plugins/brendan-beads)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nCalculating and visualizing distances of beads to surface\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `brendan-beads` via [pip]:\n\n    pip install brendan-beads\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/xfolka/brendan-beads.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"brendan-beads\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/xfolka/brendan-beads/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Main Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-stress",
    "name": "napari-stress",
    "display_name": "napari STRESS",
    "version": "0.4.2",
    "created_at": "2022-06-02",
    "modified_at": "2025-06-13",
    "authors": [
      "Johannes Soltwedel",
      "Ben J. Gross",
      "Elijah Shelton",
      "Carlos Gomez",
      "Otger Campas"
    ],
    "author_emails": [
      "johannes_richard.mueller@tu-dresden.de"
    ],
    "license": "Copyright (c) 2022, Johannes M...",
    "home_pypi": "https://pypi.org/project/napari-stress/",
    "home_github": "https://github.com/campaslab/napari-stress",
    "home_other": null,
    "summary": "Interactive surface analysis in napari for measuring mechanical stresses in biological tissues",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "dask",
      "distributed",
      "joblib",
      "mpmath",
      "napari",
      "napari-tools-menu>=0.1.15",
      "numpy<2.0.0",
      "pandas",
      "scikit-image",
      "scipy>=1.9.0",
      "seaborn",
      "tqdm",
      "napari-vedo-bridge>=0.2.2",
      "vedo>=2023.5.0",
      "vispy",
      "deprecation",
      "gdist",
      "pygeodesic",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "[![License](https://img.shields.io/pypi/l/napari-stress.svg?color=green)](https://github.com/campaslab/napari-stress/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-stress.svg?color=green)](https://pypi.org/project/napari-stress)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stress.svg?color=green)](https://python.org)\n[![tests](https://github.com/campaslab/napari-stress/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/campaslab/napari-stress/actions/workflows/test_and_deploy.yml)\n[![codecov](https://codecov.io/gh/campaslab/napari-stress/branch/main/graph/badge.svg?token=ZXQGREJAT9)](https://codecov.io/gh/campaslab/napari-stress)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-stress.svg)](https://pypistats.org/packages/napari-stress)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stress)](https://www.napari-hub.org/plugins/napari-stress)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6607329.svg)](https://doi.org/10.5281/zenodo.6607329)\n\n# napari-stress\n\nThis plugin provides tools for the analysis of surfaces in Napari, such as utilities to determine and refine the surface-representation of objects using a ray-casting approach and calculate the curvature of surfaces.\nIt re-implements code in Napari that was written for [Gross et al. (2021): STRESS, an automated geometrical characterization of deformable particles for in vivo measurements of cell and tissue mechanical stresses](https://www.biorxiv.org/content/10.1101/2021.03.26.437148v1)\nand has been made open source in [this repository](https://github.com/campaslab/STRESS).\n\n![](https://github.com/campaslab/napari-stress/raw/main/docs/imgs/function_gifs/spherical_harmonics.gif)\n\n## Usage\n\nFor documentation on how to use napari-stress both interactively from the napari-viewer or from code, please visit the [**documentation**](https://campaslab.github.io/napari-stress/intro.html)\n\n\n## Installation\n\nCreate a new conda environment with the following command.\nIf you have never used conda before, please [read this guide first](https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html).\n\n```\nconda create -n napari-stress Python=3.9 napari jupyterlab -c conda-forge\nconda activate napari-stress\n```\n\nYou can then install napari-stress using pip:\n\n```\npip install napari-stress\n```\n\n## Issues\n\nTo report bugs, request new features or get in touch, please [open an issue](https://github.com/campaslab/napari-stress/issues) or tag `@EL_Pollo_Diablo` on [image.sc](https://forum.image.sc/).\n\n## See also\n\nThere are other napari plugins with similar / overlapping functionality\n\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\n* [napari-pymeshlab](https://www.napari-hub.org/plugins/napari-pymeshlab)\n* [napari-process-points-and-surfaces](https://www.napari-hub.org/plugins/napari-process-points-and-surfaces)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [pytest], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-stress\" is free and open source software\n\n## Acknowledgements\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany‚Äôs Excellence Strategy ‚Äì EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[pytest]: https://docs.pytest.org/en/7.0.x/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Rescale image data",
      "Droplet reconstruction toolbox",
      "Fit spherical harmonics",
      "Lebedev quadrature",
      "Measure curvature",
      "Measure Gauss-Bonnet error",
      "Measure tissue and cell-scale stresses",
      "Stress analysis toolbox",
      "Mean curvature on ellipsoid",
      "Measure intensity along normals",
      "Measure intensity on surface",
      "Calculate patch-fitted curvature on surface",
      "Trace-refine surface points",
      "Create surface from points",
      "Create points from surface vertices",
      "Fit ellipsoid to pointcloud (points)",
      "Fit ellipsoid to pointcloud (major axis)",
      "Fit least squares ellipsoid",
      "Expand points on ellipsoid",
      "Pairwise point distance",
      "Calculate normal vectors on pointcloud",
      "Calculate normal vectors on surface",
      "Move points along vector (relative)",
      "Move points along vector (absolute)"
    ],
    "contributions_sample_data": [
      "Droplet pointcloud",
      "4d Droplet pointcloud",
      "4d Droplet image"
    ]
  },
  {
    "normalized_name": "napari-tapenade-processing",
    "name": "napari-tapenade-processing",
    "display_name": "Tapenade Processing",
    "version": "0.0.14",
    "created_at": "2024-08-13",
    "modified_at": "2025-06-13",
    "authors": [
      "Jules Vanaret"
    ],
    "author_emails": [
      "jules.vanaret@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tapenade-processing/",
    "home_github": "https://github.com/jules-vanaret/napari-tapenade-processing",
    "home_other": null,
    "summary": "A visual pipeline to process images with Tapenade in Napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tifffile",
      "natsort",
      "tapenade>=0.0.18",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# :herb: napari-tapenade-processing\n\n[![License MIT](https://img.shields.io/pypi/l/napari-tapenade-processing.svg?color=green)](https://github.com/jules-vanaret/napari-tapenade-processing/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tapenade-processing.svg?color=green)](https://pypi.org/project/napari-tapenade-processing)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tapenade-processing.svg?color=green)](https://python.org)\n[![tests](https://github.com/jules-vanaret/napari-tapenade-processing/workflows/tests/badge.svg)](https://github.com/jules-vanaret/napari-tapenade-processing/actions)\n[![codecov](https://codecov.io/gh/jules-vanaret/napari-tapenade-processing/branch/main/graph/badge.svg)](https://codecov.io/gh/jules-vanaret/napari-tapenade-processing)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tapenade-processing)](https://napari-hub.org/plugins/napari-tapenade-processing)\n\n<img src=\"https://github.com/GuignardLab/tapenade/blob/main/imgs/tapenade3.png\" width=\"100\">\n\nA collection of methods to process images of deep 3D/3D+time tissues in Napari.\n\n`napari-tapenade-processing` is a [napari] plugin that is part of the [Tapenade](https://github.com/GuignardLab/tapenade) project. Tapenade is a tool for the analysis of dense 3D tissues acquired with deep imaging microscopy. It is designed to be user-friendly and to provide a comprehensive analysis of the data.\n\nIf you use this plugin for your research, please [cite us](https://github.com/GuignardLab/tapenade/blob/main/README.md#how-to-cite).\n\n## Overview\n\n<img src=\"imgs/napari_preproc_demo.gif\"/>\n\nWhile working with large and dense 3D and 3D+time gastruloid datasets, we found that being able to visualise and interact with the data dynamically greatly helped processing it.\nDuring the pre-processing stage, dynamical exploration and interaction led to faster tuning of the parameters by allowing direct visual feedback, and gave key biophysical insight during the analysis stage.\n\nFrom a given set of raw images, segmented object instances, and object mask, the plugin allows the user to quickly run all pre-processing functions from our main pipeline with custom parameters while being able to see and interact with the result of each step. For large datasets that are cumbersome to manipulate or cannot be loaded in Napari, the plugin provides a macro recording feature: the users can experiment and design their own pipeline on a smaller subset of the dataset, then run it on the full dataset without having to load it in Napari.\n\n<img src=\"imgs/Fig_Napari_preprocessing.png\">\n\n## Installation\n\nThe plugin obviously requires [napari] to run. If you don't have it yet, follow the instructions [here](https://napari.org/stable/tutorials/fundamentals/installation.html).\n\nThe simplest way to install `napari-tapenade-processing` is via the [napari] plugin manager. Open Napari, go to `Plugins > Install/Uninstall Packages...` and search for `napari-tapenade-processing`. Click on the install button and you are ready to go!\n\nYou can also install `napari-tapenade-processing` via [pip]:\n\n    pip install napari-tapenade-processing\n\nTo install latest development version :\n\n    pip install git+https://github.com/jules-vanaret/napari-tapenade-processing.git\n\n## Usage\n\n### General overview of the plugin within Napari\n\n<img src=\"imgs/proc_0.png\">\n\nTo start a pre-processing pipeline, follow these steps:\n\n1. First, load your images in Napari. You can drag and drop them from your file explorer to the Napari viewer, or open them using the `File > Open files...` menu.\n2. Click on the `Plugins > Tapenade Processing` menu to open the plugin.\n3. The image you have loaded will be displayed as individual layers in the Layer List. They can be clicked-on to reveal a set of visual parameters (see 4) that can be adjusted. By double-clicking on a layer name, you can change it. Right-clicking a layer will give you several options. The little eye icon next to the layer name can be clicked to hide the layer.\n4. You can adjust visual parameters for each layer, like the contrast limits, the colormap, the opacity, the blending mode, etc.\n5. If you want to switch between 2D and 3D views, click on the `Toggle 2D/3D view` button (it resembles a square when in 2D mode, or a cube when in 3D mode).\n6. You can toggle the grid view (as shown in the example image) by clicking on the `Toggle grid mode` button. By right-clicking the button, you can parametrize the grid view (e.g number of columns, number of rows, etc).\n7. The plugin is composed of three tabs. The first tab is dedicated to pre-processing functions, the second tab is dedicated to the macro recording feature, and the third tab is dedicated to advanced parameters.\n\n### Tab 1: The pre-processing functions\n\n<img src=\"imgs/proc_1.png\" width=300>\n\nThe pre-processing tab is composed of the following elements:\n\n1. A combo box to select the pre-processing function to apply from a list.\n2. A set of comboxes that allow you to select the layers to apply the function on. If a function does not require a specific layer, the combo box will be greyed out. `Image` layers correspond to integer or float data, `Labels` layers correspond to integer data and represent segmented object instances, `Mask` layers correspond to boolean data and usually represent the sample's large scale mask (inside/outside). All layers must have data of the same shape (same number of dimensions and same dimensions). Layers can be 3D or 3D+time, respectively with the ZYX or TZYX order.\n(2') If a layer does not appear in a combo box, but is present in the Layer List, you can click on the `Refresh` button to update the list of layers.\n3. A set of parameters that you can tune to adjust the function's behaviour. The parameters are specific to each function. In case of doubt, you can click on the little `[?]` button next to the widget to get a tooltip with a description.\n4. A `Run function` button to apply the function with the current parameters to the previously selected layers.\n\n\n### Tab 2: The macro recording feature\n\n#### A. Recording a macro\n\n<img src=\"imgs/proc_macro_1.png\" width=300>\n\nTo record a macro, click on the `Macro recording` tab and follow these steps:\n\n1. Click on `Choose directory` to select a folder where the macro file will be saved.\n2. Click on `Start recording macro` to start recording the functions you will apply. At this point, you can start applying sequences of functions to images/segementations/masks that you have already loaded in Napari or that you load in the middle of the recording. \n\n<img src=\"imgs/proc_macro_2.png\" width=300>\n\n3. When you are finished, click `Stop recording and save macro`. It will be saved in the JSON (`.json`) format, and the name will follow the pattern `recorder_parameters_YYYY-MM-DD_HH-MM-SS.json`.\n\n#### B. Running a macro\n\nMacros allow you to run a sequence of functions in batch on folders of input TIFF images (either different frames of the same 3D+t image, or several 3D images). The input images should be in the same folder, and the output will be saved in a folder of your choice. The output of each function will be saved in a separate folder, and the name of the folder will be linked to the name of the function.\n\n<img src=\"imgs/proc_macro_3.png\" width=300>\n\nTo run a macro, click on the `Macro recording` tab and follow these steps:\n\n1. Click on `Select file` to choose the macro file you want to run.\n2. After specifying the path to the macro file, several path entries with names like `Path to folder ([...]) N` (e.g `Path to folder (['Image'] 1`) will appear. Click on the `Choose directory` button to select the folder where the input images (TIFF files) are located.\n3. Click on `Choose directory` under `Path to save outputs folders of tifs` to select the folder where the results of the pipeline will be saved. Each function call will generate a folder whose name will be linked to the name of the function.\n4. You can click the `Compress when saving` checkbox to save the output TIFF images in a compressed format using ZLIB compression. \n5. Choose the number of workers to use for parallel processing. The default value is 1, which means that the functions will be run sequentially on the images. If you have a multi-core CPU, you can increase this value to speed up the processing. Be careful that setting this value too high can lead to memory issues.\n6. Click on `Run macro` to start the processing. You will see as many folders as there are steps in your pipeline, containing the results on each frame.\n\n\n### Tab 3: Advanced parameters\n\n<img src=\"imgs/proc_2.png\" width=300>\n\nThe advanced parameters tab is composed of the following elements:\n\n1. A checkbox `New layers overwrite previous ones`: whether the output of the pre-processing functions should be saved as new layers or overwrite the previous ones that were used as input. This can be useful to save memory when you don't need to compare the input and output of a function.\n\n## Demo dataset\n\nA demo dataset is available [here](https://amubox.univ-amu.fr/s/MRdFy3KqQNjpyHa).\n\n### Content\n\nThis test dataset is composed of a folder `folder_raw_data` which contains 5 separate frames (3D images), and a macro Json file `recorder_parameters.json`.  \n\n### How to use\n\n - Download the folder `folder_raw_data`. \n - Load one of the image from the folder (either drag and drop, or `File>Open file(s)`) and start creating your own pipeline.\n - To try batch processing through the macro feature, click on the `Macro recording tab`, choose a path to save the macro Json file, click on `Start recording macro`, and perform a sequence of function runs of your choice. When you are finished, click `Stop recording and save macro`. Then specify the path to your macro file below (alternatively, a valid Json file is also made available), the folder where the rest of the frames are located, and the folder where the results of the pipeline will be saved. Click on run macro. You should see as many folders as there are steps in your pipeline, containing the results on each frame.\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-tapenade-processing\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jules-vanaret/napari-tapenade-processing/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Tapenade Processing"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-vedo-bridge",
    "name": "napari-vedo-bridge",
    "display_name": "napari vedo bridge",
    "version": "0.3.0",
    "created_at": "2023-07-03",
    "modified_at": "2025-06-13",
    "authors": [
      "Johannes Soltwedel",
      "Marco Musy"
    ],
    "author_emails": [
      "johannes_richard.soltwedel@tu-dresden.de"
    ],
    "license": "Copyright (c) 2023, Johannes S...",
    "home_pypi": "https://pypi.org/project/napari-vedo-bridge/",
    "home_github": "https://github.com/jo-mueller/napari-vedo-bridge",
    "home_other": null,
    "summary": "Transfer mesh data between napari and vedo for interactive processing",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "vedo>=2024.5.2",
      "napari-timelapse-processor",
      "napari",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-vedo-bridge\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vedo-bridge.svg?color=green)](https://github.com/jo-mueller/napari-vedo-bridge/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-vedo-bridge.svg?color=green)](https://pypi.org/project/napari-vedo-bridge)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vedo-bridge.svg?color=green)](https://python.org)\n[![tests](https://github.com/jo-mueller/napari-vedo-bridge/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-vedo-bridge/actions)\n[![codecov](https://codecov.io/gh/jo-mueller/napari-vedo-bridge/branch/main/graph/badge.svg)](https://codecov.io/gh/jo-mueller/napari-vedo-bridge)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vedo-bridge)](https://napari-hub.org/plugins/napari-vedo-bridge)\n\nTo be able to use interactive processing of meshes in napari, this plugin provides a bridge to the vedo library. It allows to transfer meshes between napari and vedo and to use the interactive processing capabilities of vedo in napari. \n\n## I/O\n\nThe plugin allows to export and import meshes and point layers. The following are supported:\n\n| napari layer type | File Format | Import | Export | Features |\n|:------------------:|:-----------:|:------:|:------:|:--------:|\n| Surface | .vtp | ‚úì | ‚úì |  ‚úì |\n| Surface | .vtk | ‚úì | ‚úì |  ‚úó |\n| Surface | .obj | ‚úì | ‚úì |  ‚úó |\n| Surface | .stl | ‚úì | ‚úì |  ‚úó |\n| Surface | .ply | ‚úì | ‚úì |  ‚úó |\n| Points | .vtp | ‚úì | ‚úì |  ‚úì |\n| Points | .vtk | ‚úì | ‚úì |  ‚úó |\n| Points | .ply | ‚úì | ‚úì |  ‚úó |\n| Points | .obj | ‚úì | ‚úì |  ‚úó |\n\n## Interactive mesh cutting\nTo interactively cut meshes in the napari-vedo MeshCutter, install the plugin (see below) and open the plugin it from the napari plugins menu (`Plugins > Mesh Cutter (napari-vedo-bridge)`). \n\nTo cut meshes you can use the following cutters:\n- `PlaneCutter`: cuts a mesh with a plane\n- `SphereCutter`: cuts a mesh with a sphere\n- `BoxCutter`: cuts a mesh with a box\n\n![](https://github.com/jo-mueller/napari-vedo-bridge/raw/main/docs/imgs/screenshot_box_cutter.png)\n\nTo send and get data into and from the plugin, you can:\n\n- Retrieve the current mesh from napari (click `Retrieve mesh from napari`) - this imports the **currently selected mesh layer** from napari\n- Load a mesh from file (click `Load mesh`)\n- Send a mesh to napari (click `Send back to napari`) - this creates a new mesh layer in napari\n\n## Mesh Processing Functions\n\nThe plugin also provides a set of mesh processing functions that can be used in napari. These functions are wrapped from the vedo library and provide various mesh processing capabilities. The following functions are available:\n\n- `compute_normals`: Compute normals for the given mesh.\n- `shrink`: Shrink the given mesh.\n- `join`: Join the given meshes.\n- `subdivide`: Subdivide the given mesh.\n- `decimate`: Decimate the given mesh.\n- `decimate_pro`: Decimate the given mesh using the Pro algorithm.\n- `decimate_binned`: Decimate the given mesh using the Binned algorithm.\n- `smooth`: Smooth the given mesh.\n- `fill_holes`: Fill holes in the given mesh.\n- `inside_points`: Get the points inside the given mesh.\n- `extrude`: Extrude the given mesh.\n- `split`: Split the given mesh into connected components.\n- `extract_largest_region`: Extract the largest region from the given mesh.\n- `binarize`: Binarize the given mesh.\n\n## Pointcloud Processing Functions\n\nThe plugin also provides a set of pointcloud processing functions that can be used in napari. These functions are wrapped from the vedo library and provide various pointcloud processing capabilities. The following functions are available:\n\n- `smooth_points`: Smooth the given points.\n- `decimate_points`: Decimate the given points.\n- `cluster_points`: Cluster the given points.\n- `remove_outliers`: Remove outliers from the given points.\n- `compute_normals_points`: Compute normals for the given points.\n- `extract_largest_cluster`: Extract the largest cluster from the given points.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-vedo-bridge` via [pip]:\n\n    pip install napari-vedo-bridge\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-vedo-bridge\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.ply",
      "*.vtp",
      "*.obj",
      "*.stl",
      "*.vtk"
    ],
    "contributions_writers_filename_extensions": [
      ".vtk",
      ".stl",
      ".vtp",
      ".obj",
      ".ply"
    ],
    "contributions_widgets": [
      "Mesh cutter",
      "Vedo mesh viewer",
      "Compute normals",
      "Shrink mesh",
      "Subdivide mesh",
      "Decimate mesh",
      "Decimate mesh (Pro algorithm)",
      "Decimate mesh (Binned algorithm)",
      "Smooth mesh",
      "Fill holes",
      "Split mesh",
      "Extract largest region",
      "Binarize mesh",
      "Smooth points (moving least-squares 1D)",
      "Smooth points (moving least-squares 2D)",
      "Remove outliers"
    ],
    "contributions_sample_data": [
      "Mouse limb 1",
      "Mouse limb 2",
      "Mouse limb 3",
      "Beethoven",
      "Bunny",
      "Cow",
      "Apple",
      "Panther"
    ]
  },
  {
    "normalized_name": "napari-file2folder",
    "name": "napari-file2folder",
    "display_name": "Save multidimensional file as folder of tifs",
    "version": "0.0.4",
    "created_at": "2024-10-28",
    "modified_at": "2025-06-12",
    "authors": [
      "Jules Vanaret"
    ],
    "author_emails": [
      "jules.vanaret@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-file2folder/",
    "home_github": null,
    "home_other": "None",
    "summary": "Save multidimensional file as folder of tifs",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "magicgui",
      "tifffile",
      "bioio",
      "bioio-ome-tiff",
      "bioio-ome-zarr",
      "bioio-nd2",
      "bioio-czi",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-file2folder\n\n[![License MIT](https://img.shields.io/pypi/l/napari-file2folder.svg?color=green)](https://github.com/GuignardLab/napari-file2folder/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-file2folder.svg?color=green)](https://pypi.org/project/napari-file2folder)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-file2folder.svg?color=green)](https://python.org)\n[![tests](https://github.com/jules-vanaret/napari-file2folder/workflows/tests/badge.svg)](https://github.com/jules-vanaret/napari-file2folder/actions)\n[![codecov](https://codecov.io/gh/jules-vanaret/napari-file2folder/branch/main/graph/badge.svg)](https://codecov.io/gh/jules-vanaret/napari-file2folder)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-file2folder)](https://napari-hub.org/plugins/napari-file2folder)\n\n<img src=\"https://github.com/GuignardLab/tapenade/blob/main/imgs/tapenade3.png\" width=\"100\">\n\nA plugin to inspect bioimages (e.g. .tif, .czi, .nd2, .lsm...) and save them as individual .tif files in a folder.\n\n`napari-file2folder` is a [napari] plugin that is part of the [Tapenade](https://github.com/GuignardLab/tapenade) project. Tapenade is a tool for the analysis of dense 3D tissues acquired with deep imaging microscopy. It is designed to be user-friendly and to provide a comprehensive analysis of the data.\n\nIf you use this plugin for your research, please [cite us](https://github.com/GuignardLab/tapenade/blob/main/README.md#how-to-cite).\n\n## Overview\n\n<img src=\"imgs/napari-file2folder-demo.gif\"/>\n\nThis plugin allows you to inspect (possibly large) bioimages by displaying their shape (number of elements in each dimension), and allowing you to save each element along a chosen dimension as a separate .tif file in a folder. This is useful when you have a large movie or stack of images and you want to save each frame or slice as a separate file. Optionally, the plugin allows the user to visualize the middle element of a given dimension to help the user decide which dimension to save as separate files.\n\nThe plugin currently supports the following file formats:\n- .tif\n- .ome.tiff\n- .zarr\n- .ome.zarr\n- .nd2\n- .lsm\n- .czi\n\nThis plugin leverages [tifffile], [bioio], and [zarr] to circumvent loading the entire images in memory, which allows inspection of very large images.\n\n> [!CAUTION]\n> When inspecting the middle element of a dimension, or when saving one element of a dimension as a separate file, the plugin loads the element in memory, which means that at least this lone element must fit in memory.\n\n## Installation\n\nThe plugin obviously requires [napari] to run. If you don't have it yet, follow the instructions [here](https://napari.org/stable/tutorials/fundamentals/installation.html).\n\nThe simplest way to install `napari-file2folder` is via the [napari] plugin manager. Open Napari, go to `Plugins > Install/Uninstall Packages...` and search for `napari-file2folder`. Click on the install button and you are ready to go!\n\nYou can install `napari-file2folder` via [pip]:\n\n    pip install napari-file2folder\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-file2folder\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[tifffile]: https://github.com/cgohlke/tifffile\n[bioio]: https://github.com/bioio-devs/bioio\n[zarr]: https://github.com/zarr-developers/zarr-python\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Save multidimensional file as folder of tifs"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-memmap-tiff",
    "name": "napari-memmap-tiff",
    "display_name": "Loading tiffs using memory map",
    "version": "1.1.0",
    "created_at": "2025-06-10",
    "modified_at": "2025-06-11",
    "authors": [
      "Matthew Einhorn"
    ],
    "author_emails": [
      "matt@einhorn.dev"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-memmap-tiff/",
    "home_github": "https://github.com/matham/napari-memmap-tiff",
    "home_other": null,
    "summary": "When installed and enabled in the options, it adds an option that when enabled will make napari load tiffs via memory mapping instead of fully into RAM.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tifffile",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-memmap-tiff\n\n[![License MIT](https://img.shields.io/pypi/l/napari-memmap-tiff.svg?color=green)](https://github.com/matham/napari-memmap-tiff/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-memmap-tiff.svg?color=green)](https://pypi.org/project/napari-memmap-tiff)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-memmap-tiff.svg?color=green)](https://python.org)\n[![tests](https://github.com/matham/napari-memmap-tiff/workflows/tests/badge.svg)](https://github.com/matham/napari-memmap-tiff/actions)\n[![codecov](https://codecov.io/gh/matham/napari-memmap-tiff/branch/main/graph/badge.svg)](https://codecov.io/gh/matham/napari-memmap-tiff)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-memmap-tiff)](https://napari-hub.org/plugins/napari-memmap-tiff)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nWhen installed and enabled in the options, it adds an option that when enabled\nwill make napari load tiffs via memory mapping instead of fully into RAM.\n\nThat is, `.tif` and `.tiff` files will be loaded into memory using memory\nmapping, which loads the data directly from disk instead of loading the file\nat once into RAM. This is beneficial for large files that may not fit into\navailable RAM.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-memmap-tiff` via [pip]:\n\n    pip install napari-memmap-tiff\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/matham/napari-memmap-tiff.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-memmap-tiff\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/matham/napari-memmap-tiff/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Enable / disable memory mapping tiffs"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-toska",
    "name": "napari-toska",
    "display_name": "Napari Topological Skeleton Analysis",
    "version": "0.2.2",
    "created_at": "2024-07-08",
    "modified_at": "2025-06-11",
    "authors": [
      "Allyson Quinn Ryan",
      "Johannes Soltwedel"
    ],
    "author_emails": [
      "Allyson Quinn Ryan <allyson_quinn.ryan@tu-dresden.de>",
      "Johannes Soltwedel <johannes_richard.soltwedel@tu-dresden.de>"
    ],
    "license": "Copyright (c) 2023, Allyson Qu...",
    "home_pypi": "https://pypi.org/project/napari-toska/",
    "home_github": "https://github.com/allysonryan/napari-toska",
    "home_other": null,
    "summary": "Extracts and analyses topological skeletons as undirected graphs",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image>=0.21.0",
      "napari-skimage-regionprops",
      "networkx",
      "scipy",
      "magicgui>=0.4.0",
      "tqdm>=4.65.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-toska\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-toska.svg?color=green)](https://github.com/allysonryan/napari-toska/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-toska.svg?color=green)](https://pypi.org/project/napari-toska)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-toska.svg?color=green)](https://python.org)\n[![tests](https://github.com/allysonryan/napari-toska/workflows/tests/badge.svg)](https://github.com/allysonryan/napari-toska/actions)\n[![codecov](https://codecov.io/gh/allysonryan/napari-toska/branch/main/graph/badge.svg)](https://codecov.io/gh/allysonryan/napari-toska)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-toska)](https://napari-hub.org/plugins/napari-toska)\n\nExtracts and analyses topological skeletons as undirected graphs. For usage instructions and API reference, please refer to the documentation:\n\n## [Documentation](https://allysonryan.github.io/napari-toska/).\n\n![](https://github.com/allysonryan/napari-toska/raw/main/docs/imgs/3d_skeleton_analysis.gif)\n\nThe functionality of the plugin comprises the following:\n\n- Extracting the topological skeleton of a binary image using the medial axis transform.\n- Extracting the netowrk of the skeleton as an undirected `networkx` graph.\n- Computing features of individual skeleton components as well as the entire skeleton network.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-toska` via [pip]:\n\n    pip install napari-toska\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-toska\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Skeleton analysis",
      "Make labeled skeleton image",
      "Label branches of single skeleton",
      "Parse single skeletons",
      "Parse all skeletons",
      "Analyze whole skeletons",
      "Measure branch lengths"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-afmreader",
    "name": "napari-afmreader",
    "display_name": "AFMReader",
    "version": "0.0.1",
    "created_at": "2025-06-07",
    "modified_at": "2025-06-07",
    "authors": [
      "TopoStats Team",
      "Max Gamill"
    ],
    "author_emails": [
      "TopoStats Team <topostats@sheffield.ac.uk>",
      "Max Gamill <mcgamill1@sheffield.ac.uk>"
    ],
    "license": "GNU GPLv3 only",
    "home_pypi": "https://pypi.org/project/napari-afmreader/",
    "home_github": "https://github.com/AFM-SPM/napari-afmreader",
    "home_other": null,
    "summary": "Napari plugin using AFMReader to load various Atomic Force Microscopy Images.",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "afmreader",
      "magicgui",
      "napari[all]",
      "qtpy",
      "tox",
      "pytest; extra == \"tests\"",
      "pytest-cov; extra == \"tests\"",
      "pytest-qt; extra == \"tests\"",
      "qtpy; extra == \"tests\"",
      "black; extra == \"dev\"",
      "codespell; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pylint; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "build; extra == \"pypi\"",
      "setuptools_scm[toml]; extra == \"pypi\"",
      "wheel; extra == \"pypi\""
    ],
    "package_metadata_description": "# napari-AFMReader\n\n<div align=\"center\">\n\n[![PyPI version](https://badge.fury.io/py/napari-afmreader.svg)](https://badge.fury.io/py/napari-afmreader)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/napari-afmreader)\n[![Code style:\nRuff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Code style: flake8](https://img.shields.io/badge/code%20style-flake8-456789.svg)](https://github.com/psf/flake8)\n[![codecov](https://codecov.io/gh/AFM-SPM/napari-afmreader/branch/dev/graph/badge.svg)](https://codecov.io/gh/AFM-SPM/napari-afmreader)\n[![pre-commit.ci\nstatus](https://results.pre-commit.ci/badge/github/AFM-SPM/napari-afmreader/main.svg)](https://results.pre-commit.ci/latest/github/AFM-SPM/napari-afmreader/main)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B-yellow)](https://fair-software.eu)\n\n</div>\n<div align=\"center\">\n\n[![Downloads](https://static.pepy.tech/badge/napari-afmreader)](https://pepy.tech/project/napari-afmreader)\n[![Downloads](https://static.pepy.tech/badge/napari-afmreader/month)](https://pepy.tech/project/napari-afmreader)\n[![Downloads](https://static.pepy.tech/badge/napari-afmreader/week)](https://pepy.tech/project/napari-afmreader)\n\n</div>\n<div align=\"center\">\n\n| [Installation](#installation) | [Usage](#usage) | [Licence](#licence) | [Citation](#citation) |\n\n</div>\n\nA [Napari](https://napari.org/) plugin to read in Atomic Force Microscopy (AFM) files using\n[AFMReader](https://github.com/AFM-SPM/AFMReader.git).\n\nYou can drag and drop your favourite AFM image files directly into the Napari viewer to use the awesome tools the image\nanalysis community have developed over at the [Napari Hub](https://www.napari-hub.org/) to analyse your images using\nopen-source software and a GUI!\n\n| File Extension | Supported by AFMReader | Description              |\n| -------------- | ---------------------- | ------------------------ |\n| `.asd`         | ‚úÖ                     | High-speed AFM format.   |\n| `.gwy`         | ‚úÖ                     | Gwyddion saved format.   |\n| `.ibw`         | ‚úÖ                     | Igor binary-wave format. |\n| `.jpk`         | ‚úÖ                     | JPK instruments format.  |\n| `.spm`         | ‚úÖ                     | Bruker spm format.       |\n| `.stp`         | ‚úÖ                     | Homemade stp format.     |\n| `.top`         | ‚úÖ                     | Homemade top format.     |\n| `.topostats`   | ‚úÖ                     | topostats output format. |\n\n## Installation\n\n### Via Napari-Hub\n\nThis software should be installable directly from Napari!\n\nAll you need to do is:\n\n1. [Install Napari](https://napari.org/stable/tutorials/fundamentals/installation.html) into an environment.\n2. Open Napari by typing `napari` into your command line with your Napari environment activated.\n\n   ```bash\n   napari\n   ```\n\n3. Go to `Plugins` > `Install/Uninstall Plugins`, and search for `napari-afmreader`.\n\n### Via Git\n\nOccasionally the Napari-Hub version of `napari-AFMReader` may not be the most up-to-date. This is when you might want\nto install both the most up-to-date `AFMReader` and `napari-AFMReader` versions via Git.\n\n`napari-AFMReader` has been designed to need minimal maintenance, with most of the new file type additions being solely\nadded to AFMReader.\n\n1. With [Git installed](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) on your machine, clone both the\n   `AFMReader` and `napari-AFMReader` repositories:\n\n   ```bash\n   git clone https://github.com/AFM-SPM/AFMReader.git\n   ```\n\n   ```bash\n   git clone https://github.com/AFM-SPM/napari-AFMReader.git\n   ```\n\n2. Activate your Python environment (e.g. Conda) and install the dependencies for each - make sure that the `AFMReader`\n   dependency is installed second to overwrite the possibly outdated `afmreader` package!\n\n   ```bash\n   cd napari-AFMReader\n   pip install .\n   cd ..\n   ```\n\n   ```bash\n   cd AFMReader\n   pip install .\n   ```\n\n3. Now when you open Napari via the `napari` command, it should use the latest version of `AFMReader`, and\n   `napari-AFMReader`.\n\n   ```bash\n   napari\n   ```\n\n## Usage\n\nThis package should be fairly straight-forward and intuitive to use, requiring you to:\n\n1. Drag and drop your supported AFM file into the Napari Viewer.\n\n2. Type in the name of the channel you would like to use. You may not need to specify a channel for e.g. `.stp`, or the\n   channel may refer to image key in the `.napari-afmreader` file.\\*.\n\n   \\*_Possible channel names will not appear at first due to the order in which AFMReader processes an image. Thus,\n   when provided with an non-existent channel name, the dialogue box will then return a list of possible channels to\n   choose from._\n\n## Licence\n\n**This software is licensed as specified by the [GPL License](COPYING) and [LGPL License](COPYING.LESSER).**\n\n## Citation\n\nPlease use the [Citation File Format](https://citation-file-format.github.io/) which is available in this repository.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.topostats",
      "*.gwy",
      "*.spm",
      "*.stp",
      "*.top",
      "*.ibw",
      "*.jpk",
      "*.asd"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nuclephaser",
    "name": "napari-nuclephaser",
    "display_name": "NuclePhaser",
    "version": "0.2.2",
    "created_at": "2025-04-02",
    "modified_at": "2025-06-07",
    "authors": [
      "Nikita Voloshin"
    ],
    "author_emails": [
      "nikita.voloshin.98@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-nuclephaser/",
    "home_github": "https://github.com/nikvo1/napari-nuclephaser",
    "home_other": null,
    "summary": "A Napari plugin to detect and count nuclei on phase contrast images",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "setuptools",
      "wheel",
      "napari",
      "ultralytics",
      "yolov5",
      "magicgui",
      "sahi",
      "scikit-image",
      "torch",
      "pathlib",
      "numpy",
      "typing",
      "pandas",
      "openpyxl",
      "seaborn",
      "opencv-python",
      "imagecodecs",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pytest-mock; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "opencv-python-headless; extra == \"testing\""
    ],
    "package_metadata_description": "# NuclePhaser: Cell Proliferation Measurement & Cell Tracking Assistant Plugin for Timelapse Images\n\n[![License MIT](https://img.shields.io/pypi/l/napari-nuclephaser.svg?color=green)](https://github.com/nikvo1/napari-nuclephaser/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nuclephaser.svg?color=green)](https://pypi.org/project/napari-nuclephaser)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nuclephaser.svg?color=green)](https://python.org)\n[![tests](https://github.com/nikvo1/napari-nuclephaser/workflows/tests/badge.svg)](https://github.com/nikvo1/napari-nuclephaser/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nuclephaser)](https://napari-hub.org/plugins/napari-nuclephaser)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA Napari plugin for automated cell nuclei detection, proliferation and population growth analysis, and single-cell tracking in brightfield and fluorescent nuclei timelapse microscopy images.\n\nnapari-nuclephaser is an open-source Napari plugin designed for scientists who need to measure cell proliferation rates, analyze population growth, and perform individual cell tracking on timelapse microscopy images. It utilizes [Ultralytics](https://docs.ultralytics.com/) YOLO object detection models and [obss/sahi](https://github.com/obss/sahi) sliced inference methods to detect cell nuclei on brightfield and fluorescent images of any size, including large whole slide ones. Learn more with [documentation](https://napari-nuclephaser.readthedocs.io/en/latest/index.html) and [paper](https://www.biorxiv.org/content/10.1101/2025.05.13.653705v1).\n\n## Nuclei detection\n\nWe trained a series of [YOLOv5](https://github.com/ultralytics/yolov5) and [YOLOv11](https://github.com/ultralytics/ultralytics) models to detect nuclei on phase contrast images. It can be used for counting cells or for individual cell tracking (using nuclei detections as tracking markers). Prominent features of this approach are:\n- Napari-nuclephaser plugin includes [obss/sahi](https://github.com/obss/sahi) functionality, allowing detection on images of arbitrary sizes.\n\n<p align=\"center\">\n\t<picture>\n\t  <source media=\"(prefers-color-scheme: dark)\" srcset=https://github.com/user-attachments/assets/aa321f17-b0e2-4161-8a69-cb732d7065a7 height=400>\n\t  <img alt=\"Image didn't load\" src=https://github.com/user-attachments/assets/fe4d6436-3490-4c06-8ddd-7c797976f407 height=400>\n\t</picture>\n</picture>\n\n- YOLO models are fast, providing reasonable inference speed even with CPU.\n- Ability to predict and automatically count nuclei on stacks of images, making it convenient for cell population growth studies and individual cell tracking.\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=https://github.com/user-attachments/assets/feba9a99-1d37-4962-a2e6-175052aa4925>\n  <img alt=\"Image didn't load\" src=\"https://github.com/user-attachments/assets/c7e4d0e6-44c1-4268-aae5-6bb78500d928\">\n</picture>\n\n- Calibration algorithm that allows measuring accuracy for each specific use case.\n\n## Calibration algorithm\n\nResult of object detection model inference is highly dependent on _confidence threshold_ parameter.\n\n<p align=\"center\">\n  <picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=https://github.com/user-attachments/assets/8a13085f-c7ea-45f0-8931-6851f21b68a0 height=\"300\">\n  <img alt=\"Image didn't load\" src=https://github.com/user-attachments/assets/89f76cd7-2db7-4241-bc35-36d23332b2b5 height=\"300\">\n  </picture>\n</p>\n\nWe created several calibration (finding optimal confidence threshold) algorithms that allow adjusting models to specific use cases (cell types, magnifications, illumination settings, cameras etc.):\n- Calibration using known number of objects on an image. Doesn't produce accuracy metrics.\n- Calibration using fluorescent nuclei stain image (for example, DAPI image). Produces accuracy metrics.\n- Calibration using manual annotation of nuclei. Produces accuracy metrics.\n\nApart from optimal confidence threshold search, these algorithms return accuracy metrics for specific use cases. Given that the calibration image is large, only part of it is used for search of threshold, while the second part is used for evaluation model's accuracy.\nAccuracy metrics are [Mean Absolute Percentage Error (MAPE)](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) and prediction-ground truth scatterplot, which shows how well model performs with different densities of cells.\n\nLearn more about calibration in [documentation](https://napari-nuclephaser.readthedocs.io/en/latest/Biological%20tasks%20guidelines/Individual%20cells%20tracking.html).\n\n<p align=\"center\">\n  <picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=https://github.com/user-attachments/assets/6d89e22b-2728-40fb-839d-3c6681e29c97>\n  <img alt=\"Image didn't load\" src=https://github.com/user-attachments/assets/6a574845-4ad2-4802-b0f8-f1d908aa585a>\n  </picture>\n</p>\n\n## Cell Proliferation Measurement & Population Growth Analysis\n\nWith NuclePhaser you can reconstruct population growth curves from timelapse images of growing cell population by counting number of nuclei on each image. Key features of this approach are:\n\n- No special equipment, reagents or dyes required, only regular culture plastic and cell growth medium, microscope with mechanical stage and a PC (even without GPU).\n- [Accuracy measurement for each specific use case](https://napari-nuclephaser.readthedocs.io/en/latest/General%20information/Confidence%20threshold%20calibration.html), so you will be sure the tool is working with appropriate precision.\n- Measuring the number of cells, not the area occupied by cells, which can be significantly influenced by spreading/narrowing of cells. \n- Complete reproducibility of results with metadata.txt files saved for each experiment.\n\n<p align=\"center\">\n  <picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=https://github.com/user-attachments/assets/47b6cee0-7f4a-440f-84ed-de2a5aa2aa36>\n  <img alt=\"Image didn't load\" src=https://github.com/user-attachments/assets/5a084f1a-f977-41fa-b4be-55f37bdf9996>\n  </picture>\n</p>\n\nFor more detailed information about how NuclePhaser can be used for cell proliferation measurement & population growth analysis, visit our [documentation](https://napari-nuclephaser.readthedocs.io/en/latest/Biological%20tasks%20guidelines/Population%20growth%20curves.html#).\n\n## Individual cell tracking\n\nNuclePhaser can be used as an assistant for individual cells tracking. This task is extremely difficult, and manual tracking is still the only method with 100% proof against false tracks. With NuclePhaser, you can significantly simplify manual tracking: instead of marking each cell on each image, you can predict nuclei location with NuclePhaser and then correct the result, which is **much** faster. Learn more in [documentation](https://napari-nuclephaser.readthedocs.io/en/latest/Biological%20tasks%20guidelines/Individual%20cells%20tracking.html).\n\n## Models\n\nCurrently only YOLOv5n, YOLOv5s, YOLOv11n and YOLOv11s models, as well as fluorescent nuclei detector YOLOv5n are downloaded automatically with pip install napari-nuclephaser. To use larger models, download them with these links:\n\n<div align=\"center\">\n\nFluorescent nuclei detectors\n| Model                    | Link |\n| :----------------------: | :-----: |\n| Fluorescence_v5n         | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v5n.pt?download=1) |\n| Fluorescence_v5s         | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v5s.pt?download=1) |\n| Fluorescence_v5m         | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v5m.pt?download=1) |\n| Fluorescence_v5l         | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v5l.pt?download=1) |\n| Fluorescence_v5x         | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v5x.pt?download=1) |\n| Fluorescence_v11n        | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v11n.pt?download=1)|\n| Fluorescence_v11s        | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v11s.pt?download=1)|\n| Fluorescence_v11m        | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v11m.pt?download=1)|\n| Fluorescence_v11l        | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v11l.pt?download=1)|\n| Fluorescence_v11x        | [Donwload](https://zenodo.org/records/15388030/files/Fluorescence_v11x.pt?download=1)|\n\nBrighfield nuclei detectors\n| Model                    | Link |\n| :----------------------: | :-----: |\n| Brightfield_v5n          | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v5n.pt?download=1)  |\n| Brightfield_v5s          | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v5s.pt?download=1)  |\n| Brightfield_v5m          | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v5m.pt?download=1)  |\n| Brightfield_v5l          | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v5l.pt?download=1)  |\n| Brightfield_v5x          | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v5x.pt?download=1)  |\n| Brightfield_v11n         | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v11n.pt?download=1) |\n| Brightfield_v11s         | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v11s.pt?download=1) |\n| Brightfield_v11m         | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v11m.pt?download=1) |\n| Brightfield_v11l         | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v11l.pt?download=1) |\n| Brightfield_v11x         | [Donwload](https://zenodo.org/records/15388030/files/Brightfield_v11x.pt?download=1) |\n\n</div>\n\n> [!NOTE]\n> Feel free to use the models published here without the plugin!\n\n# Plugin functionality\nnapari-nuclephaser plugin offers following widgets:\n- Widget for inference on single image. Result can be in the form of points or boxes with or without confidence scores. Automatically returns number of cells in the name of result layer.\n- Widget for inference on stack of images. Optionally can create .csv or .xlsx file at given location with counting results.\n- Widget for calibration using known number of cells.\n- Widget for calibration using fluorescent nuclei image (fluorescent nuclei detection model is used as a perfect predictor).\n- Widget for calibration using manual annotations.\n- Widget for transforming Napari Points layer into Labels layer, which allows turning detection in tracking algorithms-digestible form (in particular, [btrack](https://github.com/quantumjot/btrack)).\n- Widget for counting number of points in Points layer.\n\nLearn more about widgets and their functionality at [documentation](https://napari-nuclephaser.readthedocs.io/en/latest/index.html).\n\n## Citation\nIf you use NuclePhaser in your work, please cite our preprint:\n```bibtex\n@article {Voloshin2025.05.13.653705,\n\tauthor = {Voloshin, Nikita and Putlyaev, Egor and Chechekhina, Elizaveta and Usachev, Vladimir and Karagyaur, Maxim and Bozov, Kirill and Grigorieva, Olga and Tyurin-Kuzmin, Pyotr and Kulebyakin, Konstantin},\n\ttitle = {NuclePhaser: a YOLO-based framework for cell nuclei detection and counting in phase contrast images of arbitrary size with support of fast calibration and testing on specific use cases},\n\tyear = {2025},\n\tdoi = {10.1101/2025.05.13.653705},\n\tURL = {https://www.biorxiv.org/content/early/2025/05/16/2025.05.13.653705},\n\teprint = {https://www.biorxiv.org/content/early/2025/05/16/2025.05.13.653705.full.pdf},\n\tjournal = {bioRxiv}\n}\n```\n\n## Installation\n\nFor detailed installation instructions, visit our [documentation](https://napari-nuclephaser.readthedocs.io/en/latest/Installation/Installation.html).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-nuclephaser\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/nikvo1/napari-nuclephaser/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Predict on single image",
      "Predict on 1-stack",
      "Predict on 2-stack",
      "Calibrate with known number",
      "Calibrate with DAPI image",
      "Calibrate with points",
      "Convert points to labels",
      "Count points on single image"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "frontveg",
    "name": "frontveg",
    "display_name": "Frontveg",
    "version": "0.3.5",
    "created_at": "2025-04-20",
    "modified_at": "2025-06-04",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@univ-angers.fr"
    ],
    "license": "Copyright (c) 2025, Herearii M...",
    "home_pypi": "https://pypi.org/project/frontveg/",
    "home_github": "https://github.com/hereariim/frontveg",
    "home_other": null,
    "summary": "Segmentation of vegetation located to close to camera",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": "==3.11.12",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "transformers==4.51.3",
      "torch>=2.3.1",
      "torchvision>=0.18.1",
      "hydra-core==1.3.2",
      "iopath>=0.1.10",
      "pillow>=9.4.0",
      "sam2==1.1.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# frontveg\n\n[![License BSD-3](https://img.shields.io/pypi/l/frontveg.svg?color=green)](https://github.com/hereariim/frontveg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/frontveg.svg?color=green)](https://pypi.org/project/frontveg)\n[![Python Version](https://img.shields.io/pypi/pyversions/frontveg.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/frontveg/workflows/tests/badge.svg)](https://github.com/hereariim/frontveg/actions)\n[![codecov](https://codecov.io/gh/hereariim/frontveg/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/frontveg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/frontveg)](https://napari-hub.org/plugins/frontveg)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA plugin for foreground vegetation segmentation, tailored for trellised vegetation row images. It uses RGB images to perform inference and allows users to manually refine the generated mask.\n\n----------------------------------\n\nThe method was developped by Herearii Metuarea, PHENET PhD at LARIS (French laboratory located in Angers, France) and Abdoul Djalil Ousseini Hamza, AgroEcoPhen Engineer at IRHS (French Institute located in INRAe Angers, France) in Imhorphen team (bioimaging research group lead) under the supervision of Eric Duch√™ne (Research Engineer), Morgane Roth (Research Engineer) and David Rousseau (Full professor). This plugin was written by Herearii Metuarea and was designed in the context of the european project PHENET.\n\n![Data Warehouse](https://github.com/user-attachments/assets/4a110408-5854-4e8c-b655-4cb588434b79)\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `frontveg` via [pip]:\n\n    pip install frontveg\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/frontveg.git\n\nGPU is mandatory for time processing and models running (especially Grounding-DINO). Please visit the official PyTorch website to get the appropriate installation command: üëâ https://pytorch.org/get-started/locally\n\n**Exemple : GPU (CUDA 12.1)**\n\n    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n## Description\n\nThis plugin is a tool to perform image inference. This plugin contained two steps of image processing. First, from RGB image, a depth map is estimated and then thresholded based on the estimated depth histogram modes to detect foreground and background regions in image. Second, a Grounding DINO model detects foliage in the foreground. The output is a binary mask where white colour are associated to foliage in the foreground.\n\nThe plugin is applicable to images of trellised plants; in this configuration, it has been applied to images of pome fruit trees (apple), stone fruit trees (apricot) and climbing plants (grapevine).\n\n![sample_example](https://github.com/user-attachments/assets/ae845e01-9f48-4bcf-98ad-bf5f6e037f01)\n\n## Contact\n\nImhorphen team, bioimaging research group\n\n42 rue George Morel, Angers, France\n\n- Pr David Rousseau, david.rousseau@univ-angers.fr\n- Abdoul Djalil Ousseini Hamza, abdoul-djalil.ousseini-hamza@inrae.fr\n- Herearii Metuarea, herearii.metuarea@univ-angers.fr\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"frontveg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/hereariim/frontveg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Frontground vegetation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pitcount-cfim",
    "name": "napari-pitcount-cfim",
    "display_name": "napari-pitcount-cfim",
    "version": "1.0.0",
    "created_at": "2025-04-24",
    "modified_at": "2025-06-04",
    "authors": [
      "Markus L. Bille"
    ],
    "author_emails": [
      "github+markus@bille.dk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-pitcount-cfim/",
    "home_github": "https://github.com/MaxusTheOne/napari-pitcount-cfim",
    "home_other": null,
    "summary": "A pipeline for stuff #TODO: Get knowledge to write a proper description Pitcount",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "QtPy",
      "pydantic",
      "xmltodict",
      "napari-czi-reader",
      "aicsimageio",
      "aicspylibczi",
      "czifile",
      "matplotlib",
      "adjustText",
      "cellpose<4.0.0",
      "tensorflow",
      "joblib",
      "torch",
      "torchvision",
      "napari[all]; extra == \"napari\"",
      "pytest; extra == \"test\"",
      "pytest-cov; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "codecov; extra == \"test\"",
      "napari[all]; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "codecov; extra == \"dev\""
    ],
    "package_metadata_description": "# napari-pitcount-cfim\n\n## License\nBSD 3-Clause\n\n## About\nThis napari plugin was developed in partnership with CFIM (Centre for Microscopy and Image Analysis, Copenhagen University).\n\nThe plugin enables image analysis for microscopy, focused on identifying pits and segmenting cells, then generating detailed statistics. It is tailored for using `.czi` files and integrates well with the [`napari-czi-reader`](https://github.com/MaxusTheOne/napari-czi-reader).\n\nFor training the VGG19 2_2 √ó Random Forest Classifier used in this plugin, visit the [pitcount-ml-training](https://github.com/MaxusTheOne/pitcount-ml-training) repository.\n\n## Features\n- Detects pits in images using a trained `torchvision` model.\n- Performs cell segmentation via Cellpose (default model: `cyto3`).\n- Calculates and outputs statistics such as:\n  - Total cell count\n  - Total pit count\n  - Percentage of cells containing pits\n  - Average number of pits per cell\n\n## Usage\n\n### Graphical Mode (GUI)\nYou can launch the plugin in napari with:\n```bash\nnapari-pitcount-cfim --dev\n```\nor open napari and activate the plugin manually.\n## Headless Mode (NO GUI)\n```bash\nnapari-pitcount-cfim --no-gui \n```\nRun --help to list all options:\n```bash\nnapari-pitcount-cfim --no-gui -h\n```\n## Command-Line Arguments\n| Argument            | Alias | Type      | Description                                                                            |\n| ------------------- | ----- | --------- | -------------------------------------------------------------------------------------- |\n| `--no-gui`          |       | flag      | Runs the pipeline without GUI. Required for headless automation.                       |\n| `--dev`             |       | flag      | Launches napari in developer mode for plugin debugging.                                |\n| `--verbosity`       | `-v`  | int (0‚Äì2) | Sets the level of console output. Default: `0`.                                        |\n| `--input-folder`    | `-i`  | str       | Input directory for image data (required with `--no-gui`).                             |\n| `--output-folder`   | `-o`  | str       | Directory to save results. Default: `'output'`.                                        |\n| `--pit-mask-folder` | `-p`  | path      | If specified, skips pit prediction and uses this directory for pit masks.              |\n| `--save-raw-data`   |       | flag      | Saves raw, unprocessed data to the output folder (only in `--no-gui` mode).            |\n| `--family-grouping` |       | str       | Grouping method for output: `default`, `file`, `folder`, or `all`. Default: `default`. |\n\n## Notes\n- --input-folder must be used with --no-gui.\n\n- --pit-mask-folder must be a valid existing directory.\n\n- Set environment variables are used internally to control behavior.\n\n## Requirements\nNapari recommends installing napari seperately, as it is not included in this package. You can install it with:\n```bash\npip install napari[all]\n```\nOr you can just\n```bash\npip install napari-pitcount-cfim[napari]\n```\n\n## Known Issues\n- The plugin might not support the formats of most model output.\n- It's not possible to link masks directly to images in the GUI.\n- The default pit model, is a stub and mostly for decoration.\n\n\n\n\n\n\n\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Analyze pit count - CFIM"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "domb-napari",
    "name": "domb-napari",
    "display_name": "domb-napari",
    "version": "0.4.1",
    "created_at": "2024-03-01",
    "modified_at": "2025-06-03",
    "authors": [
      "Borys Olifirov"
    ],
    "author_emails": [
      "omnia.fatum@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/domb-napari/",
    "home_github": "https://github.com/wisstock/domb-napari",
    "home_other": null,
    "summary": "napari plugin for analyzing the redistribution of fluorescence-labeled proteins",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "domb",
      "dipy",
      "numba",
      "pybaselines"
    ],
    "package_metadata_description": "domb-napari\n===========\n\n[![Stand With Ukraine](https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner-direct-single.svg)](https://stand-with-ukraine.pp.ua)\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/domb-napari)](https://napari-hub.org/plugins/domb-napari)\n![PyPI - Version](https://img.shields.io/pypi/v/domb-napari)\n![PyPI - License](https://img.shields.io/pypi/l/domb-napari)\n[![DOI](https://zenodo.org/badge/722100876.svg)](https://doi.org/10.5281/zenodo.14843770)\n<!-- ![Website](https://img.shields.io/website?up_message=domb.bio%2Fnapari&up_color=%2323038C93&url=https%3A%2F%2Fdomb.bio%2Fnapari%2F) -->\n\n__napari Toolkit of Department of Molecular Biophysics <br /> Bogomoletz Institute of Physiology of NAS of Ukraine, Kyiv,  Ukraine__\n\nThis plugin offers widgets specifically designed to analyze the redistribution of fluorescence-labeled proteins in widefield epifluorescence time-lapse acquisitions. It is particularly useful for studying various phenomena, including:\n- Calcium-dependent translocation of neuronal calcium sensors.\n- Synaptic receptor traffic during long-term plasticity induction.\n- Membrane protein tracking.\n\n![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/translocation.gif)\n__Hippocalcin (neuronal calcium sensor) redistributes in dendritic branches upon NMDA application__\n\n---\n\n## Preprocessing\n### Dual-view Stack Registration\nRegistration of four-channel image stacks, including two excitation wavelengths and two emission pathbands, acquired with a dual-view beam splitter. This setup detects different spectral pathbands using distinct sides of the camera matrix.\n\n- `offset img` - input for a four-channel time-lapse image stack.\n- `reference img` - an optional four-channel reference image (e.g., fluorescence beads image), used for offset estimation if `use reference img` is selected.\n- `input crop` - number of pixels that will be deleted from each side of input stack frames to discard misalignment artifacts from the dual-view system.\n- `output crop` - number of pixels that will be deleted from each side of output stack frames to discard registration artifacts.\n\n\n### Multichannel Stack Preprocessing\n- `stack order` -  represents the order of axes in the input data array: T (time), C (color), X, and Y (image dimensions). If the input image stack has four dimensions (time, channel, x-axis, y-axis), channels will be split into individual three-dimensional images (time, x-axis, y-axis), each labeled with the `_ch%index%` suffix.\n- `median filter` - provides frame-by-frame image smoothing with a kernel of size specified in `median kernel`.\n- `background subtraction` -  compensates for background fluorescence intensity. Background intensity is estimated frame by frame as the 0.5 percentile of frame intensity.\n- If the `photobleaching correction` option is selected, the image will undergo correction using either an exponential (method `exp`) or bi-exponential (method `bi_exp`) fitting.\n- Image stacks can be cropped according to start and stop indexes specified in `frames range` if `drop frames` is selected.\n\n![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/stack_preprocessing.png)\n\n---\n\n## Detection of Fluorescence Redistribution\nA set of widgets designed for preprocessing multispectral image stacks and detecting redistributions in fluorescence intensity. These widgets specifically analyze differential \"red-green\" image series to identify changes in fluorescence intensity.\n\nInspired by [Dovgan et al., 2010](https://pubmed.ncbi.nlm.nih.gov/20704590/) and [Osypenko et al., 2019](https://www.sciencedirect.com/science/article/pii/S0969996119301974?via%3Dihub).\n\n### Red-Green Series\nPrimary method for detecting fluorescence-labeled targets redistribution. This widget returns a series of differential images, each representing the intensity difference between the current frame and the previous one, output image labeled with the `_red-green` suffix.\n\nParameters:\n\n- `left frames` - specifies the number of previous frames used for pixel-wise averaging.\n- `space frames` - determines the number of frames between the last left frame and the first right frame.\n- `right frames` - specifies the number of subsequent frames used for pixel-wise averaging.\n\n`normalize by int`  function normalizes the differential images relative to the absolute intensity of the input image stack, which helps to reduce background noise amplitude.\n\nIf `save MIP` is selected, the maximal intensity projection (MIP) of the differential image stack will be saved with the `_red-green-MIP` suffix.\n\n![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/rg_series.png)\n\n### ŒîF/F Series\n_In progress._\n\n---\n\n## Masking\n### Dots Pattern Masking\nCreates labels for bright dot elements on an image, such as pre- and postsynaptic fluorescence markers (e.g., Bassoon/Synaptobrevin for presynapses, PSD-95/Homer for postsynapses, etc.). It returns a labels layer with the `_dots-labels` suffix.\n\nThe widget detects the location on the MIP (Maximum Intensity Projection) of the input time series image and applies simple round masks to each detected dot. Watershed segmentation is then used to prevent the merging of overlapping masks.\n\nParameters:\n\n- `background level` - Background level for filtering out low-intensity elements. This is specified as a percentile of the MIP intensity.\n- `detection level` - Minimum intensity of dots, specified as a percentile of the MIP's maximum intensity.\n- `mask diameter` - Diameter in pixels for the round mask of each individual dot.\n- `minimal distance` - Minimum distance in pixels between the centers of individual round masks.\n\n![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/dots_masking.png)\n__Hippocalcin (green) and PSD95 (magents) in dendritic branches__\n\n\n### Up Masking\nGenerates labels for regions with high intensity based on raw or -red-green images. Returns a labels layer with the `_up-labels` suffix.\n\nThe widget provides two detection modes:\n\n- Global masking with a fixed threshold for the entire image.\n- In-ROIs masking with a loop over individual ROIs in the input `ROIs mask` with separate detections.\n\nParameters:\n\n- `det frame index` - index of the frame from the input image used for label detection.\n- `det th` - treshold value for detecting bright sites, where the intensity on the selected frame is normalized in the range of -1 to 0.\n- `in ROIs det` - option for activating in-ROIs masking.\n- `in ROIs det method` - method for in-ROIs masking; otsu provides simple Otsu thresholding, while the threshold method is identical to global detection on nomilized detection frame.\n- `in_ROIs_det_th_corr` - caling factor for the det th threshold value for in-ROIs masking.\n- `final opening fp` - footprint size in pixels for mask filtering using morphological opening (disabled if set to 0).\n- `final dilation fp` - footprint size in pixels for mask morphological dilation (disabled if set to 0).\n- `save total up mask` - if selected, a total up mask (containing all ROIs) will be created with the _up-mask suffix.\n\n![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/up_labels.png)\n__Gplobal up labels__\n\nThe In-ROIs masking option can be particularly useful for co-localization detection. By applying a broad reference mask to several target images, you can create more precise labels for ROIs in specified cell compartments. The following examples demonstrate the detection of mutual locations for static PSD-95 enriched sites (postsynaptic membranes) and HPCA translocation sites only in the vicinity of synapses, using `_dots-labels` for PSD95-mRFP images.\n\n_Note: In the In-ROIs masking mode, labels of detected sites correspond to the matching labels from the input ROIs mask._\n\nIn-ROIs masking (reference)|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/up_labels_1.png)\n:------------------:|:-------------------------:\n__In-ROIs maskin (translocation)__|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/up_labels_2.png)\n__Masks overlay__|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/up_labels_overlay.png)\n\n\n### Intensity Masking\nExtension of __Up Masking__ widget. Detects regions with increasing (`masking mode` - `up`) or decreasing (`masking mode` - `down`) intensity in `-red-green` images. Returns a labels layer with either `_up-labels` or `_down-labels` suffix, depending on the mode.\n\n![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/int_labels.png)\n\n---\n\n## 3-cube E-FRET Approach\nWidgets for detection and analysis of F√∂rster resonance energy transfer on multispectral image stacks.\n\nBased on notation and approaches from [Zal and Gascoigne, 2004](https://pubmed.ncbi.nlm.nih.gov/15189889/), [Chen et al., 2006](https://pubmed.ncbi.nlm.nih.gov/16815904/) and [Kamino et al., 2023](https://pubmed.ncbi.nlm.nih.gov/37014867/).\n\n\n### E-FRET Crosstalk Estimation\n_In progress._ \n\nEstimation of the crosstalk/bleedthrough of fluorescence between the donor and acceptor‚Äôs spectral channels.\n\n```math\nF_c = I_{DA} - a (I_{AA} - c I_{DD}) - d (I_{DD} - b I_{AA})\n```\n\n```math\nF_c = I_{DA} - a I_{AA} - d I_{DD} \\; \\text{if} \\; b \\approx c \\approx 0\n```\n\n```math\na = \\frac{I_{DA(A)}}{I_{AA(A)}}\n```\n\n```math\nb = \\frac{I_{DD(A)}}{I_{AA(A)}}\n```\n\n```math\nc = \\frac{I_{AA(D)}} {I_{DD(D)}}\n```\n\n```math\nd = \\frac{I_{DA(D)}} {I_{DD(D)}}\n```\n\n```math\nb \\approx c \\approx 0\n```\n\nParameters:\n- `DD img` - donor emission channel image acquired with the donor excitation wavelength.\n- `DA img` - donor emission channel image acquired with the acceptor excitation wavelength.\n- `AD img` - acceptor emission channel image acquired with the donor excitation wavelength.\n- `AA img` - acceptor emission channel image acquired with the acceptor excitation wavelength.\n- `mask` - .\n- `presented_fluorophore` - .\n- `saving_path` - .\n\n\n### E-FRET G-factor Estimation\n_In progress._ \n\n```math\nG = \\frac{(I_{DA} - a I_{AA} - d I_{DD}) - (I_{DA}^{post} - a I_{AA}^{post} - d I_{DD}^{post})}{I_{DD}^{post} - I_{DD}} = \\frac{F_c - F_{c}^{post}}{I_{DD}^{post} - I_{DD}} = \\frac{\\Delta F_c}{\\Delta I_{DD}}\n```\n\n\n```math\n\\Delta F_c = G \\cdot \\Delta I_{DD}\n```\n\n### E-FRET Estimation\nEstimation of the E-FRET with 3-cube approach.\n\n```math\nE_{app} = \\frac{R}{R+G}\n```\n\n```math\nR = \\frac{F_c}{I_{DD}}\n```\n\n\n__ECFP and EYFP Setup:__\n\n- Microscope Olympus IX71\n- Camera PCO Sensicam QE\n- Cube Chroma 69008\n- Dual-view system with Chroma 505DCXR beam splitter\n- Donor excitation wavelength 435 nm\n- Acceptor excitation wavelength 505 nm\n\n__TagBFP and mBaoJin Setup:__\n\n- Microscope Olympus IX71\n- Camera PCO Sensicam QE\n- Cube Chroma 69002\n- Dual-view system with Chroma 505DCXR beam splitter\n- Donor excitation wavelength 405 nm\n- Acceptor excitation wavelength 495 nm\n\nThis method utilizes default values of `a` and `d` coefficients and the `G`-factor for TagBFP and mBaoJin pair. \n\nParameters:\n\n- `DD img` - donor emission channel image acquired with the donor excitation wavelength.\n- `AD img` - acceptor emission channel image acquired with the donor excitation wavelength.\n- `AA img` - acceptor emission channel image acquired with the acceptor excitation wavelength.\n- `output type` - type of output image: sensitized emission (`Fc`), apparent FRET efficiency (`Eapp`), or FRET efficiency with photobleaching correction (`Ecorr`).\n\nIf the `save normalized` option is selected, an additional image will be saved. This image is normalized to the absolute intensity of the `AA img`, which results in reduced background noise amplitude.\n\n_Note: normalized images are useful for visual control and mask building only; they are not representative for quantitative analysis._\n\nRaw Eapp| ![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/fret_raw.png)\n:-:|:-:\n__Normalized Eapp__|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/fret_norm.png)\n\n\n---\n\n\n## Exo/Endo-cytosis Monitoring with pH-Sensitive Tag\nA set of widgets designed for the analysis of image series containing the pH-sensitive fluorescence protein Superecliptic pHluorin (SEP).\n\nInsipred by [Fujii et al., 2017](https://pubmed.ncbi.nlm.nih.gov/28474392/) and [Sposini et al., 2020](https://www.nature.com/articles/s41596-020-0371-z).\n\n\n### SEP image preprocessing\nProcesses image series obtained through repetitive pH exchange methods (such as U-tube or ppH approaches). `pH 1st frame` option indicates the 1st frame pH. By default frames with odd indexes, including index 0, are interpreted as images acquired at pH 7.0, representing total fluorescence intensity (saved with the suffix `_total`). Even frames are interpreted as images obtained at acidic pH (5.5-6.0), representing intracellular fluorescence only (saved with the suffix `_intra`).\n\nIf `calc surface img` is selected, an additional total fluorescence image with subtracted intracellular intensity will be saved as the cell surface fluorescence fraction (suffix `_surface`). The input image should be a 3-dimensional single-channel time-lapse.\n\nThe `calc projections` option allows obtaining individual pH series projections (pixel-wise series MIP - pixel-wise series average) for the detection of individual exo/endocytosis events.\n\n\n---\n\n\n## Plotting and Data Frame Saving\n### ROIs Profiles\nThis widget builds a plot with mean intensity profiles for each Region of Interest (ROI) in labels. It uses either absolute intensity (if `absolute intensity` is selected) or relative intensities (ŒîF/F0).\n\n- `time scale` - sets the number of seconds between frames for x-axis scaling.\n- `values mod` - the mode of output profile calculation. Options are `ŒîF/F0` (relative intensity changes), `ŒîF` (absolute intensity changes), or `abs` (absolute intensity value)\n- `ŒîF win` - if selected `use_simple_baseline`, the baseline intensity for ŒîF/F0 profiles is estimated as the mean intensity of the specified number of initial profile points. Othervise, this paramater specify half-size of the moving median baseline estimator (`noisy_median` from `pybaselines` package).\n- `profiles crop` - if selected, only a specified range of intensity profile indexes will be plotted, corresponding to the start and stop indexes from `profiles range`.\n\nAdditionally, you can save ROI intensity profiles as .csv files using the `save data frame` option and specifying the `saving path`. The output data frames named %img_name%_lab_prof.csv will include the following columns:\n\n- `id` - unique image ID, the name of the input `napari.Image` object.\n- `roi` - ROI number, consecutively numbered starting from 1.\n- `int` - ROI mean intensity, either raw or ŒîF/F0, according to the selected intensity option.\n- `dist` - average distance in px to the ROI from the frame, (if `save ROIs distances in data frame` option is selected).\n- `index` - frame index.\n- `time` - frame time point, adjusted according to the `time scale`.\n\n_Note: the data frame will contain information for all ROIs; amplitude filtering and crop options pertain to plotting only._\n\nAbsolute intensity         | ![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/rois_abs.png)\n:-------------------------:|:-------------------------:\n__ŒîF/F0__|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/rois_df.png)\n\n\n### Multiple Images Stat Profiles\nThis widget builds a plot displaying the average intensity of all Regions of Interest (ROIs) specified in `lab`. It can handle up to three images (`img 0`, `img 1`, and `img 2`) as inputs, depending on the selected `profiles num`.\n\n`time scale`, `values mod`, and `ŒîF win` parameters are identical as described in the __ROIs profiles__ widget.\n\nThe `stat method` allows estimation of intensity and associated errors using the following methods:\n- `se` - mean ¬± standard error of the mean.\n- `iqr` - median ¬± interquartile range.\n- `ci` - mean ¬± 95% confidence interval (t-distribution).\n\nAbsolute intensity         | ![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/stat_abs.png)\n:-------------------------:|:-------------------------:\n__ŒîF/F0__|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/stat_df.png)\n\n\n### Multiple Labels Stat Profiles\nThis widget builds a plot displaying the averaged intensity of all Regions of Interest (ROI) for one target `img`. It can handle up to three labels (`lab 0`, `lab 1`, and `lab 2`), depending on the selected `profiles num`.\n\n`time scale`, `values mod`, and `ŒîF win` parameters are identical as described in the __ROIs profiles__ widget.\n\nThe `stat method` allows estimation of intensity and associated errors using the following methods:\n- `se` - mean +/- standard error of the mean.\n- `iqr` - median +/- interquartile range.\n- `ci` - mean +/- 95% confidence interval based on the t-distribution.\n\nAbsolute intensity         | ![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/stat_lab_abs.png)\n:-------------------------:|:-------------------------:\n__ŒîF/F0__|![](https://raw.githubusercontent.com/wisstock/domb-napari/master/images/stat_lab_df.png)\n\n### Save Data Frame\nThis widget allows saving the data frame with the following columns:\n\n- `id` - unique image ID, the name of the input `napari.Image` object.\n- `lab_id` - unique label ID, the name of the input `napari.Labels` object.\n- `roi` - ROI number, consecutively numbered starting from 1.\n- `dist` - average distance in px to the ROI from the frame, (if `save ROIs distances in data frame` option is selected).\n- `index` - frame index.\n- `time` - frame time point, adjusted according to the `time scale`.\n- `abs_int` - absolute intensity value.\n- `dF_int` - absolute intensity changes (ŒîF).\n- `dF/F0_int` - relative intensity changes (ŒîF/F0).\n\n\n---\n\n\n## How to Cite\nIf you use this plugin in your work, please cite the following paper:\n\n```\n@article{Olifirov2025,\n  title = {Local Iontophoretic Application for Pharmacological Induction of Long-Term Synaptic Depression},\n  volume = {15},\n  ISSN = {2331-8325},\n  url = {http://dx.doi.org/10.21769/BioProtoc.5338},\n  DOI = {10.21769/bioprotoc.5338},\n  number = {1373},\n  journal = {BIO-PROTOCOL},\n  publisher = {Bio-Protocol,  LLC},\n  author = {Olifirov,  Borys and Fedchenko,  Oleksandra and Dovgan,  Alexandr and Babets,  Daria and Krotov,  Volodymyr and Cherkas,  Volodymyr and Belan,  Pavel},\n  year = {2025}\n}\n```\n\nor zenodo:\n```\n@misc{https://doi.org/10.5281/zenodo.14843770,\n  doi = {10.5281/ZENODO.14843770},\n  url = {https://zenodo.org/doi/10.5281/zenodo.14843770},\n  author = {wisstock,  },\n  title = {wisstock/domb-napari: Zenodo release v0.3.0},\n  publisher = {Zenodo},\n  year = {2025},\n  copyright = {MIT License}\n}\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Dual-view stack registration",
      "Multichannel stack preprocessing",
      "Red-green series",
      "E-FRET crosstalk estimation",
      "E-FRET G-factor estimation",
      "E-FRET estimation",
      "SEP preprocessing",
      "Dot-patterns masking",
      "Up masking",
      "Intensity masking",
      "ROIs profiles",
      "Multiple img stat profiles",
      "Multiple labels stat profiles",
      "Save DataFrame"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tmidas",
    "name": "napari-tmidas",
    "display_name": "T-MIDAS",
    "version": "0.2.2",
    "created_at": "2025-03-05",
    "modified_at": "2025-06-03",
    "authors": [
      "Marco Meer"
    ],
    "author_emails": [
      "marco.meer@pm.me"
    ],
    "license": "Copyright (c) 2025, Marco Meer...",
    "home_pypi": "https://pypi.org/project/napari-tmidas/",
    "home_github": "https://github.com/macromeer/napari-tmidas",
    "home_other": null,
    "summary": "A plugin for batch processing of confocal and whole-slide microscopy images of biological tissues",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "tqdm",
      "qtpy",
      "scikit-image",
      "pyqt5",
      "tqdm",
      "scikit-image",
      "ome-zarr",
      "napari-ome-zarr",
      "torch",
      "torchvision",
      "timm",
      "opencv-python",
      "cmake",
      "nd2",
      "pylibCZIrw",
      "readlif",
      "tiffslide",
      "hydra-core",
      "eva-decord",
      "acquifer-napari",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-tmidas\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-tmidas.svg?color=green)](https://github.com/macromeer/napari-tmidas/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tmidas.svg?color=green)](https://pypi.org/project/napari-tmidas)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tmidas.svg?color=green)](https://python.org)\n[![tests](https://github.com/macromeer/napari-tmidas/workflows/tests/badge.svg)](https://github.com/macromeer/napari-tmidas/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tmidas)](https://napari-hub.org/plugins/napari-tmidas)\n<!-- [![codecov](https://codecov.io/gh/macromeer/napari-tmidas/branch/main/graph/badge.svg)](https://codecov.io/gh/macromeer/napari-tmidas) -->\nThe `napari-tmidas` plugin consists of a growing collection of pipelines for fast batch processing of confocal and whole slide microscopy images of biological tissues. This is a WIP and based on the CLI version of [T-MIDAS](https://github.com/MercaderLabAnatomy/T-MIDAS).\n\n## Features\nCurrently, napari-tmidas provides pipelines as widgets for batch image conversion / cropping / processing, ROI colocalization and label inspection (cf. [Usage](#usage) below).\n\n## Installation\n\n(Video installation guides: https://www.youtube.com/@macromeer/videos)\n\nFirst, install Napari in a virtual environment:\n\n    mamba create -y -n napari-tmidas -c conda-forge python=3.11\n    mamba activate napari-tmidas\n    python -m pip install \"napari[all]\"\n\nNow you can install `napari-tmidas` via [pip]:\n\n    pip install napari-tmidas\n\nIt is recommended though to install the **latest development version**. Please also execute this command from time to time in the activated environment to benefit from newly added features:\n\n    pip install git+https://github.com/macromeer/napari-tmidas.git\n\nTo use the Batch Crop Anything pipeline, we need to install **Segment Anything 2** (2D/3D):\n\n    cd /opt # if the folder does not exist: mkdir /opt && cd /opt\n    git clone https://github.com/facebookresearch/sam2.git && cd sam2\n    pip install -e .\n    curl -L https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt -o checkpoints/sam2.1_hiera_large.pt\n    mamba install -c conda-forge ffmpeg # we also need ffmpeg\n\nIf you want to batch compress image data using [Zstandard](https://github.com/facebook/zstd), use the package manager of your operating system to install it:\n\n   ~~sudo apt-get install zstd~~    # Pre-installed on Linux :man_shrugging:\n\n    brew install zstd            # for macOS (requires [Homebrew](https://brew.sh/)\n    pip install zstandard        # Windows with Python >= 3.7\n\n\n\nAnd you are done! \n\n## Usage\n\nTo use the plugin, start napari in the activated virtual environment with this terminal command:\n\n    mamba run -n napari-tmidas napari\n\nYou can then find the installed plugin in the Plugins tab.\n\n### Microscopy Image Conversion\n\nYou can start this pipeline via `Plugins > T-MIDAS > Batch Microscopy Image Conversion`. Currently, this pipeline supports the conversion of `.nd2, .lif, .ndpi, .czi` and acquifer data. After scanning a folder of your choice for microscopy image data, select a file in the first column of the table and preview and export any image data it contains.\n\n\n<img src=\"https://github.com/user-attachments/assets/e377ca71-2f30-447d-825e-d2feebf7061b\" alt=\"Microscopy Image Conversion Widget\" style=\"width:75%; height:auto;\">\n\n\n### Image Processing\n\n1. After opening `Plugins > T-MIDAS > Batch Image Processing`, enter the path to the folder containing the images to be processed (currently supports TIF, later also ZARR). You can also filter for filename suffix.\n\n![image](https://github.com/user-attachments/assets/41ecb689-9abe-4371-83b5-9c5eb37069f9)\n\n2. As a result, a table appears with the found images. You can click on them to inspect them in the viewer.\n\n![image](https://github.com/user-attachments/assets/8360942a-be8f-49ec-bc25-385ee43bd601)\n\n3. Next, select a processing function, set parameters if applicable and `Start Batch Processing`.\n\n![image](https://github.com/user-attachments/assets/05929660-6672-4f76-89da-4f17749ccfad)\n\n4. You can click on the images in the table to show them in the viewer. For example first click on one of the `Original Files`, and then the corresponding `Processed File` to see an overlay.\n\n<img src=\"https://github.com/user-attachments/assets/cfe84828-c1cc-4196-9a53-5dfb82d5bfce\" alt=\"Image Processing Widget\" style=\"width:75%; height:auto;\">\n\n\nNote that whenever you click on an `Original File` or `Processed File` in the table, it will replace the one that is currently shown in the viewer. So naturally, you'd first select the original image, and then the processed image to correctly see the image pair that you want to inspect.\n\n\n#### Processing Function Credits\n\nThe image processing capabilities are powered by several excellent open-source tools:\n- [Cellpose 4](https://github.com/MouseLand/cellpose): Advanced cell segmentation\n- [Trackastra](https://github.com/weigertlab/trackastra): Cell tracking and analysis\n- [CAREamics](https://github.com/CAREamics/careamics): Content-aware image restoration and enhancement\n\n### Batch Label Inspection\nIf you have already segmented a folder full of images and now you want to maybe inspect and edit each label image, you can use the `Plugins > T-MIDAS > Batch Label Inspection`, which automatically saves your changes to the existing label image once you click the `Save Changes and Continue` button (bottom right).\n\n<img src=\"https://github.com/user-attachments/assets/0bf8c6ae-4212-449d-8183-e91b23ba740e\" alt=\"Batch Label Inspection Widget\" style=\"width:75%; height:auto;\">\n\n### Crop Anything\nThis pipeline combines the Segment Anything Model (SAM) for automatic object detection with an interactive interface for selecting and cropping multiple objects from images. To launch the widget, open `Plugins > T-MIDAS > Batch Crop Anything`. Cropping works like this: Enter 2D view and go to the first z slice where the object to be cropped is appearing. Activate/select the points layer and click on the object. Terminal shows progress. You can then proceed to select another object (always do this in 2D mode)\n\n<img src=\"https://github.com/user-attachments/assets/6d72c2a2-1064-4a27-b398-a9b86fcbc443\" alt=\"Crop Anything Widget\" style=\"width:75%; height:auto;\">\n\n\n\n\n### ROI Colocalization\nThis pipeline quantifies colocalization between labeled regions of interest (ROIs) across multiple image channels. It determines the extent of overlap between ROIs in a reference channel and those in one or two other channels. The output is a table of colocalization counts. Optionally, the size of reference channel ROIs, as well as the total or median size of colocalizing ROIs in the other channels, can be included. Colocalization is determined using Boolean masking. The number of colocalizing instances is determined by counting unique label IDs within the overlapping regions. Typically, the reference channel contains larger structures, while other channels contain smaller, potentially nested, structures. For example, the reference channel might contain cell bodies, with the second and third channels containing nuclei and sub-nuclear objects, respectively.\n\n<img src=\"https://github.com/user-attachments/assets/2f9022a0-7b88-4588-a448-250f07a634d7\" alt=\"ROI Colocalization Widget\" style=\"width:75%; height:auto;\">\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-tmidas\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/macromeer/napari-tmidas/issues\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Batch Image Processing",
      "Batch Label inspection",
      "Batch Microscopy Image Conversion",
      "Batch Crop Anything",
      "Batch ROI Colocalization Analysis"
    ],
    "contributions_sample_data": [
      "T-MIDAS"
    ]
  },
  {
    "normalized_name": "napari-bootstrapper",
    "name": "napari-bootstrapper",
    "display_name": "Bootstrapper",
    "version": "0.2.0",
    "created_at": "2025-05-23",
    "modified_at": "2025-06-02",
    "authors": [
      "Vijay Venu Thiyagarajan"
    ],
    "author_emails": [
      "vvenu@utexas.edu"
    ],
    "license": "Copyright (c) 2025, Vijay Venu...",
    "home_pypi": "https://pypi.org/project/napari-bootstrapper/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin to quickly generate dense ground truth with sparse labels",
    "categories": [
      "Annotation",
      "Segmentation",
      "Dataset"
    ],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "scikit-image",
      "torch",
      "numba",
      "gunpowder",
      "magicgui",
      "qtpy",
      "pyqtgraph",
      "matplotlib",
      "napari",
      "tqdm",
      "lsds",
      "mwatershed",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-bootstrapper\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bootstrapper.svg?color=green)](https://github.com/ucsdmanorlab/napari-bootstrapper/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bootstrapper.svg?color=green)](https://pypi.org/project/napari-bootstrapper)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bootstrapper.svg?color=green)](https://python.org)\n[![tests](https://github.com/ucsdmanorlab/napari-bootstrapper/workflows/tests/badge.svg)](https://github.com/ucsdmanorlab/napari-bootstrapper/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bootstrapper)](https://napari-hub.org/plugins/napari-bootstrapper)\n\n- [Introduction](#introduction)\n- [Installation](#installation)\n- [Getting Started](#getting-started)\n- [Citation](#citation)\n- [Issues](#issues)\n- [Funding](#funding)\n\n## Introduction\n\n`napari-bootstrapper` is a tool to quickly generate dense 3D labels using sparse 2D labels within napari.\n\nDense 3D segmentations are generated using the 2D->3D method described in the preprint titled [_Sparse Annotation is Sufficient for Bootstrapping Dense Segmentation_](https://www.biorxiv.org/content/10.1101/2024.06.14.599135v2). In the preprint, we show sparse 2D annotations made in ~10 minutes on a single section can generate dense 3D segmentations that are reasonably good starting points for refining or bootstrapping.\n\nThis plugin is limited to the 2D->3D method and is intended for small volumes that can fit in memory. For more complex bootstrapping workflows, dedicated 3D models, and block-wise processing of large volumes, we recommend using the [_Bootstrapper_](https://github.com/ucsdmanorlab/bootstrapper) CLI tool.\n\n## Installation\n\nWe recommend installing `napari-bootstrapper` via conda and [pip]:\n\n1. Create a new environment called `napari-bootstrapper`:\n\n```bash\nconda create -n napari-bootstrapper -c conda-forge python==3.11 napari pyqt\n```\n\n2. Activate the newly-created environment:\n\n```\nconda activate napari-bootstrapper\n```\n\n3. You can install `napari-bootstrapper` via [pip]:\n\n```bash\npip install napari-bootstrapper\n```\n   - Or you can install the latest development version from github:\n\n```bash\npip install git+https://github.com/ucsdmanorlab/napari-bootstrapper.git\n```\n\n\n## Getting Started\nRun the following in your terminal:\n```bash\nconda activate napari-bootstrapper\nnapari\n```\n\n## Citation\n\nIf you find Bootstrapper useful in your research, please consider citing our **[preprint](https://www.biorxiv.org/content/10.1101/2024.06.14.599135v1)**:\n```\n@article {Thiyagarajan2024.06.14.599135,\n\tauthor = {Thiyagarajan, Vijay Venu and Sheridan, Arlo and Harris, Kristen M. and Manor, Uri},\n\ttitle = {Sparse Annotation is Sufficient for Bootstrapping Dense Segmentation},\n\tyear = {2024},\n\tdoi = {10.1101/2024.06.14.599135},\n\tURL = {https://www.biorxiv.org/content/10.1101/2024.06.14.599135v2},\n}\n```\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/ucsdmanorlab/napari-bootstrapper/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n## Funding\nChan-Zuckerberg Imaging Scientist Award DOI https://doi.org/10.37921/694870itnyzk from the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundation (funder DOI 10.13039/100014989).\n\nNSF NeuroNex Technology Hub Award (1707356), NSF NeuroNex2 Award (2014862)\n\n![image](https://github.com/ucsdmanorlab/bootstrapper/assets/64760651/4b4a6029-e1ba-42bb-ab8b-d9357cc46239)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Bootstrapper",
      "Delete Labels",
      "Merge Labels",
      "Split Labels",
      "Morph Labels",
      "Filter Labels"
    ],
    "contributions_sample_data": [
      "cremi_c",
      "fluo_c2dl_huh7"
    ]
  },
  {
    "normalized_name": "brainglobe-segmentation",
    "name": "brainglobe-segmentation",
    "display_name": "brainglobe-segmentation",
    "version": "1.3.3",
    "created_at": "2023-11-06",
    "modified_at": "2025-05-30",
    "authors": [
      "Adam Tyson",
      "Horst Obenhaus"
    ],
    "author_emails": [
      "\"Adam Tyson",
      "Horst Obenhaus\" <code@adamltyson.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/brainglobe-segmentation/",
    "home_github": "https://github.com/brainglobe/brainglobe-segmentation",
    "home_other": null,
    "summary": "Segmentation of anatomical structures in a common coordinate space",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "brainglobe-atlasapi>=2.0.1",
      "brainglobe-napari-io>=0.3.0",
      "brainglobe-utils>=0.5.0",
      "napari!=0.6.0,>=0.5",
      "numpy",
      "pandas[hdf5]",
      "qtpy",
      "scikit-image",
      "scipy",
      "tifffile",
      "qt-niu",
      "black; extra == \"dev\"",
      "gitpython; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "napari-time-slicer; extra == \"dev\""
    ],
    "package_metadata_description": "[![Python Version](https://img.shields.io/pypi/pyversions/brainglobe-segmentation.svg)](https://pypi.org/project/brainglobe-segmentation)\n[![PyPI](https://img.shields.io/pypi/v/brainglobe-segmentation.svg)](https://pypi.org/project/brainglobe-segmentation)\n[![Wheel](https://img.shields.io/pypi/wheel/brainglobe-segmentation.svg)](https://pypi.org/project/brainglobe-segmentation)\n[![Development Status](https://img.shields.io/pypi/status/brainglobe-segmentation.svg)](https://github.com/brainglobe/brainglobe-segmentation)\n[![Tests](https://img.shields.io/github/actions/workflow/status/brainglobe/brainglobe-segmentation/test_and_deploy.yml?branch=main)](https://github.com/brainglobe/brainglobe-segmentation/actions)\n[![codecov](https://codecov.io/gh/brainglobe/brainglobe-segmentation/graph/badge.svg?token=WP9KTPZE5R)](https://codecov.io/gh/brainglobe/brainglobe-segmentation)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n[![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)\n\n# brainglobe-segmentation\n\nSegmentation of anatomical structures in a common coordinate space\n\n## Installation\n**PyPI**\n```\npip install brainglobe-segmentation\n```\n\n**conda**\n```\nconda install -c conda-forge brainglobe-segmentation\n```\n\nN.B. Your data will need to be registered to an anatomical atlas first.\n\n## Usage\nSee [user guide](https://brainglobe.info/documentation/brainglobe-segmentation/index.html).\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## Citing brainglobe-segmentation\n\nIf you find brainglobe-segmentation useful, and use it in your research, please let us know and also cite the paper:\n\n> Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 [doi.org/10.1038/s41598-021-04676-9](https://doi.org/10.1038/s41598-021-04676-9)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Region/track segmentation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "brainreg",
    "name": "brainreg",
    "display_name": "brainreg",
    "version": "1.0.13",
    "created_at": "2023-12-14",
    "modified_at": "2025-05-30",
    "authors": [
      "Adam Tyson",
      "Charly Rousseau",
      "Stephen Lenzi"
    ],
    "author_emails": [
      "\"Adam Tyson",
      "Charly Rousseau",
      "Stephen Lenzi\" <code@adamltyson.com>"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/brainreg/",
    "home_github": "https://github.com/brainglobe/brainreg",
    "home_other": null,
    "summary": "Automated multi-atlas whole-brain microscopy registration",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "brainglobe-atlasapi>=2.0.1",
      "brainglobe-space>=1.0.0",
      "brainglobe-utils>=0.5.0",
      "fancylog",
      "numpy",
      "scikit-image>=0.24.0",
      "brainglobe-napari-io>=0.3.2; extra == \"napari\"",
      "brainglobe-segmentation>=1.0.0; extra == \"napari\"",
      "magicgui; extra == \"napari\"",
      "napari-plugin-engine>=0.1.4; extra == \"napari\"",
      "napari[pyqt5]!=0.6.0,>=0.5; extra == \"napari\"",
      "pooch>1; extra == \"napari\"",
      "qtpy; extra == \"napari\"",
      "black; extra == \"dev\"",
      "check-manifest; extra == \"dev\"",
      "gitpython; extra == \"dev\"",
      "napari[pyqt5]!=0.6.0,>=0.5; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "setuptools_scm; extra == \"dev\"",
      "tox; extra == \"dev\""
    ],
    "package_metadata_description": "[![Python Version](https://img.shields.io/pypi/pyversions/brainreg.svg)](https://pypi.org/project/brainreg)\n[![PyPI](https://img.shields.io/pypi/v/brainreg.svg)](https://pypi.org/project/brainreg)\n[![Wheel](https://img.shields.io/pypi/wheel/brainreg.svg)](https://pypi.org/project/brainreg)\n[![Development Status](https://img.shields.io/pypi/status/brainreg.svg)](https://github.com/brainglobe/brainreg)\n[![Tests](https://img.shields.io/github/actions/workflow/status/brainglobe/brainreg/test_and_deploy.yml?branch=main)](https://github.com/brainglobe/brainreg/actions)\n[![codecov](https://codecov.io/gh/brainglobe/brainreg/branch/main/graph/badge.svg?token=FbPgwBIGnd)](https://codecov.io/gh/brainglobe/brainreg)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n\n# brainreg\n\nbrainreg is an update to [amap](https://github.com/SainsburyWellcomeCentre/amap_python) (which is itself a port\nof the [original Java software](https://www.nature.com/articles/ncomms11879)) to include multiple registration backends, and to support the many atlases provided by [brainglobe-atlasapi](https://github.com/brainglobe/brainglobe-atlasapi).\nIt also comes with an optional [napari plugin](https://github.com/brainglobe/brainreg-napari) if you'd rather use brainreg with through graphical interface.\n\nDocumentation for both the command-line tool and graphical interface can be found [here](https://brainglobe.info/documentation/brainreg/index.html).\n\nFor segmentation of bulk structures in 3D space (e.g. injection sites, Neuropixels probes), please see [brainglobe-segmentation](https://github.com/brainglobe/brainglobe-segmentation).\n\n## Details\n\nThe aim of brainreg is to register the template brain (e.g. from the [Allen Reference Atlas](https://mouse.brain-map.org/static/atlas)) to the sample image.\nOnce this is complete, any other image in the template space can be aligned with the sample (such as region annotations, for segmentation of the sample image).\nThe template to sample transformation can also be inverted, allowing sample images to be aligned in a common coordinate space.\n\nTo do this, the template and sample images are filtered, and then registered in a three step process (reorientation, affine registration, and freeform registration).\nThe resulting transform from template to standard space is then applied to the atlas.\n\nFull details of the process are in the [original aMAP paper](https://www.nature.com/articles/ncomms11879).\n\n![An illustrated overview of the registration process](https://user-images.githubusercontent.com/13147259/143553945-a046e918-7614-4211-814c-fc840bb0159d.png)\n\n## Installation\n\nTo install both the command line tool and the napari plugin, run\n\n```bash\npip install brainreg[napari]\n```\n\nin your desired Python environment.\nTo only install the command line tool with no GUI (e.g. to run brainreg on an HPC cluster), just run:\n\n```bash\npip install brainreg\n```\n\n### Installing on macOS\n\nIf you are using macOS, please run\n\n```bash\nconda install -c conda-forge niftyreg\n```\n\nin your environment before installing, to ensure all dependencies are installed.\n\n## Command line usage\n\n### Basic usage\n\n```bash\nbrainreg /path/to/raw/data /path/to/output/directory -v 5 2 2 --orientation psl\n```\n\nFull command-line arguments are available with `brainreg -h`, but please\n[get in touch](mailto:code@adamltyson.com?subject=brainreg) if you have any questions.\n\n### Mandatory arguments\n\n- Path to the directory of the images. This can also be a text file pointing to the files.\n- Output directory for all intermediate and final results.\n- You must also specify the voxel sizes with the `-v` flag, see [specifying voxel size](https://brainglobe.info/documentation/general/image-definition.html#voxel-sizes) for details.\n\n### Atlas\n\nBy default, brainreg will use the 25um version of the [Allen Mouse Brain Atlas](https://mouse.brain-map.org/).\nTo use another atlas (e.g. for another species, or another resolution), you must use the `--atlas` flag, followed by the string describing the atlas, e.g.:\n\n```bash\n--atlas allen_mouse_50um\n```\n\nTo find out which atlases are available, once brainreg is installed, please run `brainglobe list`.\nThe name of the resulting atlases is the string to pass with the `--atlas` flag.\n\n### Input data orientation\n\nIf your data does not match the BrainGlobe default orientation (the origin voxel is the most anterior, superior, left-most voxel), then you must specify the orientation by using the `--orientation` flag.\nWhat follows must be a string in the [brainglobe-space](https://github.com/brainglobe/brainglobe-space) \"initials\" form, to describe the origin voxel.\n\nIf the origin of your data (first, top left voxel) is the most anterior, superior, left part of the brain, then the orientation string would be \"asl\" (anterior, superior, left), and you would use:\n\n```bash\n--orientation asl\n```\n\n### Registration options\n\nTo change how the actual registration performs, see [registration parameters](https://brainglobe.info/documentation/brainreg/user-guide/parameters.html)\n\n### Additional options\n\n- `-a` or `--additional` Paths to N additional channels to downsample to the same coordinate space.\n- `--sort-input-file` If set to true, the input text file will be sorted using natural sorting. This means that the file paths will be sorted as would be expected by a human and not purely alphabetically.\n- `--brain_geometry` Can be one of `full` (default) for full brain registration, `hemisphere_l` for left hemisphere data-set and `hemisphere_r` for right hemisphere data-set.\n\n### Misc options\n\n- `--n-free-cpus` The number of CPU cores on the machine to leave unused by the program to spare resources.\n- `--debug` Debug mode. Will increase verbosity of logging and save all intermediate files for diagnosis of software issues.\n- `--save-original-orientation` Option to save the registered atlas with the same orientation as the input data.\n\n## Visualising results\n\nIf you have installed the optional [napari](https://github.com/napari/napari) plugin, you can use napari to view your data.\nThe plugin automatically fetches the [brainglobe-napari-io](https://github.com/brainglobe/brainglobe-napari-io) which provides this functionality.\nIf you have installed only the command-line tool you can still manually install [brainglobe-napari-io](https://github.com/brainglobe/brainglobe-napari-io) and follow the steps below.\n\n### Sample space\n\nOpen napari and drag your brainreg output directory (the one with the log file) onto the napari window.\n\nVarious images should then open, including:\n\n- `Registered image` - the image used for registration, downsampled to atlas resolution\n- `atlas_name` - e.g. `allen_mouse_25um` the atlas labels, warped to your sample brain\n- `Boundaries` - the boundaries of the atlas regions\n\nIf you downsampled additional channels, these will also be loaded.\nMost of these images will not be visible by default - click the little eye icon to toggle visibility.\n\n**Note:** If you use a high resolution atlas (such as `allen_mouse_10um`), then the files can take a little while to load.\n\n![GIF illustration of loading brainreg output into napari for visualisation](https://raw.githubusercontent.com/brainglobe/napari-brainreg/master/resources/sample_space.gif)\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## Citing brainreg\n\nIf you find brainreg useful, and use it in your research, please let us know and also cite the paper:\n\n> Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Lenzi, S. C., Obenhaus, H. A., Claudi, F., Branco, T.,  Margrie, T. W. (2022). Accurate determination of marker location within whole-brain microscopy images. Scientific Reports, 12, 867 [doi.org/10.1038/s41598-021-04676-9](https://doi.org/10.1038/s41598-021-04676-9)\n\nPlease also cite aMAP (the original pipeline from which this software is based):\n\n>Niedworok, C.J., Brown, A.P.Y., Jorge Cardoso, M., Osten, P., Ourselin, S., Modat, M. and Margrie, T.W., (2016). AMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data. Nature Communications. 7, 1‚Äì9. <https://doi.org/10.1038/ncomms11879>\n\nLastly, if you can, please cite the BrainGlobe Atlas API that provided the atlas:\n\n>Claudi, F., Petrucco, L., Tyson, A. L., Branco, T., Margrie, T. W. and Portugues, R. (2020). BrainGlobe Atlas API: a common interface for neuroanatomical atlases. Journal of Open Source Software, 5(54), 2668, <https://doi.org/10.21105/joss.02668>\n\nFinally, **don't forget to cite the developers of the atlas that you used (e.g. the Allen Brain Atlas)!**\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Atlas Registration"
    ],
    "contributions_sample_data": [
      "Low resolution brain"
    ]
  },
  {
    "normalized_name": "cellfinder",
    "name": "cellfinder",
    "display_name": "cellfinder",
    "version": "1.7.0",
    "created_at": "2024-01-03",
    "modified_at": "2025-05-30",
    "authors": [
      "Adam Tyson",
      "Christian Niedworok",
      "Charly Rousseau"
    ],
    "author_emails": [
      "\"Adam Tyson",
      "Christian Niedworok",
      "Charly Rousseau\" <code@adamltyson.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/cellfinder/",
    "home_github": "https://github.com/brainglobe/cellfinder",
    "home_other": null,
    "summary": "Automated 3D cell detection in large microscopy images",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "brainglobe-utils>=0.5.0",
      "brainglobe-napari-io>=0.3.4",
      "dask[array]",
      "fancylog>=0.0.7",
      "natsort",
      "numba",
      "numpy",
      "scikit-image",
      "scikit-learn",
      "keras>=3.7.0",
      "torch>=2.4.1",
      "tifffile",
      "tqdm",
      "qt-niu",
      "black; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pyinstrument; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "pytest-timeout; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "pooch>=1; extra == \"dev\"",
      "brainglobe-napari-io; extra == \"napari\"",
      "magicgui; extra == \"napari\"",
      "napari-ndtiffs; extra == \"napari\"",
      "napari-plugin-engine>=0.1.4; extra == \"napari\"",
      "napari[pyqt5]; extra == \"napari\"",
      "pooch>=1; extra == \"napari\"",
      "qtpy; extra == \"napari\""
    ],
    "package_metadata_description": "[![Python Version](https://img.shields.io/pypi/pyversions/cellfinder.svg)](https://pypi.org/project/cellfinder)\n[![PyPI](https://img.shields.io/pypi/v/cellfinder.svg)](https://pypi.org/project/cellfinder)\n[![Downloads](https://pepy.tech/badge/cellfinder)](https://pepy.tech/project/cellfinder)\n[![Wheel](https://img.shields.io/pypi/wheel/cellfinder.svg)](https://pypi.org/project/cellfinder)\n[![Development Status](https://img.shields.io/pypi/status/cellfinder.svg)](https://github.com/brainglobe/cellfinder)\n[![Tests](https://img.shields.io/github/actions/workflow/status/brainglobe/cellfinder/test_and_deploy.yml?branch=main)](https://github.com/brainglobe/cellfinder/actions)\n[![codecov](https://codecov.io/gh/brainglobe/cellfinder/branch/main/graph/badge.svg?token=nx1lhNI7ox)](https://codecov.io/gh/brainglobe/cellfinder)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](https://brainglobe.info/community/developers/index.html)\n[![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)\n\n# cellfinder\n\ncellfinder is software for automated 3D cell detection in very large 3D images (e.g., serial two-photon or lightsheet volumes of whole mouse brains).\nThere are three different ways to interact and use it, each with different user interfaces and objectives in mind.\nFor more details, head over to [the documentation website](https://brainglobe.info/documentation/cellfinder/index.html).\n\nAt a glance:\n\n- There is a command-line interface called [brainmapper](https://brainglobe.info/documentation/brainglobe-workflows/brainmapper/index.html) that integrates [with `brainreg`](https://github.com/brainglobe/brainreg) for automated cell detection and classification. You can install it through [`brainglobe-workflows`](https://brainglobe.info/documentation/brainglobe-workflows/index.html).\n- There is a [napari plugin](https://brainglobe.info/documentation/cellfinder/user-guide/napari-plugin/index.html) for interacting graphically with the cellfinder tool.\n- There is a [Python API](https://brainglobe.info/documentation/cellfinder/user-guide/cellfinder-core.html) to allow users to integrate BrainGlobe tools into their custom workflows.\n\n## Installation\n\nYou can find [the installation instructions](https://brainglobe.info/documentation/cellfinder/installation.html#installation) on the BrainGlobe website, which will go into more detail about the installation process if you want to minimise your installation to suit your needs.\nHowever, we recommend that users install `cellfinder` either through installing BrainGlobe version 1, or (if you also want the command-line interface) installing `brainglobe-workflows`.\n\n```bash\n# If you want to install all BrainGlobe tools, including cellfinder, in a consistent manner with one command:\npip install brainglobe>=1.0.0\n# If you want to install the brainmapper CLI tool as well:\npip install brainglobe-workflows>=1.0.0\n```\n\nIf you only want the `cellfinder` package by itself, you can `pip install` it alone:\n\n```bash\npip install cellfinder>=1.0.0\n```\n\nBe sure to specify a version greater than version `v1.0.0` - prior to this version the `cellfinder` package had a very different structure that is incompatible with BrainGlobe version 1 and the other tools in the BrainGlobe suite.\nSee [our blog posts](https://brainglobe.info/blog/) for more information on the release of BrainGlobe version 1.\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## Citation\nIf you find this package useful, and use it in your research, please cite the following paper:\n> Tyson, A. L., Rousseau, C. V., Niedworok, C. J., Keshavarzi, S., Tsitoura, C., Cossell, L., Strom, M. and Margrie, T. W. (2021) ‚ÄúA deep learning algorithm for 3D cell detection in whole mouse brain image datasets‚Äô PLOS Computational Biology, 17(5), e1009074\n[https://doi.org/10.1371/journal.pcbi.1009074](https://doi.org/10.1371/journal.pcbi.1009074)\n\n**If you use this, or any other tools in the brainglobe suite, please\n [let us know](https://brainglobe.info/contact.html), and\n we'd be happy to promote your paper/talk etc.**\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Cell detection",
      "Train network",
      "Curation"
    ],
    "contributions_sample_data": [
      "Sample data"
    ]
  },
  {
    "normalized_name": "napari-activelearning",
    "name": "napari-activelearning",
    "display_name": "Active Learning",
    "version": "0.1.0",
    "created_at": "2024-07-31",
    "modified_at": "2025-05-30",
    "authors": [
      "Fernando Cervantes (The Jackson Laboratory)"
    ],
    "author_emails": [
      "fernando.cervantes@jax.org"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-activelearning/",
    "home_github": "https://github.com/TheJacksonLaboratory/activelearning",
    "home_other": null,
    "summary": "An active learning plugin for fine tuning of deep learning models.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "dask[array]",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tifffile",
      "tensorstore==0.1.59",
      "ome-zarr==0.9.0",
      "zarr<3.0.0,>=2.12.0",
      "zarrdataset>=0.2.1",
      "cellpose>=3.0.0; extra == \"cellpose\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# Active Learning tools for ML models fine-tuning\nActive learning tools for fine-tuning ML models\n\n[![License MIT](https://img.shields.io/pypi/l/napari-activelearning.svg?color=green)](https://github.com/TheJacksonLaboratory/activelearning/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-activelearning.svg?color=green)](https://pypi.org/project/napari-activelearning)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-activelearning.svg?color=green)](https://python.org)\n[![tests](https://github.com/TheJacksonLaboratory/activelearning/workflows/tests/badge.svg)](https://github.com/TheJacksonLaboratory/activelearning/actions)\n[![codecov](https://codecov.io/gh/TheJacksonLaboratory/activelearning/branch/main/graph/badge.svg)](https://codecov.io/gh/TheJacksonLaboratory/napari-activelearning)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-activelearning)](https://napari-hub.org/plugins/napari-activelearning)\n\nA plugin for running a complete active learning workflow\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-activelearning` via [pip]:\n\n    pip install napari-activelearning\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-activelearning\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Image groups manager",
      "Acquisition function manager",
      "Label groups manager"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-czi-reader",
    "name": "napari-czi-reader",
    "display_name": "napari-czi-reader",
    "version": "1.2.0",
    "created_at": "2025-05-27",
    "modified_at": "2025-05-30",
    "authors": [
      "Markus L. Bille"
    ],
    "author_emails": [
      "github+markus@bille.dk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-czi-reader/",
    "home_github": "https://github.com/MaxusTheOne/napari-czi-reader",
    "home_other": null,
    "summary": "A Czi reader plugin for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "aicsimageio",
      "aicspylibczi",
      "pint",
      "xmltodict",
      "pytest; extra == \"test\"",
      "pytest-cov; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "codecov; extra == \"test\"",
      "napari[all]; extra == \"dev\"",
      "build; extra == \"dev\"",
      "twine; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "codecov; extra == \"dev\""
    ],
    "package_metadata_description": "\nPackage for reading .czi into napari.\n\nSplit from https://github.com/MaxusTheOne/napari-pitcount-cfim\n\nTasks:\n- [ ] Add config\n- [ ] Fix bioformats crashing this reader, even though it is not used\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.czi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "allencell-segmenter-ml",
    "name": "allencell-segmenter-ml",
    "display_name": "allencell-segmenter-ml",
    "version": "1.0.0",
    "created_at": "2024-09-20",
    "modified_at": "2025-05-29",
    "authors": [],
    "author_emails": [],
    "license": "Allen Institute Software Licen...",
    "home_pypi": "https://pypi.org/project/allencell-segmenter-ml/",
    "home_github": "https://github.com/AllenCell/allencell-ml-segmenter#README.md",
    "home_other": null,
    "summary": "A plugin to leverage ML segmentation in napari",
    "categories": [],
    "package_metadata_requires_python": "<3.11,>=3.9",
    "package_metadata_requires_dist": [
      "npe2>=0.6.2",
      "numpy",
      "hydra-core==1.3.2",
      "bioio==1.1.0",
      "bioio-base==1.0.4",
      "tifffile<2025.2.18,>=2023.4.12",
      "watchdog",
      "cyto-dl>=0.4.5",
      "scikit-image!=0.23.0",
      "napari>=0.4.18; extra == \"napari\"",
      "pyqt5; extra == \"napari\"",
      "pytest<8.0.0; extra == \"test-lint\"",
      "pytest-cov; extra == \"test-lint\"",
      "pytest-qt; extra == \"test-lint\"",
      "qtpy; extra == \"test-lint\"",
      "pyqt5; extra == \"test-lint\"",
      "black>=24.2.0; extra == \"test-lint\"",
      "pytest-xvfb; sys_platform == \"linux\" and extra == \"test-lint\"",
      "responses; extra == \"test-lint\"",
      "mypy; extra == \"test-lint\"",
      "toml; extra == \"test-lint\"",
      "bumpver; extra == \"test-lint\"",
      "napari>=0.4.18; extra == \"test-lint\"",
      "magicgui; extra == \"test-lint\"",
      "black>=24.2.0; extra == \"dev\"",
      "coverage>=7.2.2; extra == \"dev\"",
      "flake8>=6.0.0; extra == \"dev\"",
      "pytest<8.0.0,>=7.2.2; extra == \"dev\"",
      "pytest-qt>=3.3.0; extra == \"dev\"",
      "pytest-cov>=2.6.1; extra == \"dev\"",
      "pyqt5>=5.15.9; extra == \"dev\"",
      "bumpver>=2023.1129; extra == \"dev\"",
      "build>=1.0.3; extra == \"dev\"",
      "twine>=5.0.0; extra == \"dev\"",
      "responses; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "linkify-it-py; extra == \"sphinx-docs\"",
      "sphinx; extra == \"sphinx-docs\"",
      "furo; extra == \"sphinx-docs\"",
      "sphinxext-opengraph; extra == \"sphinx-docs\"",
      "sphinx_inline_tabs; extra == \"sphinx-docs\"",
      "sphinx_copybutton; extra == \"sphinx-docs\"",
      "myst_parser; extra == \"sphinx-docs\"",
      "sphinx_togglebutton; extra == \"sphinx-docs\"",
      "sphinx_design; extra == \"sphinx-docs\""
    ],
    "package_metadata_description": "# Allencell-segmenter-ml\n\n[![Test and lint](https://github.com/AllenCell/allencell-segmenter-ml/actions/workflows/test_lint.yaml/badge.svg?branch=main&event=push)](https://github.com/AllenCell/allencell-segmenter-ml/actions/workflows/test_lint_pr.yaml)\n\n\n## What is Allen Cell Segmenter ML\nA napari plugin for deep-learning based segmentation of cellular structures.\n\n![SegmenterML-plugin_fig1_output.png](docs%2Fuser_docs%2Fimages%2FSegmenterML-plugin_fig1_output.png)\n\n- **Available at no cost** ‚Äî available on PyPI\n- **User-friendly** ‚Äî leverage napari as a fast 3D viewer with interactive plugin interface\n- **Beginner-friendly** ‚Äî new to machine learning? This plugin simplifies the application of machine learning in the segmentation process through the 3 main modules:\n  - **Curation**: curate training datasets\n  - **Training**: iteratively train custom segmentation model(s) (UNET) to target cellular structure with wide morphological variability\n  - **Prediction & Thresholding**: generate segmentation prediction on 2D and 3D cell image data\n\n\n##  üì∞ News\n\n - **[2024.09.24]** :tada: Initial release of the plugin and Megaseg models!\n - **[2024.05.29]** :tada: v1.0.0 Released on PyPi\n\n\n## User Documentation\n[See our full user documentation on our github pages site.](https://allencell.github.io/allencell-segmenter-ml/index.html)\n\n\n## üõ†Ô∏è Installation\n\n### System and Data Requirements\n\n[Please click here to check out our latest System and Data requirements.](https://allencell.github.io/allencell-segmenter-ml/1_Get-started/1_prerequisites.html)\n\n\n### Installation Steps\n[Please click here for our latest installation steps.](https://allencell.github.io/allencell-segmenter-ml/1_Get-started/2_installation.html)\n\n\n## Models\n[More information about the pre-trained models we provide with our plugin, and citation information, can be found here.](https://allencell.github.io/allencell-segmenter-ml/1_Get-started/4_pretrained-models.html)\n\n## License\n\nDistributed under the terms of the [Allen Institute Software License].\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[@napari]: https://github.com/napari\n[Allen Institute Software License]: https://github.com/AllenCell/allencell-segmenter-ml/blob/main/LICENSE\n[file an issue]: https://github.com/AllenCell/allencell-ml-segmenter/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[PyTorch]: https://pytorch.org/get-started/locally/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Allen Cell Segmenter - ML"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "harpy-analysis",
    "name": "harpy-analysis",
    "display_name": "harpy",
    "version": "0.2.0",
    "created_at": "2024-12-09",
    "modified_at": "2025-05-29",
    "authors": [
      "dambi"
    ],
    "author_emails": [],
    "license": "Academic Non-commercial Softwa...",
    "home_pypi": "https://pypi.org/project/harpy-analysis/",
    "home_github": "https://github.com/saeyslab/harpy",
    "home_other": null,
    "summary": "single-cell spatial proteomics analysis that makes you happy",
    "categories": [],
    "package_metadata_requires_python": "<3.13,>=3.10",
    "package_metadata_requires_dist": [
      "crick",
      "dask<=2024.11.2,>=2024.4.1",
      "datasets>=2.16.0",
      "distributed",
      "flowsom",
      "geopandas>=1.0.1",
      "lazy-loader>=0.4",
      "leidenalg>=0.9.1",
      "magicgui",
      "nptyping",
      "ome-zarr>=0.9.0",
      "omegaconf==2.3.0",
      "pyrootutils",
      "rasterio>=1.3.2",
      "scanpy>=1.9.1",
      "seaborn>=0.12.2",
      "session-info2",
      "spatialdata-io>=0.1.6",
      "spatialdata>=0.2.6",
      "universal-pathlib",
      "voronoi-diagram-for-polygons>=0.1.6",
      "xarray-dataclasses>=1.9.1",
      "xarray>=2024.10.0",
      "basicpy>=1.0.0; extra == 'basic'",
      "jax>=0.4.6; extra == 'basic'",
      "jaxlib>=0.4.6; extra == 'basic'",
      "asv; extra == 'benchmark'",
      "cellpose>=2.2.3; extra == 'cellpose'",
      "hydra-colorlog>=1.2.0; extra == 'cli'",
      "hydra-core>=1.2.0; extra == 'cli'",
      "hydra-submitit-launcher>=1.2.0; extra == 'cli'",
      "submitit>=1.4.5; extra == 'cli'",
      "flowsom; extra == 'clustering'",
      "scikit-learn>=1.3.1; extra == 'clustering'",
      "asv; extra == 'dev'",
      "bokeh; extra == 'dev'",
      "cellpose>=2.2.3; extra == 'dev'",
      "datasets; extra == 'dev'",
      "flowsom; extra == 'dev'",
      "hydra-colorlog>=1.2.0; extra == 'dev'",
      "hydra-core>=1.2.0; extra == 'dev'",
      "hydra-submitit-launcher>=1.2.0; extra == 'dev'",
      "instanseg-torch>=0.0.8; extra == 'dev'",
      "ipython; extra == 'dev'",
      "ipywidgets; extra == 'dev'",
      "joypy; extra == 'dev'",
      "myst-nb; extra == 'dev'",
      "napari-spatialdata>=0.2.6; extra == 'dev'",
      "napari[all]>=0.4.18; extra == 'dev'",
      "nbconvert; extra == 'dev'",
      "opencv-python; extra == 'dev'",
      "pre-commit; extra == 'dev'",
      "pytest; extra == 'dev'",
      "pytest-cov; extra == 'dev'",
      "pytest-qt; extra == 'dev'",
      "scikit-learn>=1.3.1; extra == 'dev'",
      "spatialdata-plot<0.2.9; extra == 'dev'",
      "sphinx-autodoc-typehints; extra == 'dev'",
      "sphinx-book-theme>=1.0.0; extra == 'dev'",
      "sphinx-copybutton; extra == 'dev'",
      "sphinx-design; extra == 'dev'",
      "sphinx-rtd-theme; extra == 'dev'",
      "sphinx>=4.5; extra == 'dev'",
      "sphinxcontrib-bibtex>=1.0.0; extra == 'dev'",
      "squidpy; extra == 'dev'",
      "submitit>=1.4.5; extra == 'dev'",
      "supervenn>=0.5.0; extra == 'dev'",
      "textalloc; extra == 'dev'",
      "tox; extra == 'dev'",
      "tqdm; extra == 'dev'",
      "twine>=4.0.2; extra == 'dev'",
      "myst-nb; extra == 'docs'",
      "sphinx-autodoc-typehints; extra == 'docs'",
      "sphinx-book-theme>=1.0.0; extra == 'docs'",
      "sphinx-copybutton; extra == 'docs'",
      "sphinx-design; extra == 'docs'",
      "sphinx-rtd-theme; extra == 'docs'",
      "sphinx>=4.5; extra == 'docs'",
      "sphinxcontrib-bibtex>=1.0.0; extra == 'docs'",
      "bokeh; extra == 'extra'",
      "cellpose>=2.2.3; extra == 'extra'",
      "flowsom; extra == 'extra'",
      "hydra-colorlog>=1.2.0; extra == 'extra'",
      "hydra-core>=1.2.0; extra == 'extra'",
      "hydra-submitit-launcher>=1.2.0; extra == 'extra'",
      "instanseg-torch>=0.0.8; extra == 'extra'",
      "ipython; extra == 'extra'",
      "ipywidgets; extra == 'extra'",
      "joypy; extra == 'extra'",
      "napari-spatialdata>=0.2.6; extra == 'extra'",
      "napari[all]>=0.4.18; extra == 'extra'",
      "nbconvert; extra == 'extra'",
      "opencv-python; extra == 'extra'",
      "scikit-learn>=1.3.1; extra == 'extra'",
      "spatialdata-plot<0.2.9; extra == 'extra'",
      "squidpy; extra == 'extra'",
      "submitit>=1.4.5; extra == 'extra'",
      "supervenn>=0.5.0; extra == 'extra'",
      "textalloc; extra == 'extra'",
      "tqdm; extra == 'extra'",
      "instanseg-torch>=0.0.8; extra == 'instanseg'",
      "napari-spatialdata>=0.2.6; extra == 'napari'",
      "napari[all]>=0.4.18; extra == 'napari'",
      "bokeh; extra == 'notebook'",
      "ipython; extra == 'notebook'",
      "ipywidgets; extra == 'notebook'",
      "joypy; extra == 'notebook'",
      "nbconvert; extra == 'notebook'",
      "spatialdata-plot<0.2.9; extra == 'notebook'",
      "supervenn>=0.5.0; extra == 'notebook'",
      "textalloc; extra == 'notebook'",
      "tqdm; extra == 'notebook'",
      "opencv-python; extra == 'opencv'",
      "cellpose>=2.2.3; extra == 'segmentation'",
      "instanseg-torch>=0.0.8; extra == 'segmentation'",
      "datasets; extra == 'test'",
      "opencv-python; extra == 'test'",
      "pytest; extra == 'test'",
      "pytest-cov; extra == 'test'",
      "pytest-qt; extra == 'test'",
      "tox; extra == 'test'"
    ],
    "package_metadata_description": "<!-- These badges won't work while the GitHub repo is private:\n[![License BSD-3](https://img.shields.io/pypi/l/harpy.svg?color=green)](https://github.com/saeyslab/harpy/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/harpy-analysis.svg?color=green)](https://python.org)\n[![codecov](https://codecov.io/gh/saeyslab/harpy/graph/badge.svg?token=7UXMDWVYFZ)](https://codecov.io/gh/saeyslab/harpy)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/harpy)](https://napari-hub.org/plugins/harpy)\n-->\n\n# **Harpy: single-cell spatial proteomics analysis that makes you happy** <img src=\"./docs/_static/img/logo.png\" align =\"right\" alt=\"\" width =\"150\"/>\n\n[![PyPI](https://img.shields.io/pypi/v/harpy-analysis.svg)](https://pypi.org/project/harpy-analysis)\n[![Build Status](https://github.com//saeyslab/harpy/actions/workflows/build.yaml/badge.svg)](https://github.com//saeyslab/harpy/actions/)\n[![documentation badge](https://readthedocs.org/projects/harpy/badge/?version=latest)](https://harpy.readthedocs.io/en/latest/)\n\nNote: This package is still under very active development.\n\n## Installation\n\n**Recommended** for end-users. Install the latest `harpy-analysis` [PyPI package](https://pypi.org/project/harpy-analysis) with the `extra` dependencies in a local Python environment:\n\n```bash\nuv venv --python=3.12 # set python version\nsource .venv/bin/activate # activate the virtual environment\nuv pip install 'harpy-analysis[extra]' # use uv to pip install dependencies\npython -c 'import harpy; print(harpy.__version__)' # check if the package is installed\n```\n\nIf you're a developer, read the contribution guide. Checkout the docs for more [installation instructions](https://github.com/saeyslab/harpy/blob/main/docs/installation.md).\n\n## Tutorials\n\nTutorials are available [here](https://harpy.readthedocs.io/en/latest/).\n\n## Usage\n\n[Learn](https://github.com/saeyslab/harpy/blob/main/docs/usage.md) how Harpy can be integrated into your workflow in different ways.\n\n## Contributing\n\nSee [here](https://github.com/saeyslab/harpy/blob/main/docs/contributing.md) for info on how to contribute to Harpy.\n\n## References\n\n- https://github.com/ashleve/lightning-hydra-template\n\n## License\n\nCheck the [license](https://github.com/saeyslab/harpy/blob/main/LICENSE). Harpy is free for academic usage.\nFor commercial usage, please contact Saeyslab.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/saeyslab/harpy/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Wizard"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-toothfairy-annotator",
    "name": "napari-toothfairy-annotator",
    "display_name": "ToothFairy Annotator",
    "version": "0.0.18",
    "created_at": "2024-03-28",
    "modified_at": "2025-05-28",
    "authors": [
      "Luca Lumetti"
    ],
    "author_emails": [
      "lumetti.luca@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-toothfairy-annotator/",
    "home_github": "https://github.com/LucaLumetti/napari-toothfairy-annotator",
    "home_other": null,
    "summary": "The plugin employed to annotate volumes employed in the ToothFairy Challenges",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-toothfairy-annotator\n\n[![License MIT](https://img.shields.io/pypi/l/napari-toothfairy-annotator.svg?color=green)](https://github.com/LucaLumetti/napari-toothfairy-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-toothfairy-annotator.svg?color=green)](https://pypi.org/project/napari-toothfairy-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-toothfairy-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/LucaLumetti/napari-toothfairy-annotator/workflows/tests/badge.svg)](https://github.com/LucaLumetti/napari-toothfairy-annotator/actions)\n[![codecov](https://codecov.io/gh/LucaLumetti/napari-toothfairy-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/LucaLumetti/napari-toothfairy-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-toothfairy-annotator)](https://napari-hub.org/plugins/napari-toothfairy-annotator)\n\nThe plugin employed to annotate volumes employed in the ToothFairy 2 Challenge\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-toothfairy-annotator` via [pip]:\n\n    pip install napari-toothfairy-annotator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/LucaLumetti/napari-toothfairy-annotator.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-toothfairy-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/LucaLumetti/napari-toothfairy-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Annotator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tiff",
    "name": "napari-tiff",
    "display_name": "napari-tiff",
    "version": "0.1.4",
    "created_at": "2024-02-14",
    "modified_at": "2025-05-27",
    "authors": [
      "Genevieve Buckley",
      "napari-tiff contributors"
    ],
    "author_emails": [
      "napari core developers <napari-core-devs@googlegroups.com>"
    ],
    "license": "Copyright (c) 2020, Genevieve ...",
    "home_pypi": "https://pypi.org/project/napari-tiff/",
    "home_github": "https://github.com/napari/napari-tiff",
    "home_other": null,
    "summary": "official napari tiff reader and writer.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "tifffile[codecs,zarr]>=2024.7.21",
      "build; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "tox; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-tiff\n\n[![License](https://img.shields.io/pypi/l/napari-tiff.svg?color=green)](https://github.com/napari/napari-tiff/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tiff.svg?color=green)](https://pypi.org/project/napari-tiff)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tiff.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari/napari-tiff/workflows/test-and-deploy/badge.svg)](https://github.com/napari/napari-tiff/actions)\n[![codecov](https://codecov.io/gh/napari/napari-tiff/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/napari-tiff)\n\nA napari plugin for tiff images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-tiff` via [pip]:\n\n    pip install napari-tiff\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-tiff\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/napari/napari-tiff/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.qpi",
      "*.pcoraw",
      "*.qptiff",
      "*.ptiff",
      "*.seq",
      "*.tf2",
      "*.ndpi",
      "*.zip",
      "*.tiff",
      "*.ome.tif",
      "*.tif",
      "*.bif",
      "*.gel",
      "*.stk",
      "*.tf8",
      "*.scn",
      "*.zif",
      "*.ptif",
      "*.svs",
      "*.eer",
      "*.lsm",
      "*.btf"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "pssr",
    "name": "pssr",
    "display_name": "PSSR2",
    "version": "2.4.0",
    "created_at": "2024-06-01",
    "modified_at": "2025-05-22",
    "authors": [
      "Hayden Stites"
    ],
    "author_emails": [],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/pssr/",
    "home_github": "https://github.com/ucsdmanorlab/PSSR2",
    "home_other": null,
    "summary": "Point-Scanning Super-Resolution 2",
    "categories": [],
    "package_metadata_requires_python": ">=3.9,<4.0",
    "package_metadata_requires_dist": [
      "czifile (>=2019.7.2,<2020.0.0)",
      "magicgui (>=0.6.0,<=0.8.2)",
      "napari (>=0.4.13,<0.5.0) ; extra == \"napari\"",
      "numpy (>=1.22.4,<2.0.0)",
      "pillow (>=9.1.0)",
      "psutil (>=5.0.0)",
      "pytorch-msssim (>=1.0.0,<2.0.0)",
      "scikit-image (>=0.18.0)",
      "scikit-optimize (>=0.9.0)",
      "tifffile (>=2019.7.26)",
      "timm (>=0.8.0)",
      "torch (>=1.11.0)",
      "tqdm (>=4.0.0,<5.0.0)"
    ],
    "package_metadata_description": "# Point-Scanning Super-Resolution 2 (**PSSR2**)\n\n**PSSR2** is a user-friendly [PyTorch](https://pytorch.org)-based workflow for super-resolution tasks using microscopy images.\nThis is the official reimplementation and extention of the methods described in the original paper: [Deep learning-based point-scanning super-resolution imaging](https://www.nature.com/articles/s41592-021-01080-z).\n**PSSR2** contains various improvements from its predecessor, which are elaborated in the following manuscript:\n[PSSR2: a user-friendly Python package for democratizing deep learning-based point-scanning super-resolution microscopy](https://bmcmethods.biomedcentral.com/articles/10.1186/s44330-024-00020-5).\nIf you utilize **PSSR2** in your publication, please consider [citing it](https://bmcmethods.biomedcentral.com/articles/10.1186/s44330-024-00020-5#citeas).\n\nThe functionality of **PSSR2** is accessible in three ways:\n\n- Directly through the [Python package](https://pypi.org/project/pssr)\n- Through the integrated [Command Line Interface](https://ucsdmanorlab.github.io/PSSR2/reference/CLI.html)\n- Through the integrated [Napari plugin](https://ucsdmanorlab.github.io/PSSR2/guide/napari.html)\n\nThe **PSSR2** User Guide and full API Reference is available in the [PSSR2 Documentation](https://ucsdmanorlab.github.io/PSSR2).\n\nIf you have never used **PSSR2** before, [Getting Started](https://ucsdmanorlab.github.io/PSSR2/guide/start.html) outlines installation and basic usage.\nFull reference and explanations of all **PSSR2** tools is available in [API Reference](https://ucsdmanorlab.github.io/PSSR2/reference/api.html).\n\nThis package is under continuous development. All code can be found at [https://github.com/ucsdmanorlab/PSSR2](https://github.com/ucsdmanorlab/PSSR2).\nIf you experience any bugs, unexpected behaviors, or have any suggestions, make sure to [open a ticket](https://github.com/ucsdmanorlab/PSSR2/issues).\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Train Model",
      "Predict Images"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "blik",
    "name": "blik",
    "display_name": "blik",
    "version": "0.9.2",
    "created_at": "2022-04-21",
    "modified_at": "2025-05-16",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "Lorenzo Gaifas <brisvag@gmail.com>"
    ],
    "license": "GPLv3",
    "home_pypi": "https://pypi.org/project/blik/",
    "home_github": "https://github.com/brisvag/blik",
    "home_other": null,
    "summary": "Python tool for visualising and interacting with cryo-ET and subtomogram averaging data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "cryohub>=0.6.4",
      "cryotypes>=0.2.0",
      "dask",
      "einops",
      "magicgui>=0.4.0",
      "morphosamplers[segment]>=0.0.10",
      "numpy",
      "packaging",
      "pandas",
      "pydantic",
      "scipy",
      "napari-label-interpolator>=0.1.1; extra == 'all'",
      "napari-properties-plotter; extra == 'all'",
      "napari-properties-viewer; extra == 'all'",
      "napari[all]>=0.6.0; extra == 'all'",
      "black; extra == 'dev'",
      "ipython; extra == 'dev'",
      "mypy; extra == 'dev'",
      "napari[all]>=0.6.0; extra == 'dev'",
      "pdbpp; extra == 'dev'",
      "pre-commit; extra == 'dev'",
      "pytest-cov; extra == 'dev'",
      "pytest-qt; extra == 'dev'",
      "pytest>=6.0; extra == 'dev'",
      "rich; extra == 'dev'",
      "ruff; extra == 'dev'",
      "napari[all]>=0.6.0; extra == 'test'",
      "pytest-cov; extra == 'test'",
      "pytest-qt; extra == 'test'",
      "pytest>=6.0; extra == 'test'"
    ],
    "package_metadata_description": "![logo](https://github.com/brisvag/blik/raw/main/docs/images/logo.png)\n\n# blik\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10090438.svg)](https://zenodo.org/doi/10.5281/zenodo.10090438)\n[![Paper DOI](https://zenodo.org/badge/DOI/10.1371/journal.pbio.3002447.svg)](https://doi.org/10.1371/journal.pbio.3002447)\n[![License](https://img.shields.io/pypi/l/blik.svg?color=green)](https://github.com/brisvag/blik/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/blik.svg?color=green)](https://pypi.org/project/blik)\n[![Python Version](https://img.shields.io/pypi/pyversions/blik.svg?color=green)](https://python.org)\n[![CI](https://github.com/brisvag/blik/actions/workflows/ci.yml/badge.svg)](https://github.com/brisvag/blik/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/brisvag/blik/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/blik)\n\n\n![blik showcase](https://private-user-images.githubusercontent.com/23482191/361246457-b7447060-7ccd-4a8c-a41c-55c1678bf089.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQ2MDQ0OTksIm5iZiI6MTcyNDYwNDE5OSwicGF0aCI6Ii8yMzQ4MjE5MS8zNjEyNDY0NTctYjc0NDcwNjAtN2NjZC00YThjLWE0MWMtNTVjMTY3OGJmMDg5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODI1VDE2NDMxOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkZTAxMmU0MjViNjA4NTRmMWRlYzRhYmJkYjNkNWRiNjcxZjRjYWI1MWJkYmMxZmFiZjZmNzFhZTE0ODkwY2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ye5hVTZ-yociOCArw2_KSlvde1MZCQuVYH2LQgul4B0)\n\n**`blik`** is a tool for visualising and interacting with cryo-ET and subtomogram averaging data. It leverages the fast, multi-dimensional [napari viewer](https://napari.org) and the scientific python stack.\n\n**DISCLAIMER**: this package is in development phase. Expect bugs and crashes. Please, report them on the issue tracker and ask if anything is unclear!\n\n## Installation\n\nYou can either install `blik` through the [napari plugin system](https://napari.org/plugins/index.html), through pip, or get both napari and blik directly with:\n\n```bash\npip install \"blik[all]\"\n```\n\nThe `[all]` qualifier also installs `pyqt5` as the napari GUI backend, and a few additional napari plugins that you might find useful in your workflow:\n- [napari-properties-plotter](https://github.com/brisvag/napari-properties-plotter)\n- [napari-properties-viewer](https://github.com/kevinyamauchi/napari-properties-viewer)\n- [napari-label-interpolator](https://github.com/brisvag/napari-label-interpolator)\n\n### Nightly build\n\nIf you'd like the most up to date `blik` possible, you can install directly from the `main` branch on github. This also uses napari `main`, so expect some instability!\n\n```\npip install \"git+https://github.com/brisvag/blik.git@main#egg=blik[all]\"\npip install \"git+https://github.com/napari/napari.git@main#egg=napari[all]\"\n```\n\n## Basic Usage\n\nFrom the command line:\n```bash\nnapari -w blik -- /path/to.star /path/to/mrc/files/*\n```\n\nThe `-w blik` is important for proper initialization of all the layers. Always open the main widget open to ensure nothing goes wrong!\n\n*`blik` is just `napari`*. Particles and images are exposed as simple napari layers, which can be analysed and manipulated with simple python, and most importantly other [napari plugins](https://napari-hub.org/).\n\n## Widgets\n\nThe main widget has a few functions:\n\n- `experiment`: quickly switch to a different experiment id (typically, everything related to an individual tomogram such as volume, particles and segmentations)\n- `new`: generate a new `segmentation`, a new manually-picked set of `particles`, or a new `surface`, `sphere`, or `filament picking` for segmentation, particle generation or volume resampling.\n- `add to exp`: add a layer to the currently selected `experiment` (just a shorthand for `layer.metadata['experiment_id'] = current_exp_id`)\n- `slice_thickness`: changes the slicing thickness in all dimensions in napari. Images will be averaged over that thickness, and all particles in the slice will be displayed.\n\nThere are also widgets for picking surfaces, spheres and filaments:\n\n- `surface`: process a previously picked `surface picking` layer to generate a surface mesh and distribute particles on it for subtomogram averaging, or resample a tomogram along the surface.\n- `sphere`: process a previously picked `sphere picking` layer to generate a sphere mesh and distribute particles on it for subtomogram averaging.\n- `filament`: process a previously picked `filament picking` layer to generate a filament and distribute particles on it for subtomogram averaging, or resample a tomogram along the filament.\n\n# References\n\nIf you use `blik`, please cite the repo on zenodo and the paper on Plos Biology: [https://doi.org/10.1371/journal.pbio.3002447](https://doi.org/10.1371/journal.pbio.3002447).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tbl",
      "*.map",
      "*.picks",
      "*.surf",
      "*.box",
      "*.rec",
      "*.em",
      "*.star",
      "*.mrcs",
      "*.cbox",
      "*.hdf",
      "*.mrc",
      "*.st"
    ],
    "contributions_writers_filename_extensions": [
      ".mrcs",
      ".star",
      ".surf",
      ".mrc",
      ".tbl",
      ".st",
      ".picks",
      ".rec"
    ],
    "contributions_widgets": [
      "Blik main widget",
      "Surface picking",
      "Sphere picking",
      "Filament picking",
      "Rotate selected particles",
      "File reader",
      "Bandpass filter",
      "Gaussian filter",
      "Power spectrum"
    ],
    "contributions_sample_data": [
      "Tomogram and particles of HIV VLPs"
    ]
  },
  {
    "normalized_name": "napari-omero",
    "name": "napari-omero",
    "display_name": "napari-omero",
    "version": "0.5.1",
    "created_at": "2021-06-24",
    "modified_at": "2025-05-15",
    "authors": [
      "Peter Sobolewski"
    ],
    "author_emails": [
      "Talley Lambert <talley.lambert@gmail.com>",
      "Will Moore <w.moore@dundee.ac.uk>",
      "Johannes Soltwedel <johannes_richard.soltwedel@tu-dresden.de>"
    ],
    "license": "GPL-2.0-or-later",
    "home_pypi": "https://pypi.org/project/napari-omero/",
    "home_github": "https://github.com/tlambert03/napari-omero",
    "home_other": null,
    "summary": "napari/OMERO interoperability",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "dask[array]>=2021.10.0",
      "napari>=0.5.0",
      "omero-marshal",
      "omero-py",
      "omero-rois",
      "qtpy>=1.10.0",
      "superqt>=0.6.7",
      "napari[all]; extra == 'all'",
      "ipython; extra == 'dev'",
      "mypy; extra == 'dev'",
      "napari[all]; extra == 'dev'",
      "pdbpp; extra == 'dev'",
      "pre-commit; extra == 'dev'",
      "pytest; extra == 'dev'",
      "pytest-cov; extra == 'dev'",
      "pytest-qt; extra == 'dev'",
      "pytest-regressions; extra == 'dev'",
      "pywin32; (sys_platform == 'win32') and extra == 'dev'",
      "rich; extra == 'dev'",
      "ruff; extra == 'dev'",
      "pytest; extra == 'test'",
      "pytest-cov; extra == 'test'",
      "pytest-qt; extra == 'test'",
      "pytest-regressions; extra == 'test'",
      "pywin32; (sys_platform == 'win32') and extra == 'test'"
    ],
    "package_metadata_description": "# napari-omero\n\n[![License](https://img.shields.io/pypi/l/napari-omero.svg?color=green)](https://github.com/tlambert03/napari-omero/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-omero.svg?color=green)](https://pypi.org/project/napari-omero)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-omero.svg?color=green)](https://python.org)\n[![CI](https://github.com/tlambert03/napari-omero/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/napari-omero/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/tlambert03/napari-omero/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-omero)\n[![conda-forge](https://img.shields.io/conda/vn/conda-forge/napari-omero)](https://anaconda.org/conda-forge/napari-omero)\n\nThis package provides interoperability between the\n[OMERO](https://www.openmicroscopy.org/omero/) image management platform, and\n[napari](https://github.com/napari/napari): a fast, multi-dimensional image\nviewer for python.\n\nIt provides a GUI interface for browsing an OMERO instance from within napari,\nas well as command line interface extensions for both OMERO and napari CLIs.\n\n![demo](https://github.com/tlambert03/napari-omero/blob/master/demo.gif?raw=true)\n\n## Features\n\n- GUI interface to browse remote OMERO data, with thumbnail previews.\n- Loads remote nD images from an OMERO server into napari\n- Upload annotsations (``Labels`, `Shapes` and `Points`) to OMERO.\n- Planes are loading on demand as sliders are moved (\"lazy loading\").\n- Loading of pyramidal images as napari multiscale layers\n- session management (login memory)\n- OMERO rendering settings (contrast limits, colormaps, active channels, current\n  Z/T position) are applied in napari\n\n> [!NOTE]\n> The user experience when working with remote images, particularly large multiscale (pyramidal) ones, like whole slide images, can be significantly improved by using napari 0.5.0 or newer and enabling the experimental asynchronous mode (n the GUI in `Preferences > Experimental > Render Images Asynchronously` or with the environmental variable `NAPARI_ASYNC=1`).\n\n### as a napari dock widget\n\nTo launch napari with the OMERO browser added, [install](#installation) this\npackage and run:\n\n```bash\nnapari-omero\n```\n\nThe OMERO browser widget can also be manually added to the napari viewer using the Plugins menu\nor programmatically using:\n\n```python\nimport napari\n\nviewer = napari.Viewer()\nviewer.window.add_plugin_dock_widget('napari-omero')\n\nnapari.run()\n```\n\n### as a napari reader contribution\n\nThis package provides a napari reader contribution that accepts OMERO resources as\n\"proxy strings\" (e.g. `omero://Image:<ID>`) or as [OMERO webclient\nURLS](https://help.openmicroscopy.org/urls-to-data.html).\n\n```python\nimport napari\nviewer = napari.Viewer()\n\n# omero object identifier string\nviewer.open(\"omero://Image:1\", plugin=\"napari-omero\")\n\n# or URLS: https://help.openmicroscopy.org/urls-to-data.html\nviewer.open(\"http://yourdomain.example.org/omero/webclient/?show=image-314\", plugin=\"napari-omero\")\n```\n\nthese will also work on the napari command line interface, e.g.:\n\n```bash\n# quotes are needed if using zsh\nnapari \"omero://Image:1\"\n# or\nnapari \"http://yourdomain.example.org/omero/webclient/?show=image-314\"\n```\n\n### as an OMERO CLI plugin\n\nThis package also serves as a plugin to the OMERO CLI\n\n```bash\nomero napari view Image:1\n```\n\n- ROIs created in napari can be saved back to OMERO via a \"Save ROIs\" button.\n- napari viewer console has BlitzGateway 'conn' and 'omero_image' in context.\n\n## installation\n\nWhile this package supports anything above python 3.9,\nIn practice, python support is limited by `omero-py` and `zeroc-ice`,\ncompatibility, which is limited to python <=3.12 at the time of writing.\n\n### from conda\n\nIt's easiest to install `omero-py` from conda, so the recommended procedure\nis to install everything from conda, using the `conda-forge` channel.\nFor example, to install the plugin, napari, and the default Qt backend, use:\n\n```sh\nconda install -c conda-forge napari-omero pyqt\n```\n\n### from pip\n\n`napari-omero` itself can be installed from pip, but you will still need\n`omero-py`\n\n```sh\nconda create -n omero -c conda-forge python=3.10 omero-py\nconda activate omero\npip install napari-omero[all]  # the [all] here is the same as `napari[all]`\n```\n\n## issues\n\n| ‚ùó  | This is alpha software & some things will be broken or sub-optimal!  |\n| --- | -------------------------------------------------------------------- |\n\n- experimental & definitely still buggy!  [Bug\n  reports](https://github.com/tlambert03/napari-omero/issues/new) are welcome!\n- remote loading can be very slow still... though this is not strictly an issue\n  of this plugin.  Datasets are wrapped as delayed dask stacks, and remote data\n  fetching time can be significant.  Enabling [asynchronous\n  rendering](https://napari.org/stable/guides/rendering.html#asynchronous-slicing) in\n  napari improves the subjective performance... but remote data loading\n  will likely always be a limitation here.\n  To try asyncronous loading, start the program with `NAPARI_ASYNC=1 napari-omero`\n  or look in the Preferences on the Experimental tab.\n  Also, keep an eye on the [napari progressive loading implementation progress](https://github.com/napari/napari/issues/5561).\n- For plugin developers: As napari-OMERO provides images as lazily-loaded [dask arrays](https://docs.dask.org/en/stable/array.html),\n  napari-plugins need to account for this when retrieving data from napari layers.\n  Keep in mind that forwarding the data to processing steps in plugins may lead to signficant loading\n  and processing times.\n\n## contributing\n\nContributions are welcome!  To get setup with a development environment:\n\n```bash\n# clone this repo:\ngit clone https://github.com/tlambert03/napari-omero.git\n# change into the new directory\ncd napari-omero\n# create conda environment\nconda env create -n napari-omero python=3.10 omero-py\n# activate the new env\nconda activate napari-omero\n\n# install in editable mode with dev dependencies\npip install -e \".[dev]\"      # quotes are needed on zsh\n```\n\nTo maintain good code quality, this repo uses\n[ruff](https://github.com/astral-sh/ruff),\n[mypy](https://github.com/python/mypy).\n\nTo enforce code quality when you commit code, you can install pre-commit\n\n```bash\n# install pre-commit which will run code checks prior to commits\npre-commit install\n```\n\nThe original OMERO data loader and CLI extension was created by [Will\nMoore](https://github.com/will-moore).\n\nThe napari reader plugin and GUI browser was created by [Talley\nLambert](https://github.com/tlambert03/)\n\n## release\n\nTo psuh a release to PyPI, one of the maintainers needs to do, for example:\n```sh\ngit tag -a v0.2.0 -m v0.2.0\ngit push upstream --follow-tags\n```\nThen, the workflow should handle everything!\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*omero*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "OMERO Browser",
      "Upload Annotations to OMERO"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cylindra",
    "name": "cylindra",
    "display_name": "cylindra",
    "version": "1.0.0b7",
    "created_at": "2024-05-07",
    "modified_at": "2025-05-14",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuha@med.kobe-u.ac.jp"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/cylindra/",
    "home_github": null,
    "home_other": "None",
    "summary": "Spectral analysis, simulation and subtomogram averaging of heterogenic cylindrical structures",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "impy-array>=2.4.5",
      "acryo>=0.4.17",
      "macro-kit>=0.4.6",
      "magicgui>=0.8.1",
      "magic-class>=0.7.16",
      "psygnal>=0.9.1",
      "superqt[iconify]>=0.6.1",
      "pydantic>=1.10.0",
      "pydantic-compat",
      "pyqtgraph>=0.12.4",
      "pyarrow>=11.0.0",
      "numpy>=1.23.0",
      "scipy>=1.11.3",
      "pandas>=1.5.0",
      "polars>=1.19.0",
      "scikit-image>=0.21.0",
      "napari>=0.5.1",
      "qtpy>=2.3.1",
      "qt-command-palette>=0.0.7",
      "matplotlib>=3.8.1",
      "rich>=13.6.0",
      "dask>=2023.12.1,<2025.0.0",
      "platformdirs>=4.3.6",
      "pytest ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "scikit-learn>=1.2.2 ; extra == 'testing'",
      "mrcfile>=1.3.0 ; extra == 'testing'",
      "tifffile>=2023.2.28 ; extra == 'testing'",
      "starfile!=0.5.10 ; extra == 'testing'",
      "imodmodel ; extra == 'testing'",
      "cookiecutter ; extra == 'testing'",
      "maturin>=1.5.0,<2.0.0 ; extra == 'testing'",
      "pyqt5 ; extra == 'all'",
      "scikit-learn>=1.2.2 ; extra == 'all'",
      "mrcfile>=1.3.0 ; extra == 'all'",
      "tifffile>=2023.2.28 ; extra == 'all'",
      "starfile ; extra == 'all'",
      "imodmodel ; extra == 'all'",
      "cookiecutter ; extra == 'all'",
      "pyqt5 ; extra == 'docs'",
      "mrcfile>=1.3.0 ; extra == 'docs'",
      "mkdocs>=1.5.3 ; extra == 'docs'",
      "mkdocs-autorefs>=0.5.0 ; extra == 'docs'",
      "mkdocs-gen-files>=0.5.0 ; extra == 'docs'",
      "mkdocs-material>=9.5.2 ; extra == 'docs'",
      "mkdocs-material-extensions>=1.3.1 ; extra == 'docs'",
      "mkdocstrings>=0.24.0 ; extra == 'docs'",
      "mkdocstrings-python>=1.7.5 ; extra == 'docs'",
      "maturin>=1.5.0,<2.0.0 ; extra == 'docs'"
    ],
    "package_metadata_description": "[![BSD 3-Clause License](https://img.shields.io/pypi/l/cylindra.svg?color=green)](https://github.com/hanjinliu/cylindra/blob/main/LICENSE)\n[![Python package index download statistics](https://img.shields.io/pypi/dm/cylindra.svg)](https://pypistats.org/packages/cylindra)\n[![PyPI version](https://badge.fury.io/py/cylindra.svg)](https://badge.fury.io/py/cylindra)\n[![codecov](https://codecov.io/gh/hanjinliu/cylindra/graph/badge.svg?token=X1F259JYT5)](https://codecov.io/gh/hanjinliu/cylindra)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/cylindra)](https://napari-hub.org/plugins/cylindra)\n\n![](https://github.com/hanjinliu/cylindra/blob/main/resources/fig.png)\n\n# cylindra\n\n`cylindra` is a GUI-integrated cryo-ET image analysis tool for cylindric periodic\nstructures such as microtubules.\n\n### [&rarr; Documentation](https://hanjinliu.github.io/cylindra/)\n\n## Installation\n\n- Use `pip`\n\n```shell\npip install cylindra -U\n```\n\n- From source\n\nIf you install from the source, you'll need Rust to compile a part of the code.\n\n```shell\ngit clone git+https://github.com/hanjinliu/cylindra\ncd cylindra\npip install -e .\n```\n\n## Usage\n\n#### Launch GUI\n\n- From shell\n\n  ```shell\n  cylindra\n  ```\n\n- From a Python interpreter\n\n  ```python\n  from cylindra import start\n\n  # launch a napari viewer with a cylindra dock widget.\n  ui = start()\n  ```\n\n#### Command line interface\n\n`cylindra` is implemented with some basic command line interface (CLI).\n\n```shell\ncylindra --help\n```\n\n## Implemented Functions\n\n- Automatic/manual fitting of splines to cylindrical structures in 3D.\n- Analyze lattice structures (such as lattice spacing and skew angle) using Cylindric\n  Fourier transformation.\n- Automatic determination of polarity, protofilament number etc.\n- Monomer mapping along splines for subtomogram averaging and alignment.\n- Microtubule seam search with or without binding proteins.\n- Subtomogram alignment with 2D constraint.\n- Tomogram simulation of cylindric structures.\n- Efficient manual picking along cylindrical structures.\n\n## Prerequisite and Recommendations\n\n- **Python &ge; 3.10**. This project follows [spec-0000](https://scientific-python.org/specs/spec-0000/).\n- **Sufficient memory size**. Most of the intense calculations are done out-of-core\n  using `dask`, so that you can even run on 8-GB memory PC in many cases. However,\n  larger memory size will make parallel processing more efficient. &ge;32 GB is\n  recommended.\n- **Images should be loaded from SSD**. Raw image stacks are loaded lazily in most of\n  the processes. Loading from HDD will slow down many analyses as well. In the latest version, you can use \"Cache image on SSD\" option to directly analyze tomograms stored in HDD.\n\n## Issues\n\nIf you encountered any bugs or have any requests, feel free to\n[report an issue](https://github.com/hanjinliu/cylindra/issues/new).\n(We'll appreciate if you find some methods are over-fitted to microtubules and do not\nwork well on other cylindric structures)\n\nFor better reproducibility, please copy your environments from `Others > cylindra info`\nand the recorded macro from `Others > Macro > Show macro`.\n\n## Citation\n\nIf you find `cylindra` useful in your work, please consider citing [our paper](https://www.biorxiv.org/content/10.1101/2024.04.30.591984v1).\n\n```\nHeterogeneous local structures of the microtubule lattice revealed by cryo-ET and non-averaging analysis\nHanjin Liu, Hiroshi Yamaguchi, Masahide Kikkawa, Tomohiro Shima\nbioRxiv 2024.04.30.591984; doi: https://doi.org/10.1101/2024.04.30.591984\n```\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "cylindra"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cut-detector",
    "name": "cut-detector",
    "display_name": "Cut Detector",
    "version": "1.6.5",
    "created_at": "2023-10-17",
    "modified_at": "2025-05-12",
    "authors": [
      "Thomas Bonte"
    ],
    "author_emails": [
      "thomas.bonte@mines-paristech.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/cut-detector/",
    "home_github": "https://github.com/15bonte/cut-detector",
    "home_other": null,
    "summary": "Automatic Cut Detector",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "cellpose==3.0.9",
      "pyimagej",
      "cnn_framework==0.0.16",
      "magicgui",
      "pydantic==1.10.12",
      "xmltodict",
      "shapely",
      "aicsimageio==4.14.0",
      "fsspec==2023.6.0",
      "charset-normalizer==3.3.0",
      "napari[all]",
      "laptrack==0.16.2",
      "scikit-learn==1.5.0",
      "numba>=0.59.1",
      "scipy<=1.14.1",
      "tensorflow<=2.18.0",
      "munch",
      "plotly",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# Cut Detector\n\n[![License BSD-3](https://img.shields.io/pypi/l/cut-detector.svg?color=green)](https://github.com/15bonte/cut-detector/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/cut-detector.svg?color=green)](https://pypi.org/project/cut-detector)\n[![Python Version](https://img.shields.io/pypi/pyversions/cut-detector.svg?color=green)](https://python.org)\n[![tests](https://github.com/15bonte/cut-detector/workflows/tests/badge.svg)](https://github.com/15bonte/cut-detector/actions)\n[![codecov](https://codecov.io/gh/15bonte/cut-detector/branch/main/graph/badge.svg)](https://codecov.io/gh/15bonte/cut-detector)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/cut-detector)](https://napari-hub.org/plugins/cut-detector)\n\nAutomatic micro-tubule cut detector.\n\nhttps://github.com/user-attachments/assets/2af2e1a6-adf9-4d63-a353-e190c4814d83\n\n---\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n<video width=\"640\" height=\"480\" controls>\n  <source src=\"https://github.com/15bonte/cut-detector-models/blob/main/demo.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n## Installation\n\n### Conda environment\n\nIt is highly recommended to create a dedicated conda environment, by following these few steps:\n\n1. Install an [Anaconda] distribution of Python. Note you might need to use an anaconda prompt if you did not add anaconda to the path.\n\n2. Open an Anaconda prompt as admin to create a new environment using [conda]. We advice to use python 3.10 and conda 23.10.0, to get conda-libmamba-solver as default solver.\n\n```bash\nconda create --name cut_detector python=3.10 conda=23.10.0\nconda activate cut_detector\n```\n\n### Package installation\n\nOnce in a dedicated environment, our package can be installed via [pip]:\n\n```bash\npip install cut_detector\n```\n\nAlternatively, you can clone the github repo to access to playground scripts.\n\n```bash\ngit clone https://github.com/15bonte/cut-detector.git\ncd cut-detector\npip install -e .\n```\n\n### GPU\n\nWe highly recommend to use GPU to speed up segmentation. To use your NVIDIA GPU, the first step is to download the dedicated driver from [NVIDIA].\n\nNext we need to remove the CPU version of torch:\n\n```bash\npip uninstall torch\n```\n\nThe GPU version of torch to be installed can be found [here](https://pytorch.org/get-started/locally/). You may choose the CUDA version supported by your GPU, and install it with conda. This package has been developed with the version 11.6, installed with this command:\n\n```bash\nconda install pytorch==1.12.1 torchvision pytorch-cuda=11.6 -c pytorch -c nvidia\n```\n\n## Update\n\nTo update cut-detector to the latest version, open an Anaconda prompt and use the following commands:\n\n```bash\nconda activate cut_detector\npip install cut-detector --upgrade\n```\n\n## Definitions\n\nEach detected cell division is labeled with one of the following categories:\n\n- NORMAL: Division happening as expected, where (at least) 1 micro-tubule cut is detected.\n- NO_MID_BODY_DETECTED: Along the cell division, no mid-body was detected on the MKLP1 channel. This category encompasses different cases: the detection may have failed, the mid-body may not express the fluorescence, or this may not actually be a division.\n- MORE_THAN_TWO_DAUGHTER_TRACKS: Tripolar division. This category encompasses both actual tripolar divisions and wrong identifications of daughter cells (mainly caused by segmentation issues).\n- NEAR_BORDER: Division close to the border of the image, hence ignored as it is likely to be difficult to detect micro-tubule cuts. A division is classified as NEAR_BORDER as soon as the distance between 1 detected mid-body and the border of the image is less than 20px.\n- NO_CUT_DETECTED: Division whose mid-body was detected, but with all micro-tubule bridges classified as \"No cut\". Likely to be at the end of the video, cells dying before the end of division, or cells going out of frame.\n- TOO_SHORT_CUT: First micro-tubule cut detected before or at 50 minutes. Ignored as this is very unlikely, so it is probably caused by a wrong division detection.\n\nDivision movies start at the maximum between:\n\n- Mother cell start frame\n- 10 frames before the end of metaphase\n\nDivision movies end at the minimum between:\n\n- Last frame of any of the daughter cells\n- Metaphase of any of the daughter cells\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\nScripts required to improve any of Cut Detector tasks can be found in the folder [developers].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"cut-detector\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/15bonte/cut-detector/issues\n[developers]: https://github.com/15bonte/cut-detector/tree/main/developers\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[Anaconda]: https://www.anaconda.com/products/distribution\n[Fiji]: https://imagej.net/software/fiji/\n[NVIDIA]: https://www.nvidia.com/Download/index.aspx?lang=en-us\n[conda]: https://docs.conda.io/en/latest/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Single Video",
      "Folder",
      "Divisions Matching",
      "Distribution Comparison"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-molecule-reader",
    "name": "napari-molecule-reader",
    "display_name": "napari molecule reader",
    "version": "0.1.4",
    "created_at": "2021-12-21",
    "modified_at": "2025-05-12",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "brisvag@gmail.com"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/napari-molecule-reader/",
    "home_github": "https://github.com/brisvag/napari-molecule-reader",
    "home_other": null,
    "summary": "A napari plugin that read molecular structure files.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "atomium",
      "numpy",
      "pandas",
      "scipy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-molecule-reader\n\n[![License](https://img.shields.io/pypi/l/napari-molecule-reader.svg?color=green)](https://github.com/brisvag/napari-molecule-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-molecule-reader.svg?color=green)](https://pypi.org/project/napari-molecule-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-molecule-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/brisvag/napari-molecule-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-molecule-reader/actions)\n[![codecov](https://codecov.io/gh/brisvag/napari-molecule-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-molecule-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-molecule-reader)](https://napari-hub.org/plugins/napari-molecule-reader)\n\nA napari plugin that read molecular structure files. It reads PDB and MMCIF files using [`atomium`](https://github.com/samirelanduk/atomium), expanding molecular assemblies to a full visualization. Data is loaded into napari as `Points` for ball representation and `Vectors` for stick representation. If multiple models or assemblies are detected, they will be loaded as separate objects.\n\nhttps://user-images.githubusercontent.com/23482191/150109390-bd7fb3b4-79b4-43da-aafc-20921714df25.mp4\n\nTODO list:\n- [] handle alternate locations (i.e: different conformations in the same pdb model)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-molecule-reader` via [pip]:\n\n    pip install napari-molecule-reader\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/brisvag/napari-molecule-reader.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-molecule-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/brisvag/napari-molecule-reader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.cif",
      "*.pdb"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-assistant",
    "name": "napari-assistant",
    "display_name": "napari-assistant",
    "version": "0.6.0",
    "created_at": "2022-03-05",
    "modified_at": "2025-05-02",
    "authors": [
      "Robert Haase",
      "Ryan Savill"
    ],
    "author_emails": [
      "robert.haase@uni-leipzig.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-assistant/",
    "home_github": "https://github.com/haesleinhuepf/napari-assistant",
    "home_other": null,
    "summary": "A pocket calculator like interface to image processing in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "toolz",
      "napari>=0.4.14",
      "magicgui",
      "numpy!=1.19.4",
      "pyperclip",
      "loguru",
      "jupytext",
      "jupyter",
      "pandas",
      "napari-time-slicer>=0.4.8",
      "napari-workflows>=0.2.10"
    ],
    "package_metadata_description": "# napari-assistant\n[![License](https://img.shields.io/pypi/l/napari-assistant.svg?color=green)](https://github.com/haesleinhuepf/napari-assistant/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-assistant.svg?color=green)](https://pypi.org/project/napari-assistant)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-assistant.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-assistant/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-assistant/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-assistant)\n[![Development Status](https://img.shields.io/pypi/status/napari-assistant.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-assistant)](https://napari-hub.org/plugins/napari-assistant)\n[![DOI](https://zenodo.org/badge/463875112.svg)](https://zenodo.org/badge/latestdoi/463875112)\n\n\nThe napari-assistant is a [napari](https://github.com/napari/napari) meta-plugin for building image processing workflows. \n\n## Usage\n\nAfter installing one or more napari plugins that use the napari-assistant as user interface, you can start it from the \nmenu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line. \n\nBy clicking on the buttons in the assistant, you can setup a workflow for processing the images.\n\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/napari-assistant-screenshot.png)\n\nWhile setting up your workflow, you can at any point select a layer from the layer list (1) and change the parameters of\nthe corresponding operation (2). The layer will update when you change parameters and also all subsequent operations. \nYou can also vary which operation is applied to the image (3). Also make sure the right input image layer is selected (4).\n\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/design_workflows.png)\n\n### Saving and loading workflows\n\nYou can also save and load workflows to disk. \n\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/save_and_load.png)\n\nAfter loading a workflow, make sure that the right input images are selected.\n\n### Code generation\n\nThe napari-assistant allows exporting the given workflow as Python script and Jupyter Notebook. \n\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/code_generator.png)\n\nFurthermore, if you have the [napari-script-editor](https://www.napari-hub.org/plugins/napari-script-editor) installed,\nyou can also send the current workflow as code to the script editor from the same menu.\n\n![img.png](https://github.com/haesleinhuepf/napari-assistant/raw/main/docs/napari_script_editor.png)\n\n### Plugin generation\n\nThere is also a Napari plugin generator available. Check out [its documentation](https://github.com/haesleinhuepf/napari-assistant-plugin-generator) to learn how napari-assistant compatible plugins can be generated directly from within the assistant.\n\n## Installation\n\nIt is recommended to install the napari-assistant via one of the plugins that use it as graphical user interface.\nYou find a complete list of plugins that use the assistant [on the napari-hub](https://www.napari-hub.org/?search=napari-assistant&sort=relevance).\nMultiple of these plugins come bundled when installing [devbio-napari](https://www.napari-hub.org/plugins/devbio-napari).\nNote: This plugin is not compatible with napari 0.6.0 and later.\n\n## For developers\n\nIf you want to make your napari-plugin accessible from the napari-assistant, consider programming functions with a simple \ninterface that consume images, labels, integers, floats and strings. Annotate input and return types, e.g. like this:\n```python\ndef example_function_widget(image: \"napari.types.ImageData\") -> \"napari.types.LabelsData\":\n    from skimage.filters import threshold_otsu\n    binary_image = image > threshold_otsu(image)\n\n    from skimage.measure import label\n    return label(binary_image)\n```\n\nFurthermore, please add your function to the napari.yaml which uses [npe2](https://github.com/napari/npe2):\n```\nname: napari-npe2-test\ndisplay_name: napari-npe2-test\ncontributions:\n  commands: \n    - id: napari-npe2-test.make_magic_widget\n      python_name: napari_npe2_test._widget:example_magic_widget\n      title: Make example magic widget\n  widgets:\n    - command: napari-npe2-test.make_magic_widget\n      display_name: Segmentation / labeling > Otsu Labeling (nnpe2t)\n```\n\nTo put it in the right button within the napari-assistant, please use one of the following prefixes for the `display_name`:\n* `Filtering / noise removal > `\n* `Filtering / background removal > `\n* `Filtering > `\n* `Image math > `\n* `Transform > `\n* `Projection > `\n* `Segmentation / binarization > `\n* `Segmentation / labeling > `\n* `Segmentation post-processing > `\n* `Measurement > `\n* `Label neighbor filters > `\n* `Label filters > `\n* `Visualization > `\n\nYou find a fully functional example [here](https://github.com/haesleinhuepf/napari-npe2-test).\n\nLast but not least, to make your napari-plugin is listed in the napari-hub when searching for \"napari-assistant\", make sure\nyou mention it in your `readme`.\n\n## Feedback welcome!\n\nThe napari-assistant is developed in the open because we believe in the open source community. Feel free to drop feedback as [github issue](https://github.com/haesleinhuepf/napari-assistant/issues) or via [image.sc](https://image.sc)\n\n## Contributing\n\nContributions are very welcome. Please ensure\nthe test coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-assistant\" is free and open source software\n\n## Acknowledgements\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany‚Äôs Excellence Strategy ‚Äì EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden. \nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Assistant",
      "_split_stack"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "empanada-napari",
    "name": "empanada-napari",
    "display_name": "empanada-napari",
    "version": "1.2",
    "created_at": "2022-03-04",
    "modified_at": "2025-04-28",
    "authors": [
      "Madeline Barry",
      "Abhishek Bhardwaj",
      "Ryan Conrad"
    ],
    "author_emails": [
      "abhishek.bhardwaj@nih.gov"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/empanada-napari/",
    "home_github": "https://github.com/volume-em/empanada-napari",
    "home_other": null,
    "summary": "Napari plugin of algorithms for Panoptic Segmentation of organelles in EM",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "torch>=1.10",
      "torchvision>=0.2",
      "zarr>=2.12",
      "albumentations>=1.2",
      "pyyaml",
      "cztile",
      "mlflow",
      "opencv-python==4.9.0.80",
      "opencv-python-headless==4.9.0.80",
      "napari==0.4.18",
      "numpy==1.22",
      "napari-plugin-engine>=0.1.4",
      "scikit-image>=0.19",
      "numba==0.58.1",
      "imagecodecs",
      "openpyxl",
      "imagehash",
      "simpleitk",
      "tqdm"
    ],
    "package_metadata_description": "# empanada-napari\n\n> [!IMPORTANT]\n> **New Version 1.2 Announcement!**\n> * New Models:\n>   * NucleoNet: A base model for EM instance nucleus segmentation\n>   * DropNet: A base model for EM Lipid Droplet instance segmentation\n> * New modules \n>   * Archive Model - archives the model into a hidden archive folder \n>   * Create Tiles - Split big images winto small tiles (with Mask/ optional)\n>   * Merge Tiles - Opposite of split tiles, create full image from tiles created by create tiles\n>   * Morph labels - applies morphological operations to labels\n>   * Count labels - counts and lists the label IDs within the dataset\n>   * Filter labels - removes small pixel/voxel area labels or labels touching the image boundaries\n>   * Export and import a model - export or import locally saved model files to use within empanada-napari\n> * Updated modules\n>   * 2D Inference - now allows ROI inference and filling holes in segmentation label maps\n>   * 3D Inference - allows erosion, dilation and fill holes in segmentation to help tackle big split merge errors\n>   * Export segmentations - now allows 3D segmentations to be exported as a single .tiff image\n>   * Pick and save finetune/training patches - now allows paired grayscale and label mask images to create training patches \n>   * Split label - now allows users to specify new label IDs \n> * Updated documentation\n>   * Check out the updated documentation [here](https://empanada.readthedocs.io/en/latest/index.html)!\n\n**The paper describing this work is now available [on Cell Systems](https://www.cell.com/cell-systems/fulltext/S2405-4712(22)00494-X).**\n\n**Documentation for the plugin, including more detailed installation instructions, can be found [here](https://empanada.readthedocs.io/en/latest/empanada-napari.html).**\n\nempanada is a tool for deep learning-based panoptic segmentation of 2D and 3D electron microscopy images of cells.\nThis plugin allows the running of panoptic segmentation models trained in empanada within [napari](https://napari.org).\nFor help with this plugin please open an [issue](https://github.com/volume-em/empanada-napari/issues), for issues with napari specifically\nraise an [issue here instead](https://github.com/napari/napari/issues).\n\n## Implemented Models\n\n  - *MitoNet*: A generalist mitochondrial instance segmentation model.\n\n## Example Datasets\n\nVolume EM datasets for benchmarking mitochondrial instance segmentation are available from\n[EMPIAR-10982](https://www.ebi.ac.uk/empiar/EMPIAR-10982/).\n\n## Installation\n\n### New Users\n\nIf you've previously installed and used conda, it's recommended (but optional) to create a new virtual \nenvironment in order to avoid dependency conflicts. \n\nempanada-napari works with python=3.9 or lower\n\nIt's recommended to have installed napari through [conda](https://docs.conda.io/en/latest/miniconda.html). Then to install this plugin:\n\n```shell\npip install empanada-napari==1.2\n```\n\nLaunch napari:\n\n```shell\nnapari\n```\n\nLook for empanada-napari under the \"Plugins\" menu.\n\n\n### Returning Users\n\nIf you installed napari into a virtual environment as suggested in the original release documentation, \nbe sure to activate it and uninstall the old empanada-napari.\n\n```shell\npip uninstall empanada-napari\n```\n\nThen install the newest version:\n\n```shell\npip install empanada-napari==1.2\n```\n\n\n![empanada](images/demo.gif)\n\n## GPU Support\n\n**Note: Mac doesn't support NVIDIA GPUS. This section only applies to Windows and Linux systems.**\n\nAs for any deep learning models, having a GPU installed on your system will significantly\nincrease model throughput (although we ship CPU optimized versions of all models with the plugin).\n\nThis plugin relies on torch for running models. If a GPU was found on your system, then you will see that the\n\"Use GPU\" checkbox is checked by default in the \"2D Inference\" and \"3D Inference\" plugin widgets. Or if when running\ninference you see a message that says \"Using CPU\" in the terminal that means a GPU is not being used.\n\nMake sure that GPU drivers are correctly installed. In terminal or command prompt:\n\n```shell\nnvidia-smi\n```\n\nIf this returns \"command not found\" then you need to [install the driver from NVIDIA](https://www.nvidia.com/download/index.aspx). Instead, if\nif the driver is installed correctly, you may need to switch to the GPU enabled version of torch.\n\nFirst, uninstall the current version of torch:\n\n```shell\npip uninstall torch\n```\n\nThen [install torch >= 1.10 using conda for your system](https://pytorch.org/get-started/locally/).\nThis command should work:\n\n```shell\nconda install pytorch cudatoolkit=11.3 -c pytorch\n```\n\n## Citing this work\n\nIf you use results generated by this plugin in a publication, please cite:\n\n```bibtex\n@article { Conrad2023,\n    author = {Conrad, Ryan and Narayan, Kedar},\n    title = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset},\n    journal = {Cell Systems},\n    year = {2023},\n    month = {Jan},\n    day = {18},\n    publisher = {Elsevier},\n    volume = {14},\n    number = {1},\n    pages = {58-71.e5},\n    issn = {2405-4712},\n    doi = {10.1016/j.cels.2022.12.006},\n    url = {https://doi.org/10.1016/j.cels.2022.12.006}\n}\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "2D Inference (Parameter Testing)",
      "3D Inference",
      "Export Segmentations",
      "Count Labels",
      "Merge Labels",
      "Delete Labels",
      "Morph Labels",
      "Split Labels",
      "Filter Labels",
      "Jump to label",
      "Find next available label",
      "Pick finetune/training patches",
      "Save finetune/training patches",
      "Finetune a model",
      "Train a model",
      "Register a model",
      "Get model info",
      "Export a model",
      "Import a model",
      "Archive a model",
      "Create Tiles",
      "Merge Tiles"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sam2long",
    "name": "napari-sam2long",
    "display_name": "SAM2Long",
    "version": "1.0.3",
    "created_at": "2025-04-14",
    "modified_at": "2025-04-22",
    "authors": [
      "Mai Hoang"
    ],
    "author_emails": [
      "maihan.hoang1208@gmail.com"
    ],
    "license": "Attribution-NonCommercial 4.0 ...",
    "home_pypi": "https://pypi.org/project/napari-sam2long/",
    "home_github": "https://github.com/maihanhoang/napari-sam2long",
    "home_other": null,
    "summary": "A plugin for interactive 3D (volumetric or time-lapse) segmentation using Meta's Segment Anything Model 2 (SAM2).",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "opencv-python",
      "pytest",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-sam2long\n\n[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)\n[![PyPI](https://img.shields.io/pypi/v/napari-sam2long.svg?color=green)](https://pypi.org/project/napari-sam2long)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sam2long.svg?color=green)](https://python.org)\n[![tests](https://github.com/maihanhoang/napari-sam2long/workflows/tests/badge.svg)](https://github.com/maihanhoang/napari-sam2long/actions)\n[![codecov](https://codecov.io/gh/maihanhoang/napari-sam2long/branch/main/graph/badge.svg)](https://codecov.io/gh/maihanhoang/napari-sam2long)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sam2long)](https://napari-hub.org/plugins/napari-sam2long)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA plugin for interactive 3D (volumetric or time-lapse) segmentation using Meta's Segment Anything Model 2 (SAM2).\n\nDesigned for bioimaging researchers working with 3D volumetric or time-lapse images, this plugin supports TIFF files in ZYX or TYX format. Users can provide input and make corrections through point clicks or manually drawn masks.\n\nThe tool leverages the [SAM2Long](https://github.com/Mark12Ding/SAM2Long) model, an optimized version of [Meta's SAM 2](https://github.com/facebookresearch/sam2) with enhancements to the memory module for improved performance on long videos. It was built to support long videos, but it remains effective for shorter videos as well.\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<h3 align=\"center\">Select object with point prompts</h3>\n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-sam2long/raw/main/assets/napari-sam2long-firstLabel.gif\" width=\"100%\" />\n</p>\n\n<h3 align=\"center\">Refine object selection with napari tools </h3>\n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-sam2long/raw/main/assets/napari-sam2long-anotherLabel.gif\" width=\"100%\" />\n</p>\n\n\n## Installation\nPlease see the official [SAM 2](https://github.com/facebookresearch/sam2) repo and the [INSTALL.md](https://github.com/facebookresearch/sam2/blob/main/INSTALL.md) for notes and FAQs on potential installation issues.\n\n1. Create a new conda environment with python>=3.10 and install napari:\n    ```bash\n    conda create -n napari-sam2long python==3.10 pip\n    conda activate napari-sam2long\n    python -m pip install \"napari[all]\"\n    ```\n\n2. Install PyTorch and TorchVision. Select preferences [here](https://pytorch.org/get-started/locally/) to find correct installation command.\n\n    Example command can look like this:\n\n    ```bash\n    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n    ```\n\n3. Install SAM2Long:\n    ```bash\n    git clone git@github.com:maihanhoang/napari-sam2long.git\n\n    cd napari-sam2long/SAM2Long && python -m pip install -e .\n    ```\n4. Install napari-SAM2Long plugin:\n    ```bash\n    cd .. && python -m pip install e .\n    ```\n\n## Usage\n\n### Segmenting & tracking first object\n1. **Open a 3D tiff** in napari, make sure it's in TYX or ZYX format.\n2. **Select** the image in the ***Input*** dropdown.\n3. **Add a new [labels layer](https://napari.org/0.5.0/howtos/layers/labels.html)** *after* the input image, then select it in the *Labels* dropdown.\n\n    The labels layer must be added after the image to ensure dimension alignment.\n4. **Select the *Model***.\n5. **Click *Initialize*** to load the image and initialize the inference state.\n6. **Define the initial object mask** on any frame:\n    - Use the mouse middle-click to prompt the model:\n        - *Middle-click* = add region\n        - *Ctrl + middle-click* = remove region\n    - Or use napari's built-in tools (paintbrush, eraser, etc.) to draw the mask manually.\n\n    If the model doesn‚Äôt segment the object accurately with point prompts, manual correction using napari tools can be useful.\n\n7. Once satisfied with the initial mask, **click *Propagate from current frame*** to obtain segmentations for all subsequent frames. The result will be added to the labels layer.\n\n    Propagation only affects future frames. It does not recompute previous ones or consider prompts from other frames. Only the current mask is used to propagate forward.\n\n### Making corrections\n8. To refine segmentation, add/remove regions use:\n    - *(Ctrl+) middle-click* prompts\n    - napari‚Äôs label tools\n9. *Propagate from current frame* to re-run the model's predictions with the new mask.\n\n    The plugin treats this as a new initial mask and discards earlier prompts on that frame.\n\n### Segmenting another object in the same image/video\n10. Save the current labels layer (to preserve previous segmentation).\n11. Click *Reset*, or add a new labels layer and select it in the *Labels* dropdown.\nThen repeat steps from Step 6 for the next object.\n\n###  Segment new image/video\n12. *Reset* inference state.\n13. Load new image and follow instructions starting from Step 1.\n    *Initialize* is necessary to load the new image.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/maihanhoang/napari-sam2long/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n## Bibliography\n[1]: Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R√§dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll√°r, & Christoph Feichtenhofer. (2024). SAM 2: Segment Anything in Images and Videos.\n\n[2]: Ding, S., Qian, R., Dong, X., Zhang, P., Zang, Y., Cao, Y., Guo, Y., Lin, D., & Wang, J. (2024). SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree. arXiv preprint arXiv:2410.16268.\n\n## License & Attribution\n\nThis project integrates code from:\n- [SAM2](https://github.com/facebookresearch/sam2) by Meta ([`Apache 2.0 License`](LICENSE-Apache-2.0))\n- [SAM2Long](https://github.com/Mark12Ding/SAM2Long) by Shuangrui Ding et al. ([`CC-BY-NC 4.0 License`](LICENSE-CC-BY-NC-4.0))\n- [napari-samv2](https://github.com/Krishvraman/napari-SAMV2) by Krishnan Venkataraman ([`BSD-3 License`](LICENSE-BSD-3))\n\n\nThe following changes were made to SAM2Long:\n- integrated SAM2Long into a napari plugin\n- modified the video predictor to support the progress bar in the plugin\n\n\nSince this project includes **SAM2Long**, it inherits the **CC-BY-NC 4.0 license**, meaning **commercial use is not allowed**.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SAM2Long"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "applet3",
    "name": "appletree",
    "display_name": "Appletree",
    "version": "0.1.dev1",
    "created_at": "2025-04-19",
    "modified_at": "2025-04-19",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@univ-angers.fr"
    ],
    "license": "Copyright (c) 2025, Herearii M...",
    "home_pypi": "https://pypi.org/project/applet3/",
    "home_github": "https://github.com/hereariim/appletree",
    "home_other": null,
    "summary": "Apple tree segmentation for young apple tree",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": "==3.10.16",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "torch>=2.5.1",
      "torchvision>=0.20.1",
      "numpy>=1.24.4",
      "tqdm>=4.66.1",
      "hydra-core>=1.3.2",
      "iopath>=0.1.10",
      "pillow>=9.4.0",
      "supervision==0.25.1",
      "transformers==4.51.3",
      "einops==0.8.1",
      "tensorflow==2.19.0",
      "accelerate==1.6.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# appletree\n\n[![License BSD-3](https://img.shields.io/pypi/l/appletree.svg?color=green)](https://github.com/hereariim/appletree/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/appletree.svg?color=green)](https://pypi.org/project/appletree)\n[![Python Version](https://img.shields.io/pypi/pyversions/appletree.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/appletree/workflows/tests/badge.svg)](https://github.com/hereariim/appletree/actions)\n[![codecov](https://codecov.io/gh/hereariim/appletree/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/appletree)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/appletree)](https://napari-hub.org/plugins/appletree)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nApple tree segmentation for young apple tree\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `appletree` via [pip]:\n\n    pip install appletree\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/appletree.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"appletree\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/hereariim/appletree/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Container Threshold",
      "Magic Threshold",
      "Autogenerate Threshold",
      "Example QWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "fitellipsoid",
    "name": "fitellipsoid",
    "display_name": "Fit Ellipsoid",
    "version": "0.0.6",
    "created_at": "2025-04-08",
    "modified_at": "2025-04-11",
    "authors": [
      "Pierre Weiss"
    ],
    "author_emails": [
      "pierre.weiss@cnrs.fr"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/fitellipsoid/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin to that fits an ellipsoid to a set of user clicked points in 3D",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "napari>=0.5.6",
      "npe2",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tifffile",
      "scipy",
      "pandas",
      "pathlib",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# fitellipsoid\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/fitellipsoid.svg?color=green)](https://github.com/pierre-weiss/fitellipsoid/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/fitellipsoid.svg?color=green)](https://pypi.org/project/fitellipsoid)\n[![Python Version](https://img.shields.io/pypi/pyversions/fitellipsoid.svg?color=green)](https://python.org)\n<!-- [![tests](https://github.com/pierre-weiss/fitellipsoid/workflows/tests/badge.svg)](https://github.com/pierre-weiss/fitellipsoid/actions)-->\n[![codecov](https://codecov.io/gh/pierre-weiss/fitellipsoid/branch/main/graph/badge.svg)](https://codecov.io/gh/pierre-weiss/fitellipsoid)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/fitellipsoid)](https://napari-hub.org/plugins/fitellipsoid)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n\nA plugin to find the best ellipsoid to fit a set of points clicked by the user.\nWith just a few clicks (10 is the absolute minimum) around the cells/nuclei boundaries, the plugin fits an ellipsoid and returns its parameters. \nThis can be used to analyze tissue geometry, mecanical stress, provide training databases for segmentation algorithms,...\n\n![FitEllipsoid widget example](https://raw.githubusercontent.com/pierre-weiss/fitellipsoid_napari/main/images/screenshot.jpg)\n\n----------------------------------\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `fitellipsoid` via [pip]:\n\n    conda create -n fitellipsoid-env python=3.11\n    conda activate fitellispoid-env\n    pip install -U 'napari[all]'\n    pip install fitellipsoid\n    napari\n\n## üß™ Usage Instructions\n\n1. **Open your 3D image stack** in Napari.\n\n2. **Launch the FitEllipsoid plugin** from the plugin menu.\n\n3. **Select the point layer** created by the plugin and begin clicking along the boundary of your object:\n   - üñ±Ô∏è **Left-click** to add a point  \n   - üñ±Ô∏è **Right-click** to remove the last added point  \n\n4. **Once you've added at least 10 points**, click on the **\"Fit Ellipsoid\"** button.\n\n5. A **blue ellipsoid** will be fitted and displayed.  \n   - ‚úÖ If the shape fits well, you're done with that object.  \n   - ‚ùå If it doesn't, return to the corresponding point layer and add or adjust points.\n\n6. **Repeat** the process for all objects you'd like to segment.  \n   The plugin automatically adds a new point layer after each fit.\n\n7. When you're finished, **save the results** as a `.csv` file and optionally export a segmentation mask.\n\n\n\n## Contributors\n\n- **Pierre Weiss** - Project lead, core plugin development\n- **Cl√©ment Cazorla** - Added the segmentation mask generation\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"fitellipsoid\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "FitEllipsoid"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-easy-augment-batch-dl",
    "name": "napari-easy-augment-batch-dl",
    "display_name": "Easy Augment Batch DL",
    "version": "0.0.6",
    "created_at": "2025-02-13",
    "modified_at": "2025-04-11",
    "authors": [
      "Brian Northan"
    ],
    "author_emails": [
      "bnorthan@gmail.com"
    ],
    "license": "Copyright (c) 2024, Brian Nort...",
    "home_pypi": "https://pypi.org/project/napari-easy-augment-batch-dl/",
    "home_github": "https://github.com/bnorthan/napari-easy-augment-batch-dl",
    "home_other": null,
    "summary": "A plugin to perform unet based deep learning with a small number of labels and augmentation",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tnia-python",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-easy-augment-batch-dl\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-easy-augment-batch-dl.svg?color=green)](https://github.com/bnorthan/napari-easy-augment-batch-dl/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-easy-augment-batch-dl.svg?color=green)](https://pypi.org/project/napari-easy-augment-batch-dl)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-easy-augment-batch-dl.svg?color=green)](https://python.org)\n[![tests](https://github.com/bnorthan/napari-easy-augment-batch-dl/workflows/tests/badge.svg)](https://github.com/bnorthan/napari-easy-augment-batch-dl/actions)\n[![codecov](https://codecov.io/gh/bnorthan/napari-easy-augment-batch-dl/branch/main/graph/badge.svg)](https://codecov.io/gh/bnorthan/napari-easy-augment-batch-dl)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-easy-augment-batch-dl)](https://napari-hub.org/plugins/napari-easy-augment-batch-dl)  \n\n\nSee [full documentation](https://true-north-intelligent-algorithms.github.io/napari-easy-augment-batch-dl/)\n\nA plugin to perform deep learning on small to medium sized image sets with UNETs, Cellpose, Stardist, SAM and friends.  In particular this plugin is useful for performing deep learning with a small number of labels and augmentation, and experimenting with different deep learning frameworks.  \n\nImportant note on dependencies:  This plugin is designed to work with different permutations of dependencies.  For example it should work if one of Pytorch, Cellpose, SAM and/or Stardist is installed but does not require all.   Thus we don't specify all the dependencies and leave it up to the user to install the permutation of DL related dependencies they would like to use.  More detailed instructions are below. \n\nIf you have any questions about dependencies splease post on the [Image.sc](Image.sc) forum. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nTo install latest development version :\n\n    pip install git+https://github.com/bnorthan/napari-easy-augment-batch-dl.git\n\nYou will also need to install the latest development version of tnia-python:\n\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/tnia-python.git\n\nYou will need to install napari and for augmentation you will need albumentations library.  Also explicitly install numpy 1.26.  (We have not tested with numpy 2.0 so it is a good idea to explicitly install numpy 1.26 to avoid another dependency installing numpy 2.x)\n\n```\n    pip install numpy==1.26\n    pip install napari[all]\n    pip install albumentations\n    pip install matplotlib\n```\n\nYou will also need one or more of stardist, cellpose, segment-everything or Yolo\n\n### Stardist\n\n#### Windows\n\n```\n    conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n    pip install \"tensorflow<2.11\"\n    pip install stardist==0.8.5\n    pip install gputools\n    pip install edt\n```\n\n#### Linux\n\n```\n    pip install tensorflow[and-cuda]\n    pip install stardist\n    pip install gputools\n    pip install edt\n```\n\n### Pytorch (for unet segmentation)\n\n```\n    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    pip install pytorch-lightning\n    pip install monai\n    pip install scipy\n    pip install tifffile\n```\n\n### Cellpose\n\n```\n    pip install cellpose\n```\n\n### SAM (Segment Anything)\n\n```\n    pip install segment-everything\n```\n\n###\n\nYou can install `napari-easy-augment-batch-dl` via [pip]:\n\n    pip install napari-easy-augment-batch-dl\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-easy-augment-batch-dl\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bnorthan/napari-easy-augment-batch-dl/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Easy Augment Batch DL"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mask-density",
    "name": "napari-mask-density",
    "display_name": "Mask Analyzer",
    "version": "0.1",
    "created_at": "2025-04-11",
    "modified_at": "2025-04-11",
    "authors": [
      "Da Kuang"
    ],
    "author_emails": [
      "kuangda@seas.upenn.edu"
    ],
    "license": "Copyright (c) 2025, Da Kuang\nA...",
    "home_pypi": "https://pypi.org/project/napari-mask-density/",
    "home_github": null,
    "home_other": "None",
    "summary": "A napari plugin for analyzing mask density in regions of interest",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-mask-density\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-mask-density.svg?color=green)](https://github.com/kuang-da/napari-mask-density/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mask-density.svg?color=green)](https://pypi.org/project/napari-mask-density)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mask-density.svg?color=green)](https://python.org)\n[![tests](https://github.com/kuang-da/napari-mask-density/workflows/tests/badge.svg)](https://github.com/kuang-da/napari-mask-density/actions)\n[![codecov](https://codecov.io/gh/kuang-da/napari-mask-density/branch/main/graph/badge.svg)](https://codecov.io/gh/kuang-da/napari-mask-density)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mask-density)](https://napari-hub.org/plugins/napari-mask-density)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nA napari plugin for analyzing mask density in regions of interest\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-mask-density` via [pip]:\n\n    pip install napari-mask-density\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mask-density\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Mask Density"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "mmv-regionseg",
    "name": "mmv-regionseg",
    "display_name": "MMV-Region Segmentation",
    "version": "0.3.0",
    "created_at": "2025-03-21",
    "modified_at": "2025-04-10",
    "authors": [
      "Peter Lampen"
    ],
    "author_emails": [
      "Peter Lampen <lampen@isas.de>"
    ],
    "license": "Copyright (c) 2025, Peter Lamp...",
    "home_pypi": "https://pypi.org/project/mmv-regionseg/",
    "home_github": "https://github.com/MMV-Lab/mmv-regionseg",
    "home_other": null,
    "summary": "Napari plugin for the segmentation of regions by flood",
    "categories": [
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "qtpy",
      "scikit-image",
      "tifffile",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# mmv-regionseg\n\n[![License BSD-3](https://img.shields.io/pypi/l/mmv-regionseg.svg?color=green)](https://github.com/MMV-Lab/mmv-regionseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/mmv-regionseg.svg?color=green)](https://pypi.org/project/mmv-regionseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/mmv-regionseg.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/mmv-regionseg/workflows/tests/badge.svg)](https://github.com/MMV-Lab/mmv-regionseg/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/mmv-regionseg/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/mmv-regionseg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/mmv-regionseg)](https://napari-hub.org/plugins/mmv-regionseg)\n\nA Napari plugin for the segmentation of regions by flood_fill\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `mmv-regionseg` via [pip]:\n\n    pip install mmv-regionseg\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MMV-Lab/mmv-regionseg.git\n\n## Documentation\n\n**MMV-RegionSeg** is a Napari plugin designed to segment three-dimensional image data based on the gray value of a selected seed point. Neighboring voxels are assigned to the same class if their intensity is similar to that of the seed point or falls within a defined tolerance range.\n\n---\n\n### Launching the Plugin\n\n1. Open Napari.\n2. Go to the **Plugins** menu.\n3. Select **MMV-RegionSeg** from the dropdown.\n\nThis opens a widget on the right-hand side of the Napari window, featuring several buttons, labels, and a slider.\n\n### Screenshot\n\nHere is a preview of the MMV-RegionSeg plugin in action:\n\n![MMV-RegionSeg Plugin Screenshot](https://raw.githubusercontent.com/MMV-Lab/MMV-RegionSeg/main/docs/images/plugin_screenshot.png)\n\n---\n\n### Loading Image Data\n\nClick the **\"Read image\"** button to load a 3D image in TIFF format. A standard OS file dialog will open. Once the image is selected, Napari will display it as an **image layer**.\n\n---\n\n### Adjusting Tolerance\n\nA **slider** below the image loading button allows you to set the gray value tolerance (range: **1‚Äì50**):\n\n- **Low tolerance**: May result in incomplete region filling.\n- **High tolerance**: May include undesired regions.\n\n> ‚ö†Ô∏è Choosing the right tolerance often requires trial and error.\n\n---\n\n### Selecting Seed Points\n\nClick **\"Select seed points\"** to activate a new **points layer** in Napari. You can then define seed points by clicking directly in the viewer.\n\n- Each seed point is visualized.\n- Multiple seed points added in one step are treated as a single class.\n- Use Napari‚Äôs **Layer Controls** to move or delete seed points.\n\n---\n\n### Segmentation Options\n\nAfter placing seed points, you can choose between two segmentation methods:\n\n#### Flood\n\nClick **\"Flood\"** to perform segmentation using  \n`skimage.segmentation.flood(...)`.  \nThis identifies neighboring voxels within the tolerance range and saves them to a new **label layer**.\n\nYou can repeat this process for other classes by selecting new seed points. Each class will have its own label layer.\n\n#### Growth\n\nClick **\"Growth\"** to visualize the segmentation **step by step**.  \nThis simulates the growth of a region, similar to a cell colony expanding in a Petri dish.\n\n---\n\n### Resetting for New Segmentation\n\nAfter a label layer is created for a class, the **points layer is removed**, allowing you to define new seed points without affecting the existing segmentation results.\n\n---\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"mmv-regionseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/MMV-Lab/mmv-regionseg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MMV-RegionSeg"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-splinebox",
    "name": "napari-splinebox",
    "display_name": "splinebox",
    "version": "0.0.4a0",
    "created_at": "2024-10-01",
    "modified_at": "2025-04-09",
    "authors": [
      "Florian Aymanns"
    ],
    "author_emails": [
      "florian.aymanns@epfl.ch"
    ],
    "license": "Copyright (c) 2024, Florian Ay...",
    "home_pypi": "https://pypi.org/project/napari-splinebox/",
    "home_github": "https://github.com/EPFL-Center-for-Imaging/napari-splinebox",
    "home_other": null,
    "summary": "A plugin to create splines with napari.",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "splinebox",
      "pandas",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-splinebox\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-splinebox.svg?color=green)](https://github.com/EPFL-Center-for-Imaging/napari-splinebox/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-splinebox.svg?color=green)](https://pypi.org/project/napari-splinebox)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-splinebox.svg?color=green)](https://python.org)\n[![tests](https://github.com/EPFL-Center-for-Imaging/napari-splinebox/workflows/tests/badge.svg)](https://github.com/EPFL-Center-for-Imaging/napari-splinebox/actions)\n[![codecov](https://codecov.io/gh/EPFL-Center-for-Imaging/napari-splinebox/branch/main/graph/badge.svg)](https://codecov.io/gh/EPFL-Center-for-Imaging/napari-splinebox)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splinebox)](https://napari-hub.org/plugins/napari-splinebox)\n\nA plugin to create splines with napari.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-splinebox` via [pip]:\n\n    pip install napari-splinebox\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/EPFL-Center-for-Imaging/napari-splinebox.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-splinebox\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/EPFL-Center-for-Imaging/napari-splinebox/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.json"
    ],
    "contributions_writers_filename_extensions": [
      ".json"
    ],
    "contributions_widgets": [
      "SplineBox"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-annotation-project",
    "name": "napari-annotation-project",
    "display_name": "napari-annotation-project",
    "version": "0.2.0",
    "created_at": "2023-11-01",
    "modified_at": "2025-04-08",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-annotation-project/",
    "home_github": "https://github.com/guiwitz/napari-annotation-project",
    "home_other": null,
    "summary": "A napari plugin to keep images and annotations as a re-loadable project",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "PyYAML",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-annotation-project\n\n[![License](https://img.shields.io/pypi/l/napari-annotation-project.svg?color=green)](https://github.com/guiwitz/napari-annotation-project/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-annotation-project.svg?color=green)](https://pypi.org/project/napari-annotation-project)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-annotation-project.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-annotation-project/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-annotation-project/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-annotation-project/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-annotation-project)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-annotation-project)](https://napari-hub.org/plugins/napari-annotation-project)\n\nThis napari plugin allows to define projects consisting of multiple images that can be annotated with labels and rectangular regions of interest (rois). Those rois can then be exported as series of cropped images and labels, typically to train Machine Learning models. Projects can be easily reopened in order to browse through images and their annotations. This package is a meant to be a *light-weight plugin which does not introduce any specific dependencies* and that should be easily installable in any environment already containing napari and other plugins.\n\n## Usage\nTo start a project, you can just drag and drop files in the file list area. This prompts for the selection of a project folder. After that, more files (also from different folders) can be dragged and dropped to be included in the project. Files can optionally be copied to the project folder but this option has to be set **before adding files**. When selecting a file in the list, it is opened (using the default image reader or a reader plugin if installed) and two layers, one for rois, and one for annotations are added.\n\nhttps://user-images.githubusercontent.com/4622767/147265874-57dcd956-4d54-4c76-9129-c1fc2837e6a4.mp4\n\n### Adding rois\nAfter selecting the ```rois``` layer, you can add rectangular rois to the image. If you need square rois of a specific size (as often needed in DL training) you can select the ```Fixed roi size``` option and then use the ```Add roi``` button. **Note that currently only 2D rois are supported**. If you work with nD images, the roi is therefore added to the **current selected 2D plane**.\n\n### Adding annotations\nAfter selecting the ```annotations``` layer, you can add annotations to your image. There are no restrictions here and you can e.g. add as many labels as you need.\n\n### Info storage\nAll relevant information on project location, project files and rois is stored in a yaml file ```Parameters.yml```. Annotations are stored as 2D tiff files in the ```annotations``` as files named after the original files. **Note that at the moment if multiple files have the same name, this will cause trouble**. This parameter file is used when re-loading an existing project.\n\nhttps://user-images.githubusercontent.com/4622767/147265984-adb6ee1f-9319-45c9-a9a4-735ade2a3905.mp4\n\n## Exporting rois\nOnce you are satisfied with your annotations and rois, you can use the rois to export only the corresponing cropped rois of both the image and annotation layers. For this you can head to the ```Export``` tab. Here you can set the location of the export folder, set the names of the folders that will contain cropped images and cropped annotations, and finally set the prefix names for these two types of files. Files are exported as tif files. \n\nhttps://user-images.githubusercontent.com/4622767/147266002-9c4485c9-5bcc-4c64-9c92-6c06775e2711.mp4\n\n## Installation\n\n\nYou can install `napari-annotation-project` via [pip]:\n\n    pip install napari-annotation-project\n\nTo install latest development version :\n\n    pip install git+https://github.com/guiwitz/napari-annotation-project.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-annotation-project\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-annotation-project/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ProjectWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-himena",
    "name": "napari-himena",
    "display_name": "Himena",
    "version": "0.0.1",
    "created_at": "2025-04-08",
    "modified_at": "2025-04-08",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin.sc@gmail.com"
    ],
    "license": "Copyright (c) 2025, Hanjin Liu...",
    "home_pypi": "https://pypi.org/project/napari-himena/",
    "home_github": "https://github.com/hanjinliu/napari-himena",
    "home_other": null,
    "summary": "Pipeline between napari and himena",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "himena",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt6; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-himena\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-himena.svg?color=green)](https://github.com/hanjinliu/napari-himena/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-himena.svg?color=green)](https://pypi.org/project/napari-himena)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-himena.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-himena/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-himena/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-himena/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-himena)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-himena)](https://napari-hub.org/plugins/napari-himena)\n[![npe2](https://img.shields.io/badge/plugin-npe2-blue?link=https://napari.org/stable/plugins/index.html)](https://napari.org/stable/plugins/index.html)\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n\nPipeline between [`napari`](https://github.com/napari/napari) and [`himena`](https://github.com/hanjinliu/himena).\n\n`napari` is a great tool for visualization, annotation and analysis of multi-dimensional\nimages. On the other hand, `himena` has a powerful plugin system that allows users to\ntechnically do anything, such as editing table and plotting.\n\n`napari-himena` connects these two ecosystems together, enabling users to send data\nback and forth, extending the functionality of the both packages.\n\n## Examples\n\n#### 1. Sending image layers to `himena` for ImageJ-like multi-measurement and Excel-like plotting.\n\nMeasuring time-course change in the image intensity with [`himena-image`](https://github.com/hanjinliu/himena-image) plugin, and plot the result using the built-in plot functions using `matplotlib`.\n\n![](https://github.com/hanjinliu/napari-himena/blob/main/assets/image-plot.gif)\n\n#### 2. Sending points and their features to `himena` for seaborn plotting.\n\nFeature dataframe can be directly sent to `himena` for `seaborn` plotting using [`himena-seaborn`](https://github.com/hanjinliu/himena-seaborn) plugin.\n\n![](https://github.com/hanjinliu/napari-himena/blob/main/assets/feature-sns.gif)\n\n## Usage\n\n#### Starting from `napari`\n\nOpen the `napari-himena` dock widget from the \"Plugin\" menu, connect to one of the\n`himena` profile (only \"default\" is available by default), and that's it!\n\n![](https://github.com/hanjinliu/napari-himena/blob/main/assets/from-napari.png)\n\n#### Starting from `himena`\n\nTo use this plugin from `himena`, you need to first register this plugin to the `himena`\nprofile\n\n```shell\n# install to the default profile\nhimena --install napari-himena\n\n# or install to a specific profile\nhimena <my-profile> --install napari-himena\n```\n\nThen all the commands will be available in `himena` and a napari viewer will be launched\nwhen it is needed. You don't need to do this if you always launch `himena` from `napari`\nplugin; it automatically register this package in the beginning.\n\n![](https://github.com/hanjinliu/napari-himena/blob/main/assets/from-himena.png)\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n## Installation\n\nYou can install `napari-himena` via [pip]:\n\n    pip install napari-himena\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-himena.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-himena\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/hanjinliu/napari-himena/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Napari-Himena Pipeline"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-stitcher",
    "name": "napari-stitcher",
    "display_name": "napari-stitcher",
    "version": "0.1.2",
    "created_at": "2024-10-18",
    "modified_at": "2025-04-07",
    "authors": [
      "Marvin Albert"
    ],
    "author_emails": [
      "marvin.albert@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-stitcher/",
    "home_github": "https://github.com/multiview-stitcher/napari-stitcher",
    "home_other": null,
    "summary": "Stitch napari image layers in 2-3D+t",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "dask",
      "magicgui",
      "multiscale_spatial_image",
      "multiview-stitcher>=0.1.24",
      "napari",
      "numpy>=1.18",
      "qtpy",
      "spatial_image",
      "tifffile>=2022.7.28",
      "tqdm",
      "xarray",
      "tox; extra == \"testing-no-gui\"",
      "multiview-stitcher[czi]>=0.1.24; extra == \"testing-no-gui\"",
      "pytest; extra == \"testing-no-gui\"",
      "pytest-cov; extra == \"testing-no-gui\"",
      "pytest-qt; extra == \"testing-no-gui\"",
      "tox; extra == \"testing\"",
      "multiview-stitcher[czi]>=0.1.24; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stitcher)](https://napari-hub.org/plugins/napari-stitcher)\n[![License {{cookiecutter.license}}](https://img.shields.io/pypi/l/napari-stitcher.svg?color=green)](https://github.com/multiview-stitcher/napari-stitcher/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-stitcher.svg?color=green)](https://pypi.org/project/napari-stitcher)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stitcher.svg?color=green)](https://python.org)\n[![tests](https://github.com/multiview-stitcher/napari-stitcher/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/multiview-stitcher/napari-stitcher/actions)\n[![codecov](https://codecov.io/gh/multiview-stitcher/napari-stitcher/branch/main/graph/badge.svg)](https://codecov.io/gh/multiview-stitcher/napari-stitcher)\n[![DOI](https://zenodo.org/badge/697999800.svg)](https://zenodo.org/doi/10.5281/zenodo.14176362)\n\n\n# napari-stitcher\n\nA napari plugin for stitching large multi-positioning datasets in 2/3D+t using [`multiview-stitcher`](https://github.com/multiview-stitcher/multiview-stitcher).\n\n![](docs/images/napari-stitcher-loaded-mosaic-annotated.png)\n<small>Image data by Arthur Michaut @ J√©r√¥me Gros Lab @ Institut Pasteur.</small>\n\n#### Quick guide:\n\n1. Directly stitch napari layers: Use napari to load, visualize and [preposition](prearrangement.md) the tiles to be stitched.\n2. When working with multi-channel data, stick to the following [naming convention](naming_convention.md): `{tile} :: {channel}`.\n3. Load either all or just a subset of the layers into the plugin.\n4. Choose registration options: registration channel, binning and more.\n5. Stitching = registration (refining the positions, optional) + fusion (joining the tiles into a single image).\n6. The registration result is shown in the viewer and the fused channels are added as new layers.\n\n## Demo\n\nhttps://github.com/user-attachments/assets/8773e49f-af18-4ff3-ab2f-2a5f1b1cadf2\n\n<small>This demo uses the awesome [`napari-threedee`](https://github.com/napari-threedee/napari-threedee) for prepositioning the tiles. Image data: [BigStitcher](https://imagej.net/plugins/bigstitcher/).</small>\n\n## Documentation\n\nHead over to the [user guide](https://multiview-stitcher.github.io/napari-stitcher/main/) for more details.\n\n## Installation\n\nYou can install `napari-stitcher` via `pip`:\n\n```bash\npip install napari-stitcher\n```\n\nFor more installation options, see the [installation docs](https://multiview-stitcher.github.io/napari-stitcher/main/installation/).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with `tox`.\n\n## License\n\nDistributed under the terms of the [BSD-3] license, \"napari-stitcher\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/multiview-stitcher/napari-stitcher/issues) along with a detailed description.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.czi"
    ],
    "contributions_writers_filename_extensions": [
      ".tif"
    ],
    "contributions_widgets": [
      "Stitcher",
      "Mosaic arrangement"
    ],
    "contributions_sample_data": [
      "Mosaic",
      "2D drifting timelapse with stage shifts and no overlap",
      "3D timelapse with stage shifts and overlap"
    ]
  },
  {
    "normalized_name": "napari-svetlana",
    "name": "napari-svetlana",
    "display_name": "Svetlana",
    "version": "1.6.1",
    "created_at": "2022-11-22",
    "modified_at": "2025-04-07",
    "authors": [
      "Cl√©ment Cazorla"
    ],
    "author_emails": [
      "clement.cazorla31@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-svetlana/",
    "home_github": null,
    "home_other": null,
    "summary": "A classification plugin for the ROIs of a segmentation mask. If you face problems opening the Napari-hub page, try replacing napari-svetlana by napari_svetlana in the URL.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "numpy",
      "albumentations==1.0.3",
      "joblib==1.2.0",
      "light-the-torch",
      "matplotlib",
      "opencv-python==4.8.1.78",
      "pyqtgraph==0.13.3",
      "PyQt5",
      "cucim==23.10.0; platform_system == \"Linux\"",
      "cupy-cuda115==10.6.0",
      "xlsxwriter",
      "pandas",
      "npe2",
      "pooch"
    ],
    "package_metadata_description": "    # napari-svetlana\n\n[![License](https://img.shields.io/pypi/l/napari_svetlana.svg?color=green)](https://bitbucket.org/koopa31/napari_svetlana/src/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari_svetlana.svg?color=green)](https://pypi.org/project/napari_svetlana)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari_svetlana.svg?color=green)](https://python.org)\n[![tests](https://bitbucket.org/koopa31/napari_svetlana/workflows/tests/badge.svg)](https://bitbucket.org/koopa31/napari_svetlana/actions)\n[![codecov](https://codecov.io/gh/koopa31/napari_svetlana/branch/main/graph/badge.svg)](https://codecov.io/gh/koopa31/napari_svetlana)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-svetlana)](https://napari-hub.org/plugins/napari-svetlana)\n[![Documentation](https://readthedocs.org/projects/svetlana-documentation/badge/?version=latest)](https://svetlana-documentation.readthedocs.io/en/latest/)\n\nThe aim of this plugin is to classify the output of a segmentation algorithm.\nThe inputs are :\n<ul>\n  <li>A folder of raw images</li>\n  <li>Their segmentation masks where each ROI has its own label.</li>\n</ul>\n\nSvetlana can process 2D, 3D and multichannel image. If you want to use it to work on cell images, we strongly\nrecommend the use of [Cellpose](https://www.cellpose.org) for the segmentation part, as it provides excellent quality results and a standard output format\naccepted by Svetlana (labels masks). \n\nIf you use this plugin please cite the [paper](https://www.nature.com/articles/s41598-024-60916-8): \n\nCazorla, C., Weiss, P., & Morin, R. (2024). Svetlana: a Supervised Segmentation Classifier for Napari.\n\n```bibtex\n@article{cazorla2024svetlana,\n  title={Svetlana a supervised segmentation classifier for Napari},\n  author={Cazorla, Cl{\\'e}ment and Morin, Renaud and Weiss, Pierre},\n  journal={Scientific Reports},\n  volume={14},\n  number={1},\n  pages={11604},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n}\n\n```\n\n\n![](https://bitbucket.org/koopa31/napari_svetlana/raw/bca8788111b38d97bd172c7caac87cc488ace699/images/Videogif.gif)\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nFirst install Napari in a Python 3.9 Conda environment following these instructions :\n\n```bash\nconda create -n svetlana_env python=3.9\nconda activate svetlana_env\nconda install pip\npython -m pip install \"napari[all]\"==0.4.17\n```\n\nThen, you can install `napari_svetlana` via [pip](https://pypi.org/project/napari-svetlana/), or directly from the Napari plugin manager (see Napari documentation):\n```bash\npip install napari_svetlana\n```\nWARNING:\n\nIf you have a Cuda compatible GPU on your computer, some computations may be accelerated\nusing [Cupy](https://pypi.org/project/cupy/). Unfortunately, Cupy needs Cudatoolkit to be installed. This library can only be installed via \nConda while the plugin is a pip plugin, so it must be installed manually for the moment:\n```bash\nconda install cudatoolkit=11.5 \n```\nAlso note that the library ([Cucim](https://pypi.org/project/cucim/)) that we use to improve these performances, computing morphological operations on GPU\nis unfortunately only available for Linux systems. Hence, if you are a Windows user, this installation is not necessary.\n\n## Tutorial\n\nMany advanced features are available in Svetlana, such as data augmentation or contextual information reduction, to optimize the performance of your classifier. Thus, we strongly encourage you to\ncheck our [Youtube tutorial](https://www.youtube.com/watch?v=u_FKuHta-RE) and\nour [documentation](https://svetlana-documentation.readthedocs.io/en/latest/).\nA button called **TRY ON DEMO IMAGE** is available in the annotation plugin and enables you to apply the YouTube\ntutorial to the same test images to learn how to use the plugin. Feel free to try it to test all the features\nthat Svetlana offers.\n\n## Similar Napari plugins\n\nJoel Luethi developed a similar method for objects classification called [napari feature classifier](https://www.napari-hub.org/plugins/napari-feature-classifier).\nAlso, [apoc](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification) by Robert Haase is available in Napari for pixels and objects classification.\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari_svetlana\" is free and open source software\n\n## Acknowledgements\n\nThe method was developed by [Cl√©ment Cazorla](https://koopa31.github.io/), [Renaud Morin](https://www.linkedin.com/in/renaud-morin-6a42665b/?originalSubdomain=fr) and [Pierre Weiss](https://www.math.univ-toulouse.fr/~weiss/). And the plugin was written by\nCl√©ment Cazorla. The project is co-funded by [Imactiv-3D](https://www.imactiv-3d.com/) and [CNRS](https://www.cnrs.fr/fr).\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://bitbucket.org/koopa31/napari_svetlana/issues?status=new&status=open) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotation",
      "Training",
      "Prediction"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-deepfinder",
    "name": "napari-deepfinder",
    "display_name": "Napari DeepFinder",
    "version": "0.0.2",
    "created_at": "2022-07-25",
    "modified_at": "2025-04-04",
    "authors": [
      "Constantin Aronssohn"
    ],
    "author_emails": [
      "cnstt@tutanota.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-deepfinder/",
    "home_github": "https://github.com/deep-finder/napari-deepfinder",
    "home_other": null,
    "summary": "A napari plugin for the DeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities. An orthoslice view has been added for an easier visualisation and annotation process.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "cryoet-deepfinder",
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "scikit-image",
      "typing",
      "pandas",
      "lxml[html_clean]",
      "pillow",
      "h5py",
      "mrcfile",
      "scipy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-deepfinder\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-deepfinder.svg?color=green)](https://github.com/deep-finder/napari-deepfinder/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-deepfinder.svg?color=green)](https://pypi.org/project/napari-deepfinder)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-deepfinder.svg?color=green)](https://python.org)\n[![tests](https://github.com/deep-finder/napari-deepfinder/workflows/tests/badge.svg)](https://github.com/deep-finder/napari-deepfinder/actions)\n[![codecov](https://codecov.io/gh/deep-finder/napari-deepfinder/branch/main/graph/badge.svg)](https://codecov.io/gh/deep-finder/napari-deepfinder)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deepfinder)](https://napari-hub.org/plugins/napari-deepfinder)\n\nA napari plugin for the DeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities.\nAn orthoslice view has been added for an easier visualisation and annotation process.\n\n**The documentation for users is available [here](https://deep-finder.github.io/napari-deepfinder/).**\n\n> [!WARNING]\n>\n> An upstream bug in `napari` versions **‚â• 0.5.0** causes the menu bar to break when closing the *Orthoslice view*.\n> \n> See this issue for more details: https://github.com/napari/napari/issues/7588.\n> \n> As a temporary workaround, you can use [napari v0.4.19](https://github.com/napari/napari/releases/tag/v0.4.19), which is not affected by this bug.\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-deepfinder` via [pip]:\n\n    pip install napari-deepfinder\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/deep-finder/napari-deepfinder.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-deepfinder\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/deep-finder/napari-deepfinder/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.xlsx",
      "*.map",
      "*.h5",
      "*.xml",
      "*.rec",
      "*.tif",
      "*.xls",
      "*.mrc",
      "*.TIF",
      "*.ods"
    ],
    "contributions_writers_filename_extensions": [
      ".mrc",
      ".xml"
    ],
    "contributions_widgets": [
      "Reorder layers automatically",
      "Denoise tomogram",
      "Annotation",
      "Orthoslice view",
      "Segmentation",
      "Clustering"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "neurofly",
    "name": "neurofly",
    "display_name": "neurofly",
    "version": "0.1.4",
    "created_at": "2024-10-30",
    "modified_at": "2025-04-03",
    "authors": [
      "Rubin Zhao"
    ],
    "author_emails": [
      "Rubin Zhao <beanli161514@gmail.com>"
    ],
    "license": "GPL-3.0-or-later",
    "home_pypi": "https://pypi.org/project/neurofly/",
    "home_github": "https://github.com/beanli161514/neurofly",
    "home_other": null,
    "summary": "A framework to annotate single neurons at whole-brain scale",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "PyQt5",
      "napari",
      "Rtree",
      "networkx",
      "tqdm",
      "magicgui",
      "brightest-path-lib",
      "tifffile",
      "scikit-image",
      "scipy",
      "torch",
      "tinygrad>=0.9.2",
      "pathlib",
      "h5py",
      "zarr"
    ],
    "package_metadata_description": "# NeuroFly: A framework for single neuron reconstruction at whole-brain scale\n\nThis package provides tools for semi-automatic neuron reconstruction. Features based on deep learning, like image segmentation and deconvolution are implemented in [tinygrad](https://github.com/tinygrad/tinygrad), which can run on almost any GPU (NVIDIA, AMD, Apple, Qualcomm, Intel).\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/main.png\" width=\"640\">\n\n## Update\n\nOur transformer-based autonomous driving model is available now!\n\n\nPress shortcut 'd' to automatically extend segments.\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/autonomous.gif\" width=\"640\">\n\n## Installation\n\nInstall the latest version\n```\npip install --upgrade git+https://github.com/beanli161514/neurofly.git\n```\nor\n```\npip install neurofly\n```\n\nYou can also install from [napari hub](https://www.napari-hub.org/plugins/neurofly), using their plugin manager with GUI.\n\n## Dataset\n\nWe provide several expert-proofread reconstruction results for testing, model training, and evaluation. [Zenodo Link](https://zenodo.org/records/13328867)\n\n\n### Content of samples\n| name           | size  | species | label type  | imaging |\n|----------------|-------|---------|-------------|---------|\n| rm009_labeled  | 629MB | macaque | skeleton    | VISoR   |\n| mouse_labeled  | 260MB | mouse   | skeleton    | VISoR   |\n| z002_labeled   | 204MB | mouse   | skeleton    | VISoR   |\n| fmost_labeled  | 370MB | mouse   | skeleton    | fMOST   |\n| RM009_noisy_1  | 65MB  | macaque | morphology  | VISoR   |\n| RM009_noisy_2  | 65MB  | macaque | morphology  | VISoR   |\n| fmost_test     | 65MB  | mouse   | morphology  | fMOST   |\n| z002_dendrites | 768MB | mouse   | morphology  | VISoR   |\n| RM009_arbor_1  | 288MB | macaque | morphology  | VISoR   |\n| RM009_axons_1  | 600MB | macaque | morphology  | VISoR   |\n| RM009_axons_2  | 600MB | macaque | morphology  | VISoR   |\n| z002           | 8.92G | mouse   | morphology* | VISoR   |\n\n$*$ annotation in progress\n### Label format\nMorphology labels are graphs saved in SQLite database with 3 tables:\n|    segments    |  nodes  |  edges  |\n|:--------------:|:-------:|:-------:|\n|       sid      |   nid   |   src   |\n|     points     |  coord  |   des   |\n| sampled_points | creator |   date  |\n|                |  status | creator |\n|                |   type  |         |\n|                |   date  |         |\n|                | checked |         |\n\nSegments are results of the segmentation stage, they are used to generate initial nodes and edges.\n\n\n## Basic usage example\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/pipeline.png\" width=\"640\">\n\nNeuroFly packaged 4 napari plugins for image browsing, image segmentation, and data annotation.\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/menu.png\" width=\"320\">\n\n### Segmentation\n\nNeuroFly supports whole brain image saved in hierarchical data structures(ims, h5, and zarr) in [Imaris File Format](https://imaris.oxinst.com/support/imaris-file-format) and small image volumes saved in single-channel tiff format. Here we use a mouse brain in our dataset named z002.zarr.zip as example.\n\nThis brain is sparsely labeled, which means only a tiny puny part of neurons are lighted and imaged. To extract these foreground singals, you can use the provided command line interface 'seg'. We provide a default weight trained on images captured by VISoR and fMOST.\n```\nseg -i z002.zarr.zip -vis -d z002.db\n```\nor use the graphical interface\n\n<img align src=\"https://github.com/beanli161514/neurofly/raw/main/assets/neuron_seger.png\" width=\"200\">\n\n\nThis process may take about 10 hours depending on your you hardware configuration. When finished, you should see the extracted segments and a database file named z002.db in your working dictionary.\n\n\nAn image block with severe contamination and the segmentation result\n\n<img align src=\"https://github.com/beanli161514/neurofly/raw/main/assets/segmentation.gif\" width=\"640\">\n\n\n### Manual connection and proofreading\n\nLaunch annotation tool from napari menu, Plugin -> neurofly -> Segs Annotator\n\n#### Load data\nLoad image file (z002.zarr.zip) and database file (z002.db), then click **refresh panorama** button to show the panorama view.\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/overall.png\" width=\"600\">\n\n\n#### Select one node as start point of annotation\nIn panorama mode, you can easily identify sparse, bright signals that are promising for reconstruction. The silde bars 'short segs filter', 'length thres', and 'point size' can be adjusted to hide noise and short segments. \n\nIf you can clearly identify foreground segments, click on one of the conspicuous segments to select it as start point of annotation. Once selected, the id of picked node will be displayed at **node selection**. Then click 'switch mode' to switch to labeling mode, and the tasks will be generated automatically.\n\n\n#### Task generation\nGiven a selected node, task generator analyses its connected component and extract all unchecked terminal nodes. The tasks are designed very simple: Connect the center node with the surrounding nodes if there should be an edge. The criterion is whether the edge aligns well with the imaged neuron fibers.\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/task_generation.png\" width=\"480\">\n\n\n#### Node operations\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/labeling_mode.png\" width=\"480\">\n\nIn each task, a center node and nearby segments are rendered, you can add/remove nodes and edges to get a reasonable local structure.\n\n\nLeft click on nodes to add/remove an edge between it and the center node\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/add_edges.gif\" width=\"480\">\n\nRight click to remove a node\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/remove_nodes.gif\" width=\"480\">\n\nPress 'g' or use left panel to switch to 'image' layer, then right click to add points\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/add_nodes.gif\" width=\"480\">\n\nUse dropdown selection in right panel to add type label for center node.\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/change_type.gif\" width=\"480\">\n\n#### Deconvolution\n\nPress 'i' or click on 'deconvolution' to deconvolve the image\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/deconv.gif\" width=\"640\">\n\n\n#### Proofreading\n\nIf you find something wrong when labeling, for example, two somas are connected together. You can use proofreading mode to check the neuron branch by branch.\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/proofreading.gif\" width=\"640\">\n\n\n\n### Performance\nNeuroFly supports rendering of more than ten million points. (tested on M3 Macbook Air and RTX 3090 workstation)\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/dense.jpg\" width=\"640\">\n\n\n\n### Export as swc file\nSwitch to panorama mode, adjust 'length_thres' to filter out short segments and keep only complete neurons. Then press 'export swc files', each neuron will be saved as one .swc file in your working dictionary.\n\n\n<img src=\"https://github.com/beanli161514/neurofly/raw/main/assets/export.jpg\" width=\"640\">\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segs Annotator",
      "Simple Viewer",
      "Skeleton Annotator",
      "Neuron Seger"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "arcos-gui",
    "name": "arcos-gui",
    "display_name": "napari ARCOS",
    "version": "0.1.5",
    "created_at": "2022-02-24",
    "modified_at": "2025-04-01",
    "authors": [
      "Benjamin Gr√§del"
    ],
    "author_emails": [
      "benjamin.graedel@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/arcos-gui/",
    "home_github": "https://github.com/bgraedel/arcos-gui",
    "home_other": null,
    "summary": "A napari plugin to detect and visualize collective signaling events",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "arcos4py>=0.3.1",
      "matplotlib>=3.3.4",
      "napari>=0.4.14",
      "numpy>=1.22.2; python_version >= \"3.10\"",
      "numpy<2,>=1.22.2; python_version < \"3.10\"",
      "pandas>=1.3.5",
      "pyarrow>=11.0.0",
      "scikit-image>=0.20.0; python_version < \"3.12\"",
      "scikit-image>=0.22.0; python_version >= \"3.12\"",
      "scipy>=1.7.3",
      "napari-timestamper",
      "mkdocs; extra == \"doc\"",
      "mkdocs-include-markdown-plugin; extra == \"doc\"",
      "mkdocs-material; extra == \"doc\"",
      "mkdocs-material-extensions; extra == \"doc\"",
      "pyqt5; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-mock; extra == \"testing\"",
      "pytest-qt; extra == \"testing\""
    ],
    "package_metadata_description": "# arcos-gui\n\n[![License](https://img.shields.io/pypi/l/arcos-gui.svg?color=green)](https://github.com/pertzlab/pertzlab/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/arcos-gui.svg)](https://pypi.org/project/arcos-gui)\n[![conda-forge](https://img.shields.io/conda/vn/conda-forge/arcos-gui)](https://anaconda.org/conda-forge/arcos-gui)\n[![Python Version](https://img.shields.io/pypi/pyversions/arcos-gui.svg?color=green?)](https://python.org)\n[![tests](https://github.com/pertzlab/arcos-gui/workflows/tests/badge.svg)](https://github.com/pertzlab/arcos-gui/actions)\n[![codecov](https://codecov.io/gh/pertzlab/arcos-gui/branch/main/graph/badge.svg)](https://codecov.io/gh/pertzlab/arcos-gui)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/arcos-gui)](https://napari-hub.org/plugins/arcos-gui)\n\nA napari plugin to detect and visualize collective signaling events\n\n----------------------------------\n- Package specific Documentation: <https://pertzlab.github.io/arcos-gui>\n- ARCOS documentation: <https://arcos.gitbook.io>\n\n**A**utomated **R**ecognition of **C**ollective **S**ignalling (ARCOS) is an algorithm to identify collective spatial events in time series data.\nIt is available as an [R (ARCOS)](https://github.com/pertzlab/ARCOS) and [python (arcos4py)](https://github.com/pertzlab/arcos4py) package.\nARCOS can identify and visualize collective protein activation in 2- and 3D cell cultures over time.\n\nThis plugin integrates ARCOS into napari. Users can import tracked time-series data in CSV format or load data from napari-layer properties (such as the ones generated with [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops). The plugin\nprovides GUI elements to process this data with ARCOS. Layers containing the detected collective events are subsequently added to the viewer.\n\nFollowing analysis, the user can export the output as a CSV file with the detected collective events or as a sequence of images to generate a movie.\n\n\n![](https://github.com/bgraedel/arcos-gui/assets/100028238/66fa2afa-6f24-4cce-b29e-4279066c6c25)\n\n[Watch full demo on youtube](https://www.youtube.com/watch?v=hG_z_BFcAiQ) (older plugin version)\n\n\n# Installation\n\nYou can install `arcos-gui` via [pip]:\n\n    pip install arcos-gui\n\nOr via [conda-forge]:\n\n    conda install -c conda-forge arcos-gui\n\n## Usage\n\nThe plugin can be started from the napari menu `Plugins > ARCOS GUI`.\nFor detailed instructions on how to use the plugin, please refer to the [Usage section of the documentation](https://pertzlab.github.io/arcos-gui/Usage).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\nSee the [Contributing Guide](https://pertzlab.github.io/arcos-gui/Contributing) for more information.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"arcos-gui\" is free and open-source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/pertzlab/arcos-gui/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/arcos-gui/\n[conda-forge]: https://anaconda.org/conda-forge/arcos-gui\n[PyPI]: https://pypi.org/\n\n## Credits\nWe were able to develop this plugin in part due to funding from the [CZI napari Plugin Foundation Grant](https://chanzuckerberg.com/science/programs-resources/imaging/napari/detecting-and-quantifying-space-time-correlations-in-cell-signaling/).\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Citation\n\nIf you use this plugin in your research, please cite the following [paper](https://doi.org/10.1083/jcb.202207048):\n\n    @article{10.1083/jcb.202207048,\n        author = {Gagliardi, Paolo Armando and Gr√§del, Benjamin and Jacques, Marc-Antoine and Hinderling, Lucien and Ender, Pascal and Cohen, Andrew R. and Kastberger, Gerald and Pertz, Olivier and Dobrzy≈Ñski, Maciej},\n        title = \"{Automatic detection of spatio-temporal signaling patterns in cell collectives}\",\n        journal = {Journal of Cell Biology},\n        volume = {222},\n        number = {10},\n        pages = {e202207048},\n        year = {2023},\n        month = {07},\n        abstract = \"{Increasing experimental evidence points to the physiological importance of space‚Äìtime correlations in signaling of cell collectives. From wound healing to epithelial homeostasis to morphogenesis, coordinated activation of biomolecules between cells allows the collectives to perform more complex tasks and to better tackle environmental challenges. To capture this information exchange and to advance new theories of emergent phenomena, we created ARCOS, a computational method to detect and quantify collective signaling. We demonstrate ARCOS on cell and organism collectives with space‚Äìtime correlations on different scales in 2D and 3D. We made a new observation that oncogenic mutations in the MAPK/ERK and PIK3CA/Akt pathways of MCF10A epithelial cells hyperstimulate intercellular ERK activity waves that are largely dependent on matrix metalloproteinase intercellular signaling. ARCOS is open-source and available as R and Python packages. It also includes a plugin for the napari image viewer to interactively quantify collective phenomena without prior programming experience.}\",\n        issn = {0021-9525},\n        doi = {10.1083/jcb.202207048},\n        url = {https://doi.org/10.1083/jcb.202207048},\n        eprint = {https://rupress.org/jcb/article-pdf/222/10/e202207048/1915749/jcb/_202207048.pdf},\n    }\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ARCOS Main Widget"
    ],
    "contributions_sample_data": [
      "ARCOS Sample Data Synthetic Datase",
      "ARCOS Sample Data Real Dataset"
    ]
  },
  {
    "normalized_name": "napari-labels",
    "name": "napari-labels",
    "display_name": "Labels Control",
    "version": "0.0.4",
    "created_at": "2025-03-26",
    "modified_at": "2025-03-28",
    "authors": [
      "Lars Kr√§mer"
    ],
    "author_emails": [
      "lars.kraemer@dkfz-heidelberg.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-labels/",
    "home_github": "https://github.com/MIC-DKFZ/napari-labels",
    "home_other": null,
    "summary": "Adding intuitive label color management, allowing custom class colors, per-class opacity, and colormap control to enhance visual clarity.",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "napari_toolkit",
      "seaborn",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-labels\n\nA plugin for flexible and intuitive color management of label layers in Napari, designed to enhance the visual clarity and control of your labeled data.\nAssign custom colors to individual classes using a color picker or RGB input, adjust per-class opacity, and apply colormaps to visually distinguish between labels with ease.\n\n## Installation\n\nYou can install `napari-labels` via [pip]:\n\n```\npip install napari-labels\n```\n\n## Seaborn Color palettes\n\nYou can use all color palette names which are valid for `seaborn.color_palette()`.\nAn overview can be found here:\n\n- https://r02b.github.io/seaborn_palettes/\n- https://www.practicalpythonfordatascience.com/ap_seaborn_palette\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-labels\" is free and open source software\n\n## Acknowledgments\n\n<p align=\"left\">\n  <img src=\"https://github.com/MIC-DKFZ/napari-labels/raw/main/imgs/Logos/HI_Logo.png\" width=\"150\"> &nbsp;&nbsp;&nbsp;&nbsp;\n  <img src=\"https://github.com/MIC-DKFZ/napari-labels/raw/main/imgs/Logos/DKFZ_Logo.png\" width=\"500\">\n</p>\n\nThis repository is developed and maintained by the Applied Computer Vision Lab (ACVL)\nof [Helmholtz Imaging](https://www.helmholtz-imaging.de/) and the\n[Division of Medical Image Computing](https://www.dkfz.de/en/medical-image-computing) at DKFZ.\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[copier]: https://copier.readthedocs.io/en/stable/\n[napari]: https://github.com/napari/napari\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Labels Control"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "unet-lungs-segmentation",
    "name": "unet-lungs-segmentation",
    "display_name": "Mouse lungs segmentation",
    "version": "1.0.9",
    "created_at": "2025-03-25",
    "modified_at": "2025-03-28",
    "authors": [
      "Quentin Chappuis",
      "Center for Imaging",
      "Ecole Polytechnique Federale de Lausanne (EPFL)"
    ],
    "author_emails": [
      "quentin.chappuis@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/unet-lungs-segmentation/",
    "home_github": "https://github.com/qchapp/lungs-segmentation.git",
    "home_other": null,
    "summary": "3D U-Net model for the segmentation of the lungs in mice CT scans.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "qtpy",
      "napari[all]>=0.4.16",
      "napari-label-focus",
      "tifffile",
      "scikit-image",
      "matplotlib",
      "csbdeep",
      "python-dotenv",
      "huggingface-hub",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# üê≠ Lungs segmentation in mice CT scans\n\nWe provide a neural network model for segmenting the lungs of the mice. The model is based on the [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) architecture.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/qchapp/lungs-segmentation/refs/heads/master/images/main_fig.png\" height=\"500\">\n</p>\n\nThe goal of our tool is to provid a reliable way to segment the lungs in mouse CT scans. The U-net model produces a binary mask representing the segmentation of the lungs.\n\n## Try the model \n\n- [Install the package](#installation)\n- [Follow the usage instructions](#usage-in-napari)\n\n## Installation\n\nWe recommend performing the installation in a clean Python environment.\n\nThe code requires `python>=3.9`, as well as `pytorch>=2.0`. Please install Pytorch first and separately following the instructions for your platform on [pytorch.org](https://pytorch.org/get-started/locally/).\n\nInstall `unet_lungs_segmentation` using *pip* after you've installed Pytorch:\n\n```sh\npip install unet_lungs_segmentation\n```\n\nor clone the repository and install with:\n\n```sh\ngit clone https://github.com/qchapp/lungs-segmentation.git\npip install -e .\n```\n\n## Models\n\nThe model weights (~1 GB) will be automatically downloaded from [Hugging Face](https://huggingface.co/qchapp/unet-lungs-segmentation-weights).\n\n\n## Usage in Napari\n\n[Napari](https://napari.org/stable/) is a multi-dimensional image viewer for python. To use our model in Napari, start the viewer with\n\n```sh\nnapari\n```\n\nTo open an image, use `File > Open files` or drag-and-drop an image into the viewer window. If you want to open medical image formats such as NIFTI directly, consider installing the [napari-medical-image-formats](https://pypi.org/project/napari-medical-image-formats/) plugin.\n\n**Sample data**: To test the model, you can run it on our provided sample image. In Napari, open the image from `File > Open Sample > Mouse lung CT scan`.\n\nNext, in the menu bar select `Plugins > Lungs segmentation (unet_lungs_segmentation)`. Select an image and run it by pressing the \"Segment lungs\" button.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/qchapp/lungs-segmentation/refs/heads/master/images/napari-screenshot.png\" height=\"500\">\n</p>\n\n## Usage as a library\n\nYou can run a model in just a few lines of code to produce a segmentation mask from an image (represented as a numpy array).\n\n```py\nfrom unet_lungs_segmentation import LungsPredict\n\nlungs_predict = LungsPredict()\nmask = lungs_predict.segment_lungs(your_image)\n```\nor if you want to apply a specific `threshold` (`float` between 0 and 1):\n```py\nmask = lungs_predict.segment_lungs(your_image, threshold)\n```\n\n## Usage as a CLI\n\nRun inference on an image from the command-line. For example:\n\n```sh\nuls_predict_image -i /path/to/folder/image_001.tif [-t <threshold>]\n```\n\nThe `<threshold>` will be applied to the predicted image in order to have a binary mask. A default threshold of 0.5 will be applied if none is given. Should be a `float` between 0 and 1.\n\nThe command will save the segmentation next to the image:\n```\nfolder/\n    ‚îú‚îÄ‚îÄ image_001.tif\n    ‚îú‚îÄ‚îÄ image_001_mask.tif\n```\n\nRun inference in batch on all images in a folder:\n\n```sh\nuls_predict_folder -i /path/to/folder/ [-t <threshold>]\n```\nWill produce:\n```\nfolder/\n    ‚îú‚îÄ‚îÄ image_001.tif\n    ‚îú‚îÄ‚îÄ image_001_mask.tif\n    ‚îú‚îÄ‚îÄ image_002.tif\n    ‚îú‚îÄ‚îÄ image_002_mask.tif\n```\n\n## Dataset\n\nOur model was trained using a dataset of `355` images coming from 17 different experiments, 2 different scanners and validated on `62` images.\n\n## Issues\n\nIf you encounter any problems, please fill an issue along with a detailed description.\n\n## License\n\nThis model is licensed under the [BSD-3](LICENSE.txt) license.\n\n## Carbon footprint of this project\n\nAs per the online tool [*Green algorithms*](http://calculator.green-algorithms.org/), the footprint of training this model was estimated to be around 584 g CO2e.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Lungs segmentation"
    ],
    "contributions_sample_data": [
      "Mouse lung CT scan"
    ]
  },
  {
    "normalized_name": "napari-camera",
    "name": "napari-camera",
    "display_name": "Camera Control",
    "version": "0.0.3",
    "created_at": "2025-03-26",
    "modified_at": "2025-03-27",
    "authors": [
      "Lars Kr√§mer"
    ],
    "author_emails": [
      "lars.kraemer@dkfz-heidelberg.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-camera/",
    "home_github": "https://github.com/MIC-DKFZ/napari-camera",
    "home_other": null,
    "summary": "Provides control over camera settings and allows saving and loading full viewer configurations to easily recreate and share consistent visual perspectives.",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari_toolkit",
      "qtpy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-camera\n\nA Napari plugin that provides control over camera settings and allows saving and loading full\nviewer configurations to easily recreate and share consistent visual perspectives.\n\n## Installation\n\nYou can install `napari-camera` via [pip]:\n\n```\npip install napari-camera\n```\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-camera\" is free and open source software\n\n# Acknowledgments\n\n<p align=\"left\">\n  <img src=\"https://github.com/MIC-DKFZ/napari-camera/raw/main/imgs/Logos/HI_Logo.png\" width=\"150\"> &nbsp;&nbsp;&nbsp;&nbsp;\n  <img src=\"https://github.com/MIC-DKFZ/napari-camera/raw/main/imgs/Logos/DKFZ_Logo.png\" width=\"500\">\n</p>\n\nThis repository is developed and maintained by the Applied Computer Vision Lab (ACVL)\nof [Helmholtz Imaging](https://www.helmholtz-imaging.de/) and the\n[Division of Medical Image Computing](https://www.dkfz.de/en/medical-image-computing) at DKFZ.\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[copier]: https://copier.readthedocs.io/en/stable/\n[napari]: https://github.com/napari/napari\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Camera Control"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-manual-registration",
    "name": "napari-manual-registration",
    "display_name": "Manual registration",
    "version": "0.0.4",
    "created_at": "2024-08-13",
    "modified_at": "2025-03-27",
    "authors": [
      "Alice Gros",
      "Jules Vanaret"
    ],
    "author_emails": [
      "jules.vanaret@univ-amu.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-manual-registration/",
    "home_github": "https://github.com/jules-vanaret/napari-manual-registration",
    "home_other": null,
    "summary": "A simple plugin to register 2 views of the same object",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "magicgui",
      "qtpy",
      "pyclesperanto_prototype",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# :herb: napari-manual-registration\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-manual-registration.svg?color=green)](https://github.com/jules-vanaret/napari-manual-registration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-manual-registration.svg?color=green)](https://pypi.org/project/napari-manual-registration)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-manual-registration.svg?color=green)](https://python.org)\n[![tests](https://github.com/jules-vanaret/napari-manual-registration/workflows/tests/badge.svg)](https://github.com/jules-vanaret/napari-manual-registration/actions)\n[![codecov](https://codecov.io/gh/jules-vanaret/napari-manual-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/jules-vanaret/napari-manual-registration)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-manual-registration)](https://napari-hub.org/plugins/napari-manual-registration)\n\n<img src=\"https://github.com/GuignardLab/tapenade/blob/main/imgs/tapenade3.png\" width=\"100\">\n\nA plugin to obtain parameters for affine transform used to register two views of the same object, e.g as obtained with dual-view microscopes. \n\n`napari-manual-registration` is a [napari] plugin that is part of the [Tapenade](https://github.com/GuignardLab/tapenade) project. Tapenade is a tool for the analysis of dense 3D tissues acquired with deep imaging microscopy. It is designed to be user-friendly and to provide a comprehensive analysis of the data.\n\nIf you use this plugin for your research, please [cite us](https://github.com/GuignardLab/tapenade/blob/main/README.md#how-to-cite).\n\n## Overview\n\nA. Registration by annotating salient landmarks|B. Registration by selecting explicit transformation parameters\n--|--\n<img src=\"imgs/napari_registration_demo2_3.gif\" width=\"100%\" />|<img src=\"imgs/napari_registration_demo_3.gif\" width=\"94%\" />\n\nWhile working with large and dense 3D and 3D+time gastruloid datasets, we found that being able to visualise and interact with the data dynamically greatly helped processing it.\nDuring the pre-processing stage, dynamical exploration and interaction led to faster tuning of the parameters by allowing direct visual feedback, and gave key biophysical insight during the analysis stage. \n\nWhen using our automatic registration tool to spatially register two views of the same organoid, we were sometimes faced with the issue that the tool would not converge to the true registration transformation. This happens when the initial position and orientation of the floating view are too far from their target values. We thus designed a Napari plugin to quickly find a transformation that can be used to initialize our registration tool close to the optimal transformation. From two images loaded in Napari representing two views of the same organoid, the plugin allows the user to \n\n1. **annotate matching salient landmarks** (e.g bright dead cells or lumen-like structures) in both the reference and floating views, from which an optimal rigid transformation can be found automatically using principal component analysis.\n\n\n\n2. **manually define a rigid transformation** by continually varying 3D rotations and translations while observing the results until a satisfying fit is found\n\n<img src=\"imgs/Fig_Napari_registration.png\">\n\n## Installation\n\nThe plugin obviously requires [napari] to run. If you don't have it yet, follow the instructions [here](https://napari.org/stable/tutorials/fundamentals/installation.html).\n\nThe simplest way to install `napari-manual-registration` is via the [napari] plugin manager. Open Napari, go to `Plugins > Install/Uninstall Packages...` and search for `napari-manual-registration`. Click on the install button and you are ready to go!\n\nYou can also install `napari-manual-registration` via [pip]:\n\n    pip install napari-manual-registration\n\nTo install latest development version :\n\n    pip install git+https://github.com/jules-vanaret/napari-manual-registration.git\n\n## Usage\n\nThe plugin provides two methods to register two views of the same object. The first method consists in manually drawing landmarks in both views, from which the optimal transformation is found automatically using principal component analysis. The second one consists in selecting manually each transformation parameter (rotation and translation) while observing the result in real-time, either in 2D or 3D (the user can switch between 2D and 3D at any time).\n\n> [!CAUTION]\n> Be aware that the visualization does not accomodate for voxel anisotropy, so we recommend using isotropic data, or to resize you anisotropic data to an isotropic voxel size (e.g by using [napari-tapenade-processing](https://github.com/GuignardLab/napari-tapenade-processing)).\n\n### A. Registration by annotating salient landmarks\n\n<img src=\"imgs/reg_2_3.png\">\n\nSteps:\n1. First, load your images in Napari. You can drag and drop them from your file explorer to the Napari viewer, or open them using the `File > Open files...` menu.\n2. Click on the `Plugins > Manual Registration` menu to open the plugin.\n3. Select the reference layer from the combo box. The reference layer is chosen to be the one that does not move.\n4. Select the floating layer from the combo box. The floating layer is the one that will be transformed.\n5. Click the `Create landmarks layers` button to create two new Labels layers that will be used to annotate the landmarks in the reference and floating views.\n6. We recommend pressing the `Format layers for landmarks matching` button so that your layers are automatically formatted for you to begin the registration process. Napari offers a wide range of customisation options for the layers appearances, so feel free to play with them if our formatting does not fit your preferences. ;)\n7. We first recommend hiding the reference layer and the reference landmarks layer by clicking on the eye icon next to the layer name in the layer list. This will allow you to focus on the floating layer and the floating landmarks layer.\n8. Click on the `landmarks_floating` layer in the layer list to select it. \n9. Click on the `Activate the paint brush` button in the layer properties widget. This will allow you to draw landmarks in the floating view.\n10. Navigate through the z-slices of your images using the slider at the bottom of the plugin window.\n11. When you have found a salient landmark in the floating view, start drawing a \"blob\" around it by clicking and dragging your mouse. You can adjust the size of the brush using the `Brush size` slider in the layer properties widget. The shape of the \"blob\" you draw does not matter, as the plugin currently only uses the center of mass of the \"blob\" to locate the landmark.\n12. Once you have drawn a landmark, click on the `+` button in the layer properties widget to increase the label value. This will allow you to draw another landmark. Change label value after each landmark you draw. Repeat steps 10 to 12 until you have annotated all the salient landmarks in the floating view.\n13. Once you have annotated all the salient landmarks in the floating view, hide the floating layers, and show the reference layers and the reference landmarks layer by clicking on the eye icon next to the layer name in the layer list.\n14. Click on the `landmarks_reference` layer in the layer list to select it.\n15. Navigate to the z-slice of the reference view that corresponds to the z-slice of the floating view where you drew the first landmark.\n16. Draw a \"blob\" around the corresponding landmark in the reference view by clicking and dragging your mouse.\n17. Increment your label value by clicking on the `+` button in the layer properties widget each time you draw a new widget. Repeat steps 15 to 17 until you have annotated all the salient landmarks in the reference view.\n18. Once you have annotated all the salient landmarks in the reference view, click on the `Run landmark registration` button. The plugin will automatically find the optimal transformation that aligns the floating landmarks to the reference landmarks using principal component analysis. \n19. If you are satisfied with the registration, choose a directory to save the transformation parameters by clicking on the `Choose directory` button. The transformation parameters will be saved in a `.json` file in this directory. Finally, click on the `Save to JSON` button to save the transformation parameters.\n\n### B. Registration by selecting explicit transformation parameters\n\n\n#### Registration with 3D view\n\nWe describe below the steps to register two views of the same object in a purely 3D manner. Note that the plugin also allows to switch between 2D and 3D at any time, and 2D view is described in the next section. \n\n<img src=\"imgs/reg_0.png\">\n\nSteps:\n1. First, load your images in Napari. You can drag and drop them from your file explorer to the Napari viewer, or open them using the `File > Open files...` menu.\n2. Click on the `Plugins > Manual Registration` menu to open the plugin.\n3. Select the reference layer from the combo box. The reference layer is chosen to be the one that does not move.\n4. Select the floating layer from the combo box. The floating layer is the one that will be transformed.\n5. We recommend pressing the `Format layers for explicit registration` button so that your layers are automatically formatted for you to begin the registration process. Napari offers a wide range of customisation options for the layers appearances, so feel free to play with them if our formatting does not fit your preferences. ;)\n6. You can now start the registration process by moving the `Translations` and `Rotations` sliders. The floating layer will be transformed in real-time according to the selected parameters. \n7. To optimize the visibility of your images, you can change the contrast limits and opacity of a layer by clicking on the layer name in the layer list and adjusting the sliders in the layer properties widget.\n8. If you wish to hide a layer, you can click on the eye icon next to the layer name in the layer list.\n9. Once you are satisfied with the registration, choose a directory to save the transformation parameters by clicking on the `Choose directory` button. The transformation parameters will be saved in a `.json` file in this directory.\n10. Finally, click on the `Save to JSON` button to save the transformation parameters.\n\n#### Registration with 2D view\n\n<img src=\"imgs/reg_1.png\">\n\nSteps (the steps 1 to 5 are the same as for the 3D registration):\n6. If you want to switch to the 2D view, click on the `Toggle 2D/3D view` button (it resembles a square when in 2D mode, or a cube when in 3D mode).\n7. Again, feel free to play with the contrast limits and opacity of the layers to optimize the visibility of your images. First click on the layer name in the layer list, then adjust the sliders in the layer properties widget.\n8. If you wish to hide a layer, you can click on the eye icon next to the layer name in the layer list.\n9. In 2D mode, a slider appears at the bottom of the plugin window. You can use it to slide through the z-slices of your images.\n10. You can now start the registration process by moving the `Translations` and `Rotations` sliders. The floating layer will be transformed in real-time according to the selected parameters.\n11. Once you are satisfied with the registration, choose a directory to save the transformation parameters by clicking on the `Choose directory` button. The transformation parameters will be saved in a `.json` file in this directory. Finally, click on the `Save to JSON` button to save the transformation parameters.\n\n## Demo dataset\n\nA demo dataset is available [here](https://amubox.univ-amu.fr/s/HLktPNLGgMF4jHT).\n\n### Content\n\nThis test dataset is composed of two 3D images `bottom_small.tif` and `top_small.tif` that correspond to two halves of the same sample.\n\n### How to use\n\n - Load the images from the folder (either drag and drop, or \"File>Open file(s)\").\n - Specify one of the images as the \"Reference layer\" (which is fixed), and the other one as the \"Layer to move\" (usually called \"floating\").\n - Choose between the \"Explicit transforms\" or \"Landmarks matching\" modes, and follow instructions on the plugin repository for further use.   \n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-manual-registration\" is free and open source software\n\n## Issues\n\nIf you encounter any problem using this plugin, please [file an issue] on the GitHub repository.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jules-vanaret/napari-manual-registration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Manual registration"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-vectorizer",
    "name": "napari-vectorizer",
    "display_name": "Labels Vectorization",
    "version": "0.0.5",
    "created_at": "2025-02-28",
    "modified_at": "2025-03-25",
    "authors": [
      "Nicola Santacroce"
    ],
    "author_emails": [
      "nicola.santacroce@protonmail.com"
    ],
    "license": "Copyright (c) 2025, Nicola San...",
    "home_pypi": "https://pypi.org/project/napari-vectorizer/",
    "home_github": "https://github.com/tha-santacruz/napari-vectorizer",
    "home_other": null,
    "summary": "A plugin to convert label layers to vector layers",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari[all]",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-vectorizer\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vectorizer.svg?color=green)](https://github.com/tha-santacruz/napari-vectorizer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-vectorizer.svg?color=green)](https://pypi.org/project/napari-vectorizer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vectorizer.svg?color=green)](https://python.org)\n[![tests](https://github.com/tha-santacruz/napari-vectorizer/workflows/tests/badge.svg)](https://github.com/tha-santacruz/napari-vectorizer/actions)\n[![codecov](https://codecov.io/gh/tha-santacruz/napari-vectorizer/branch/main/graph/badge.svg)](https://codecov.io/gh/tha-santacruz/napari-vectorizer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vectorizer)](https://napari-hub.org/plugins/napari-vectorizer)\n\nA plugin to convert label layers to vector layers\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-vectorizer` via [pip]:\n\n    pip install napari-vectorizer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/tha-santacruz/napari-vectorizer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-vectorizer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/tha-santacruz/napari-vectorizer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Label Vectorization"
    ],
    "contributions_sample_data": [
      "Napari Vectorizer"
    ]
  },
  {
    "normalized_name": "napari-locan",
    "name": "napari-locan",
    "display_name": "napari-locan",
    "version": "0.7.0",
    "created_at": "2023-10-29",
    "modified_at": "2025-03-17",
    "authors": [
      "napari-locan Developers"
    ],
    "author_emails": [],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-locan/",
    "home_github": "https://github.com/super-resolution/napari-locan",
    "home_other": null,
    "summary": "Use locan methods in napari for single-molecule localization microscopy data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "locan>=0.18",
      "matplotlib",
      "napari",
      "napari-matplotlib",
      "numpy",
      "qtpy",
      "locan[http]>=0.18; extra == \"test\"",
      "pytest; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "black~=25.0; extra == \"dev\"",
      "build; extra == \"dev\"",
      "coverage[toml]; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pandas-stubs; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "twine; extra == \"dev\"",
      "furo; extra == \"docs\"",
      "ipython; extra == \"docs\"",
      "myst-nb; extra == \"docs\"",
      "napari>=0.4.17; extra == \"docs\"",
      "sphinx; extra == \"docs\"",
      "sphinx-autodoc-typehints; extra == \"docs\"",
      "sphinx-copybutton; extra == \"docs\""
    ],
    "package_metadata_description": "![logo](./docs/_static/logo.png) napari-locan\n==================================================\n\n[![License](https://img.shields.io/github/license/super-resolution/napari-locan)](https://github.com/super-resolution/napari-locan/blob/main/LICENSE.md)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-locan)](https://napari-hub.org/plugins/napari-locan)\n[![PyPI](https://img.shields.io/pypi/v/napari-locan.svg?color=green)](https://pypi.org/project/napari-locan)\n[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/napari-locan)](https://anaconda.org/conda-forge/napari-locan)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-locan.svg?color=green)](https://python.org)\n[![test-py-matrix](https://github.com/super-resolution/napari-locan/actions/workflows/test_py_matrix.yml/badge.svg)](https://github.com/super-resolution/napari-locan/actions/workflows/test_py_matrix.yml)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![codecov](https://codecov.io/gh/super-resolution/napari-locan/branch/main/graph/badge.svg)](https://codecov.io/gh/super-resolution/napari-locan)\n[![Documentation Status](https://readthedocs.org/projects/napari-locan/badge/?version=latest)](https://napari-locan.readthedocs.io/en/latest/?badge=latest)\n\nLoad, visualize and analyze single-molecule localization microscopy (SMLM) data.\n\nnapari-locan is a napari plugin that implements a subset of methods from [locan],\na python-based library with code for analyzing SMLM data.\nLocan provides extended functionality that is better suited for script- or\nnotebook-based analysis procedures.\nnapari-locan is well suited for exploratory data analysis within napari.\n\nFor details on usage and development of napari-locan please read the [documentation].\n\n## Installation\n\nMake sure to have Qt bindings installed in your python environment of choice.\n\nYou can install napari-locan from PyPI:\n\n    pip install napari-locan\n\nor from conda-forge:\n\n    mamba install -c conda-forge napari-locan\n\nPlease read the [documentation on installation] for more details.\n\n## Usage\n\n![](https://github.com/super-resolution/napari-locan/raw/main/docs/resources/screenshot_0.png?raw=true)\n\nPlease read the [documentation] for details.\n\n## Contributing\n\nContributions are very welcome.\nPlease read the [documentation on development] for details.\n\n## Credit\n\nThe plugin was developed in the Department of Biotechnology and Biophysics,\nW√ºrzburg University, Germany.\nIt is based on locan. So credit goes to the [locan developers]\nand can be [cited](https://github.com/super-resolution/napari-locan/blob/main/CITATION.cff).\n\n## License\n\nDistributed under the terms of the\n[BSD-3](http://opensource.org/licenses/BSD-3-Clause)\nlicense, \"napari-locan\" is free and open source software.\nSee the [LICENSE](https://github.com/super-resolution/napari-locan/blob/main/LICENSE.md) file for details.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[locan]: https://github.com/super-resolution/locan\n[locan developers]: https://github.com/super-resolution/locan\n\n[documentation]: https://napari-locan.readthedocs.io\n[documentation on installation]: https://napari-locan.readthedocs.io/en/latest/source/installation.html\n[documentation on development]: https://napari-locan.readthedocs.io/en/latest/source/development.html\n[file an issue]: https://github.com/super-resolution/napari-locan/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SMLM data",
      "Show information",
      "Show metadata",
      "Show properties",
      "Show data statistics",
      "Show property distribution",
      "Load",
      "Save",
      "Filter specifications",
      "Select",
      "Render points 2d",
      "Render points 3d",
      "Render image 2d",
      "Render image 3d",
      "Render features",
      "Region of interest",
      "Compute cluster",
      "Render collection 2d",
      "Render collection features",
      "Run script",
      "Project"
    ],
    "contributions_sample_data": [
      "Nuclear Pore Complex (2D)",
      "Tubulin (2D)",
      "Nuclear Pore Complex (2D-point cloud)",
      "Tubulin (2D-point cloud)"
    ]
  },
  {
    "normalized_name": "brainglobe-utils",
    "name": "brainglobe-utils",
    "display_name": "BrainGlobe",
    "version": "0.7.0",
    "created_at": "2024-07-31",
    "modified_at": "2025-03-10",
    "authors": [
      "Adam Tyson"
    ],
    "author_emails": [
      "Adam Tyson <code@adamltyson.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/brainglobe-utils/",
    "home_github": "https://github.com/brainglobe/brainglobe-utils",
    "home_other": "https://brainglobe.info",
    "summary": "Shared general purpose tools for the BrainGlobe project",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "brainglobe-atlasapi>=2.0.1",
      "brainglobe-space",
      "configobj",
      "natsort",
      "nibabel>=2.1.0",
      "numba",
      "numpy",
      "dask",
      "pandas",
      "psutil",
      "pyarrow",
      "PyYAML",
      "scikit-image",
      "scipy",
      "slurmio",
      "tifffile",
      "tqdm",
      "qt-niu",
      "qtpy; extra == \"qt\"",
      "superqt; extra == \"qt\"",
      "brainglobe-utils[qt]; extra == \"napari\"",
      "napari[all]; extra == \"napari\"",
      "black; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pyqt5; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "scikit-image; extra == \"dev\"",
      "setuptools_scm; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "pooch; extra == \"dev\"",
      "brainglobe-utils[napari]; extra == \"dev\""
    ],
    "package_metadata_description": "# brainglobe-utils\n\nShared general purpose tools for the BrainGlobe project, including [citation generation](#citations-for-brainglobe-tools).\n\n## Installation\n\n```bash\npip install brainglobe-utils\n```\n\nTo also include the dependencies required for Qt widgets, use:\n\n```bash\npip install brainglobe-utils[qt]\n```\n\nFor development, clone this repository and install the dependencies with:\n\n```bash\npip install -e .[dev]\n```\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## Citations for BrainGlobe tools\n\n`brainglobe-utils` comes with the `cite-brainglobe` command line tool, to write citations for BrainGlobe tools for you so you don't need to worry about fetching the data yourself.\nYou can read about [how to use the tool](https://brainglobe.info/documentation/brainglobe-utils/citation-module.html) on the documentation website.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "brainmapper"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-biopb",
    "name": "napari-biopb",
    "display_name": "single-cell segmentation",
    "version": "0.1.4",
    "created_at": "2025-02-02",
    "modified_at": "2025-03-07",
    "authors": [
      "Ji Yu"
    ],
    "author_emails": [
      "jyu@uchc.edu"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-biopb/",
    "home_github": "https://github.com/jiyuuchc/napari-biopb",
    "home_other": null,
    "summary": "Performing image analysis by visiting biopb endpoints",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "biopb",
      "opencv-python-headless",
      "grpcio_tools",
      "vedo",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-biopb\n\n[![License MIT](https://img.shields.io/pypi/l/napari-biopb.svg?color=green)](https://github.com/jiyuuchc/napari-biopb/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-biopb.svg?color=green)](https://pypi.org/project/napari-biopb)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-biopb.svg?color=green)](https://python.org)\n[![tests](https://github.com/jiyuuchc/napari-biopb/workflows/tests/badge.svg)](https://github.com/jiyuuchc/napari-biopb/actions)\n[![codecov](https://codecov.io/gh/jiyuuchc/napari-biopb/branch/main/graph/badge.svg)](https://codecov.io/gh/jiyuuchc/napari-biopb)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-biopb)](https://napari-hub.org/plugins/napari-biopb)\n\nPerforming single-cell segmentation by visiting [biopb](https://github.com/jiyuuchc/biopb) endpoints\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-biopb` via [pip]:\n\n    pip install napari-biopb\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jiyuuchc/napari-biopb.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-biopb\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/jiyuuchc/napari-biopb/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BioPB"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-omaas",
    "name": "napari-omaas",
    "display_name": "napari OMAAS",
    "version": "1.0.4",
    "created_at": "2022-08-09",
    "modified_at": "2025-03-06",
    "authors": [
      "Ruben Lopez"
    ],
    "author_emails": [
      "rjlopez2@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-omaas/",
    "home_github": "https://github.com/rjlopez2/napari-omaas",
    "home_other": null,
    "summary": "napari-OMAAS stands for Optical Mapping Acquisition and Analysis Software",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "tqdm",
      "superqt",
      "magicgui",
      "qtpy",
      "opencv-python-headless",
      "sif_parser",
      "napari_matplotlib",
      "napari-tiff",
      "napari-mat-file-reader",
      "opticalmapping",
      "scikit-image",
      "matplotlib",
      "pandas",
      "scipy",
      "tifffile",
      "toml",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "sphinx; extra == \"docs\"",
      "sphinxcontrib-napoleon; extra == \"docs\"",
      "sphinxcontrib-bibtex; extra == \"docs\"",
      "sphinxcontrib-video; extra == \"docs\"",
      "sphinx-autobuild; extra == \"docs\"",
      "sphinx-copybutton; extra == \"docs\"",
      "sphinx-codeautolink; extra == \"docs\"",
      "furo; extra == \"docs\"",
      "myst_nb>=1.0.0; extra == \"docs\"",
      "jupytext; extra == \"docs\"",
      "jupyter-cache; extra == \"docs\"",
      "cupy-cuda11x; extra == \"gpu\""
    ],
    "package_metadata_description": "# napari-omaas\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-omaas.svg?color=green)](https://github.com/rjlopez2/napari-omaas/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-omaas.svg?color=green)](https://pypi.org/project/napari-omaas)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-omaas.svg?color=green)](https://python.org)\n[![tests](https://github.com/rjlopez2/napari-omaas/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-omaas/actions)\n[![codecov](https://codecov.io/gh/rjlopez2/napari-omaas/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-omaas)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-omaas)](https://napari-hub.org/plugins/napari-omaas)\n\nnapari-OMAAS stands for **Optical Mapping Acquisition and Analysis Software** for panoramic heart imaging.\n\nThis napari plugin intends to be an analysis and acquisition tool for optical mapping in potentiometric (V<sub>m</sub>) or calcium (Ca<sup>2+</sup>) fluorescence signals obtained from panoramic imaging of intact hearts.\n\n It supports reading images in `.sif` format and binary files generated from Andor Technologies cameras powered by the [sif_parser] python module.\n\n\n\n<!-- ```{admonition} Experimental ‚ùóÔ∏èüê≤üß™üî≠üêóüí£üö®ü™≤‚ò£Ô∏è‚ùóÔ∏è\n:class: warning\nThis plugin is in early development/experimental stage so expect braking changes and bugs at anytime.\n``` -->\n## Examples\n\n<br /> \n\n### Plot profile\n\nThe following example ilustrate how to perform normalization (pixelwise) on a time serie image and plot its 2d profile along the *t* dimension withing the averaged ROI selected pixels.\n\n![](https://github.com/rjlopez2/napari-omaas/blob/documentation/example_imgs/Oct-31-2023%2016-45-55_plot_profile.gif?raw=true)\n\n----------------------------------\n\n### APD estimation \n\nThe next example shows how to compute action potential duration (APD) in the same image stack.\n\n![](https://github.com/rjlopez2/napari-omaas/blob/documentation/example_imgs/Oct-31-2023%2016-49-02_APD_analysis.gif?raw=true)\n\n----------------------------------\n\nYou can also perform additional operations on images, such as normalization, temporal/spatial filters, segmentation, but also apply more advanced image processing methods such as motion tracking/compensation, etc.\n\n----------------------------------\n\n## Roadmap\n\nThis plugin was aimed to have two major components: **analysis** and **acquisition**. Bellow is a list of the current features that napari-omaas supports:\n\n### Analysis Features\n    \n- [x] Read sif files from Andor Technologies.\n- [x] Display time profile of ROIs on image sequences.\n- [x] Normalize images.\n    - [x] Perform peak analysis of action potential / Calcium traces.\n    - [x] Add motion correction.\n    - [x] APD analysis.\n    - [x] Create activation maps.\n    - [x] Segment images.\n    - [x] Automatic crop and alignment of heart ROIs.\n- [x] Export results, metadata and analysis log.\n\n### Acquisition Features\n\n- [ ] Control Zyla camera for the acquisition of data\n    - [ ] test using the PYME module\n- [ ] Real-time analysis(?)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nAlso review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n\n## Contributing\n\nContributions are very welcome. Run tests with [tox], ensuring\nthe coverage remains the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-omaas\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] and a  detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/rjlopez2/napari-omaas/issues\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[sif_parser]: https://pypi.org/project/sif-parser/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.sif"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Optical Mapping Analysis"
    ],
    "contributions_sample_data": [
      "Example SIF Dataset (Singel illumination RED)",
      "Example spool Dataset (Singel illumination RED)",
      "Example spool Dataset (Dual illumination RED/BLUE)"
    ]
  },
  {
    "normalized_name": "platemapper",
    "name": "platemapper",
    "display_name": "ndev PlateMapper",
    "version": "0.1.1",
    "created_at": "2025-03-05",
    "modified_at": "2025-03-06",
    "authors": [
      "Tim Monko"
    ],
    "author_emails": [
      "timmonko@gmail.com"
    ],
    "license": "Copyright (c) 2025, Tim Monko\n...",
    "home_pypi": "https://pypi.org/project/platemapper/",
    "home_github": null,
    "home_other": "None",
    "summary": "Map microplate treatments to image metadata.",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas[output-formatting]",
      "seaborn",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# platemapper\n\n[![License BSD-3](https://img.shields.io/pypi/l/platemapper.svg?color=green)](https://github.com/ndev-kit/platemapper/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/platemapper.svg?color=green)](https://pypi.org/project/platemapper)\n[![Python Version](https://img.shields.io/pypi/pyversions/platemapper.svg?color=green)](https://python.org)\n[![tests](https://github.com/ndev-kit/platemapper/workflows/tests/badge.svg)](https://github.com/ndev-kit/platemapper/actions)\n[![codecov](https://codecov.io/gh/ndev-kit/platemapper/branch/main/graph/badge.svg)](https://codecov.io/gh/ndev-kit/platemapper)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/platemapper)](https://napari-hub.org/plugins/platemapper)\n\nMap microplate treatments to metadata.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `platemapper` via [pip]:\n\n    pip install platemapper\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"platemapper\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "popidd-io",
    "name": "popidd-io",
    "display_name": "POPIDD IO",
    "version": "0.0.3",
    "created_at": "2024-09-30",
    "modified_at": "2025-03-05",
    "authors": [
      "Ferran Cardoso Rodriguez"
    ],
    "author_emails": [
      "ferran.cardoso@icr.ac.uk"
    ],
    "license": "GNU GENERAL PUBLIC LICENSE\n   ...",
    "home_pypi": "https://pypi.org/project/popidd-io/",
    "home_github": null,
    "home_other": "None",
    "summary": "A simple plugin to read digital pathology images and annotations Made at the IPU (ICR/RMH).",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari",
      "numpy",
      "zarr<3",
      "dask",
      "pathlib",
      "tifffile==2025.1.10",
      "imagecodecs",
      "geopandas",
      "pyarrow",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pre-commit; extra == \"testing\""
    ],
    "package_metadata_description": "# popidd-io\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/popidd-io.svg?color=green)](https://github.com/IntegratedPathologyUnit-ICR/popidd-io/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/popidd-io.svg?color=green)](https://pypi.org/project/popidd-io)\n[![Python Version](https://img.shields.io/pypi/pyversions/popidd-io.svg?color=green)](https://python.org)\n[![tests](https://github.com/IntegratedPathologyUnit-ICR/popidd-io/workflows/tests/badge.svg)](https://github.com/IntegratedPathologyUnit-ICR/popidd-io/actions)\n[![codecov](https://codecov.io/gh/IntegratedPathologyUnit-ICR/popidd-io/branch/main/graph/badge.svg)](https://codecov.io/gh/IntegratedPathologyUnit-ICR/popidd-io)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/popidd-io)](https://napari-hub.org/plugins/popidd-io)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14185575.svg)](https://doi.org/10.5281/zenodo.14185575)\n\n\n\nA simple plugin to read digital pathology images and annotations.\nMade by Ferran Cardoso at the Integrated Pathology Unit (ICR/RMH).\n\nThis is still an experimental and in-development project,\nso expect considerable additions and changes to existing methods.\nDocumentation and tests will be added in the coming weeks.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\n\nSetup conda environment\n\n    mamba create -n popidd_io python pip pyqt\n\nActivate conda environment\n\n    mamba activate popidd_io\n\n### End users\n\nYou can install `popidd-io` via [pip]:\n\n    pip install popidd-io\n\n### Development\n\nInstall test version from project base directory\n\n    pip install -e \".[testing]\"\n\nRun dev environment with\n\n    python developing.py\n\nBefore contributing, please install and use pre-commit hooks:\n\n    pip install pre-commit\n    pre-commit install\n\n## Description\n\nThis plugin brings support for brightfield and fluorescence images to Napari,\nas well as adding support for polygonal annotations in geoJSON files saved by QuPath.\n\nBrightfield images are loaded as a single layer, incorporating also resolution\ninformation if found on the metadata.\nFluorescence images are separated into channels annotated using the information\npresent in the image metadata.\nSupport for this latter modality is still ongoing and will improve in the coming weeks.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nMade by Ferran Cardoso Rodriguez with the help of colleagues at the Integrated Pathology Unit.\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"popidd-io\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.qptiff",
      "*.geojson",
      "*.tif",
      "*.ndpi",
      "*.parquet",
      "*.svs"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Image Loader",
      "Annotation Loader"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-filter-labels-by-prop",
    "name": "napari-filter-labels-by-prop",
    "display_name": "Filter labels by properties",
    "version": "0.1.0",
    "created_at": "2025-01-24",
    "modified_at": "2025-03-04",
    "authors": [
      "Lo√Øc Sauteur"
    ],
    "author_emails": [
      "loic.sauteur@unibas.ch"
    ],
    "license": "Copyright (c) 2025, Lo√Øc Saute...",
    "home_pypi": "https://pypi.org/project/napari-filter-labels-by-prop/",
    "home_github": "https://github.com/loicsauteur/napari-filter-labels-by-prop",
    "home_other": null,
    "summary": "A simple plugin to filter labels by properites.",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "matplotlib",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-filter-labels-by-prop\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-filter-labels-by-prop.svg?color=green)](https://github.com/loicsauteur/napari-filter-labels-by-prop/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-filter-labels-by-prop.svg?color=green)](https://pypi.org/project/napari-filter-labels-by-prop)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-filter-labels-by-prop.svg?color=green)](https://python.org)\n[![tests](https://github.com/loicsauteur/napari-filter-labels-by-prop/workflows/tests/badge.svg)](https://github.com/loicsauteur/napari-filter-labels-by-prop/actions)\n[![codecov](https://codecov.io/gh/loicsauteur/napari-filter-labels-by-prop/branch/main/graph/badge.svg)](https://codecov.io/gh/loicsauteur/napari-filter-labels-by-prop)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-filter-labels-by-prop)](https://napari-hub.org/plugins/napari-filter-labels-by-prop)\n\nA simple plugin to filter labels by properties.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n## Description\n\nThis plugin provides the possibility to filter segmentation objects by measurements\n(shape and intensity). E.g. you segmented your cells, and you want to exclude segmentation objects\nthat have a mean intensity below a certain value.\n\nIt is intended for 2D and 3D images.\n\nYou can interactively set minimum and maximum thresholds on measurement properties, and\nnapari will show a preview of the selection.\n\nMeasurements are based on `scikit-image regionprops`. However, not all properties are\nimplemented, and they are more restricted for 3D images.\n\n## Usage: Quick start\n\n![](https://github.com/loicsauteur/napari-filter-labels-by-prop/raw/main/resources/preview_filter_labels.gif)\n\n1. Start napari\n2. Start the plugin from the menu: `Plugins > Filter labels by properties`\n3. Add a label image\n4. (optionally) Add a corresponding intensity image with the same (Z)YX shape\n5. In the widget, select the property you want to filter on\n6. Adjust the min/max sliders\n7. When you are ready to create a new label layer click the `Create labels` button in the widget\n\n### Usage notes:\n\nWhen dealing with more than 100 label objects in an image, the filtering view update is\ntriggered only once you release the sliders.\n\nAnother similar plugin you could consider checking out:\n[napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops).\n\nPixel/Voxel size are read from the napari layer scale attribute (defaults to 1 if not specified when adding the layer).\nYou can manually enter the size and press the `Set` button, which will set the layer scale,\nand measure the shape properties with calibrated units\n\nThe \"Measure projected shape properties\" option is only available for 3D images.\nIt measures additional properties of Z-projected labels (including: \"area\", \"convex_area\", \"circularity\" and \"perimeter\").\n\nThe \"Measure cytoplasm and cell compartments\" is intended for label images that represent nuclei.\nWith this option selected, cytoplasm and cell masks will be created by a dilation of 5 units (pixels or calibrated).\nMeasurement in those compartments will be made and be used to filter on.\n`Create labels` will also add the respective cytoplasm and cell mask layers to the napari viewer.\n\n<!--\n         ## TODO: add feature measurement also to layer.features?\n-->\n## Installation\n\nYou can install `napari-filter-labels-by-prop` via [pip]:\n\n    pip install napari-filter-labels-by-prop\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/loicsauteur/napari-filter-labels-by-prop.git\n\n<!--\nInstall Test dependencies\n    `pip install -e \".[testing]\"`\n-->\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-filter-labels-by-prop\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/loicsauteur/napari-filter-labels-by-prop/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Filter labels by properties"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sediment",
    "name": "napari-sediment",
    "display_name": "napari Sediment",
    "version": "0.4.0",
    "created_at": "2024-09-23",
    "modified_at": "2025-02-26",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-sediment/",
    "home_github": "https://github.com/guiwitz/napari-sediment",
    "home_other": null,
    "summary": "A plugin to process hyperspectral images of sediments",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy<2",
      "zarr<3",
      "magicgui",
      "qtpy",
      "napari-guitils",
      "napari-convpaint>=0.6.0",
      "superqt",
      "natsort",
      "spectral",
      "matplotlib",
      "scikit-image",
      "scikit-learn",
      "PyYAML",
      "microfilm",
      "dask",
      "distributed",
      "tqdm",
      "cmap",
      "colour-science",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "torch; extra == \"classifier\"",
      "torchvision; extra == \"classifier\""
    ],
    "package_metadata_description": "# napari-sediment\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-sediment.svg?color=green)](https://github.com/guiwitz/napari-sediment/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sediment.svg?color=green)](https://pypi.org/project/napari-sediment)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sediment.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-sediment/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-sediment/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-sediment/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-sediment)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sediment)](https://napari-hub.org/plugins/napari-sediment)\n\nThis napari plugin is designed to hpyerspectral images of sediment cores. It is composed of three interfaces allowing the user to:\n\n- import HDR images\n- normalize the images using white and dark references\n- mask unwanted regions\n- perform spectral dimensionality reduction via minimum noise fraction analysis\n- perform spatial dimensionality reduction based on pixel purity indices\n- identify representative end-members by clustering pure pixels\n- select relevant regions in spectra to compute absorption indices and create absorption maps \n\n### Pre-processing: Sediment widget\n\nThe sediment widget allows the user to import an HDR image and to normalize it using white and dark references. The widget also allows the user to mask unwanted regions of the images.\n\n## Documentation\n\nYou can find a detailed documentation [here](https://guiwitz.github.io/napari-sediment).\n## Installation\n\nCreate a conda environment and activate it. We highly recommend to use the new conda version called mamba to speed up the installation process. You can install it from [here](https://github.com/conda-forge/miniforge#mambaforge). If you don't use mamba, replace the mamba command by conda in the following instructions:\n\n    mamba create -n sediment python=3.9 napari pyqt -c conda-forge\n    mamba activate sediment\n\nThen you can install `napari-sediment` use:\n\n    pip install git+https://github.com/guiwitz/napari-sediment.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-sediment\" is free and open source software\n\n## Authors\n\nThis plugin has been developed by Guillaume Witz at the Data Science Lab of the University of Bern in collaboration with Petra Zahajsk√°, Institue of Geography of the University of Bern. Funding for development was provided by Prof. Martin Grosjean, Institute of Geography of the University of Bern.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-sediment/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.hdr",
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Sediment",
      "HyperAnalysis",
      "SpectralIndices",
      "BatchIndex",
      "BatchPreproc",
      "Concatenation",
      "Convert",
      "Demo Data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-wsi",
    "name": "napari-wsi",
    "display_name": "WSI Reader",
    "version": "1.2.1",
    "created_at": "2023-02-01",
    "modified_at": "2025-02-26",
    "authors": [
      "Philipp Plewa"
    ],
    "author_emails": [
      "Philipp Plewa <philipp.plewa@astrazeneca.com>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-wsi/",
    "home_github": "https://github.com/AstraZeneca/napari-wsi",
    "home_other": null,
    "summary": "A plugin to read whole-slide images within napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.11",
    "package_metadata_requires_dist": [
      "dask>=2025.1",
      "magicgui>=0.10",
      "numpy>=1.26",
      "pillow>=11.1",
      "typing-extensions>=4.6.1",
      "universal-pathlib>=0.2",
      "zarr>=3.0",
      "colorspacious>=1.1.2; extra == 'all'",
      "openslide-python>=1.4; extra == 'all'",
      "pandas>=2.0; extra == 'all'",
      "rasterio>=1.4; extra == 'all'",
      "shapely>=2.0; extra == 'all'",
      "wsidicom>=0.22; extra == 'all'",
      "openslide-python>=1.4; extra == 'openslide'",
      "rasterio>=1.4; extra == 'rasterio'",
      "colorspacious>=1.1.2; extra == 'wsidicom'",
      "pandas>=2.0; extra == 'wsidicom'",
      "shapely>=2.0; extra == 'wsidicom'",
      "wsidicom>=0.22; extra == 'wsidicom'"
    ],
    "package_metadata_description": "# napari-wsi\n\n[![PyPI](https://img.shields.io/pypi/v/napari-wsi.svg?color=green)](https://pypi.org/project/napari-wsi)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-wsi)](https://napari-hub.org/plugins/napari-wsi)\n[![Tests](https://github.com/AstraZeneca/napari-wsi/actions/workflows/main.yml/badge.svg)](https://github.com/AstraZeneca/napari-wsi/actions)\n![Maturity Level-1](https://img.shields.io/badge/Maturity%20Level-ML--1-yellow)\n\nA plugin to read whole-slide images within [napari].\n\n---\n\n## Installation via pip\n\nYou can install `napari-wsi` via [pip]:\n\n```bash\npip install \"napari-wsi[all]>=1.0\"\n```\n\nThis automatically installs all optional backends, as a shortcut for:\n\n```bash\npip install \"napari-wsi[openslide,rasterio,wsidicom]>=1.0\"\n```\n\nIn addition, to be able to read images using the `openslide` backend, it is\nrequired to install the OpenSlide library itself, for example by installing the\n[openslide-bin] python package (also via [pip]).\n\n## Installation via conda\n\nYou can also install `napari-wsi` via [conda]:\n\n```bash\nconda install -c conda-forge \"napari-wsi>=1.0\"\n```\n\nThis already installs all optional dependencies, including OpenSlide.\n\n# Description\n\nThis [napari] plugin provides a widget for reading various whole-slide image\nformats using a common [zarr] store inteface, based on the libraries\n[openslide], [rasterio], and [wsidicom].\n\n# Quickstart\n\nAfter installation, open the `Plugins` menu in the viewer and select\n`WSI Reader` to open the widget. Then select a `Backend` to use, select a `Path`\nto open, and click `Load`.\n\n![The napari viewer displaying a sample image.](./resources/sample_data.jpg)\n\nIf `sRGB` is selected in the `Color Space` menu and an ICC profile is attached\nto the given image, a transformation to this color space will be applied when\nthe image data is read. Otherwise, the raw RGB image data will be displayed.\n\nThis plugin can also be used to open image files via drag and drop into the\nviewer window. The file suffixes '.bif', '.ndpi', '.scn', '.svs' are registered\nwith the `openslide` backend, while the suffixes '.tif' and '.tiff' are\nregistered with the `rasterio` backend. These files can also be opened directly\nfrom the command line or from a python script:\n\n```bash\nnapari CMU-1.svs\n```\n\n```python\nfrom napari import Viewer\n\nviewer = Viewer()\nviewer.open(\"CMU-1.svs\", plugin=\"napari-wsi\")\n```\n\nIt is also possible to use the different backend classes directly, in which case\nsome more features are available, for example:\n\n```python\nfrom napari import Viewer\nfrom napari_wsi.backends.openslide import OpenSlideStore\n\nviewer = Viewer()\n\n# Display the image in the sRGB color space and a physical coordinate system:\nstore = OpenSlideStore(\"CMU-1.svs\", color_space=\"sRGB\")\n(layer,) = store.to_viewer(viewer, spatial_transform=True)\nassert layer.metadata[\"color_space\"] == \"sRGB\"\n\n# Display a scale bar to indicate milli- or micrometers, depending on the zoom level:\nviewer.scale_bar.visible = True\nviewer.scale_bar.colored = True\n```\n\n```python\nfrom napari import Viewer\nfrom napari_wsi.backends.wsidicom import WSIDicomStore\nfrom requests.auth import HTTPBasicAuth\nfrom wsidicom import WsiDicomWebClient\n\nviewer = Viewer()\nclient = WsiDicomWebClient.create_client(\"...\", auth=HTTPBasicAuth(\"...\", \"...\"))\nstore = WSIDicomStore(client=client, study_uid=\"...\", series_uids=\"...\")\nstore.to_viewer(viewer)\n```\n\nThe sample images used above are part of the OpenSlide test data (see [Aperio]\nand [DICOM]).\n\n# Known Issues & Other Notes\n\n- This plugin is prototype research software and there may be **breaking\n  changes** with each release of the plugin, which is also the case for current\n  releases of the [napari] viewer itself.\n- The `wsidicom` backend supports loading annotations together with the image\n  data. However, this may take several minutes, depending on the number and\n  complexity of the annotations. When loading more than a few thousand polygon\n  annotations, make sure that the experimental \"[triangles] speedup\" setting is\n  enabled.\n\n[Aperio]: https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/\n[conda]: https://conda-forge.org/\n[DICOM]: https://openslide.cs.cmu.edu/download/openslide-testdata/DICOM/\n[napari]: https://github.com/napari/napari\n[openslide]: https://github.com/openslide/openslide-python\n[openslide-bin]: https://pypi.org/project/openslide-bin/\n[pip]: https://github.com/pypa/pip\n[rasterio]: https://github.com/rasterio/rasterio\n[triangles]: https://napari.org/island-dispatch/blog/triangles_speedup_beta.html\n[wsidicom]: https://github.com/imi-bigpicture/wsidicom\n[zarr]: https://github.com/zarr-developers/zarr-python\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.ndpi",
      "*.scn",
      "*.bif",
      "*.svs"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "WSI Reader"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "mmv-h4tracks",
    "name": "mmv_h4tracks",
    "display_name": "MMV_H4Tracks",
    "version": "1.2.0",
    "created_at": "2024-03-22",
    "modified_at": "2025-02-24",
    "authors": [
      "Lennart Kowitz",
      "Justin Sonneck"
    ],
    "author_emails": [
      "Lennart Kowitz <lennart.kowitz@isas.de>",
      "Justin Sonneck <justin.sonneck@isas.de>"
    ],
    "license": "Copyright (c) 2022, lennart ko...",
    "home_pypi": "https://pypi.org/project/mmv-h4tracks/",
    "home_github": "https://github.com/MMV-Lab/mmv_h4tracks",
    "home_other": null,
    "summary": "Human in the loop 2d cell migration analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "npe2",
      "napari-plugin-engine>=0.1.4",
      "napari",
      "zarr",
      "cellpose==2.1.0",
      "matplotlib",
      "aicsimageio",
      "scipy>=1.11.0"
    ],
    "package_metadata_description": "# MMV_H4Tracks\n\n[![License](https://img.shields.io/pypi/l/mmv_h4tracks.svg?color=green)](https://github.com/MMV-Lab/mmv_h4tracks/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/mmv_h4tracks.svg?color=green)](https://pypi.org/project/mmv_h4tracks)\n[![Python Version](https://img.shields.io/pypi/pyversions/mmv_h4tracks.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/mmv_h4tracks/workflows/tests/badge.svg)](https://github.com/MMV-Lab/mmv_h4tracks/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/mmv_h4tracks/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/mmv_h4tracks)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/mmv_h4tracks)](https://napari-hub.org/plugins/mmv_h4tracks)\n\nMMV_H4Tracks (Human4Tracks) is a plugin to use with napari for segmenting and tracking cells, which additionally enables user-friendly manual curation of segmentation and tracks and various options for analyzing and evaluating the results.\n\nWe have tested MMV_H4Tracks intensively under Linux and Windows, for Mac there may be problems with parallel computing, which are on our roadmap and will be fixed in a future version.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!-- ## Usage\nLoad a zarr-file consisting of Image, Label and Tracks layer. -->\n\n## Installation\n\nYou can install `mmv_h4tracks` via [pip]:\n\n    pip install mmv_h4tracks\n\n\nBy default, CPU is used for segmentation computing. We did our best to optimize the CPU computing time, but still recommend GPU computing for better performance. For more detailed instructions on how to install GPU support look [here](https://github.com/MouseLand/cellpose#gpu-version-cuda-on-windows-or-linux).\n\n<!-- \n\nTo install latest development version :\n\n    pip install git+https://github.com/MMV-Lab/mmv_h4tracks.git -->\n\n\n## Documentation\nThis plugin was developed to analyze 2D cell migration. It includes the function of segmenting 2D videos using [Cellpose](https://github.com/MouseLand/cellpose) (both CPU and GPU implemented) and then tracking them using different automatic tracking algorithms, depending on the use case. For both segmentation and tracking, we have implemented user-friendly options for manual curation after automatic processing. In conjunction with napari's inherent functionalities, our plugin provides the capability to automatically track data and subsequently process the tracks in three different ways based on the reliability of the automated results. Firstly, any potentially existing incorrect tracks can be rectified in a user-friendly manner, thereby maximizing the evaluation of available information. Secondly, unreliable tracks can be selectively deleted, and thirdly, individual tracks can be manually or semi-automatically created for particularly challenging data, ensuring reliable results. In addition, the manually curated results can be compared with the automatic results in order to obtain a score for the quality of the automatic results. In essence, our tool aims to offer a valuable supplement to the existing fully automated tracking tools and a user-friendly means to analyze videos where fully automated tracking has been previously challenging.\n\nCommon metrics such as speed, cell size, velocity, etc... can then be extracted, plotted and exported from the tracks obtained in this way. Furthermore, the plugin incorporates a functionality to assess the automatic tracking outcomes using a [quality score](https://doi.org/10.1371/journal.pone.0144959). Since automated tracking may not be consistently 100% accurate, presenting a quality measure alongside scientific discoveries becomes essential. This supplementary metric offers researchers valuable insights into the dependability of the produced tracking results, fostering informed data interpretation and decision-making in the analysis of cell migration.\n\nMore detailed information and instructions on each topic can be found in the following sections.\n\n### Get started\n\nTo load your raw data, you can simply drag & drop them into napari. Ensure that the 'Image' combobox displays the correct layer afterward, see example: \n\n![Comboboxes](https://github.com/MMV-Lab/mmv_h4tracks/blob/main/docs/figures/combobox.png?raw=true)\n\nTo load your own segmentation, you can do equivalent.\n\nThe \"save as\" button can be used to save the existing layers (raw, segmentation, tracks) in a single .zarr file, which can be loaded again later using the \"load\" button. The \"save\" button overwrites the loaded .zarr file.\n\nThe computation mode is used to set how many of the available CPU cores (40% or 80%) are to be used for computing the CPU segmentation and tracking and therefore has a direct impact on performance.\n\n\n### Segmentation\n\nFor segmentation, we use the state of the art instance segmentation method Cellpose. We provide a model that we trained and has proven successful for our application ([see more information](https://doi.org/10.1038/s41467-023-43765-3)).\n\n\n#### Automatic instance segmentation\n\nTo start automatic segmentation, a model must first be selected. Automatic segmentation can then be started via \"Run Segmentation\". The \"Preview\" option offers the possibility of segmenting the first 5 frames first in order to obtain an estimate of the expected results, as the computation - depending on the data and hardware - can be time-consuming.\n\n\n##### Custom models\n\nThe plugin supports adding custom Cellpose models. To do so, simply click on \"Add custom Cellpose model\", enter a name to be displayed, select the model path and pass the required parameters. Click [here](https://cellpose.readthedocs.io/en/latest/api.html#id0) for more information about the parameters.\n\n\nTo train your own Cellpose model, [this](https://cellpose.readthedocs.io/en/latest/train.html) might be helpful.\nIn future versions, we plan to support fine-tuning of Cellpose models within the plugin. \n\n\n#### Manual curation\n\nWe provide different options to correct the automatic segmentation:\n\n- `Remove cell` - Click on a cell to remove it. Be aware that removing a cell will split the track the cell belongs to, potentially affecting subsequent tracking.\n- `Next free ID` - Loads the next free label ID, then a false negative cell can be manually annotated using the paint mode.\n- `Select ID` - Click on a cell to load its ID, then this cell can be corrected manually using the paint mode.\n- `Merge cell` - Click on 2 different fragments of the same cell to harmonize their ID. Note: This has no effect on the annotation itself.\n- `Separate` - Click on a cell to assign a new ID to it.\n\n\n### Tracking\n\nThe plugin supports both coordinate-based (LAP) and overlap-based tracking. Overlap-based tracking requires more computation, but can also be used in particularly complicated data for individual cells.\nIn our experience, coordinate-based tracking has proven itself in cases with reliable segmentation. Overlap-based tracking serves as a useful complement in cases where the segmentation is not of sufficient quality.\n\nIf necessary, overlap-based tracking can also be used for single cells. To do this, simply click on the cell after clicking the button.\n\n#### Manual curation\n\nTo correct tracks, the plugin allows you to link or unlink them. For both options, first click on the corresponding button and then on the cell in the respective frame. The action must then be confirmed using the previously clicked button, which now displays \"confirm\".\n\nTo unlink, all you need to do is click on the cell in the first and last frame. So if the cell is tracked from frame 1-100 and the track between frames 1-10 is to be deleted, it is sufficient to click on the cell in frames 1 and 10. If the track is to be deleted between frames 40-60, it is sufficient to click in frames 40 and 60. In this scenario, the rest of the track is then split, i.e. once into a track from frame 1-40 and once into a track from frame 60-100.\n\nIn contrast, to link cells, the corresponding cell in each frame must be clicked. This must be done for all frames, so the track must be gapless.\n\n#### Visualize & filter tracks\n\nThe displayed tracks can be filtered by entering specific track IDs. Click on the \"Show all tracks\" button to display all tracks again.\n\nIndividual tracks can be deleted using the delete function. Note: These are permanently deleted and cannot be restored without re-tracking. In addition, all displayed tracks can be deleted.\n\n### Analysis\n\nThe plugin supports the calculation of various metrics, which can be divided into two categories: migration-based (such as speed, direction, ...) and shape-based (such as size, eccentricity, ...). Through the use of these metrics, a comprehensive understanding of the available data can be obtained.\n\nAll these metrics can be exported to a .csv file. In addition, the tracks can be filtered with a movement minimum (in pixels) and a minimum track length (in frames). Note: All existing tracks are exported in any case, but their results are presented separately.\n \nThe plugin offers the option of filtering the existing tracks according to the metrics. To do this, the corresponding metric can be selected in the plot area and a scatter plot of the data points will be generated using the plot button. Individual data points (/tracks) that are to be displayed can be circled with the mouse and all tracks that are not circled will be hidden. Note: No tracks are deleted in this process. Hiding tracks triggers the filter function in the tracking section, the \"Show all tracks\" button can display all tracks again as described above.\n\n\n### Evaluation\n\nTo be aware of the accuracy of your automatic tracking and segmentation results, we have implemented an option to evaluate your automatic results. Evaluation is always carried out against the latest results of automatic segmentation and automatic tracking or previously created results loaded via the plugin's own load function. We may implement the option to evaluate external segmentations in the future, but for now you can use save and load as a workaround.\n\nTo evaluate results, at least two consecutive frames must be manually corrected first. The plugin saves the previously mentioned automatic or loaded results in the background, so no activation via button or similar is necessary before manual correction.\n\nThe range of frames to be evaluated can be set, for which the results for segmentation and tracking can be calculated independently of each other. \n\n\n#### Segmentation evaluation\n\nIn order to evaluate the segmentation results, a segmentation must first be loaded via the plugin's load function (drag & drop via napari is not sufficient) or computed within the plugin. This can then be corrected manually. For IoU, Dice and Average Precision 50 scores are then calculated for the frames specified by the user. These results are not exported automatically and must therefore be noted down by users themselves.\n\n#### Tracking evaluation\n\nAs for the evaluation of the segmentation, tracking results loaded via the plugin or obtained within the plugin are required. At least 2 consecutive frames must be corrected manually so that a score can be calculated for the quality of the tracking results. More information can be found [here](https://doi.org/10.1371/journal.pone.0144959).\n\n\n### Assistant\n\nThe assistant tab serves to facilitate the identification of errors within segmentation and tracking. It is divided into filters and segmentation adaptation.\n\nRecommended filter strategy:\n1. \"Show noteworthy tracks\" to discover tracks that are not close to the edge and emerge or disappear after the movie starts. Tracks must be gapless, and resulting hits can be indications of errors.\n2. \"Show small cells\" to double-check if there is any noise/pollution segmented.\n3. \"Show untracked cells\" to identify untracked segmented instances. \n\nThe other filters can provide additional support.\n\nThe segmentation adaptation functions supplement useful functions with respect to segmentation. \"Align segmentation IDs\" adapts the label IDs with regard to the tracking IDs (the label IDs are then no longer arbitrary). \"Relabel cells\" ensures that the labels within a frame are unique, which helps eliminate accidental errors during manual segmentation correction.\n\n\n## Hotkeys\n\nHere's an overview of the hotkeys. All of them can also be found in the corresponding tooltips. \n\n- `W` - Load next free segmentation ID\n- `G` - Overlap-based single cell tracking \n- `H` - Separate cells\n- `Q` - Select cell ID \n\n\n## Development plan\n\nWe will continue to develop the plugin and implement new features in the future. Some of our plans in arbitrary order:\n\n- Feedback (progress bar) for computationally intensive functions\n- Support of lineages\n- Support training custom Cellpose models within the plugin\n- Model optimization to further optimize segmentation computation\n- Support evaluation of external segmentations\n- Improve robustness of Mac computing\n- ...\n\nIf you have a feature request, please [file an issue].\n\n## Resources\n\nThe following resources may be of interest:\n\n- [napari](https://napari.org/)\n- [Cellpose](https://doi.org/10.1038/s41592-020-01018-x)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"mmv_h4tracks\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MMV-Lab/mmv_h4tracks/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MMV_H4Tracks"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cell-aap",
    "name": "cell-AAP",
    "display_name": "cell-AAP",
    "version": "0.0.9",
    "created_at": "2024-05-28",
    "modified_at": "2025-02-20",
    "authors": [
      "Anish Virdi"
    ],
    "author_emails": [],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/cell-aap/",
    "home_github": null,
    "home_other": "None",
    "summary": null,
    "categories": [],
    "package_metadata_requires_python": "<3.13,>=3.11",
    "package_metadata_requires_dist": [
      "napari[all]>=0.4.19",
      "numpy==1.26.4",
      "opencv-python>=4.9.0",
      "tifffile>=2024.2.12",
      "torch>=2.3.1",
      "torchvision>=0.18.1",
      "scikit-image>=0.22.0",
      "qtpy>=2.4.1",
      "pillow>=10.3.0",
      "scipy>=1.3.0",
      "timm>=1.0.7",
      "pandas>=2.2.2",
      "superqt>=0.6.3",
      "btrack>=0.6.5",
      "seaborn>=0.13.2",
      "openpyxl>=3.1.4",
      "joblib>=1.0",
      "scikit-learn>=0.22",
      "cython<3,>=0.27"
    ],
    "package_metadata_description": "# Cellular Annotation & Perception Pipeline\n\n![](https://github.com/anishjv/cell-AAP/blob/main/images/figure2.png?raw=true)\n![](https://github.com/anishjv/cell-AAP/blob/main/images/rpe1_u2os.png?raw=true)\n\n\n\n\nUtilities for the semi-automated generation of instance segmentation annotations to be used for neural network training. Utilities are built ontop of [UMAP](https://github.com/lmcinnes/umap), [HDBSCAN](https://arxiv.org/abs/1911.02282) and a finetuned encoder version of FAIR's [Segment Anything Model](https://github.com/facebookresearch/segment-anything/tree/main?tab=readme-ov-file) developed by Computational Cell Analytics for the project [micro-sam](https://github.com/computational-cell-analytics/micro-sam/tree/master/micro_sam/sam_annotator). In addition to providing utilies for annotation building, we train networks using FAIR's [detectron2](https://github.com/facebookresearch/detectron2) to \n1. Demonstrate the efficacy of our utilities. \n2. Be used for microscopy annotation of supported cell lines \n\nCell-line specific models currently include:\n1. HeLa\n2. U2OS\n\nModels have demonstrated performance efficacy on:\n1. HT1080 (HeLa model)\n2. RPE1 (U2OS model)\n\nWe've also developed a napari application for the usage of these pre-trained networks.\n\n\n# Installation \nWe highly recommend installing cell-AAP in a clean conda environment. To do so you must have [miniconda](https://docs.anaconda.com/free/miniconda/#quick-command-line-install) or [anaconda](https://docs.anaconda.com/free/anaconda/) installed.\n\nIf a conda distribution has been installed:\n\n1. Create and activate a clean environment \n\n        conda create -n cell-aap-env python=3.11.0\n        conda activate cell-app-env\n\n2. Within this environment install pip\n\n        conda install pip\n\n3. Then install cell-AAP from PyPi\n\n        pip install cell-AAP --upgrade\n\n4. Finally detectron2 must be built from source, atop cell-AAP\n    \n        #For MacOS\n        CC=clang CXX=clang++ ARCHFLAGS=\"-arch arm64\" python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n\n        #For other operating systems \n        python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n\n\n\n# Napari Plugin Usage\n\n1. To open napari simply type \"napari\" into the command line, ensure that you are working the correct environment\n2. To instantiate the plugin navigate to the \"Plugins\" menu and select \"cell-AAP\"\n3. You should now see the Plugin, where you can select an image, display it, and run inference on it. \n\n\n# Configs Best Practices\n\nIf running inference on large volumes of data, i.e. timeseries data >= 300 MB in size, we recommend to proceed in the following manner. \n\n1. Assemble a small, < 100 MB, substack of your data using python or a program like [ImageJ](https://imagej.net/ij/download.html)\n2. Use this substack to find the optimal parameters for your data, (Number of Cells, Network confidence threshold)\n3. Run Inference over the volume using the discovered optimal parameters\n\n\n# Interpreting Results \n\nOnce inference is complete the following colors indicate class prediction\n- Red: Non-mitotic\n- Blue: Mitotic\n\nFor analysis purposes, masks in the semantic and instance segmentations have the following value mapping:\nSemantic\n- 1: Non-mitotic\n- 100: Mitotic\n\nInstance\n- $2x$: Non-mitotic\n- $2x-1$: Mitotic\n\n\n\n\n\n\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Inference GUI",
      "Batch Inference GUI"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-melt-pool-tracker",
    "name": "napari-melt-pool-tracker",
    "display_name": "Melt Pool Tracker",
    "version": "0.1.3",
    "created_at": "2023-11-02",
    "modified_at": "2025-02-20",
    "authors": [
      "Florian Aymanns"
    ],
    "author_emails": [
      "florian.aymanns@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-melt-pool-tracker/",
    "home_github": "https://github.com/EPFL-Center-for-Imaging/napari-melt-pool-tracker",
    "home_other": null,
    "summary": "Plugin for tracking the width and depth of the melt pool and keyhole in x-ray images of laser powder bed fusion experiments.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "h5py",
      "napari-cursor-tracker",
      "napari",
      "pandas",
      "scikit-image",
      "tifffile",
      "scipy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "<img style=\"float: right;\" src=\"https://imaging.epfl.ch/resources/logo-for-gitlab.svg\">\n\n\n# napari-melt-pool-tracker\nDeveloped by the [EPFL Center for Imaging](https://imaging.epfl.ch/) for the [Thermomechanical Metallurgy Laboratory](https://www.epfl.ch/labs/lmtm/) in Sep 2023.\nPlugin for tracking the width and depth of the melt pool and keyhole in x-ray images of laser powder bed fusion experiments.\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-melt-pool-tracker.svg?color=green)](https://github.com/EPFL-Center-for-Imaging/napari-melt-pool-tracker/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-melt-pool-tracker.svg?color=green)](https://pypi.org/project/napari-melt-pool-tracker)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-melt-pool-tracker.svg?color=green)](https://python.org)\n[![tests](https://github.com/EPFL-Center-for-Imaging/napari-melt-pool-tracker/workflows/tests/badge.svg)](https://github.com/EPFL-Center-for-Imaging/napari-melt-pool-tracker/actions)\n[![codecov](https://codecov.io/gh/EPFL-Center-for-Imaging/napari-melt-pool-tracker/branch/main/graph/badge.svg)](https://codecov.io/gh/EPFL-Center-for-Imaging/napari-melt-pool-tracker)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-melt-pool-tracker)](https://napari-hub.org/plugins/napari-melt-pool-tracker)\n[![DOI](https://zenodo.org/badge/700413345.svg)](https://zenodo.org/doi/10.5281/zenodo.11366048)\n\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-melt-pool-tracker` via [pip]:\n\n    pip install napari-melt-pool-tracker\n\n# Getting Started with napari-melt-pool-tracker\n\n## Reading Data\n\n- The `napari-melt-pool-tracker` plugin can read h5 files from the ID19 and TOMCAT beam lines.\n- When opening an h5 file in napari, select the \"Melt Pool Tracker\" as the reader for the mentioned beamlines.\n- Once the data is loaded, you have the option to save the layer as a tif file if needed.\n\n## Pre-processing\n\n- For large images, it is recommended to crop them in both time and space to include only the relevant parts of the image stack.\n\n## 1. Determine Laser Speed and Position\n\n- This step helps identify the laser in the images for later reslicing the stack with a moving window following the laser.\n- It generates a projection of the stack along the y-axis, creating an x-t image where the laser's position appears as an oblique line in the projection.\n\n**To perform this step:**\n\n1. Select the stack you want to work on using the \"Input\" drop-down menu.\n2. Choose one of three projection modes:\n   - Default: Maximum projection along y.\n   - Pre mean: Divide each frame by the mean projection along the t-axis (to remove background) and then perform a maximum projection along y.\n   - Post median: Perform a maximum projection along y and then divide the projected images by a median-filtered version in the x-direction (to remove horizontal strips).\n3. Click \"Run\" to generate a new layer with the projected image and a shapes layer with a line.\n4. Select the line layer, use the \"Select vertices\" tool to match the line with the laser in the projected image.\n\n## 2. Reslice with Moving Window\n\n- This step reslices the stack with a moving window that follows the laser's position.\n\n**To perform this step:**\n\n1. Select the input stack using the \"Stack\" drop-down menu.\n2. Choose the line layer with the laser's position using the \"Line\" drop-down menu.\n3. Adjust the \"Left margin\" and \"Right margin\" sliders to set the size of the window to the left and right of the laser's position.\n4. Click \"Run\" to create three new layers: a resliced stack, a shapes layer indicating the laser's position based on your previous annotation, and a shapes layer with lines indicating the window's position in the original image.\n5. If the window size doesn't fit the melt pool correctly, adjust it using the margin sliders. Disable the \"Auto run\" checkbox for large stacks to control when reslicing occurs.\n\n## 3. Filter Image\n\n- This step aims to reduce noise in the images by applying a median filter.\n\n**To filter the image:**\n\n- Select the resliced layer as the input.\n- Use the \"Kernel\" sliders to set the size of the median filter along different axes.\n- Disable \"Auto run\" for large stacks due to the computational cost. After median filtering, the function applies Otsu thresholding to remove the background. Adjust the contrast as needed.\n\n## 4. Calculate Radial Gradient\n\n- This step calculates the gray value gradient in the radial direction with respect to a point on the surface, forming the origin. You can set the horizontal position of the origin using the position slider.\n\n**To calculate the radial gradient:**\n\n- Select the resliced and filtered stack as input.\n- Adjust the contrast for the new radial gradient layer.\n\n## 5. Annotate\n\n- Annotation of points is done using the [napari-cursor-tracker](https://www.napari-hub.org/plugins/napari-cursor-tracker) plugin.\n\n**To annotate points:**\n\n- Select any of the resliced layers as your reference image.\n- Change the name in the \"Name of the tracked point\" text box to define the point you want to track, e.g., 'MP depth'.\n- Click \"Add new layer\" to create a new points layer with the specified name, automatically selected as the active layer.\n- Start tracking by pressing 't' on your keyboard. Enable \"Auto play when tracking is started\" for automatic playback.\n- Adjust playback parameters as needed. Setting \"Loop mode\" to 'once' is advised to prevent overwriting tracked points. You can track points manually by scrolling through slices/frames (hold down `ctrl`) and saving your cursor positions at each index change.\n\n## Saving and Processing Results\n\n- You can save the 'window_coordinates' layer and point layers with tracked points as CSV files for further processing with external software.\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-melt-pool-tracker\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.h5"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Melt pool tracker"
    ],
    "contributions_sample_data": [
      "Melt Pool Tracker"
    ]
  },
  {
    "normalized_name": "in-silico-fate-mapping",
    "name": "in-silico-fate-mapping",
    "display_name": "In Silico Fate Mapping",
    "version": "0.1.3",
    "created_at": "2023-02-27",
    "modified_at": "2025-02-14",
    "authors": [
      "Jordao Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/in-silico-fate-mapping/",
    "home_github": "https://github.com/royerlab/in-silico-fate-mapping",
    "home_other": null,
    "summary": "TODO",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "scikit-learn",
      "zarr",
      "magicgui",
      "qtpy",
      "napari",
      "click",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# in silico fate mapping\n\n[![License BSD-3](https://img.shields.io/pypi/l/in-silico-fate-mapping.svg?color=green)](https://github.com/royerlab/in-silico-fate-mapping/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/in-silico-fate-mapping.svg?color=green)](https://pypi.org/project/in-silico-fate-mapping)\n[![Python Version](https://img.shields.io/pypi/pyversions/in-silico-fate-mapping.svg?color=green)](https://python.org)\n[![tests](https://github.com/royerlab/in-silico-fate-mapping/workflows/tests/badge.svg)](https://github.com/royerlab/in-silico-fate-mapping/actions)\n[![codecov](https://codecov.io/gh/royerlab/in-silico-fate-mapping/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/in-silico-fate-mapping)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/in-silico-fate-mapping)](https://napari-hub.org/plugins/in-silico-fate-mapping)\n\n\nInteractive in silico fate mapping from tracking data.\n\nThis napari plugin estimates the cell fates from tracking data by building a radial regression model per time point. The user can select an area of interest using a `Points` layer; the algorithm will advent the probed coordinates forward (or backward) in time, showing the estimated fate.\n\nVideo example below:\n\nhttps://user-images.githubusercontent.com/21022743/216478216-89c1c35f-2ce4-44e8-adb8-9aeea75b5833.mp4\n\n## Installation\n\nWe suggest you create a fresh conda environment to avoid conflicts with your existing package.\nTo do this, you need to:\n\n    conda create -n fatemap python=3.11\n    conda activate fatemap\n\nAnd then, you can install `in-silico-fate-mapping` via [pip] and other additional useful packages:\n\n    pip install napari-ome-zarr napari[all] in-silico-fate-mapping\n\nTo install the latest development version :\n\n    pip install git+https://github.com/royerlab/in-silico-fate-mapping.git\n\n\n## IO file format\n\nThis plugin does not depend on a specific file format, the only requirement is using a `Track` layer from napari.\n\nDespite this, we ship a reader and writer interface. It supports `.csv` files with the following reader `track_id, t, (z), y, x`, `z` is optional.\nSuch that each tracklet has a unique `track_id` and it's composed of a sequence o time and spatial coordinates.\n\nThis is extremely similar to how napari store tracks, more information can be found [here](https://napari.org/stable/howtos/layers/tracks.html).\n\nDivisions are not supported at the moment.\n\n## Usage Example\n\n### Minimal example\n\nMinimal example using a track file following the convention described above.\n\n```python3\nimport napari\nimport pandas as pd\nfrom in_silico_fate_mapping.fate_mapping import FateMapping\n\ntracks = pd.read_csv(\"tracks.csv\")\n\nfate_map = FateMapping(radius=5, n_samples=25, bind_to_existing=False, sigma=1)\nfate_map.data = tracks[[\"track_id\", \"t\", \"z\", \"y\", \"x\"]]\n\nsource = tracks[tracks[\"t\"] == 0].sample(n=1)\n\ntracks = fate_map(source[[\"t\", \"z\", \"y\", \"x\"]])\n\nnapari.view_tracks(tracks)\nnapari.run()\n```\n\n### Zebrahub example\n\nZebrafish embryo tail example. This example requires the package `napari-ome-zarr`.\n\n```python3\nimport napari\nimport pandas as pd\nfrom in_silico_fate_mapping import FateMappingWidget\n\nimage_path = \"http://public.czbiohub.org/royerlab/zebrahub/imaging/single-objective/ZSNS001_tail.ome.zarr\"\ntracks_path = \"http://public.czbiohub.org/royerlab/zebrahub/imaging/single-objective/ZSNS001_tail_tracks.csv\"\n\nviewer = napari.Viewer()\nviewer.window.add_dock_widget(FateMappingWidget(viewer))\n\nviewer.open(image_path, plugin=\"napari-ome-zarr\")\n\ntracks = pd.read_csv(tracks_path)\nviewer.add_tracks(tracks[[\"track_id\", \"t\", \"z\", \"y\", \"x\"]])\nviewer.add_points(name=\"Markers\", ndim=4)\n\nnapari.run()\n```\n\n## Citing\n\nIf used please cite:\n\n```\n@article{lange2023zebrahub,\n  title={Zebrahub-Multimodal Zebrafish Developmental Atlas Reveals the State Transition Dynamics of Late Vertebrate Pluripotent Axial Progenitors},\n  author={Lange, Merlin and Granados, Alejandro and VijayKumar, Shruthi and Bragantini, Jordao and Ancheta, Sarah and Santhosh, Sreejith and Borja, Michael and Kobayashi, Hirofumi and McGeever, Erin and Solak, Ahmet Can and others},\n  journal={bioRxiv},\n  pages={2023--03},\n  year={2023},\n  publisher={Cold Spring Harbor Laboratory}\n}\n```\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/royerlab/in-silico-fate-mapping/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [
      ".csv"
    ],
    "contributions_widgets": [
      "Fate Mapping"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napam",
    "name": "napam",
    "display_name": "NaPaM",
    "version": "0.1.4",
    "created_at": "2024-04-26",
    "modified_at": "2025-02-10",
    "authors": [
      "Jaison John"
    ],
    "author_emails": [
      "jjohn@stjude.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napam/",
    "home_github": "https://github.com/JB4Jaison/napam",
    "home_other": null,
    "summary": "A plugin that allows you to run macros (i.e. python scripts) on the images for any kind of image processing.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "QScintilla",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "qtpy; extra == \"testing\"",
      "QScintilla; extra == \"testing\""
    ],
    "package_metadata_description": "# napam\n\n[![License BSD-3](https://img.shields.io/pypi/l/napam.svg?color=green)](https://github.com/JB4Jaison/napam/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napam.svg?color=green)](https://pypi.org/project/napam)\n[![Python Version](https://img.shields.io/pypi/pyversions/napam.svg?color=green)](https://python.org)\n[![codecov](https://codecov.io/gh/JB4Jaison/NaPaM/graph/badge.svg?token=5NEDOJB5V1)](https://codecov.io/gh/JB4Jaison/NaPaM)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napam)](https://napari-hub.org/plugins/napam)\n\nA plugin that allows you to run macros on the images for any kind of image processing.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napam` via [pip]:\n\n    pip install napam\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n# Demo\nThis is to demonstrate how the plugin can be used to run any script on an image.\n<video src=\"https://github.com/user-attachments/assets/24bebe67-8186-4189-8679-2148cbe26859\" width=\"352\" height=\"720\"></video>\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napam\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Napari Macro"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-segselect",
    "name": "napari-segselect",
    "display_name": "SegSelect",
    "version": "0.1.3",
    "created_at": "2025-01-31",
    "modified_at": "2025-02-06",
    "authors": [
      "Benedikt Wimmer"
    ],
    "author_emails": [
      "b.wimmer@bioc.uzh.ch"
    ],
    "license": "Copyright (c) 2025, Benedikt W...",
    "home_pypi": "https://pypi.org/project/napari-segselect/",
    "home_github": "https://github.com/bwmr/napari-segselect",
    "home_other": null,
    "summary": "Select a connected component from a membrain-seg segmentation.",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "mrcfile",
      "scipy",
      "napari; extra == \"testing\"",
      "ruff; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-segselect\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-segselect.svg?color=green)](https://github.com/bwmr/napari-segselect/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-segselect.svg?color=green)](https://pypi.org/project/napari-segselect)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segselect.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segselect)](https://napari-hub.org/plugins/napari-segselect)\n\nSelect a connected component from a [membrain-seg](https://github.com/teamtomo/membrain-seg) segmentation.\n\n## Usage\n\n1. Run `membrain-seg` with the `--store-connected-components` flag (optional, otherwise connected components will be calculated while opening)\n2. Open the segmentation in Napari, find out which component numbers correspond to your feature.\n    ![Label Layer](images/image2.png)\n3. Enter these numbers and a feature name in the widget, press run. \n    ![Widget](images/image3.png)\n4. Save the resulting layer using naparis built-in dialog. \n5. Now you have a standalone binary segmentation of your feature of interest.\n    ![Output](images/image4.png)\n\n\n## Installation\n\nYou can install `napari-segselect` via [pip]:\n\n    pip install napari-segselect\n   \nOr directly from GitHub:\n\n    pip install git+https://github.com/bwmr/napari-segselect.git\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-segselect\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/bwmr/napari-segselect/issues\n\n[napari]: https://github.com/napari/napari\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mrc"
    ],
    "contributions_writers_filename_extensions": [
      ".mrc"
    ],
    "contributions_widgets": [
      "Select Label"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-brainbow-diagnose",
    "name": "napari-brainbow-diagnose",
    "display_name": "Napari Brainbow Diagnose",
    "version": "0.2.0",
    "created_at": "2023-01-25",
    "modified_at": "2025-02-04",
    "authors": [
      "Clement Caporal"
    ],
    "author_emails": [
      "clement.caporal@polytechnique.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-brainbow-diagnose/",
    "home_github": "https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose",
    "home_other": null,
    "summary": "Visualize and Diagnose brainbow dataset in color space.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "pooch",
      "matplotlib",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "pre-commit; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-brainbow-diagnose\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-brainbow-diagnose.svg?color=green)](https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-brainbow-diagnose.svg?color=green)](https://pypi.org/project/napari-brainbow-diagnose)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brainbow-diagnose.svg?color=green)](https://python.org)\n[![tests](https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/workflows/tests/badge.svg)](https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/actions)\n[![codecov](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/branch/main/graph/badge.svg)](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-brainbow-diagnose)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-brainbow-diagnose)](https://napari-hub.org/plugins/napari-brainbow-diagnose)\n\nExplore image in channel coordinate spaces.\n\n\n**Original motivation**: Brainbow dataset have unique features that need to be addressed by specialized tools.\nThis plugin allows you to visualize the distribution of the channel ratio interactively in the image space and channel spaces.\n\nYou can also use this plugin along with the [`napari-cluster-plotter` plugin](https://github.com/BiAPoL/napari-clusters-plotter?tab=readme-ov-file#installation) to interact with individual objects.\n\n![demo_gif](https://raw.githubusercontent.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/main/docs/demo_napari-brainbow-diagnose.gif)\n\n## Available Channel space transformation\n\nThe following channel spaces are available:\n\n![image|width=10](https://github.com/user-attachments/assets/0dae9090-da16-4653-b466-a08289e061ea)\n\n\nFrom Cartesian RGB:\n- (a) Maxwell triangle (ternary plot) [illustration](https://en.wikipedia.org/wiki/Ternary_plot)\n- (c) Hue-Saturation wheel [illustration (g)](https://en.wikipedia.org/wiki/File:Hsl-hsv_models.svg)\n- (e) Spherical coordinates (Theta, Phi and Radius) [illustration](https://en.wikipedia.org/wiki/Spherical_coordinate_system)\n- (g,i) Hue-Saturation-Value planes [illustration (b)(f)](https://en.wikipedia.org/wiki/File:Hsl-hsv_models.svg)\n\n## Example Notebooks\n\nYou can use this plugin to visualize channel space of:\n- interactively every voxel in the image (see [demo notebook](docs/demo.ipynb))\n- interactively every object (aka center point) in the image (see [demo notebook](docs/cluster_plotter_compatibility.ipynb)). To use this notebook you need to install [`napari-cluster-plotter` plugin](https://github.com/BiAPoL/napari-clusters-plotter?tab=readme-ov-file#installation).\n- Not interactive in matplotlib to export figures: (see [demo notebook](docs/plot_color_space_matplotlib.ipynb))\n\n## Example Datasets\n\nIf you want to use your dataset, you have to format it such as each channel is in one distinct `napari.Layers`\nYou can open test dataset to try this plugin in `File > Open Sample > napari-brainbow-diagnose`.\n\n- The RGB Cube is an array with shape (3x256x256x256) cube : Great to check how the plugin work when all color are represented\n- ChroMS Cortex Sample is an array with shape (3x256x256x256) #Hugo : Real life brainbow image (Cortex E18 Emx1Cre) !\n\nOnce you have your layers you can use the dropdown and select the corresponding layer. It is advised to match the `red, green, blue` order so the ratio you see on the napari viewer corresponds to the Hue-Saturation Wheel of the plugin.\n\n## Installation\n\nYou can install `napari-brainbow-diagnose` via [pip]:\n\n    pip install napari-brainbow-diagnose\n\nIf you want to use [`napari-cluster-plotter` plugin](https://github.com/BiAPoL/napari-clusters-plotter?tab=readme-ov-file#installation)  you also need to install it manually\n\nTo install latest development version :\n\n    pip install git+https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-brainbow-diagnose\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/LaboratoryOpticsBiosciences/napari-brainbow-diagnose/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Load a Points layer",
      "Add color features to existing Points layer",
      "Diagnose Brainbow Image",
      "Extract RGB features from Brainbow image",
      "RGB features to all channel spaces",
      "Brainbow Tooltip"
    ],
    "contributions_sample_data": [
      "RGB Cube",
      "Chroms Cortex Sample"
    ]
  },
  {
    "normalized_name": "mmv-playground",
    "name": "MMV-playground",
    "display_name": "MMV-playground",
    "version": "0.1.4",
    "created_at": "2025-01-30",
    "modified_at": "2025-01-31",
    "authors": [
      "Peter Lampen"
    ],
    "author_emails": [
      "lampen@isas.de"
    ],
    "license": "Copyright (c) 2024, Peter Lamp...",
    "home_pypi": "https://pypi.org/project/mmv-playground/",
    "home_github": "https://github.com/MMV-Lab/MMV-playground",
    "home_other": null,
    "summary": "Napari plugin for 2D microscopy segmentation",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "aicssegmentation",
      "itk",
      "napari",
      "numpy",
      "qtpy",
      "scikit-image",
      "scipy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# MMV-playground\n\n[![License BSD-3](https://img.shields.io/pypi/l/MMV-playground.svg?color=green)](https://github.com/MMV-Lab/MMV-playground/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/MMV-playground.svg?color=green)](https://pypi.org/project/MMV-playground)\n[![Python Version](https://img.shields.io/pypi/pyversions/MMV-playground.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/MMV-playground/workflows/tests/badge.svg)](https://github.com/MMV-Lab/MMV-playground/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/MMV-playground/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/MMV-playground)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/MMV-playground)](https://napari-hub.org/plugins/MMV-playground)\n\nThis plugin is aimed at researchers in biology and medicine who want to segment and analyze 2D microscopy images. It offers intuitive tools for common pre-processing and analysis tasks.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `MMV-playground` via [pip]:\n\n    pip install MMV-playground\n\nTo install latest development version :\n\n    pip install git+https://github.com/MMV-Lab/MMV-playground.git\n\n## Documentation\n\nThis plugin for the graphics software Napari is designed to analyze two-dimensional microscopy images. Images should be provided in grayscale format, as colored images are not supported. The plugin includes seven core functions for image analysis:\n\n1. Intensity normalization  \n2. Smoothing  \n3. Background correction  \n4. Spot-shape filter  \n5. Filament-shape filter  \n6. Thresholding  \n7. Topology-preserving thinning  \n\nWe provide an [introduction](https://github.com/MMV-Lab/MMV-playground/blob/main/docs/introduction.md) to explain the different functions of this plugin.\n\n### **How to Start and Use the Plugin**\n\nTo start the plugin, open Napari, go to the \"Plugins\" menu, and select \"MMV-playground (MMV-playground).\" The MMV-playground interface will appear on the right-hand side of the Napari window, displaying seven buttons, each corresponding to one of the available functions. Clicking a button opens a dialog box where you can select an image, adjust the parameters for the chosen function, and execute it by pressing the \"Run\" button. Clicking the function button again collapses the dialog box.\n\n### Screenshot\n\nHere is a preview of the MMV-playground plugin in action:\n\n![MMV-playground Plugin Screenshot](https://raw.githubusercontent.com/MMV-Lab/MMV-playground/main/docs/images/plugin_screenshot.png)\n\n---\n\n### **Intensity Normalization**\n\nThis function adjusts the image intensity to enhance contrast and improve uniformity. Two percentage values are required:  \n- *Lower percentage (0‚Äì5%)*: Defines the darkest portion of the image to ignore as background noise, which is set to a fixed value.  \n- *Upper percentage (95‚Äì100%)*: Specifies the brightest portion of the image to be capped, preventing overexposure.  \n\nThe plugin calculates the respective percentiles based on these values. Intensities below the lower percentile are clipped, and those above the upper percentile are also capped. Finally, all pixel intensities are rescaled to the range [0, 1].\n\n---\n\n### **Smoothing**\n\nThis function reduces noise to enhance image clarity. Two methods are available:  \n- *Gaussian smoothing*  \n- *Edge-preserving smoothing*: Retains edges (e.g., cell boundaries) while reducing noise.  \n\n---\n\n### **Background Correction**\n\nThis function removes uneven illumination or background artifacts using a filter. The key parameter is:  \n- *Kernel size (1‚Äì100)*: Determines the spatial scale of the background correction. Smaller kernel sizes remove local noise, while larger sizes correct for broader illumination variations.  \n\nThe [scipy.ndimage.white_tophat] function is used to perform the correction, making this method effective for images with dark backgrounds.\n\n---\n\n### **Spot-Shape Filter**\n\nThis filter detects circular structures, such as cell nuclei or fluorescent spots. It is based on the [scipy.ndimage.gaussian_laplace] function and requires:  \n- *Sigma (œÉ)*: Controls the size of the spots to detect. Smaller sigma values target smaller spots, while larger values focus on larger structures.\n\n---\n\n### **Filament-Shape Filter**\n\nThis filter highlights elongated structures like cytoskeletal fibers or blood vessels. Using the [aicssegmentation.core.vessel.vesselness2D] function, the key parameter is:  \n- *Sigma (œÉ)*: Specifies the width of the detected filaments. Lower values detect thinner structures, while higher values identify thicker ones.\n\n---\n\n### **Thresholding**\n\nThis function segments the image into binary regions by separating the signal from the background. Users can choose one of four thresholding methods:  \n- *Otsu*: Best for images with clear separation between background and signal intensities.  \n- *Li*: Suitable for uniformly illuminated images.  \n- *Triangle*: Effective for asymmetrical intensity distributions.  \n- *Sauvola*: Ideal for images with uneven illumination.  \n\nThe result is a binary image where pixels above the threshold are set to 1 (signal), and all others are set to 0 (background).\n\n---\n\n### **Topology-Preserving Thinning**\n\nThis function extracts the skeleton of structures while maintaining their connectivity. Two parameters are required:  \n- *Minimum thickness (0.5‚Äì5)*: Defines the smallest allowable thickness of structures before thinning.  \n- *Thin (1‚Äì5)*: Controls the degree of thinning, reducing structure width while preserving topology (e.g., network connections).  \n\nThis is particularly useful for analyzing vascular or cellular networks.\n\n---\n\n### **What Is Missing?**\n\nCurrently, unit tests are not implemented, and internal documentation of the source code is still in progress.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"MMV-playground\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MMV-Lab/MMV-playground/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[scipy.ndimage.gaussian_filter]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter.html\n[itk.GradientAnisotropicDiffusionImageFilter]: https://itk.org/Doxygen/html/classitk_1_1GradientAnisotropicDiffusionImageFilter.html\n[scipy.ndimage.white_tophat]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.white_tophat.html\n[scipy.ndimage.gaussian_laplace]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_laplace.html\n[aicssegmentation.core.vessel.vesselness2D]: https://allencell.github.io/aics-segmentation/aicssegmentation.core.html\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MMV-playground"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ehooke",
    "name": "napari-ehooke",
    "display_name": "eHooke",
    "version": "0.0.17",
    "created_at": "2023-03-02",
    "modified_at": "2025-01-29",
    "authors": [
      "Ant√≥nio Brito"
    ],
    "author_emails": [
      "antmsbrito95@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-ehooke/",
    "home_github": "https://github.com/antmsbrito/napari-ehooke",
    "home_other": null,
    "summary": "eHooke implementation within napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy<2.0",
      "magicgui==0.6.1",
      "napari[all]",
      "tensorflow<=2.15.0",
      "napari-skimage-regionprops",
      "stardist-napari==2022.12.6",
      "scikit-learn",
      "scikit-image==0.20.0",
      "pandas",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-ehooke\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-ehooke.svg?color=green)](https://github.com/antmsbrito/napari-ehooke/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ehooke.svg?color=green)](https://pypi.org/project/napari-ehooke)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ehooke.svg?color=green)](https://python.org)\n[![tests](https://github.com/antmsbrito/napari-ehooke/workflows/tests/badge.svg)](https://github.com/antmsbrito/napari-ehooke/actions)\n[![codecov](https://codecov.io/gh/antmsbrito/napari-ehooke/branch/main/graph/badge.svg)](https://codecov.io/gh/antmsbrito/napari-ehooke)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ehooke)](https://napari-hub.org/plugins/napari-ehooke)\n\neHooke implementation within napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-ehooke` via [pip]:\n\n    pip install napari-ehooke\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/antmsbrito/napari-ehooke.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-ehooke\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/antmsbrito/napari-ehooke/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Compute base mask",
      "Compute features",
      "Compute cells",
      "Filter cells"
    ],
    "contributions_sample_data": [
      "Phase contrast S. aureus",
      "Membrane dye S.aureus",
      "DNA dye S.aureus"
    ]
  },
  {
    "normalized_name": "napari-zplane-depth-colorizer",
    "name": "napari-zplane-depth-colorizer",
    "display_name": "napari-zplane-depth-colorizer",
    "version": "1.0.1",
    "created_at": "2024-10-14",
    "modified_at": "2025-01-29",
    "authors": [
      "Mai Hoang"
    ],
    "author_emails": [
      "maihan.hoang1208@gmail.com"
    ],
    "license": "Copyright (c) 2024, Mai Hoang\n...",
    "home_pypi": "https://pypi.org/project/napari-zplane-depth-colorizer/",
    "home_github": "https://github.com/maihanhoang/napari-zplane-depth-colorizer",
    "home_other": null,
    "summary": "A simple plugin to color and merge z-planes of 3D data to give depth information.",
    "categories": [
      "Annotation",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-zplane-depth-colorizer\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-zplane-depth-colorizer.svg?color=green)](https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-zplane-depth-colorizer.svg?color=green)](https://pypi.org/project/napari-zplane-depth-colorizer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-zplane-depth-colorizer.svg?color=green)](https://python.org)\n[![tests](https://github.com/maihanhoang/napari-zplane-depth-colorizer/workflows/tests/badge.svg)](https://github.com/maihanhoang/napari-zplane-depth-colorizer/actions)\n[![codecov](https://codecov.io/gh/maihanhoang/napari-zplane-depth-colorizer/branch/main/graph/badge.svg)](https://codecov.io/gh/maihanhoang/napari-zplane-depth-colorizer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-zplane-depth-colorizer)](https://napari-hub.org/plugins/napari-zplane-depth-colorizer)\n\nA simple plugin for 3d+t files that visualizes z-planes in 3 colors for depth information. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\nThis plugin can colorize and provide depth information on single-channel 3D+t files. The color coding enhances the visibility of structures and the detection/annotations of dynamic events. \n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/plugin_demo.gif\" width=\"100%\" />\n</p>\n\nIdentifying dynamic events such as cellular divisions can be challenging in 3D time-lapses of developing tissues such as organoids or embryos. Visualizing and annotating such events in dense 3D stacks obtained by light-sheet or two-photon microscopy where nuclei are almost in contact is especially challenging. With this visualization method, the tissue section can be \"augmented\" in 3D by visualizing the surrounding tissue layers in different colors. \n\nThis plugin is for bioimaging researchers who need to annotate events in time lapses of dense tissues. It supports 3D+t stacks in the TZYX format. \n\n## Installation\n\nYou can install `napari-zplane-depth-colorizer` via [pip]:\n\n    pip install napari-zplane-depth-colorizer\n\nTo install latest development version :\n\n    pip install git+https://github.com/maihanhoang/napari-zplane-depth-colorizer.git\n\n## Quick Start\nA sample file can be found at `src/napari_zplane_depth_colorizer/data/3D+t.tif`\n\n1. Open a 3D+t tif in napari, make sure it is in TZYX format\n2. If it's a 4D file, it will automatically appear in the drop-down menu of *Input*\n3. Use default parameters for *Z-Projection Type* and *Depth Range*\n4. Click *Merge to RGB*\n5. Result will appear in the viewer and in *Output*\n6. To save, select the colored stack in *Output*, select saving format (*Save as*), and click *Save RGB*\n\n## Parameters\nThere are two depth augmentation parameters for each RGB color channel: Z-Projection Type and Depth Range. Depth Range indicates which and how many z-planes are projected for a channel. The Z-Projection Type specifies which type of projection is applied. The **How it works** section below might help you understand the parameters better.\n\n### Z-Projection Types \nThe Z-projection types are the same ones found in [Fiji](https://imagej.net/software/fiji/), except for \"Raw. \"Raw is the original stack without any projection.\n\n### Depth Range\n- Depth Range is a range and consists of two numbers [range_start, range_end]. The range is inclusive and requires range_start <= range_end. \n\n- Exception if Z-Projection Type is \"Raw\" since no z-projection is applied. The Depth Range for Raw can either be empty (equals 0 then) or a single number. \n\n- The Depth Range indicates the z-planes relative to the reference plane that are projected and then assigned to a color channel. The reference plane is the plane for which the color merging is currently computed.\n\n- In the illustration below, the current reference plane is the 4. plane (indicated by the color wheel in the right stack). A Depth Range of [-3, -1] for the red channel denotes the first three planes above the reference plane. Similarly, a Depth Range of [1, 2] for the blue channel indicates the first two planes below the reference plane. Negative numbers are for the planes above, and positive numbers are for ones below the reference plane. 0 is the same as the reference plane (the green plane in the illustration).   \n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/entire_stack_zproj.jpg\" width=\"100%\" />\n</p>\n\n\n## How it works\n### Creating a colored image from a single-channel stack\n\nTo colorize a single-channel stack, multiple z-planes are assigned to different color (RGB) channels and then overlaid to create a composite colored image. For a simple z-stack with three planes, the 1. plane is assigned to red, the 2. to green, and the 3. plane to blue. They are overlaid/merged to create a single composite RGB-color plane. In the composite image, multiple z-planes are displayed simultaneously, with the colors providing the depth information. In this example, red indicates that cells are in the upper, while blue cells are in the deeper z-planes. The colors make it easier to detect and track cells that move or split in the z-direction.  \n\n\n<!---------\nIn other words, instead of having multiple channels from different fluorescence labels, the z-planes are treated as different channels and then merged to create a composite image.\n\n\n<img src=\"./assets/3_plane_stack.gif\" alt=\"Demo GIF\" style=\"width:100%;\">\n-------->\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/3_plane_stack.gif\" width=\"100%\" />\n</p>\n\nOn the left is an image of a single plane of an organoid and on the right is the colorized plane. To create the colors, the same plane was overlaid with the direct upper and lower plane, as shown in the illustration above. \n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/img_gray.png\" width=\"49%\" />\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/img_color.png\" width=\"49%\" /> \n</p>\n\n### Colorizing the entire stack\nTo obtain a colorized stack of the same size as the original stack, the merging is applied for each z-plane. At the boundaries, the colors can differ, as there are no further z-planes to assign to a color channel. For example, the first plane in the colorized stack consists of an overlay of only two planes in green and blue, which results in a blue/green/cyan-colored image. Similarly, the last plane of the colorized stack is a red/green/yellowish image since it lacks a blue channel.    \n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/entire_stack.gif\" width=\"100%\" />\n</p>\n\n### Varying the depth with z-projections\nIt is also possible to vary the depth and overlay more than three planes. To do this, a z-projection is applied to the planes before assigning them to a color and merging. In the illustration below, a different number of planes is assigned to each color. Before merging the three RGB color channels, a z-projection is applied if more than one plane is assigned to a color. In the illustration below, three planes are selected for the red color channel. A z-projection (e.g. average intensity) is applied to generate a single plane for the red channel. This z-projected plane is merged with the raw, green plane and the z-projected plane of the blue channel. \n\n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/entire_stack_zproj.jpg\" width=\"100%\" />\n</p>\n\nFor the entire stack:\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/entire_stack_zproj.gif\" width=\"100%\" />\n</p>\n\nOn the left is an image of a single plane of an organoid, and on the right is the colorized plane. To create the colors, the same plane was overlaid with three upper and two lower z-planes, as shown in the illustration above. \n\n<p align=\"middle\">\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/img_gray.png\" width=\"49%\" />\n  <img src=\"https://github.com/maihanhoang/napari-zplane-depth-colorizer/raw/main/assets/img_color_zproj.png\" width=\"49%\" /> \n</p>\n\n## Acknowledgements\n[Sham Tlili](https://scholar.google.com/citations?user=8ykCpTIAAAAJ&hl=fr) provided the sample data and developed the visualization method implemented in this plugin\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-zplane-depth-colorizer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/maihanhoang/napari-zplane-depth-colorizer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Z-Color"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "epitools",
    "name": "epitools",
    "display_name": "EpiTools",
    "version": "0.0.13",
    "created_at": "2023-05-04",
    "modified_at": "2025-01-27",
    "authors": [
      "Yanlan Mao"
    ],
    "author_emails": [
      "\"Daniel R. Matthews\" <d.matthews@ucl.ac.uk>",
      "Giulia Paci <g.paci@ucl.ac.uk>",
      "Pablo Vicente Munuera <p.munuera@ucl.ac.uk>",
      "\"Patrick J. Roddy\" <patrick.roddy@ucl.ac.uk>",
      "\"Paul J. Smith\" <paul.j.smith@ucl.ac.uk>",
      "Yanlan Mao <y.mao@ucl.ac.uk>"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/epitools/",
    "home_github": "https://github.com/epitools/epitools",
    "home_other": null,
    "summary": "Quantifying 2D cell shape and epithelial tissue dynamics",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "PartSeg",
      "magicgui",
      "matplotlib",
      "napari",
      "networkx",
      "numpy",
      "pandas",
      "scikit-image>=0.20",
      "scipy",
      "black; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pyqt5; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "myst-parser; extra == \"docs\"",
      "pydata-sphinx-theme; extra == \"docs\"",
      "pytz; extra == \"docs\"",
      "sphinx-autobuild; extra == \"docs\"",
      "sphinx<5; extra == \"docs\"",
      "sphinx_autodoc_typehints; extra == \"docs\"",
      "sphinxcontrib-video; extra == \"docs\"",
      "types-pytz; extra == \"docs\"",
      "btrack[napari]>=0.6.1; extra == \"wf\"",
      "napari-segment-blobs-and-things-with-membranes; extra == \"wf\""
    ],
    "package_metadata_description": "[![Licence](https://img.shields.io/pypi/l/epitools.svg?color=green)](https://raw.githubusercontent.com/epitools/epitools/main/LICENCE.md)\n[![PyPI](https://img.shields.io/pypi/v/epitools.svg?color=green)](https://pypi.org/project/epitools)\n[![Python Version](https://img.shields.io/pypi/pyversions/epitools.svg?color=green)](https://python.org)\n[![tests](https://github.com/epitools/epitools/actions/workflows/test.yml/badge.svg)](https://github.com/epitools/epitools/actions/workflows/test.yml)\n[![Documentation](https://readthedocs.org/projects/epitools/badge/?version=latest)](https://epitools.readthedocs.io/en/latest/?badge=latest)\n[![coverage](https://coveralls.io/repos/github/epitools/epitools/badge.svg?branch=main)](https://coveralls.io/github/epitools/epitools?branch=main)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/epitools)](https://napari-hub.org/plugins/epitools)\n\n# Welcome to EpiTools!\n\nEpiTools is a Python package and associated [napari](https://napari.org/stable/) plugin to extract the membrane signal from epithelial tissues and analyze it with the aid of computer vision.\n\nThe development of EpiTools was inspired by the challenges in analyzing time-lapses of growing Drosophila imaginal discs.\n\nThe folded morphology, the very small apical cell surfaces and the long time series required a new automated cell recognition to accurately study growth dynamics.\n\n## Installation\n\nFirst, install [napari](https://napari.org/index.html#installation).\n\nThe recommended way to install `EpiTools` is via\n[pip](https://pypi.org/project/pip)\n\n```sh\npython -m pip install epitools\n```\n\nTo install the latest development version of `EpiTools` clone this repository\nand run\n\n```sh\npython -m pip install -e .\n```\n\nIf working on Apple Silicon make sure to also install the following package from\n[conda-forge](https://conda-forge.org).\n\n```sh\nconda install -c conda-forge pyqt\n```\n\n### Recommended Companion Napari Plugins\n\nTo also install the recommended plugins for the `EpiTools` workflow run\n\n```sh\npython -m pip install epitools[wf]\n```\n\nand\n\n```sh\npython -m pip install -e .[wf]\n```\n\nWhen installing with Apple Mac OS X terminal, you might need to add '\"' to [wf] as in:\n\n```sh\npython -m pip install -e .\"[wf]\"\n```\n\nIf working on Apple Silicon make sure to also install the following package from\n[conda-forge](https://conda-forge.org)\n\n```sh\nconda install -c conda-forge cvxopt\n```\n\nwhich is required for [btrack](https://github.com/quantumjot/btrack).\n\n## Issues\n\nIf you encounter any problems, please\n[file an issue](https://github.com/epitools/epitools/issues) along with a\ndetailed description.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox](https://tox.wiki),\nplease ensure the coverage at least stays the same before you submit a pull request.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [
      ".tif",
      ".tiff"
    ],
    "contributions_widgets": [
      "Projection (selective plane)",
      "Projection (2 channel with reference channel)",
      "Segmentation (local minima seeded watershed)",
      "Cell statistics",
      "Calculate quality metrics"
    ],
    "contributions_sample_data": [
      "EpiTools"
    ]
  },
  {
    "normalized_name": "mousechd-napari",
    "name": "mousechd-napari",
    "display_name": "MouseCHD",
    "version": "0.0.4",
    "created_at": "2023-11-18",
    "modified_at": "2025-01-16",
    "authors": [
      "Hoa Nguyen"
    ],
    "author_emails": [
      "ntthoa.uphcm@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/mousechd-napari/",
    "home_github": "https://github.com/hnguyentt/mousechd-napari",
    "home_other": null,
    "summary": "A tool for heart segmentation and congenital heart defect detection in mice.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "setuptools",
      "packaging",
      "mousechd",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\""
    ],
    "package_metadata_description": "# Napari plugin for MouseCHD project\n\n![](https://raw.githubusercontent.com/hnguyentt/mousechd-napari/master/assets/demo.gif)\n\n*Tool for heart segmentation and congenital heart defect detection in mice.*\n\n## Installation\n\nThere are several ways to run the plugin: (1) from bundle, (2) Containers (Docker or Apptainer), (3) from code\n\n### From Bundle\n\n(1) Install Napari by following this instruction https://napari.org/stable/tutorials/fundamentals/installation.html#install-as-a-bundled-app\n\n(2) Install `mousechd-napari` plugin:\n    * Run Napari\n    * `Plugins` --> `Install/Uninstall Plugins ...` --> Search for `mousechd-napari` --> Click on `install`.\n\n(3) Restart Napari to run the plugin\n\n\n### Containers\n#### Docker\n##### Pull docker\n```\nsudo docker pull hoanguyen93/mousechd-napari\n```\n\n##### Run MouseCHD Plugin\n* Run plugin with local resources:\n\n    ```\n    sudo docker run --gpus all -v <path/to/home/on/host>:<path/to/home/on/host> -it --rm -p 9876:9876 -p 6006:6006 hoanguyen93/mousechd-napari\n    ```\n\n    <details>\n    <summary>Example:</summary>\n\n    ```\n    sudo docker run --gpus all -v /home/hnguyent:/home/hnguyent -it --rm -p 9876:9876 -p 6006:6006 hoanguyen93/mousechd-napari\n    ```\n\n    </details>\n\n    Open this link on your browser: [http://localhost:9876/](http://localhost:9876/)\n\n* Run plugin with server resources:\n\n    * Follow [this instruction](./docs/server_setup.md) to setup running on server.\n    * Copy ~/.ssh folder to a temporary location, for example in ~/Downloads: `cp -r ~/.ssh ~/Downloads/`\n    * Change ownership for temporary ~/Downloads/.ssh folder: `chown -R root:root ~/Downloads/.ssh`\n    * Run docker: `udo docker run --gpus all -v <path/to/home/on/host>:<path/to/home/on/host> -v /home/hnguyent/Downloads/.ssh:/root/.ssh:ro -it --rm -p 9876:9876 -p 6006:6006 hoanguyen93/mousechd-napari`\n    * Open this link on your browser: [http://localhost:9876/](http://localhost:9876/)\n\n##### Known issues\n* The plugin in docker container can't display 3D view, please choose 2D view to display the sample and images.\n\n#### Apptainer (Singularity)\nIf you want to run with server resource, follow [this instruction](./docs/server_setup.md) to setup running on server.\n\n* Download Apptainer image `mousechd-napari.sif` from (Zenodo)[https://zenodo.org/records/14652180] or simply: `wget https://zenodo.org/records/14652180/files/mousechd-napari.sif`\n* Run the plugin: \n```\napptainer exec \\\n    --nv \\\n    --bind /tmp/.X11-unix:/tmp/.X11-unix \\\n    --env DISPLAY=$DISPLAY \\\n    <path/to/mousechd-napari.sif> napari\n```\n\n### From code\n\n```bash\nconda create -n mousechd_napari python=3.9\nconda activate mousechd_napari\npip install \"napari[all]\"\npip install mousechd-napari\nnapari\n```\n\n## How to use\nPlease find details instruction in folder [docs](https://github.com/hnguyentt/mousechd-napari/tree/master/docs)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.nii.gz",
      "*.nrrd",
      "*.dcm"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MouseCHD"
    ],
    "contributions_sample_data": [
      "micro-CT scan"
    ]
  },
  {
    "normalized_name": "segment-embryo",
    "name": "segment-embryo",
    "display_name": "Embryo Segmentation",
    "version": "0.5",
    "created_at": "2024-11-27",
    "modified_at": "2025-01-15",
    "authors": [
      "Volker Baecker"
    ],
    "author_emails": [
      "volker.baecker@mri.cnrs.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/segment-embryo/",
    "home_github": "https://github.com/MontpellierRessourcesImagerie/segment-embryo.git",
    "home_other": null,
    "summary": "3D Segment ascidian embryos using cellpose. The images are first rescaled to be isotropic and then downscaled, before cellpose is used. The resulting labels are upscaled to match the original data.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "cellpose",
      "czifile",
      "napari-czifile2",
      "set-calibration",
      "big-fish",
      "napari-bigfish",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "numpy; extra == \"testing\""
    ],
    "package_metadata_description": "# segment-embryo\n\n[![License MIT](https://img.shields.io/pypi/l/segment-embryo.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/segment-embryo/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/segment-embryo.svg?color=green)](https://pypi.org/project/segment-embryo)\n[![Python Version](https://img.shields.io/pypi/pyversions/segment-embryo.svg?color=green)](https://python.org)\n[![tests](https://github.com/MontpellierRessourcesImagerie/segment-embryo/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/segment-embryo/actions)\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/segment-embryo/branch/main/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/segment-embryo)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/segment-embryo)](https://napari-hub.org/plugins/segment-embryo)\n\n<img src=\"https://github.com/user-attachments/assets/4459f48c-435f-489d-b27c-25a4ea390871\" align='left' width=\"30%\"></img> 3D Segment ascidian embryos using cellpose. The images are first rescaled to be isotropic and then downscaled, before cellpose is used. The resulting labels are upscaled to match the original data.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `segment-embryo` via [pip]:\n\n    pip install segment-embryo\n\n## Usage\n\nWe will segment the cells of an embryo and count the mRNA spots tagged in another channel per cell.\n\n### 1. Opening an image\n\nDrag a tif- or czi-file from your file-browser and drop it into the napari window. The image will opened. The napari-plugin [napari-czifile2](https://github.com/BodenmillerGroup/napari-czifile2) is used to open czi-files.\n\n### 2. Checking and modifying the voxel size \n\nOpen the plugin [Scale-Tool plugin](https://pypi.org/project/set-calibration/) from the ``Plugins``-menu. Make sure the voxel-size values are set correctly in nm. Correct the values and the unit if necessary and press the ``Apply to all`` button.\n\n<img src=\"https://github.com/user-attachments/assets/409b669b-675f-495f-ba11-c6ded0442b2b\" align='left' width=\"30%\"></img> \n\n### 3. Segmenting the cells of the embryo\n\n<img src=\"https://github.com/user-attachments/assets/1b464f39-47ff-412d-bdb8-c2d61837af39\" align='right' width=\"30%\"></img> \n\nClose the [Scale-Tool plugin](https://pypi.org/project/set-calibration/) and open the ``Embryo Segmentation``-plugin from the ``Plugins``-menu. Select the layer containing the membranes as ``input image`` and the layer containing the nuclei as ``nuclei image``. Press the ``run`` button and wait until the segmentation is finished. When the segmentation is finished a ``Labels layer`` with the result will be added to the layers list. \n\n<img src=\"https://github.com/user-attachments/assets/4dcf8aad-22ef-4a20-a6a6-50bcb81c7011\" align='left' width=\"20%\"></img> \n\n### 4. Exporting and curating the labels\n\nIf the labels need curation, select  the labels layer and save it to a tiff-file via the menu ``File>Save Selected Layers...``. Import the saved labels into the [Morphonet 2 standalone client](https://morphonet.org/downloads), make the corrrections and export the corrected labels to a tiff-file. Open the tiff-file in napari.\n\n### 5. Detecting spots\n\nClose the ``Embryo Segmentation`` plugin, select the layer containing the mRNA-spots and open the [Detect FISH spots-plugin](https://www.napari-hub.org/plugins/napari-bigfish) from the ``Plugins``-menu. Estimate the spot size in xy and z in ImageJ and enter the values into the corresponding fields. You can either estimate the threshold let the software find a threshold value. Press the ``detect spots button``. Depending on the result you might want to modify the threshold-value and run the plugin again.\n\n<img src=\"https://github.com/user-attachments/assets/6507eb3e-d732-4d69-8c63-02d83f8975f1\" align='left' width=\"30%\"></img> \n\n### 6. Counting Spots per cell\n\nIn the ``Spot Counting`` section of the plugin, select the spots layer as input and the labels layer for the ``cytoplasm labels`` and also for the ``nuclei labels``. \n\n![image](https://github.com/user-attachments/assets/445da47f-c3fc-4437-803d-e635500bf1af)\n\nPress the ``Count Spots`` button! You will obtain a table with the numbers of spots per cell. If you provide the image twice, for the cytoplasm as well as for the nuclei, the spots will all be counted inside the nuclei. If you also want to know per cell how many spots are in the nucleus and how many are in the cytoplasm, you need to provide a mask or label image for the nuclei. You can obtain one by thresholding the nucleus image or by selecting ``segment nuclei`` in step 3.\n\n\n<img src=\"https://github.com/user-attachments/assets/7f325bc5-baaf-4b74-b240-f174b81d0e96\" align='left' width=\"30%\"></img>\n<img src=\"https://github.com/user-attachments/assets/463072c7-d647-4d23-90d9-15de3398bf31\" align='right' width=\"30%\"></img>\n\nYou can copy the data from the table to your spreadsheat software. Activate the table by clicking into it and press ``ctrl+a`` to select all rows and columns. Press ``ctrl+c`` to copy the data to the clipboard and ``ctrl+v`` to paste it into your spreadsheet.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"segment-embryo\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Embryo Segmentation Widget"
    ],
    "contributions_sample_data": [
      "Embryo Segmentation"
    ]
  },
  {
    "normalized_name": "napari-svg",
    "name": "napari-svg",
    "display_name": "napari SVG",
    "version": "0.2.1",
    "created_at": "2021-05-01",
    "modified_at": "2025-01-14",
    "authors": [
      "Nicholas Sofroniew",
      "napari core devs"
    ],
    "author_emails": [
      "Nicholas Sofroniew <sofroniewn@gmail.com>",
      "napari core devs <napari-core-devs@googlegroups.com>"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-svg/",
    "home_github": "https://github.com/napari/napari-svg",
    "home_other": null,
    "summary": "A plugin for writing svg files with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "imageio>=2.5.0",
      "numpy>=1.16.0",
      "vispy>=0.6.4",
      "napari>=0.4; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-svg\n\n[![License](https://img.shields.io/pypi/l/napari-svg.svg?color=green)](https://github.com/napari/napari-svg/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-svg.svg?color=green)](https://pypi.org/project/napari-svg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-svg.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari/napari-svg/workflows/tests/badge.svg)](https://github.com/napari/napari-svg/actions)\n[![codecov](https://codecov.io/gh/napari/napari-svg/branch/master/graph/badge.svg)](https://codecov.io/gh/napari/napari-svg)\n\nA plugin for writing svg files with napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-svg` via [pip]:\n\n    pip install napari-svg\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-svg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/napari/napari-svg/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".svg"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "lasso-3d",
    "name": "lasso-3d",
    "display_name": "Lasso",
    "version": "0.0.1",
    "created_at": "2025-01-09",
    "modified_at": "2025-01-09",
    "authors": [
      "Lorenz Lamm"
    ],
    "author_emails": [
      "lorenz.lamm@gmail.com"
    ],
    "license": "Copyright (c) 2024, Lorenz Lam...",
    "home_pypi": "https://pypi.org/project/lasso-3d/",
    "home_github": "https://github.com/LorenzLamm/lasso-3d",
    "home_other": null,
    "summary": "3D lasso tool to select large 3D areas",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "magicgui",
      "membrain-seg",
      "napari-mrcfile-reader",
      "numpy",
      "pyqt5",
      "qtpy",
      "scipy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# lasso-3d\n\n[![License BSD-3](https://img.shields.io/pypi/l/lasso-3d.svg?color=green)](https://github.com/LorenzLamm/lasso-3d/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/lasso-3d.svg?color=green)](https://pypi.org/project/lasso-3d)\n[![Python Version](https://img.shields.io/pypi/pyversions/lasso-3d.svg?color=green)](https://python.org)\n[![tests](https://github.com/LorenzLamm/lasso-3d/workflows/tests/badge.svg)](https://github.com/LorenzLamm/lasso-3d/actions)\n[![codecov](https://codecov.io/gh/LorenzLamm/lasso-3d/branch/main/graph/badge.svg)](https://codecov.io/gh/LorenzLamm/lasso-3d)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/lasso-3d)](https://napari-hub.org/plugins/lasso-3d)\n\n3D lasso tool to select large 3D areas\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Lasso tool\n\nThis repository allows to draw 3D lassos, generate masks from these, and then mask out the image.\nFor instructions on how to use the plugin, please refer to the [Usage instructions](./docs/Usage.md).\n\n<div style=\"text-align: center;\">\n<img src=\"https://github.com/user-attachments/assets/88851e09-6f10-4219-9b45-6f608c3e10b6\" alt=\"lasso_gif\" width=\"75%\" />\n</div>\n\nHow it works: A polygon is drawn and a mask is generated via:\n### Mask via rotation\nSteps:\n1. Rotate and project polygon to 2D and create a pixel mask\n2. Create a 3D mask by stacking the pixel mask along z\n3. Rotate 3D mask s.t. it is aligned with the original polygon\n\nThis performed more efficiently than the other methods:\n\n### Mask via projection\nSteps:\n1. Project all points onto the hyperplane defined by the polygon\n2. Rotate all points and the polygon s.t. they are in a horizontal plane and remove z component\n3. Create a binary pixel mask of the polygon\n4. Check which point projections are within the polygon mask\n5. reshape mask to original tomogram size\n\n### Mask via mesh voxelization\nSteps:\n1. Move polygon along its normal in both directions until end of tomogram shape --> front & back polygons\n2. Define a surface by combining front & back polygons into a triangular mesh\n3. Voxelize the surface, giving the outline of the cone\n4. Fill holes to receive a filled cone\n\n### Mask via attaching slices\nSteps:\n1. Rotate and project polygon to 2D and generate a pixel mask (2D)\n2. Get indices of pixel mask and rotate them back to 3D space\n3. Do that for many pixel mask, varying the z-component --> will be moved into tomogram along the polygon normal\n4. Binary closing to get rid of holes from integer conversion\n\n## Installation\n\npip install .\n<!-- You can install `lasso-3d` via [pip]:\n\n    pip install lasso-3d\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/LorenzLamm/lasso-3d.git -->\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"lasso-3d\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/LorenzLamm/lasso-3d/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Lasso Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-3dtimereg",
    "name": "napari-3dtimereg",
    "display_name": "3D Movies Registration",
    "version": "0.0.11",
    "created_at": "2024-04-05",
    "modified_at": "2025-01-08",
    "authors": [
      "Ga√´lle Letort"
    ],
    "author_emails": [
      "gaelle.letort@pasteur.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-3dtimereg/",
    "home_github": null,
    "home_other": null,
    "summary": "Registration of 3D movies applied to all channels",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari",
      "magicgui",
      "qtpy",
      "tifffile",
      "imaris_ims_file_reader",
      "czifile",
      "itk==5.3.0",
      "itk-registration",
      "itk-elastix",
      "pydantic<=1.10.14"
    ],
    "package_metadata_description": "# napari-3dtimereg\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-3dtimereg.svg?color=green)](https://gitlab.pasteur.fr/gletort/napari-3dtimereg/-/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-3dtimereg.svg?color=green)](https://pypi.org/project/napari-3dtimereg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-3dtimereg.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-3dtimereg)](https://napari-hub.org/plugins/napari-3dtimereg)\n\nTemporal registration of 2D/3D movies on one channel based on [itk-elastix](https://pypi.org/project/itk-elastix/), and transpose alignement to the other channels.\n\nAdaptated from [multireg](https://gitlab.pasteur.fr/gletort/multireg) for temporal movies.\nFor a tutorial on using `elastix` for registration, see [this tutorial](https://m-albert.github.io/elastix_tutorial/intro.html).\n\n\n----------------------------------\n## Installation\n\n* You can install the plugin directly in `Napari` by going to `Plugins>Install/Uninstall plugins` and search for `napari-3dtimereg`\n\n* Or you can install `napari-3dtimereg` via [pip]:\n\n    pip install napari-3dtimereg\n\n\n## Usage\n\nYou can launch `3dtimereg` in napari by going to `Plugins>Do 3D movie registration (napari-3dtimereg)`.\n\n### Choose movie and reference chanel\n\nFirst, choose select the movie that you want to register. The plugin will create a folder `aligned` in the folder of your selected movie where the results will be saved.\n\nChoose the color chanel on which to calculate the registration (`reference chanel`). Color chanels are numbered from 0 to nchanels, and you can see their respective number in the layer list on the left panel of Napari. Click on `Update` when the correct chanel is selected to go to the registration calculation step.\n\n### Calculate temporal alignement\n\nThe registration is calculated iteratively from one frame to another. Thus the first frame is not moved and all the other frames are aligned to it.\nYou can tune several parameters in this plugin:\n\n![parameters screenshot](./imgs/parameters.png \"Registration parameters\")\n\nThe other parameters are parameters to use [itk-elastix](https://elastix.lumc.nl/) to calculate the registration.\n* `show log`: to see the log of Elastix calculation\n* `do rigid`: performs a rigid (affine) transformation step, that allowed to correct for translations/rotations.\n* `do bspline`: performs a b-spline based transformation step, that allowed for local deformations in the image.\n* `show advanced parameters`: to control the parameters used in the rigid and/or bspline transformations. These parameters control the size of the local registrations calculated, the resolutions at which the transformations are calculated, and can thus greatly impact the results.\n* `final order`: is the final order of the B-Splines used for the registration. \n* `resolution`: is the number of consecutives resolutions at which the registration will be made. First the registration is made at the lowest level of resolution, correcting global deformations/motions, then at each step, the registration is done on higher resolution, allowing to correct for more local deformations.\n* `final spacing`: is the physical spacing of the smallest resolution.\n* `iterations`: are the maximum number of iterations allowed to minimize the distance between the two images for each resolution and type of registration.\n\nIf both rigid and bspline transformations, the program first applies the rigid transformation to allow for a global registration of the images. Then it will performs the second step of b-spline transformation that can includes local deformations.\n\nFor each frame, after calculating the registration on the reference chanel, the plugin will apply the calculated transformation to all the other color chanels of the initial movie. All results are saved as separated images in the `aligned` folder during the computation.\n\n### Create the final aligned movie\n\nWhen all frames have been processed, each color chanel and each frame have been saved in the `aligned` folder as separated images. This is usefull to calculate the registration on large movies without having to keep all the intermediates and calculated images in memory. You can directly use these separated images, or reconstruct a single composite movie of the result.\n\nIf you click on `Concatenate aligned images` on the plugin interface, the plugin will create a single composite movie from the aligned images, save it and delete the separated images in the `aligned` folder. \n\n## License\n\nDistributed under the terms of the [BSD-3] license, \"napari-3dtimereg\" is free and open source software\n\n\n[napari]: https://github.com/napari/napari\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Do 3D movie registration"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-swc-editor",
    "name": "napari-swc-editor",
    "display_name": "napari-swc-editor",
    "version": "0.0.5",
    "created_at": "2024-12-03",
    "modified_at": "2025-01-05",
    "authors": [
      "Cl√©ment Caporal"
    ],
    "author_emails": [
      "caporal.clement@gmail.com"
    ],
    "license": "Copyright (c) 2024, Cl√©ment Ca...",
    "home_pypi": "https://pypi.org/project/napari-swc-editor/",
    "home_github": "https://github.com/LaboratoryOpticsBiosciences/napari-swc-editor",
    "home_other": null,
    "summary": "Use point and shape layer to edit swc format in napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "pandas",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-swc-editor\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-swc-editor.svg?color=green)](https://github.com/LaboratoryOpticsBiosciences/napari-swc-editor/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-swc-editor.svg?color=green)](https://pypi.org/project/napari-swc-editor)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-swc-editor.svg?color=green)](https://python.org)\n[![tests](https://github.com/LaboratoryOpticsBiosciences/napari-swc-editor/workflows/tests/badge.svg)](https://github.com/LaboratoryOpticsBiosciences/napari-swc-editor/actions)\n[![codecov](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-swc-editor/branch/main/graph/badge.svg)](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-swc-editor)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-swc-editor)](https://napari-hub.org/plugins/napari-swc-editor)\n\nUse point and shape layer to edit swc format in napari.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Features\n\n\nhttps://github.com/user-attachments/assets/cba1820f-d0b5-436c-a981-62bae0e1a6ba\n\n\n\n\n### IO\n#### READER\n- Your .swc should follow the following specs: http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html\n- the reader will create 2 napari layer: `point_layer` and `shape_layer`. Only `point_layer` is interactive, `shape_layer` is used to render path between swc points.\n- The raw swc can be accessed in the point layer metadata. Such as `point_layer.metadata[\"raw_swc\"]`\n- A `pd.DataFrame` object is also saved in the metadata: `point_layer.metadata[\"swc_data\"]`\n#### WRITER\n- With the `point_layer` selected, you can use napari interface to save with `.swc` extension name.\n- You can also do it in command line: `napari.save_layers('test.swc', [point_layer])`\n### Napari Interface\n#### Structure ID and point symbol\nIn swc, structure id allow to label the type of neuron structure the point belongs to. In this plugin by default, the points will follow this symbol mapping:\n```python\nSWC_SYMBOL = {\n    0: \"clobber\",  # undefined\n    1: \"star\",  # soma\n    2: \"disc\",  # axon\n    3: \"triangle_down\",  # basal dendrite\n    4: \"triangle_up\",  # apical dendrite\n}\n```\n![image](https://github.com/user-attachments/assets/618aa000-370d-43f9-8645-8a3b7e9b9739)\n\nYou can also visualize the swc data in a table using the widget under `Plugin > SWC Editor Widget`\n\n![image](https://github.com/user-attachments/assets/ed43f4c2-582b-4bc1-bbb1-54e8d9487f1d)\n\nWhen using the \"Show swc table\" you will have an interactive table widget:\n- left-click on table: highlight + center on the corresponding point\n- **double**-left-click on table: highlight + center on the correspongind point **+ zoom**\n- selection on the point layer: highlight the corresponding row on the table\n\n#### SWC Edition\n**ALL INTERACTIONS ARE ONLY BOUND TO THE `point_layer`**\n**THERE IS NO CTRL-Z (please save your progress)**\n- **Add point**: You can edit the \"r\" and the \"structure_id\" using the `point_size` and `symbol` ![image](https://github.com/user-attachments/assets/44255691-ffa0-4f63-8368-499b0c8ff6a4)\n- **Remove point**: (Select the point and press `1` or `suppr` or `delete`) All the link pointing to this point will be removed\n- **Add edge**: Select 2 or more point(s) and press on your keyboard `l` (aka: link).\n- **Remove edge**: Select 1 or more point(s) and press on your keyboard `u` (aka: unlink).\n\nIf you want to link point as you are adding them you have two solutions:\n- press \"CTRL\" while you add points, this will create a link with the previously selected point\n- use the `Plugin > SWC Editor Widget` Checkbox (\"link previous node with new node (same as using CTRL+Click)\"): when selected, all new points will be selected with the previously selected point\n\nhttps://github.com/user-attachments/assets/273f1221-2882-4a7c-ab7f-6d3ecb7f3fa6\n\n## Installation\n\nYou can install `napari-swc-editor` via [pip]:\n\n    pip install napari-swc-editor\n\n\n\n\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/LaboratoryOpticsBiosciences/napari-swc-editor.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-swc-editor\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/LaboratoryOpticsBiosciences/napari-swc-editor/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.swc"
    ],
    "contributions_writers_filename_extensions": [
      ".swc"
    ],
    "contributions_widgets": [
      "SWC Editor Widget"
    ],
    "contributions_sample_data": [
      "sample-napari-swc-editor",
      "empty-napari-swc-editor"
    ]
  },
  {
    "normalized_name": "napari-annotator",
    "name": "napari-annotator",
    "display_name": "Annotator",
    "version": "0.1.1",
    "created_at": "2022-03-07",
    "modified_at": "2025-01-03",
    "authors": [
      "Lo√Øc Sauteur"
    ],
    "author_emails": [
      "loic.sauteur@unibas.ch"
    ],
    "license": "Copyright (c) 2025, Lo√Øc Saute...",
    "home_pypi": "https://pypi.org/project/napari-annotator/",
    "home_github": "https://github.com/loicsauteur/napari-annotator",
    "home_other": null,
    "summary": "A lightweight plugin extending label layer control",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari>=0.5.5",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-annotator\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-annotator.svg?color=green)](https://github.com/loicsauteur/napari-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-annotator.svg?color=green)](https://pypi.org/project/napari-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/loicsauteur/napari-annotator/workflows/tests/badge.svg)](https://github.com/loicsauteur/napari-annotator/actions)\n[![codecov](https://codecov.io/gh/loicsauteur/napari-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/loicsauteur/napari-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-annotator)](https://napari-hub.org/plugins/napari-annotator)\n\nA lightweight plugin extending label layer control.\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n## Description\nThis lightweight plugin helps you navigate your labels layer. It is intended to ease your manual annotation work.\n![Overview image](resources/image1.png)\n- Select a label from the list.\n- Toggle the visibility of individual label entries.\n- Move to the centroid of a label at the current zoom.\n- Change the color of individual labels.\n- Erase all drawn pixels of a given label.\n- Restore an erased label.\n\nVersion >=0.1.0 works for napari version >= 0.5.5\n\nVersion <0.1.0 should work for napari version < 0.4.19\n\n## Usage\nStart the plugin `Plugins > Annotator (Annotator)`.\n\nThe plugin will list available labels once a labels layer is selected and labels drawn.\n\nColor shuffling for labels will not work, since the plugin sets the color mode of the layer to `direct`.\nBut you can always change the color of individual labels, using the color picker.\n\n## Known limitations\n1. Locating / moving to the center of a label only works on 2D/3D label layers, i.e.:\n   1. single- / multi-channel 2D label layers.\n   2. single-channel 3D label layers (the third dimension being either Z or T).\n2. (Theoretical) maximum of 20'000 labels supported.\n<!-- increasing the number is possible, but will introduce bigger lag, as each color/visibility change re-creates the colormap.-->\n3. Restoring an erased labels is lost after switching between layers.\n\n\n\n\n## Installation\n\nYou can install `napari-annotator` via [pip]:\n\n    pip install napari-annotator\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/loicsauteur/napari-annotator.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-annotator\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\nOr open a thread on [forum.image.sc](https://forum.image.sc) with a detailed description\nand a [@loicsauteur](https://github.com/loicsauteur) tag.\n\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/loicsauteur/napari-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-vector-graphics",
    "name": "napari-vector-graphics",
    "display_name": "Napari Vector Graphics",
    "version": "0.1.0",
    "created_at": "2025-01-02",
    "modified_at": "2025-01-02",
    "authors": [
      "Jord√£o Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "Copyright (c) 2024, Jord√£o Bra...",
    "home_pypi": "https://pypi.org/project/napari-vector-graphics/",
    "home_github": "https://github.com/JoOkuma/napari-vector-graphics",
    "home_other": null,
    "summary": "Helper plugin to export napari viewer content as SVG",
    "categories": [
      "Utilities",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "drawsvg",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "opencv-python-headless; extra == \"all\""
    ],
    "package_metadata_description": "# napari-vector-graphics\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vector-graphics.svg?color=green)](https://github.com/JoOkuma/napari-vector-graphics/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-vector-graphics.svg?color=green)](https://pypi.org/project/napari-vector-graphics)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vector-graphics.svg?color=green)](https://python.org)\n[![tests](https://github.com/JoOkuma/napari-vector-graphics/workflows/tests/badge.svg)](https://github.com/JoOkuma/napari-vector-graphics/actions)\n[![codecov](https://codecov.io/gh/JoOkuma/napari-vector-graphics/branch/main/graph/badge.svg)](https://codecov.io/gh/JoOkuma/napari-vector-graphics)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vector-graphics)](https://napari-hub.org/plugins/napari-vector-graphics)\n\nHelper plugin to export napari viewer content as SVG\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-vector-graphics` via [pip]:\n\n    pip install napari-vector-graphics\n\nIf you want to be able to vectorize segmentation layers, you will need `python-opencv-headless`.\nTo install it, run:\n\n    pip install \"napari-vector-graphics[all]\"\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/JoOkuma/napari-vector-graphics.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-vector-graphics\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/JoOkuma/napari-vector-graphics/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Vector Graphics Screenshots"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nyxus",
    "name": "napari-nyxus",
    "display_name": "napari-nyxus",
    "version": "0.2.1",
    "created_at": "2023-05-05",
    "modified_at": "2024-12-30",
    "authors": [
      "Jesse McKinzie"
    ],
    "author_emails": [
      "Jesse.McKinzie@axleinfo.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-nyxus/",
    "home_github": "https://github.com/PolusAI/napari-nyxus",
    "home_other": null,
    "summary": "A napari plugin for calculating features from intensity-label image data",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "napari",
      "pandas",
      "numpy",
      "pandas",
      "nyxus>=0.5.0",
      "imagecodecs",
      "magicgui",
      "napari-workflows",
      "qtpy",
      "superqt",
      "napari-skimage-regionprops>=0.10.1",
      "matplotlib",
      "filepattern>=2.0.0"
    ],
    "package_metadata_description": "# Nyxus Napari\n\nNyxus Napari is a Napari plugin for running feature calculations on image-segmentation image pairs, using the\nNyxus application to compute features. Nyxus is a feature-rich, highly optimized, Python/C++ application capable \nof analyzing images of arbitrary size and assembling complex regions of interest (ROIs) split across multiple image tiles and files. \n\nFor more information on Nyxus, see https://github.com/PolusAI/nyxus.\n \n# Installation \n\nTo install Napari, it is recommended to first create a separate Conda environment. \n\n```\nconda create -y -n napari-env -c conda-forge python=3.9\nconda activate napari-env\n```\n\nAfter creating the Conda environment,\ninstall Napari using pip\n\n```\npython -m pip install \"napari[all]\"\npython -m pip install \"napari[all]\" --upgrade\n```\n\nor using conda\n\n```\nconda install -c conda-forge napari\nconda update napari\n```\n\nNext, Nyxus must be installed. Note that the version of Nyxus must be greater than or equal to `0.50` to run the Napari plugin.\n\n`pip install nyxus`\n\nor build from source using the instructions at https://github.com/PolusAI/nyxus#building-from-source using the conda build for the\npython API.\n\nAfter installing Napari and Nyxus, the Nyxus Napari plugin can be installed by cloning this repo and then building the plugin from the source. \nAn example of this process is provided below.\n\n```\ngit clone https://github.com/PolusAI/napari-nyxus.git\ncd napari_nyxus\npip install -e .\n```\n\nNapari can then be ran by running \n\n```\nnapari\n````\n\n# Use\nAfter installing the plugin, start Napari by running the command `napari` from the command line. Once the Napari \nGUI is loaded, the Nyxus plugin can be loaded from the `Plugins` menu in the toolbar by going to Plugins -> nyxus-napari.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/plugin_menu.png)\n\nA widget will appear in the Napari viewer to run Nyxus.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/nyxus_loaded.png)\n\nAs shown by the example above, Nyxus will take in Intensity and Segmentation images. These parameters can either be a stack\nof images or a single image pair. To load an image pair, use File -> Open File(s)... and select the images to load.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/open_image.png)\n\n\nNote that this method can also be used to open a stack of image, by using File -> Open Folder... instead of images. \n\nIf the segmentation is loaded as an Image type in the napari viewer, it must first be converted to the Labels type. The image can converted as shown below.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/convert_to_labels.png)\n\nThe loaded files can then be selected with the Intensity and Segmentation drop down menus. Other parameters can also be changed,\nsuch as which features to calculate. For more information on the available features, see https://nyxus.readthedocs.io/en/latest/featurelist.html.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/setup_calculation.png)\n\nAfter running Nyxus, the feature calculations will also appear in the Napari viewer.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/feature_results.png)\n\nThe Nyxus Napari plugin provides functionality to interact with the table containing the feature calculations. First, click on the segmentation image and then select `show selected` in the layer controls. \n\n\nThen, if a value is clicked in the `label` column of the table, the respective ROI will be highlighted in the segmentation image in the viewer.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/click_label.png)\n\nTo select the ROI and have it added to a separate Labels image, the label in the table can be double clicked. Each double clicked label will be added to the same Labels image as show below. To unselect, the ROI, double click its respective label again.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/double_click_label.png)\n\nThis feature can also be used in the opposite way, i.e. if an ROI is clicked in the segmentation image, the respective row in the \nfeature table will be highlighted.\n\nIf one of the column headers are double clicked, a colormap will be generated in the Napari viewer showing the values of the features in the clicked\ncolumn. For example, if `Intensity` features are calculated, the `INTEGRATED_INTENSITY` column can be clicked and the colormap will appear.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/feature_colormap.png)\n\nOnce the colormap is loaded, a slider will appear in the window with the minimum value being the minimum value of the feature colormap and the \nmaximum value of the slider is the maximum value of the colormap. By adjusting the ranges in the slider, a new label image will appear in the viewer\nthat contains the ROIs who's features values fall within the slider values.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/slider_feature.png)\n\nThe new labels resulting from the range slider selector can then be used to run Nyxus on by using the labels image as the `Segmentation` parameter.\n\n![](https://github.com/PolusAI/napari-nyxus/raw/main/docs/source/img/run_on_colormap_labels.png)\n\n# Limitations\n\nWhile Nyxus Napari provides batched processing for large sets of images where each individual image will fit into RAM, \nit does not provide functionality to handle large single images that do not fit into RAM or that are larger than the \nlimitations of Napari. For large images, it is recommended to install the Python or CLI version of Nyxus. \nFor more information, see https://github.com/PolusAI/nyxus. \n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Nyxus"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-clemreg",
    "name": "napari-clemreg",
    "display_name": "napari-clemreg",
    "version": "0.2.1",
    "created_at": "2021-06-01",
    "modified_at": "2024-12-26",
    "authors": [
      "Daniel Krentzel"
    ],
    "author_emails": [
      "dkrentzel@pm.me"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-clemreg/",
    "home_github": "https://github.com/krentzd/napari-clemreg",
    "home_other": null,
    "summary": "A plugin for registering multimodal image volumes based on common segmented structures of interest with point-clouds.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "typing_extensions",
      "setuptools",
      "packaging",
      "numpy==1.22.0",
      "magicgui==0.7.3",
      "scipy==1.10.1",
      "napari",
      "scikit-image==0.21.0",
      "h5py==3.9.0",
      "matplotlib==3.7.3",
      "imageio==2.31.5",
      "tifffile==2023.7.10",
      "probreg==0.3.6",
      "open3d==0.17.0",
      "transforms3d==0.4.1",
      "tqdm==4.66.1",
      "empanada-dl==0.1.7",
      "torch==2.0.1",
      "magicgui==0.7.3",
      "connected-components-3d==3.12.3"
    ],
    "package_metadata_description": "# napari-clemreg\n### An automated point cloud based registration algorithm for correlative light and volume electron microscopy\n\n[![License](https://img.shields.io/pypi/l/napari-clemreg.svg?color=green)](https://github.com/krentzd/napari-clemreg/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-clemreg.svg?color=green)](https://pypi.org/project/napari-clemreg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-clemreg.svg?color=green)](https://python.org)\n\n[//]: # ([![codecov]&#40;https://codecov.io/gh/krentzd/napari-clemreg/branch/master/graph/badge.svg&#41;]&#40;https://codecov.io/gh/krentzd/napari-clemreg&#41;)\n[//]: # ([![tests]&#40;https://github.com/krentzd/napari-clemreg/workflows/tests/badge.svg&#41;]&#40;https://github.com/krentzd/napari-clemreg/actions&#41;)\n\n## Installation\n### Local Installation\n\nTo install `napari-clemreg` it is recommended to create a fresh [conda] environment with Python 3.9:\n\n```\nconda create -n clemreg_env python=3.9\n```\nNext, install `napari` with the following command via [pip]: \n\n```\npip install \"napari[all]\"\n```\n\nFinally, `napari-clemreg` can be installed with:\n```\npip install napari-clemreg\n```\nWhen installing `napari-clemreg` on a Windows machine, the following error might appear:\n```\nerror Microsoft Visual C++ 14.0 is required\n```\nEnsure that [Visual Studios C++ 14.00](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16) is installed\n\n### Docker Container\nIf you would like to run `napari-clemreg` in a docker container instead of installing it as above, please follow the instructions in our [Docker guide](docker_guide.md)\n\n## Usage\nCLEM-reg is the combination of 5 main steps, MitoNet segmentation, LoG segmentation,\npoint cloud sampling, point cloud registration and lastly image warping. These 5 steps \ncan be run all at once using the run registration widget shown below with the tick next to it.\nAlternatively, they can be run individually with the numbered widgets.\n\n![clemreg_widget_options.png](docs%2Fimages%2Fclemreg_widget_options.png)\n\n### Run Registration\n\n\n\n![registration_labels.png](docs%2Fimages%2FCLEMreg-fig.png)\n\n1. **Moving Image** - Here you select your light microscopy (LM) data which will\nbe warped to align with the fixed electron microscopy (EM) image.\n2. **Fixed Image** - Here you select your EM data which will\nact as the reference point for the LM to be aligned to.\n3. **Registration Algorithm** - Here you can decide which type of registration algorith\nwill be used for the registration of inputted LM and EM. In terms of speed of each algorithm\nthe following is the generally true, Rigid CPD > Affine CPD > BCPD.\n4. **MitoNet Segmentation Parameters** - Here are the advanced options for the segmentation\nof the mitochondria in the EM data.\n   1. Prediction Across Three Axis - By selecting this option MitoNet will run segmentation\nacross all three axis of the EM volume and then these three predictions will be aggregate.\n5. **LoG Segmentation Parameters** - Here are the advanced options for the segmentation of \nthe mitochondria in the LM data.\n   1. Sigma - Sigma value for the Laplacian of Gaussian filter.\n   2. Threshold - Threshold value for the segmenting the LM data.\n6. **Point Cloud Sampling** - Here are the advanced options for the point cloud sampling of the \nsegmentations of the LM and EM data.\n   1. Sampling Frequency - Frequency of point sampling from the fixed and moving segmentation. The greater the value the more points in the point cloud.\n   2. Sigma - Sigma value for the canny edge filter.\n7. **Point Cloud Registration** - Here are the advanced options for the registration of the point clouds\nof both the LM and EM data.\n   1. Voxel Size - The size voxel size of each point. Smaller the size the less memory consumption.\n   2. Subsampling - Downsampling of the point clouds to reduce memory consumption. Greater the number the fewer points in the point cloud.\n   3. Maximum Iterations - The number of round of point cloud registration. If too small it won't converge on an opitmal registration.\n8. **Image Warping** - Here are the advanced options for the image warping of the moving LM images.\n   1. Interpolation Order - The order of the spline interpolation.\n   2. Aproximate Grid - Controls the \"resolution\" of the grid onto which you're warping. A higher value reduces the step size between coordinates.\n   3. Sub-division Factor - Controls the size of the chunk when applying the warping.\n9. **Save Parameters** - Here you can select the option to save the advanced options you've selected\nto a JSON file which can be kept for reproducibility as well as running the registration again.\n10. **Visualise Intermediate Results** - Here you can select to view the outputs of each step as they\nare completed.\n\n### Split Registration\nAs well as being able to run all the steps of CLEM-reg in one widget (the `Run registration` widget),\nyou are also able to do all these steps independently using the `Split Registration` functionality. \n\nThere are four separate widgets that encapsulate the 5 steps of CLEM-reg each of which have\ntheir own unique input and output:\n1. `MitoNet Segmentation` \n   - **Input**: EM Image\n   - **Output**: EM Segmentation\n2. `LoG Segmentation`\n   - **Input**: LM Image\n   - **Output**: LM Segmentation\n3. `Point Cloud Sampling`\n   - **Input**: LM Segmentation & EM Segmentation\n   - **Output**: LM Point Cloud & LM Point Cloud\n4. `Point Cloud Registration & Image Warping`\n   - **Input**: EM Image, LM Image, LM Point Cloud & EM Point Cloud\n   - **Output**: Registered LM Image, Registered LM Point Cloud\n\n### Registering Multiple LM Channels\nOne can register multiple LM channels at once by doing the following.\n\n1. Start by splitting the LM channels into the separate layers by right-clicking on\nthe layer and then selecting `Split Stack`.\n![merged-channel-split-options.png](docs%2Fimages%2Fmerged-channel-split-options.png)\nThis will result in each of the channels having their own individual layer. \n\n2. Once this is done we must link all the LM layers together, this is done \nby selecting all the layers which will highlight them in blue, once again right-clicking\non the layer and then selecting `Link Layers.`\n![split-channels-link-layers.png](docs%2Fimages%2Fsplit-channels-link-layers.png)\n\n3. When you finally go to run CLEM-reg ensure that for the `Moving Image`\nyou select the LM layer that contains mitochondria.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-clemreg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/krentzd/napari-clemreg/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[conda]: https://docs.conda.io/en/latest/\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Run registration",
      "1) Electron Microscopy (EM) Segmentation",
      "2) Fluorescence Microscopy (FM) Segmentation",
      "3) Point Cloud Sampling",
      "4) Point Cloud Registration & Image Warping"
    ],
    "contributions_sample_data": [
      "Sample Benchmark Data"
    ]
  },
  {
    "normalized_name": "faser",
    "name": "faser",
    "display_name": "faser",
    "version": "0.3.6",
    "created_at": "2022-05-23",
    "modified_at": "2024-12-25",
    "authors": [
      "Johannes Roos",
      "Stephane Bancelin"
    ],
    "author_emails": [
      "Johannes Roos <jhnnsrs@gmail.com>",
      "Stephane Bancelin <stephane.bancelin@cnrs.fr>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/faser/",
    "home_github": "https://github.com/jhnnsrs/faser",
    "home_other": null,
    "summary": "Faser is a tool for vectorial psf simulation",
    "categories": [],
    "package_metadata_requires_python": "<3.13,>=3.9",
    "package_metadata_requires_dist": [
      "matplotlib>=3.9.2",
      "numpy<2",
      "psygnal>=0.11.1",
      "pydantic>2",
      "python-slugify>=8.0.4",
      "qtpy>=2.4.2",
      "superqt>=0.6.7",
      "tifffile>=2024.8.30",
      "rich-click>=1.8.3; extra == 'cli'",
      "pyqt5>=5.15.11; extra == 'full'",
      "rich-click>=1.8.3; extra == 'full'",
      "numba>=0.60.0; extra == 'numba'"
    ],
    "package_metadata_description": "# faser\n\nfaser is a Python-based software package designed to simulate the excitation\npoint spread function (PSF) of optical microscopes. Faser  calculates PSFs for high NA\nfocusing by using a vectorial model of the electromagnetic field, enabling exploration of the impact\nof geometrical and optical parameters on imaging performance in advanced applications. The\nsoftware supports various beam profiles, including those used in STED microscopy, and allows for\nthe simulation of common experimental conditions such as a cranial window and a coverslip tilt.\n\nWe provide to prefered ways to use faser:\n\n## Faser as a Napari Plugin\n\nThe recommended way to install faser is as a Napari plugin, which provides a user-friendly GUI for interactively exploring the PSF simulation. This can be done via:\n\n```bash\npip install faser napari[pyqt5]\n```\n\n#### Usage\n\nYou can run the GUI application via (or just as a Napari plugin)\n\n```bash\nqtfaser\n```\n\nFor more information on how to use the GUI, please refer to the [preprint](https://faser.readthedocs.io/en/latest/).\n\n## Faser as a standalone application\n\nAlternatively, you can install the package as a standalone application only, that you can run in enironments without Napari or GUI support:\n\n```bash\npip install faser[cli]\n```\nThis will install the package as a standalone application that can be run from the command line via:\n\n```bash\nfaser\n```\nWe generally recommend the GUI application for most users, as it provides a more user-friendly interface.\nHowever faser can also be used as a library, and the CLI application is useful for scripting and batch processing.\n\n#### Usage\n\nTo simulate the PSF, with a specific numerical aperture (NA) and a beam profile, you can run the following command:\n\n```bash\nfaser --na 1.4 --window=NO\n```\n\nFor more information and options you can run:\n\n```bash\nfaser --help\n```\n\nTo display the GUI interface and the available options:\n\n```\n  __                     \n / _| __ _ ___  ___ _ __ \n| |_ / _` / __|/ _ \\ '__|\n|  _| (_| \\__ \\  __/ |   \n|_|  \\__,_|___/\\___|_|   \n\nGenerating PSF with config\nfaser‚ûú  faser git:(master) ‚úó uv run faser --help\n                                                                                                                                                                                                                                                   \n Usage: faser [OPTIONS]                                                                                                                                                                                                                            \n                                                                                                                                                                                                                                                   \n‚ï≠‚îÄ Options ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ --config                     FILENAME         Path to a JSON file                                                                                                                                                                               ‚îÇ\n‚îÇ --detector_gaussian_noise    FLOAT            Detector Gaussian noise                                                                                                                                                                           ‚îÇ\n‚îÇ --gaussian_beam_noise        FLOAT            Gaussian_beam noise                                                                                                                                                                               ‚îÇ\n‚îÇ --add_noise                  NOISE                                                                                                                                                                                                              ‚îÇ\n‚îÇ --loaded_phase_mask          OPTIONAL         Loaded Phasemak                                                                                                                                                                                   ‚îÇ\n‚îÇ --p                          FLOAT            Ratio between Donut (p) and Bottle (1-p) intensity                                                                                                                                                ‚îÇ\n‚îÇ --mask_offset_y              FLOAT            Y offset of the phase mask in regard to pupil center                                                                                                                                              ‚îÇ\n‚îÇ --mask_offset_x              FLOAT            X offset of the phase mask in regard to pupil center                                                                                                                                              ‚îÇ\n‚îÇ --ring_radius                FLOAT            Radius of the ring phase mask (on unit pupil)                                                                                                                                                     ‚îÇ\n‚îÇ --rc                         FLOAT            Ring charge (should be odd to produce bottle)                                                                                                                                                     ‚îÇ\n‚îÇ --vc                         FLOAT            Vortex charge (should be integer to produce donut)                                                                                                                                                ‚îÇ\n‚îÇ --epsilon                    FLOAT            Ellipticity of the polarization (in ¬∞)                                                                                                                                                            ‚îÇ\n‚îÇ --psi                        FLOAT            Direction of the polarization (in ¬∞)                                                                                                                                                              ‚îÇ\n‚îÇ --ampl_offset_y              FLOAT            Y offset of the amplitude profile in regard to pupil center                                                                                                                                       ‚îÇ\n‚îÇ --ampl_offset_x              FLOAT            X offset of the amplitude profile in regard to pupil center                                                                                                                                       ‚îÇ\n‚îÇ --waist                      FLOAT            Diameter of the input beam on the objective pupil (in ¬µm)                                                                                                                                         ‚îÇ\n‚îÇ --wavelength                 FLOAT            Wavelength of light (in ¬µm)                                                                                                                                                                       ‚îÇ\n‚îÇ --polarization               POLARIZATION                                                                                                                                                                                                       ‚îÇ\n‚îÇ --mode                       MODE                                                                                                                                                                                                               ‚îÇ\n‚îÇ --aberration_offset_y        FLOAT            Y offset of the aberration function in regard to pupil center                                                                                                                                     ‚îÇ\n‚îÇ --aberration_offset_x        FLOAT            X offset of the aberration function in regard to pupil center                                                                                                                                     ‚îÇ\n‚îÇ --a24                        ABERRATIONFLOAT  Secondary spherical                                                                                                                                                                               ‚îÇ\n‚îÇ --a12                        ABERRATIONFLOAT  Primary spherical                                                                                                                                                                                 ‚îÇ\n‚îÇ --a9                         ABERRATIONFLOAT  Oblique Trefoil                                                                                                                                                                                   ‚îÇ\n‚îÇ --a8                         ABERRATIONFLOAT  Horizontal Coma                                                                                                                                                                                   ‚îÇ\n‚îÇ --a7                         ABERRATIONFLOAT  Vertical Coma                                                                                                                                                                                     ‚îÇ\n‚îÇ --a6                         ABERRATIONFLOAT  Vertical Trefoil                                                                                                                                                                                  ‚îÇ\n‚îÇ --a5                         ABERRATIONFLOAT  Vertical Astigmatism                                                                                                                                                                              ‚îÇ\n‚îÇ --a4                         ABERRATIONFLOAT  Defocus                                                                                                                                                                                           ‚îÇ\n‚îÇ --a3                         ABERRATIONFLOAT  Oblique Astigmatism                                                                                                                                                                               ‚îÇ\n‚îÇ --a2                         ABERRATIONFLOAT  Horizontal Tilt                                                                                                                                                                                   ‚îÇ\n‚îÇ --a1                         ABERRATIONFLOAT  Vertical Tilt                                                                                                                                                                                     ‚îÇ\n‚îÇ --a0                         ABERRATIONFLOAT  Piston                                                                                                                                                                                            ‚îÇ\n‚îÇ --wind_offset_y              FLOAT            Y offset of the cranial window in regard to pupil center                                                                                                                                          ‚îÇ\n‚îÇ --wind_offset_x              FLOAT            X offset of the cranial window in regard to pupil center                                                                                                                                          ‚îÇ\n‚îÇ --wind_depth                 FLOAT            Depth of the cranial window (in mm)                                                                                                                                                               ‚îÇ\n‚îÇ --wind_radius                FLOAT            Diameter of the cranial window (in mm)                                                                                                                                                            ‚îÇ\n‚îÇ --window                     WINDOW                                                                                                                                                                                                             ‚îÇ\n‚îÇ --tilt                       FLOAT            Tilt angle of the coverslip (in ¬∞)                                                                                                                                                                ‚îÇ\n‚îÇ --depth                      FLOAT            Imaging depth in the sample (in ¬µm)                                                                                                                                                               ‚îÇ\n‚îÇ --collar                     FLOAT            Correction collar setting to compensate coverslip thickness                                                                                                                                       ‚îÇ\n‚îÇ --thickness                  FLOAT            Thickness of the coverslip (in ¬µm)                                                                                                                                                                ‚îÇ\n‚îÇ --n3                         FLOAT            Refractive index of the sample                                                                                                                                                                    ‚îÇ\n‚îÇ --n2                         FLOAT            Refractive index of the coverslip                                                                                                                                                                 ‚îÇ\n‚îÇ --n1                         FLOAT            Refractive index of the immersion medium                                                                                                                                                          ‚îÇ\n‚îÇ --wd                         FLOAT            Working Distance of the objective lens (in ¬µm)                                                                                                                                                    ‚îÇ\n‚îÇ --na                         FLOAT            Numerical Aperture of Objective Lens                                                                                                                                                              ‚îÇ\n‚îÇ --normalize                  NORMALIZE                                                                                                                                                                                                          ‚îÇ\n‚îÇ --nphi                       INTEGER          Integration sted of the aximutal angle on the pupil                                                                                                                                               ‚îÇ\n‚îÇ --ntheta                     INTEGER          Integration sted of the focalization angle                                                                                                                                                        ‚îÇ\n‚îÇ --nz                         INTEGER          Discretization of Z axis - better be odd number for perfect 0                                                                                                                                     ‚îÇ\n‚îÇ --nxy                        INTEGER          Discretization of image plane - better be odd number for perfect 0                                                                                                                                ‚îÇ\n‚îÇ --l_obs_z                    FLOAT            Observation scale in Z (in ¬µm)                                                                                                                                                                    ‚îÇ\n‚îÇ --l_obs_xy                   FLOAT            Observation scale in XY (in ¬µm)                                                                                                                                                                   ‚îÇ\n‚îÇ --help                                        Show this message and exit.                                                                                                                                                                       ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n```\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Faser (Helper)",
      "Faser (Generator)"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cellseg3d",
    "name": "napari_cellseg3d",
    "display_name": "CellSeg3D",
    "version": "0.2.2",
    "created_at": "2022-06-25",
    "modified_at": "2024-12-23",
    "authors": [
      "Cyril Achard",
      "Maxime Vidal",
      "Mackenzie Mathis"
    ],
    "author_emails": [
      "Cyril Achard <cyril.achard@epfl.ch>",
      "Maxime Vidal <maxime.vidal@epfl.ch>",
      "Mackenzie Mathis <mackenzie@post.harvard.edu>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-cellseg3d/",
    "home_github": "https://github.com/AdaptiveMotorControlLab/CellSeg3D",
    "home_other": null,
    "summary": "Plugin for cell segmentation in 3D",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari[all]>=0.4.14",
      "QtPy",
      "scikit-image>=0.19.2",
      "matplotlib>=3.4.1",
      "tifffile>=2022.2.9",
      "imagecodecs>=2023.3.16",
      "torch>=1.11",
      "monai[einops,nibabel]>=0.9.0",
      "itk",
      "tqdm",
      "pyclesperanto-prototype",
      "tqdm",
      "matplotlib",
      "pydensecrf2",
      "pyqt5; extra == \"pyqt5\"",
      "pyside2; extra == \"pyside2\"",
      "pyside6; extra == \"pyside6\"",
      "onnx; extra == \"onnx-cpu\"",
      "onnxruntime; extra == \"onnx-cpu\"",
      "onnx; extra == \"onnx-gpu\"",
      "onnxruntime-gpu; extra == \"onnx-gpu\"",
      "wandb; extra == \"wandb\"",
      "isort; extra == \"dev\"",
      "black; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "tuna; extra == \"dev\"",
      "twine; extra == \"dev\"",
      "jupyter-book; extra == \"docs\"",
      "pytest; extra == \"test\"",
      "pytest_qt; extra == \"test\"",
      "pytest-cov; extra == \"test\"",
      "coverage; extra == \"test\"",
      "tox; extra == \"test\"",
      "twine; extra == \"test\"",
      "onnx; extra == \"test\"",
      "onnxruntime; extra == \"test\""
    ],
    "package_metadata_description": "# CellSeg3D: self-supervised (and supervised) 3D cell segmentation, primarily for mesoSPIM data!\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cellseg3d)](https://www.napari-hub.org/plugins/napari-cellseg3d)\n[![PyPI](https://img.shields.io/pypi/v/napari-cellseg3d.svg?color=green)](https://pypi.org/project/napari-cellseg3d)\n[![Downloads](https://static.pepy.tech/badge/napari-cellseg3d)](https://pepy.tech/project/napari-cellseg3d)\n[![Downloads](https://static.pepy.tech/badge/napari-cellseg3d/month)](https://pepy.tech/project/napari-cellseg3d)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/AdaptiveMotorControlLab/CellSeg3D/raw/main/LICENSE)\n[![codecov](https://codecov.io/gh/AdaptiveMotorControlLab/CellSeg3D/branch/main/graph/badge.svg?token=hzUcn3XN8F)](https://codecov.io/gh/AdaptiveMotorControlLab/CellSeg3D)\n<a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/838605d0-9723-4e43-83cd-6dbfe4adf36b/cellseg-logo.png?format=1500w\" title=\"cellseg3d\" alt=\"cellseg3d logo\" width=\"350\" align=\"right\" vspace = \"80\"/>\n\n\n**A package for 3D cell segmentation with deep learning, including a napari plugin**: training, inference, and data review. In particular, this project was developed for analysis of confocal and mesoSPIM-acquired (cleared tissue + lightsheet) tissue datasets, but is not limited to this type of data. [Check out our preprint for more information!](https://www.biorxiv.org/content/10.1101/2024.05.17.594691v1)\n\n\n![demo](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/0d16a71b-3ff2-477a-9d83-18d96cb1ce28/full_demo.gif?format=500w)\n\n\n## Installation\n\n üíª See the [Installation page](https://adaptivemotorcontrollab.github.io/CellSeg3D/welcome.html) in the documentation for detailed instructions.\n\n## Documentation\n\nüìö Documentation is available at [https://AdaptiveMotorControlLab.github.io/CellSeg3D\n](https://adaptivemotorcontrollab.github.io/CellSeg3D/welcome.html)\n\n\nüìö For additional examples and how to reproduce our paper figures, see: [https://github.com/C-Achard/cellseg3d-figures](https://github.com/C-Achard/cellseg3d-figures)\n\n## Quick Start\n\n```\npip install napari_cellseg3d\n```\n\nTo use the plugin, please run:\n```\nnapari\n```\nThen go into `Plugins > napari-cellseg3d`, and choose which tool to use.\n\n- **Review (label)**: This module allows you to review your labels, from predictions or manual labeling, and correct them if needed. It then saves the status of each file in a csv, for easier monitoring.\n- **Inference**: This module allows you to use pre-trained segmentation algorithms on volumes to automatically label cells and compute statistics.\n- **Train**:  This module allows you to train segmentation algorithms from labeled volumes.\n- **Utilities**: This module allows you to perform several actions like cropping your volumes and labels dynamically, by selecting a fixed size volume and moving it around the image; fragment images into smaller cubes for training; or converting labels from instance to segmentation and the opposite.\n\n## Why use CellSeg3D?\n\nThe strength of our approach is we can match supervised model performance with purely self-supervised learning, meaning users don't need to spend (hundreds) of hours on annotation. Here is a quick look of our key results. TL;DR see panel **f**, which shows that with minmal input data we can outperform supervised models:\n\n\n![FIG1 (1)](https://github.com/user-attachments/assets/0d970b45-79ff-4c58-861f-e1e7dc9abc65)\n\n**Figure 1. Performance of 3D Semantic and Instance Segmentation Models.**\n**a:** Raw mesoSPIM whole-brain sample, volumes and corresponding ground truth labels from somatosensory (S1) and visual (V1) cortical regions.\n**b:** Evaluation of instance segmentation performance for baseline\nthresholding-only, supervised models: Cellpose, StartDist, SwinUNetR, SegResNet, and our self-supervised model WNet3D over three data subsets.\nF1-score is computed from the Intersection over Union (IoU) with ground truth labels, then averaged. Error bars represent 50% Confidence Intervals\n(CIs).\n**c:** View of 3D instance labels from supervised models, as noted, for visual cortex volume in b evaluation.\n**d:** Illustration of our WNet3D architecture showcasing the dual 3D U-Net structure with our modifications.\n\n\n## News\n\n**New version: v0.2.2**\n\n- v0.2.2:\n  - Updated the Colab Notebooks for training and inference\n  - New models available in the inference demo notebook\n  - CRF optional post-processing adjustments (and pip install directly)\n- v0.2.1:\n  - Updated plugin default behaviors across the board to be more readily applicable to demo data\n  - Threshold value in inference is now automatically set by default according to performance on demo data on a per-model basis\n  - Added a grid search utility to find best thresholds for supervised models\n\n- v0.2.0:\n  - Changed project name to \"napari_cellseg3d\" to avoid setuptools deprecation\n  - Small API changes for training/inference from a script\n  - Some fixes to WandB integration and csv saving after training\n\nPrevious additions:\n\n- v0.1.2: Fixed manifest issue for PyPi\n- Improved training interface\n- Unsupervised model : WNet3D\n  - Generate labels directly from raw data!\n  - Can be trained in napari directly or in Google Colab\n  - Pretrained weights for mesoSPIM whole-brain cell segmentation\n- WandB support (install wandb and login to use automatically when training)\n- Remade and improved documentation\n  - Moved to Jupyter Book\n  - Dedicated installation page, and working ARM64 install for macOS Silicon users\n- New utilities\n- Many small improvements and many bug fixes\n\n\n\n\n## Requirements\n\n**Compatible with Python 3.8 to 3.10.**\nRequires **[napari]**, **[PyTorch]** and **[MONAI]**.\nCompatible with Windows, MacOS and Linux.\nInstallation should not take more than 30 minutes, depending on your internet connection.\n\nFor PyTorch, please see [the PyTorch website for installation instructions].\n\nA CUDA-capable GPU is not needed but very strongly recommended, especially for training.\n\nIf you get errors from MONAI regarding missing readers, please see [MONAI's optional dependencies] page for instructions on getting the readers required by your images.\n\n### Install note for ARM64 (Silicon) Mac users\n\nTo avoid issues when installing on the ARM64 architecture, please follow these steps.\n\n1) Create a new conda env using the provided conda/napari_CellSeg3D_ARM64.yml file :\n\n        git clone https://github.com/AdaptiveMotorControlLab/CellSeg3d.git\n        cd CellSeg3d\n        conda env create -f conda/napari_CellSeg3D_ARM64.yml\n        conda activate napari_CellSeg3D_ARM64\n\n\n2) Install a Qt backend (PySide or PyQt5)\n3) Launch napari, the plugin should be available in the plugins menu.\n\n\n\n## Issues\n\n**Help us make the code better by reporting issues and adding your feature requests!**\n\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Testing\n\nYou can generate docs locally by running ``make html`` in the docs/ folder.\n\nBefore testing, install all requirements using ``pip install napari-cellseg3d[test]``.\n\n``pydensecrf`` is also required for testing.\n\nTo run tests locally:\n\n- Locally : run ``pytest napari_cellseg3d\\_tests`` in the plugin folder.\n- Locally with coverage : In the plugin folder, run ``coverage run --source=napari_cellseg3d -m pytest`` then ``coverage xml`` to generate a .xml coverage file.\n- With tox : run ``tox`` in the plugin folder (will simulate tests with several python and OS configs, requires substantial storage space)\n\n## Contributing\n\nContributions are very welcome.\n\nPlease ensure the coverage at least stays the same before you submit a pull request.\n\nFor local installation from Github cloning, please run:\n\n```\npip install -e .\n```\n\n## License\n\nDistributed under the terms of the [MIT] license.\n\n\"napari-cellseg3d\" is free and open source software.\n\n[napari-hub]: https://www.napari-hub.org/plugins/napari-cellseg3d\n\n[file an issue]: https://github.com/AdaptiveMotorControlLab/CellSeg3D/issues\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[Installation page]: https://adaptivemotorcontrollab.github.io/CellSeg3D/source/guides/installation_guide.html\n[the PyTorch website for installation instructions]: https://pytorch.org/get-started/locally/\n[PyTorch]: https://pytorch.org/get-started/locally/\n[MONAI's optional dependencies]: https://docs.monai.io/en/stable/installation.html#installing-the-recommended-dependencies\n[MONAI]: https://docs.monai.io/en/stable/installation.html#installing-the-recommended-dependencies\n\n## Citation\n\n```\n@article {Achard2024,\n\tauthor = {Achard, Cyril and Kousi, Timokleia and Frey, Markus and Vidal, Maxime and Paychere, Yves and Hofmann, Colin and Iqbal, Asim and Hausmann, Sebastien B. and Pages, Stephane and Mathis, Mackenzie W.},\n\ttitle = {CellSeg3D: self-supervised 3D cell segmentation for microscopy},\n\telocation-id = {2024.05.17.594691},\n\tyear = {2024},\n\tdoi = {10.1101/2024.05.17.594691},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tURL = {https://www.biorxiv.org/content/early/2024/05/17/2024.05.17.594691},\n\teprint = {https://www.biorxiv.org/content/early/2024/05/17/2024.05.17.594691.full.pdf},\n\tjournal = {bioRxiv}\n}\n```\n## Acknowledgements\n\nThis plugin was developed by originally Cyril Achard, Maxime Vidal, Mackenzie Mathis.\nThis work was funded, in part, from the Wyss Center to the [Mathis Laboratory of Adaptive Intelligence](https://www.mackenziemathislab.org/).\nPlease refer to the documentation for full acknowledgements.\n\n## Plugin base\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Labeling",
      "Inference",
      "Training",
      "Utilities",
      "Help/About..."
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-update-checker",
    "name": "napari-update-checker",
    "display_name": "napari update-checker",
    "version": "0.1.0",
    "created_at": "2024-12-23",
    "modified_at": "2024-12-23",
    "authors": [
      "napari team"
    ],
    "author_emails": [
      "napari team <napari-steering-council@googlegroups.com>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-update-checker/",
    "home_github": "https://github.com/napari/napari-plugin-manager",
    "home_other": null,
    "summary": "Updates checker plugin for napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "qtpy",
      "superqt",
      "pip",
      "PyQt5; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "virtualenv; extra == \"testing\"",
      "sphinx>6; extra == \"docs\"",
      "sphinx-autobuild; extra == \"docs\"",
      "sphinx-external-toc; extra == \"docs\"",
      "sphinx-copybutton; extra == \"docs\"",
      "sphinx-favicon; extra == \"docs\"",
      "myst-nb; extra == \"docs\"",
      "napari-sphinx-theme>=0.3.0; extra == \"docs\""
    ],
    "package_metadata_description": "# napari-update-checker\n\n[![License](https://img.shields.io/pypi/l/napari-update-checker.svg?color=green)](https://github.com/napari/update-checker/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-update-checker.svg?color=green)](https://pypi.org/project/napari-update-checker)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-update-checker.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari/update-checker/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/napari/update-checker/actions/workflows/test_and_deploy.yml)\n[![codecov](https://codecov.io/gh/napari/update-checker/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/update-checker)\n\n[napari] update checker to query for new napari versions.\n\nYou can read the documentation at [napari.org/update-checker](https://napari.org/update-checker).\n\n## Overview\n\nThe `napari-update-checker` is a plugin that checks for newer versions of napari available either on [conda-forge], or on [PyPI], by querying the repository release tags.\n\n![Screenshot of the napari-update-checker interface, showcasing the plugin](https://raw.githubusercontent.com/napari/update-checker/refs/heads/main/images/description.png)\n\n`napari-update-checker` knows how to detect if napari was installed using `conda` or `pip` or if it was installed using the application bundle to provide the correct documentation on how to update napari to the latest version.\n\n## Widget\n\n![Screenshot of the napari-update-checker widget](https://raw.githubusercontent.com/napari/update-checker/refs/heads/main/images/widget.png)\n\n## License\n\nDistributed under the terms of the [BSD-3] license, \"napari-update-checker\" is free and open source\nsoftware.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[file an issue]: https://github.com/napari/update-checker/issues\n[conda-forge]: https://anaconda.org/conda-forge/napari\n[PyPI]: https://pypi.org/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[napari]: https://github.com/napari/napari\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Check updates"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-console",
    "name": "napari-console",
    "display_name": "napari-console",
    "version": "0.1.3",
    "created_at": "2021-01-21",
    "modified_at": "2024-12-20",
    "authors": [
      "napari team"
    ],
    "author_emails": [
      "napari team <napari-steering-council@googlegroups.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-console/",
    "home_github": "https://github.com/napari/napari-console",
    "home_other": null,
    "summary": "A plugin that adds a console to napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "IPython>=7.7.0",
      "ipykernel>=5.2.0",
      "qtconsole!=4.7.6,!=5.4.2,>=4.5.1",
      "qtpy>=1.7.0",
      "PySide2!=5.15.0,>=5.13.2; (python_version < \"3.11\" and platform_machine != \"arm64\") and extra == \"pyside2\"",
      "PySide6<6.5; python_version < \"3.12\" and extra == \"pyside6-experimental\"",
      "PyQt6>6.5; extra == \"pyqt6\"",
      "PyQt6!=6.6.1; platform_system == \"Darwin\" and extra == \"pyqt6\"",
      "napari-console[pyside2]; extra == \"pyside\"",
      "PyQt5!=5.15.0,>=5.13.2; extra == \"pyqt5\"",
      "napari-console[pyqt5]; extra == \"pyqt\"",
      "napari-console[pyqt]; extra == \"qt\"",
      "napari[pyqt]; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-console (WIP, under active development)\n\n[![License](https://img.shields.io/pypi/l/napari-console.svg?color=green)](https://github.com/napari/napari-console/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-console.svg?color=green)](https://pypi.org/project/napari-console)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-console.svg?color=green)](https://python.org)\n[![tests](https://github.com/napari/napari-console/workflows/tests/badge.svg)](https://github.com/napari/napari-console/actions)\n[![codecov](https://codecov.io/gh/napari/napari-console/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/napari-console)\n\nA plugin that adds a console to napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Local variables\n\nIn napari-console 0.0.8 and earlier, the console `locals()` namespace only\ncontained a reference to the napari viewer that enclosed the console.\n\nSince version 0.0.9, it instead contains everything in the enclosing frame that\ncalled napari. That is, if your Python code is:\n\n```python\nimport napari\nimport numpy as np\nfrom scipy import ndimage as ndi\n\nimage = np.random.random((500, 500))\nlabels = ndi.label(image > 0.7)[0]\n\nviewer, image_layer = napari.imshow(image)\nlabels_layer = viewer.add_labels(labels)\n\nnapari.run()\n```\n\nThen the napari console will have the variables `np`, `napari`, `ndi`, `image`,\n`labels`, `viewer`, `image_layer`, and `labels_layer` in its namespace.\n\nThis is implemented by inspecting the Python stack when the console is first\ninstantiated, finding the first frame that is outside of the `napari_console`,\n`napari`, and `in_n_out` modules, and passing the variables in the frame's\n`f_locals` and `f_globals` to the console namespace.\n\nIf you want to disable this behavior (for example, because you are embedding\nnapari and the console within some larger application), you can add\n`NAPARI_EMBED=1` to your environment variables before instantiating the\nconsole.\n\n## Installation\n\nYou can install `napari-console` via [pip]:\n\n    pip install napari-console\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-console\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/sofroniewn/napari-console/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-imagej",
    "name": "napari-imagej",
    "display_name": "napari-imagej",
    "version": "0.2.0",
    "created_at": "2023-05-24",
    "modified_at": "2024-12-20",
    "authors": [
      "ImageJ2 developers"
    ],
    "author_emails": [
      "ImageJ2 developers <ctrueden@wisc.edu>"
    ],
    "license": "BSD-2-Clause",
    "home_pypi": "https://pypi.org/project/napari-imagej/",
    "home_github": "https://github.com/imagej/napari-imagej",
    "home_other": null,
    "summary": "ImageJ functionality from napari",
    "categories": [],
    "package_metadata_requires_python": "<3.13,>=3.9",
    "package_metadata_requires_dist": [
      "confuse>=2.0.0",
      "imglyb>=2.1.0",
      "jpype1>=1.4.1",
      "labeling>=0.1.12",
      "magicgui>=0.5.1",
      "napari>=0.4.17",
      "numpy",
      "pandas",
      "pyimagej>=1.4.1",
      "scyjava>=1.9.1",
      "superqt>=0.7.0",
      "xarray<2024.10.0",
      "qtconsole!=5.4.2",
      "typing_extensions!=4.6.0",
      "build; extra == \"dev\"",
      "myst-parser; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "pyqt5; extra == \"dev\"",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-env; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "sphinx; extra == \"dev\"",
      "sphinx-copybutton; extra == \"dev\"",
      "sphinx-rtd-theme; extra == \"dev\"",
      "qtpy; extra == \"dev\"",
      "validate-pyproject[all]; extra == \"dev\""
    ],
    "package_metadata_description": "# napari-imagej\n\n### A [napari] plugin for access to [ImageJ2]\n\n[![License](https://img.shields.io/pypi/l/napari-imagej.svg?color=green)](https://github.com/imagej/napari-imagej/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-imagej.svg?color=green)](https://pypi.org/project/napari-imagej)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-imagej.svg?color=green)](https://python.org)\n[![tests](https://github.com/imagej/napari-imagej/workflows/tests/badge.svg)](https://github.com/imagej/napari-imagej/actions)\n[![codecov](https://codecov.io/gh/imagej/napari-imagej/branch/main/graph/badge.svg)](https://codecov.io/gh/imagej/napari-imagej)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-imagej)](https://napari-hub.org/plugins/napari-imagej)\n\n**napari-imagej** aims to provide access to all [ImageJ2] functionality through the [napari] graphical user interface. It builds on the foundation of [PyImageJ], a project allowing ImageJ2 access from Python.\n\n**With napari-imagej, you can access:**\n\n1. The napari-imagej widget, providing *headless access* to:\n   * [ImageJ2 Commands] - 100+ image processing algorithms\n   * [ImageJ Ops] - 500+ *functional* image processing algorithms\n   * [SciJava Scripts] - migrated from Fiji or ImageJ2, or written yourself!\n2. The ImageJ user interface, providing access to *the entire ImageJ ecosystem* within napari.\n\nSee the [project roadmap](https://github.com/orgs/imagej/projects/2) for future directions.\n\n## Getting Started\n\nLearn more about the project [here](https://napari-imagej.readthedocs.io/en/latest/), or jump straight to [installation](https://napari-imagej.readthedocs.io/en/latest/Install.html)!\n\n## Usage\n\n* [Image Processing with ImageJ Ops](https://napari-imagej.readthedocs.io/en/latest/examples/ops.html)\n* [Puncta Segmentation with SciJava Scripts](https://napari-imagej.readthedocs.io/en/latest/examples/scripting.html)\n\n## Troubleshooting\n\nThe [FAQ](https://napari-imagej.readthedocs.io/en/latest/Troubleshooting.html) outlines solutions to many common issues.\n\nFor more obscure issues, feel free to reach out on [forum.image.sc](https://forum.image.sc).\n\nIf you've found a bug, please [file an issue]!\n\n## Contributing\n\nWe welcome any and all contributions made onto the napari-imagej repository.\n\nDevelopment discussion occurs on the [Image.sc Zulip chat](https://imagesc.zulipchat.com/#narrow/stream/328100-scyjava).\n\nFor technical details involved with contributing, please see [here](https://napari-imagej.readthedocs.io/en/latest/Development.html)\n\n## License\n\nDistributed under the terms of the [BSD-2] license,\n\"napari-imagej\" is free and open source software.\n\n## Citing\n\n_napari-imagej: ImageJ ecosystem access from napari_, Nature Methods, 2023 Aug 18\n\nDOI: [10.1038/s41592-023-01990-0](https://doi.org/10.1038/s41592-023-01990-0)\n\n[Apache Software License 2.0]: https://www.apache.org/licenses/LICENSE-2.0\n[black]: https://github.com/psf/black\n[BSD-2]: https://opensource.org/licenses/BSD-2-Clause\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[conda]: https://docs.conda.io/\n[conda-forge]: https://conda-forge.org/\n[file an issue]: https://github.com/imagej/napari-imagej/issues\n[flake8]: https://flake8.pycqa.org/\n[GNU GPL v3.0]: https://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: https://www.gnu.org/licenses/lgpl-3.0.txt\n[ImageJ2]: https://imagej.net/software/imagej2\n[ImageJ2 Commands]: https://github.com/imagej/imagej-plugins-commands\n[ImageJ Ops]: https://imagej.net/libs/imagej-ops\n[install mamba]: https://mamba.readthedocs.io/en/latest/installation.html\n[isort]: https://pycqa.github.io/isort/\n[mamba]: https://mamba.readthedocs.io/\n[MIT]: https://opensource.org/licenses/MIT\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari]: https://github.com/napari/napari\n[napari hub]: https://www.napari-hub.org/\n[npe2]: https://github.com/napari/npe2\n[pip]: https://pypi.org/project/pip/\n[pull request]: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests\n[PyImageJ]: https://github.com/imagej/pyimagej\n[PyPI]: https://pypi.org/\n[SciJava Scripts]: https://imagej.net/scripting\n[tox]: https://tox.readthedocs.io/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.xml"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ImageJ2"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "multireg",
    "name": "multireg",
    "display_name": "Multiplex Registration",
    "version": "0.0.18",
    "created_at": "2023-06-09",
    "modified_at": "2024-12-19",
    "authors": [
      "Ga√´lle Letort"
    ],
    "author_emails": [
      "gaelle.letort@pasteur.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/multireg/",
    "home_github": null,
    "home_other": null,
    "summary": "Registration of 3D multiplex images with one common chanel",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari<=0.4.18",
      "magicgui",
      "qtpy",
      "pyqt5",
      "tifffile",
      "imaris-ims-file-reader",
      "czifile",
      "itk==5.3.0",
      "itk-registration",
      "itk-elastix"
    ],
    "package_metadata_description": "# multireg\n\n[![License BSD-3](https://img.shields.io/pypi/l/multireg.svg?color=green)](https://gitlab.pasteur.fr/gletort/multireg/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/multireg.svg?color=green)](https://pypi.org/project/multireg)\n[![Python Version](https://img.shields.io/pypi/pyversions/multireg.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/multireg)](https://napari-hub.org/plugins/multireg)\n\nRegistration of 3D multiplex images with one common chanel, based on itk-elastix.\n\nNapari plugin to align 3D stacks that have one common field of view in one chanel used to calculate the alignement. The plugin will apply the registration to all other chanels and output one final stack with all the aligned chanels.\n\nThe stacks **must have one common chanel** (typically cell junctions and nuclei) that is used to calculate the registration transformation. It can be rotated, translated, deformed, and with a wider field of view. \nThen the calculated transformation is applied to all the other chanels for each stack.\n\nThe final result is **one multi-chanel 3D stack**, with the first chanel being an average (or not) of the common chanel and each other chanel the registered chanels from the multiple stacks. The common chanel can be averaged between the different chanels, which improves its quality.\n\nThe plugin save and load files to a folder named `aligned` and created in the same directory as the source images.\n\nExample of usage of this module is in the case of imaging the same cells with washing out or moving the sample in between. The corresponding cells will not be at the same position in the new stacks, and can even be deformed by the procedure. This plugin realign the images based on one common chanel on which the transformation is calculated. \n\n----------------------------------\n## Installation\n\n* You can install the plugin directly in `Napari` by going to `Plugins>Install/Uninstall plugins` and search for `multireg`\n\n* Or you can install `multireg` via [pip]:\n\n    pip install multireg\n\n\n## Usage\n\nYou can launch `multireg` in napari by going to `Plugins>multireg: do multiplex registration`.\n\n### Fixed image\nIt will open a prompt to ask you to select the reference (fixed) image, compared to which all other images will be aligned.\nThen you have to choose the `reference chanel` that will be used in all the stacks to calculate the alignement. So this chanel should be common to all stacks.\n\n![](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/plugin_step0.png)\n\n#### Reference points\nThe first part of the registration relies on reference points manually selected, because the common field of view can be quite far from each other in the acquisition. So first a affine registration is applied to bring close the region of interest between the two stacks to match. \n<br> *Note that if your stacks did not move a lot then you could calculate the transformation without using the reference points. There's an option in the alignement calculation panel for this.*\n\n![](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/plugin_fixedpoints.png)\n\nYou have to manually placed a few reference points (4-5 should be enough). Try to spread them in the image (in x,y and z) on landmarks to recognize them in other images. \n\nTo add a new reference point, click on the \"plus\" sign in the left panel. To select one, click on the arrow icon (or press 3), then on the point. You can move the point in x and y. To move it in z, press `u` for up and `d` for down. \n\nWhen all points are placed, save them. The **points have to be saved** to be correctly loaded by the alignement calculation step.\nThen click on `Fixed points done` to continue to the next step.\n\n\n### Moving images\nThen you can choose one of the images you want to align with the reference image. Its chanel that is common to the fixed image should be the same chanel, selected in the first step (the `reference chanel`). Select the file of the moving image to align by clicking on `select file`. This will open the new image and go to the step of placing the moving points in this image.\n\nWhen you will have process all the moving images, you can click on `All done` to finish the plugin by creating the [resulting stack](#create-resulting-image).\n\n![moving image step](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/plugin_movingimg.png)\n\n#### Moving points\nYou now have to locate where the region of interest (the fixed image) is in your new image and find the landmarks referenced in the fixed image are in this new image. This allows the plugin to put together the region of interest in the two images in a first step, before to fine-tune the registration.\n\nFor each point placed in the fixed image, place the corresponding point in the moving image. By default, the moving points are placed close to the fixed points. \n* Each point must have the same label (number) as its corresponding fixed points to associate them correctly. You can change a point label by selecting it and putting the new value in `param` and clicking on `update`.\n\n\n* When a point is selected, you can drag it to its desired location. To move it in the Z direction, you can press `u` to move it to the next Z (up direction) and `d` (down) to the previous Z. The viewed slice will also move, following the point new position, when you do so.\n\n* You can click on `side_by_side_view` to see the two images (fixed and moving) with their placed points at the same time.\n\n* You can click on `two_windows_view` to see the fixed image and points in a separate Napari window. This allows to have visualize separatly the fixed and moving images and points, and thus to see different z-slices or zoom for each image. The new window will be closed automatically by the plugin if you unselect this option or when you click on `Moving done`.\n\n![two window](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/twowin.png)\n\nWhen all the moving points have been correctly placed, click on `Save points` to save this positions and let it be usable by the alignement step. The points **have to be saved** in the point file to be correctly loaded in the alignement step.\n\n### Alignement calculation\nThis step is the core of the plugin. The transformation necessary to change the moving image to match with the fixed image on the `reference chanel` is calculated based on [itk-elastix](https://pypi.org/project/itk-elastix/) python module. It is decomposed in two steps. \n\n1. First a global **affine registration** is performed, based on the correspondance between the reference and moving points (`do rigid` option). This allows to locate the fixed image postion within the moving image and apply a first **shearing, scaling, rotation and translation** to super-impose the region of interest. \n\n2. The second step fine-tunes the registration. It doesn't use the reference points (except if rigid transformation was not selected) anymore but calculate the matching based on the images local intensities. **Non-rigid transformation** based on B-spline is performed at this step, thus allowing to compensate for **local deformations** in the moving image (`do bspline` option).\n\nThe option `use reference points` determines if the previously placed reference points should be used or if the registration is only based on intensities matching. It's possible to use only the intensities if the two images are not so far away from each other. The reference points will be used only in the first pass (either rigid or bspline) when both are selected. If only one is selected, the points will be used on the selected transformation.\n\nThe option `strong_weight_points` allows to give more importance to reference points than to intensities matching when calculating the registration. The weights will be 0.2 for the intensity metric and 0.8 for points metric. Note that if both rigid and bspline transformations are selected, the second transformation (bspline) do not use the points.\n\n![apply alignement step](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/interm.png)\n\n\nYou can click on `show advanced parameters` to tune the parameters of the non rigid transformation. After calculating the registration, the plugin will add a new layer, which is the moving image after alignement, so you can check the sucess of the regristration. `show intermediate_layer` will also add the moving image aligned after the first step only (the points matching with affine registration).\n\n![calculate alignement step](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/align.png)\n\n\n### Apply alignement\nOnce the calculated registration is satisfying, you can apply it to all the chanels of your moving image, or only to a few. By default, all chanels are selected in the `Apply alignement` panel, but you can unselect the chanels that you don't want to align in the parameter `align chanels`. \nWhen you click on `Align images`, the plugin will apply the transformation on the selected chanels of the moving image and save each of them in the `aligned` folder as individual `.tif` files. \n\n![apply alignement step](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/goalign.png)\n\n### Create resulting image\nThis step allows to save a single 3D multi-chanels stack with all the aligned chanels. \n\nThe common chanel present in all the images can be averaged together after alignement to obtain a much less noisy image. By default, the aligned `reference chanel` of all the images are averaged together to create the final image first chanel. However, it is possible to unselect some images in the first panel (`average chanels` parameter) if you do not wish to use all the images or do an average.\n\n![create result image](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/create.png)\n\nThen each aligned chanel of all the images that were not the reference chanel are stacked together in the final resulting image. Here also, if you don't want to keep all the other chanels in the resulting image, you can unselect the one that you don't want stacked, in the `add_chanels` parameter. \nAll the aligned chanels have been previously saved in the `aligned` folder. If `delete_files` is checked (default) all these interemediate files will be deleted and only the final resulting stack will be saved in that folder.\n\nYou will end-up with a final 3D multi-chanels stack, saved as a `.tif` file in the `aligned` folder, with the same name as your fixed image. It can have a lot of chanels if you stacked together multiple images.\nIn napari, you can separate the chanels by right clicking on the layer and select `Split stack`. \nIn Fiji, you can make the stack as a composite to see the chanels with different colors.\n\n![final image](https://gitlab.pasteur.fr/gletort/multireg/raw/main/imgs/reslayer.png)\n\n## License\nDistributed under the terms of the [BSD-3] license,\n\"multireg\" is free and open source software\n\n## Plugin initialization\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Do multiplex registration"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bbox",
    "name": "napari-bbox",
    "display_name": "Bounding Box",
    "version": "0.0.9",
    "created_at": "2023-03-24",
    "modified_at": "2024-12-19",
    "authors": [
      "David Bauer"
    ],
    "author_emails": [
      "dbauer@brc.hu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-bbox/",
    "home_github": "https://github.com/bauerdavid/napari-bbox",
    "home_other": null,
    "summary": "A new layer for bounding boxes in 2+ dimensions",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "napari>=0.4.15",
      "packaging",
      "pandas",
      "npe2",
      "vispy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-bbox\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bbox.svg?color=green)](https://github.com/bauerdavid/napari-bbox/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bbox.svg?color=green)](https://pypi.org/project/napari-bbox)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bbox.svg?color=green)](https://python.org)\n[![tests](https://github.com/bauerdavid/napari-bbox/workflows/tests/badge.svg)](https://github.com/bauerdavid/napari-bbox/actions)\n[![codecov](https://codecov.io/gh/bauerdavid/napari-bbox/branch/main/graph/badge.svg)](https://codecov.io/gh/bauerdavid/napari-bbox)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bbox)](https://napari-hub.org/plugins/napari-bbox)\n\nA new layer for bounding boxes in 2+ dimensions\n\n> **Note**: This plugin was originally part of [Annotation Toolbox](https://www.napari-hub.org/plugins/napari-nD-annotator), and was separated to allow other plugins to utilize it.\n\n----------------------------------\n\n## Demo\n\n\nhttps://user-images.githubusercontent.com/36735863/227506511-d672ce5c-eab5-436f-a7fd-6080e118a9a8.mp4\n\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-bbox` via [pip]:\n\n    pip install napari-bbox\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/bauerdavid/napari-bbox.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bbox\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bauerdavid/napari-bbox/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Bounding Box Creator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cellpose-counter",
    "name": "cellpose-counter",
    "display_name": "CellPose Counter",
    "version": "0.1.8",
    "created_at": "2024-11-21",
    "modified_at": "2024-12-10",
    "authors": [
      "Nicolas Buitrago"
    ],
    "author_emails": [
      "nsb5@rice.edu"
    ],
    "license": "Copyright (c) 2024, Szablowski...",
    "home_pypi": "https://pypi.org/project/cellpose-counter/",
    "home_github": null,
    "home_other": "None",
    "summary": "Cell/nuclei counter using cellpose models",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari[all]>=0.5.4",
      "cellpose>=3.1.0",
      "accelerate>=1.1.1",
      "napari-czifile2>=0.2.7",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# cellpose-counter\n\n[![License BSD-3](https://img.shields.io/pypi/l/cellpose-counter.svg?color=green)](https://github.com/szablowskilab/cellpose-counter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/cellpose-counter.svg?color=green)](https://pypi.org/project/cellpose-counter)\n[![Python Version](https://img.shields.io/pypi/pyversions/cellpose-counter.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/cellpose-counter)](https://napari-hub.org/plugins/cellpose-counter)\n\nA Napari plugin for cell/nuclei counting from a region or interest using\ncellpose models.\n\n----------------------------------\n\n## Installation\n\nOption 1: via [pip](https://pip.pypa.io/en/stable/) (or pip alternatives like\n[uv](https://docs.astral.sh/uv/)):\n\nBelow is a minimally working example of setting up a new virtual environment and\ninstalling the counter module with uv on Unix based systems.\n\n```bash\nuv venv # create virtual environment in .venv\nsource .venv/bin/activate\n\nuv pip install \"napari[all]\" cellpose-counter\n```\n\nOption 2: via Docker/Podman. The provide [Dockerfile](./Dockerfile) can be used\nto install Napari and the counter plugin along with a preconfigured Xpra server\nusing the napari-xpra image. Below is an example of building the image and\nrunning the application with GPU support.\n\n```bash\npodman build -t cellpose-counter .\npodman run -it -d \\\n    -p 9876:9876 \\\n    -e XPRA_START=\"python3 -m napari -w cellpose-counter\" \\\n    --device nvidia.com/gpu=all\n```\n\nThen, navigate to `http://localhost:9876` to view the application in a virtual\nmachine.\n\nNote: There is a known issue installing the plugin directly from Napari. Please\nsee [this issue](https://github.com/szablowskilab/cellpose-counter/issues/12)\nfor more updates.\n\n## GPU Acceleration\n\nTo enable GPU acceleration, you will need a CUDA capable GPU along with the\n[CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) and [cudNN library](https://developer.nvidia.com/cudnn).\n\nFor instructions on installing cuda toolkit and cudNN, see:\n\n1. [cuda toolkit installation for Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#fedora)\n1. [cudNN installation for Linux](https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html)\n\nOnce these are installed, update the pytorch package by first uninstalling torch\n(if already instsalled).\n\n```bash\nuv pip uninstall torch\n```\n\nThen install a torch version that is compatible with your CUDA version. For example,\n\n```bash\nuv pip install torch --index-url https://download.pytorch.org/whl/cu118\n```\n\nAfter installation, you can verify in an interactive python console with:\n\n```python3\nimport torch\ntorch.cuda.is_available()\n```\n\n## Usage\n\nTo open Napari with the cellpose counter loaded, run `napari -w cellpose-counter`.\n\nA dock widget will be open on the right side of the Napari interface. There\nyou can view options for restoring images (using the cellpose denoise module),\nand counting cells/nuclei in a region of interest (ROI).\n\nA few important notes:\n\n1. Images in TIFF or CZI file formats may be used.\n1. Images must be grayscale or single channel. RGB images may be loaded, but\nshould be split. You can do this by right clicking on the image and select\n`split rgb` or `split stack`.\n1. ROIs can be drawn using the shape layer tools. Only a single ROI can be drawn\nper shape layer (otherwise only the first draw ROI will be used).\n1. ROIs should be square or rectangular. You can draw ROIs as polygons or other\nshapes, but a bounding box will be made from these shapes anyway.\n1. For long running processes such as image restoration or counting, it may seem\nlike Napari is not doing anything. Notifications are shown in the viewer to\ndisplay import information and a small activity indicator can be seen in the\nbottom right hand corner. If this indicator is spinning, then work is being done\neven if it doesn't look like it.\n1. In case of a large number of uncounted nuclei, consider modifying the\nsegmentation parameters, or use the `Continue Counting` option to re-run the\nsegmentation on uncounted nuclei.\n\n## Updating\n\n1. via Napari plugin manager. Select cellpose-counter plugin and update button.\n\n1. via pip (or uv, ..., etc.)\n\n```bash\nuv pip install cellpose-counter --upgrade\n```\n\n## Contributing\n\nAll contributions are welcome. Please submit an issue for feedback or bugs.\n\n## Citations\n\nThis plugin is built on top of the Cellpose segmentation and denoising models.\nIf you use this plugin, please cite the following paper:\n\n```bitex\n@article{stringer2021cellpose,\ntitle={Cellpose: a generalist algorithm for cellular segmentation},\nauthor={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},\njournal={Nature Methods},\nvolume={18},\nnumber={1},\npages={100--106},\nyear={2021},\npublisher={Nature Publishing Group}\n}\n```\n\n## License\n\n[BSD-3](./LICENSE)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Counter"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "qhyccd-capture",
    "name": "qhyccd-capture",
    "display_name": "qhyccd-capture",
    "version": "0.0.4.0",
    "created_at": "2024-10-23",
    "modified_at": "2024-12-10",
    "authors": [
      "QHYCCD"
    ],
    "author_emails": [
      "lq@qhyccd.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/qhyccd-capture/",
    "home_github": null,
    "home_other": "None",
    "summary": "The basic operations for QHYCCD series cameras",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python",
      "PyQt5",
      "matplotlib",
      "astropy",
      "psutil",
      "photutils<1.14.0,>=1.11.0",
      "pybind11",
      "pyqtgraph",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\""
    ],
    "package_metadata_description": "# qhyccd-capture\n\n## Project Introduction\n\n`qhyccd-capture` is a basic operation library for handling QHYCCD series cameras. This library provides functionalities to interact with QHYCCD cameras, including camera connection, parameter setting, image capture, and display. This project is a [napari] plugin, aimed at simplifying the use of the camera through a graphical user interface.\n\n## Features\n\n- **Camera Connection**: Supports loading the corresponding QHYCCD dynamic link libraries on different operating systems (such as Windows, Linux, macOS) and initializing camera resources.\n- **Parameter Setting**: Provides the functionality to set camera parameters, such as exposure time, gain, offset, USB bandwidth, etc.\n- **Image Capture**: Supports single-frame mode exposure and retrieves image data.\n- **Image Display**: Displays captured images through napari, supports distributed display, single display, and sequence display modes.\n- **Histogram and White Balance**: Provides histogram equalization and white balance adjustment functions.\n- **ROI (Region of Interest)**: Supports creating and applying ROIs to operate on specific areas.\n- **Video Recording**: Supports video recording and saves in various video formats.\n- **Temperature Control**: Supports temperature control and displays temperature.\n- **CFW Control**: Supports CFW control and displays CFW status.\n- **Star Point Resolution**: Supports star point resolution and displays the results.\n\n![qhyccd-capture Êèí‰ª∂ÁïåÈù¢ÊòæÁ§∫](https://raw.githubusercontent.com/LiuQiang-AI/qhyccd-capture/main/src/qhyccd_capture/images/image.png)\n\n## Installation\nYou can install via pip:\n\n    pip install qhyccd-capture\n\nTo install the latest development version:\n\n    pip install git+https://github.com/nightliar-L/qhyccd-capture.git\n\n## Dependency Installation\n#### Astrometry.net \nCurrently, astrometry.net only supports the Ubuntu system.\n\n    sudo apt-get install astrometry.net\n    sudo apt-get install astrometry-data-tycho2\n    sudo vim ~/.bashrc\n    # Add the following content\n    export PATH=$PATH:/usr/local/astrometry/bin\n\n## Version Changes\n\n- 2024-10-23 Version 0.0.1 Initial version\n- 2024-10-24 Version 0.0.2 Fixed some issues introduced by the release\n- 2024-10-24 Version 0.0.3 Optimized some functions and processing logic\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"qhyccd-capture\" is free and open source software\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.raw"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "qhyccd-capture"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-organoid-counter",
    "name": "napari-organoid-counter",
    "display_name": "napari organoid counter",
    "version": "0.2.5",
    "created_at": "2022-04-13",
    "modified_at": "2024-12-05",
    "authors": [
      "christinab12"
    ],
    "author_emails": [
      "christina.bukas@helmholtz-muenchen.de"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-organoid-counter/",
    "home_github": "https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter",
    "home_other": null,
    "summary": "A plugin to automatically count lung organoids using Deep Learning.",
    "categories": [],
    "package_metadata_requires_python": "<3.11,>=3.9",
    "package_metadata_requires_dist": [
      "napari[all]<0.5.0,>=0.4.17",
      "napari-aicsimageio>=0.7.2",
      "torch>=2.3.1",
      "torchvision>=0.18.1",
      "openmim",
      "mmengine>=0.10.4",
      "mmdet>=3.3.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# Napari Organoid Counter - Version 0.2 is out! \n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-organoid-counter)](https://napari-hub.org/plugins/napari-organoid-counter)\n![stability-stable](https://img.shields.io/badge/stability-stable-green.svg)\n[![DOI](https://zenodo.org/badge/476715320.svg)](https://zenodo.org/badge/latestdoi/476715320)\n[![License](https://img.shields.io/pypi/l/napari-organoid-counter.svg?color=green)](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-organoid-counter.svg?color=green)](https://pypi.org/project/napari-organoid-counter)\n[![Python Version](https://img.shields.io/badge/python-3.9%20%7C%203.10-blue)](https://python.org)\n[![tests](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/workflows/tests/badge.svg)](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/actions)\n[![codecov](https://codecov.io/gh/HelmholtzAI-Consultants-Munich/napari-organoid-counter/branch/main/graph/badge.svg)](https://codecov.io/gh/HelmholtzAI-Consultants-Munich/napari-organoid-counter)\n\n\nA napari plugin to automatically count lung organoids from microscopy imaging data. Note: this plugin only supports single channel grayscale images.\n\n***Hold it for the demo!***\n\n![Alt Text](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/main/readme-content/demo-plugin-v2.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nThis plugin has been tested with python 3.9 and 3.10 - you may consider using conda to create your dedicated environment before running the `napari-organoid-counter`.\n\n1. You can install `napari-organoid-counter` via [pip](https://pypi.org/project/napari-organoid-counter/):\n\n    ``` pip install napari-organoid-counter```\n\n   To install latest development version :\n\n    ```pip install git+https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter.git```\n\n2. Additionally, you will then need to install one additional dependency:\n\n     ``` mim install \"mmcv<2.2.0,>=2.0.0rc4\" ```\n\nFor installing on a Windows machine directly from within napari, follow the instuctions [here](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/main/readme-content/How%20to%20install%20on%20a%20Windows%20machine.pdf). Step 2 additionally needs to be performed here too (mim install \"mmcv<2.2.0,>=2.0.0rc4\").\n\n## What's new in v2?\nCheckout our *What's New in v2* [here](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/main/.napari/DESCRIPTION.md#whats-new-in-v2).\n\n## How to use?\nAfter installing, you can start napari (either by typing ```napari``` in your terminal or by launching the application) and select the plugin from the drop down menu.\n\nFor more information on this plugin, its' intended audience, as well as Quickstart guide go to our [Quickstart guide](https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/blob/main/.napari/DESCRIPTION.md#quickstart).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [pytest], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-organoid-counter\" is free and open source software\n\n## Dependencies\n\n\n```napari-organoid-counter``` uses the ```napari-aicsimageio```<sup>[1]</sup> <sup>[2]</sup> plugin for reading and processing CZI images.\n\n[1] Eva Maxfield Brown, Dan Toloudis, Jamie Sherman, Madison Swain-Bowden, Talley Lambert, AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\n\n[2] Eva Maxfield Brown, Talley Lambert, Peter Sobolewski, Napari-AICSImageIO Contributors (2021). Napari-AICSImageIO: Image Reading in Napari using AICSImageIO [Computer software]. GitHub. https://github.com/AllenCellModeling/napari-aicsimageio\n\nThe latest version also uses models developed with the ```mmdetection``` package <sup>[3]</sup>, see [here](https://github.com/open-mmlab/mmdetection)\n\n[3] Chen, Kai, et al. \"MMDetection: Open mmlab detection toolbox and benchmark.\" arXiv preprint arXiv:1906.07155 (2019).\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/HelmholtzAI-Consultants-Munich/napari-organoid-counter/issues\n\n[napari]: https://github.com/napari/napari\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n## Citing\n\nIf you use this plugin for your work, please cite it using the following:\n\n> Christina Bukas, Harshavardhan Subramanian, & Marie Piraud. (2023). HelmholtzAI-Consultants-Munich/napari-organoid-counter: v0.2.0 (v0.2.0). Zenodo. https://doi.org/10.5281/zenodo.7859571\n> \nbibtex:\n```\n@software{christina_bukas_2022_6457904,\n  author       = {Christina Bukas, Harshavardhan Subramanian, & Marie Piraud},\n  title        = {{HelmholtzAI-Consultants-Munich/napari-organoid- \n                   counter: second release of the napari plugin for lung\n                   organoid counting}},\n  month        = apr,\n  year         = 2023,\n  publisher    = {Zenodo},\n  version      = {v0.2.0},\n  doi          = {10.5281/zenodo.7859571},\n  url          = {https://doi.org/10.5281/zenodo.7859571}\n}\n```\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.json"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Organoid-Counter"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-k2-wavebreaker",
    "name": "napari-k2-WaveBreaker",
    "display_name": "K2 Wave Breaker",
    "version": "0.2.5",
    "created_at": "2024-12-02",
    "modified_at": "2024-12-03",
    "authors": [
      "Sam Kris Vanspauwen"
    ],
    "author_emails": [
      "sam.vanspauwen.dk@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-k2-wavebreaker/",
    "home_github": null,
    "home_other": "None",
    "summary": "later",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "plotly",
      "pandas",
      "scikit-learn",
      "opencv-python",
      "matplotlib",
      "pathvalidate",
      "seaborn",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-k2-WaveBreaker\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-k2-WaveBreaker.svg?color=green)](https://github.com/SamKVs/napari-k2-WaveBreaker/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-k2-WaveBreaker.svg?color=green)](https://pypi.org/project/napari-k2-WaveBreaker)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-k2-WaveBreaker.svg?color=green)](https://python.org)\n[![tests](https://github.com/SamKVs/napari-k2-WaveBreaker/workflows/tests/badge.svg)](https://github.com/SamKVs/napari-k2-WaveBreaker/actions)\n[![codecov](https://codecov.io/gh/SamKVs/napari-k2-WaveBreaker/branch/main/graph/badge.svg)](https://codecov.io/gh/SamKVs/napari-k2-WaveBreaker)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-k2-WaveBreaker)](https://napari-hub.org/plugins/napari-k2-WaveBreaker)\n\n<div>\n    <img src=\"static/Logo.png\">\n</div>\n<h2>About this plugin</h2>\n\n\nThis Napari plugin was designed for the detection and quantification of periodic biological structures.\nAs this plugin has not been uploaded to napari-hub as of this moment it **cannot be installed on a pre-compiled, bundled \nversion of Napari**. Therefore Napari will need to be installed as a python package \n(<a href=\"https://napari.org/stable/tutorials/fundamentals/installation.html\">more info about Napari installation</a>). \nFurther information about the installation and licensing of the plugin can be found below. A detailed manual on the usage of the plugin can be found below as well. \n\n**If you have any questions or need help with the installation, please do hesitate to use the issues tab.**\n\n**In case you need a tutorial on how to use the plugin, please use the \"tutorial request\" label in the issues tab to reach out to me:**\n\n\n<h3>Guide</h3>\n\nActin is the most abundant protein in eukaryotic cells. As it is part of the cytoskeleton its function is essential for \nthe maintenance of the cell's morphological structure. In neurons, it was only recently that researchers started paying \nattention to the peculiar subcellular organization and localization of actin. First focussing on the dendritic spines, \nlater expanding to the axon.  \n\nThe axon initial segment (AIS) is defined as the most proximal 30-60 ¬µm of the axon and is known for its sturdy \nactin-betaIV cytoskeletal structure which is known to facilitate the densely packed ion channels, regulatory and\nscaffolding proteins on the membrane. The recent popularity of superresolution microscopy techniques like STORM and STED\nhas made the study of the localization of these proteins relatively easy and straightforward.\n\n&nbsp;\n<p align=\"center\">\n    <img src=\"static/Figure 1.svg\" width=\"100%\">\n</p>\n&nbsp;\n\nBecause of this property of the AIS many ion channels are localized either perpendicular to the actin rings\nor attached to a scaffolding protein called Ankyrin G which is localized in between two actin rings. This results in ion \nchannels like the Kv 1.1 (displayed below) appearing similar to superresolution images of actin.\n\n&nbsp;\n<div align=\"center\">\n    <img src=\"static/AIS.png\" width=\"100%\" style=\"mix-blend-mode: screen\">\n    <i align=\"center\" style=\"font-size: 9px\"> Example image of a rat hippocampal neuron AIS immunostained for Kv1.1.\nImage made on a Zeiss AxioImager Z1 equipped with a STEDYCON scanhead detector for confocal and super-resolution imaging,\nfitted with 4 APDs. Post-acquisition, image was deconvolved using Huygens Deconvolution Software  </i>\n</div>\n\n&nbsp;\n\nThis plugin was designed to detect and quantify the distance and the goodness of periodicity between cellular periodic structures. \nAdditionally, it can be used to detect and quantify the periodicity shift between two periodic stuctures.\n\n&nbsp;\n\n\n\n<a href=\"static/WaveBreaker User Manual.pdf\">\n    <img src=\"static/UM BUT.svg\" width=\"50%\">  \n</a>\n\n<a href=\"static/TEMPLATE AUTOCORRELATION 0.17-0.21 (x10).xlsx\">\n    <img src=\"static/AC EX BUT.svg\" width=\"50%\">  \n</a>\n\n<a href=\"static/TEMPLATE CROSSCORRELATION (x15).xlsx\">\n    <img src=\"static/CC EX BUT.svg\" width=\"50%\">  \n</a>\n\n<a href=\"static/Post-processing.py\">\n    <img src=\"static/EX%20PY.svg\" width=\"50%\">\n</a>\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-k2-WaveBreaker` via [pip]:\n\n    pip install napari-k2-WaveBreaker\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-k2-autocorrelation\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please file an issue along with a detailed description or reach out to me.\n\n[napari]: https://github.com/napari/napari\n\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n\n[@napari]: https://github.com/napari\n\n[MIT]: http://opensource.org/licenses/MIT\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n\n[tox]: https://tox.readthedocs.io/en/latest/\n\n[pip]: https://pypi.org/project/pip/\n\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari-k2-WaveBreaker"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bluesky",
    "name": "napari-bluesky",
    "display_name": "Bluesky",
    "version": "0.0.2",
    "created_at": "2024-11-29",
    "modified_at": "2024-11-29",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "bluesky@kyleharrington.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-bluesky/",
    "home_github": "https://github.com/kephale/napari-bluesky",
    "home_other": null,
    "summary": "A plugin for posting to Bluesky from napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "atproto",
      "appdirs",
      "python-dotenv",
      "pillow",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-bluesky\n\n[![License MIT](https://img.shields.io/pypi/l/napari-bluesky.svg?color=green)](https://github.com/kephale/napari-bluesky/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bluesky.svg?color=green)](https://pypi.org/project/napari-bluesky)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bluesky.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-bluesky/workflows/tests/badge.svg)](https://github.com/kephale/napari-bluesky/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-bluesky/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-bluesky)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bluesky)](https://napari-hub.org/plugins/napari-bluesky)\n\nA plugin for posting to Bluesky from napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/napari-plugin-template#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-bluesky` via [pip]:\n\n    pip install napari-bluesky\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-bluesky.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-bluesky\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/kephale/napari-bluesky/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Bluesky post"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spfluo",
    "name": "napari-spfluo",
    "display_name": "single particle",
    "version": "0.1.0",
    "created_at": "2024-10-04",
    "modified_at": "2024-11-26",
    "authors": [
      "Jean Plumail"
    ],
    "author_emails": [
      "Jean Plumail <jplumail@unistra.fr>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-spfluo/",
    "home_github": "https://github.com/jplumail/napari-spfluo",
    "home_other": null,
    "summary": "A plugin to use spfluo within napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "numpy",
      "qtpy",
      "scikit-image",
      "spfluo[ab-initio-reconstruction]"
    ],
    "package_metadata_description": "# napari-spfluo\n\n[![License MIT](https://img.shields.io/pypi/l/napari-spfluo.svg?color=green)](https://github.com/jplumail/napari-spfluo/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spfluo.svg?color=green)](https://pypi.org/project/napari-spfluo)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spfluo.svg?color=green)](https://python.org)\n[![tests](https://github.com/jplumail/napari-spfluo/workflows/tests/badge.svg)](https://github.com/jplumail/napari-spfluo/actions)\n[![codecov](https://codecov.io/gh/jplumail/napari-spfluo/branch/main/graph/badge.svg)](https://codecov.io/gh/jplumail/napari-spfluo)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spfluo)](https://napari-hub.org/plugins/napari-spfluo)\n\nA plugin to use spfluo within napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-spfluo` via [pip]:\n\n    pip install napari-spfluo\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jplumail/napari-spfluo.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-spfluo\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jplumail/napari-spfluo/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Ab initio reconstruction",
      "Manual ab initio reconstruction",
      "Symmetrize",
      "Rotate",
      "Separate",
      "DBSCAN",
      "HDBSCAN",
      "Threshold",
      "Merge labels",
      "Filter set"
    ],
    "contributions_sample_data": [
      "generate anisotropic data"
    ]
  },
  {
    "normalized_name": "napari-cryoet-data-portal",
    "name": "napari-cryoet-data-portal",
    "display_name": "CryoET Data Portal",
    "version": "0.5.0",
    "created_at": "2023-08-14",
    "modified_at": "2024-11-25",
    "authors": [
      "Andy Sweet"
    ],
    "author_emails": [
      "andrewdsweet@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-cryoet-data-portal/",
    "home_github": "https://github.com/chanzuckerberg/napari-cryoet-data-portal",
    "home_other": null,
    "summary": "List, preview, and open data from the CZII CryoET Data Portal",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "cmap",
      "cryoet_data_portal~=4.0",
      "fsspec[http,s3]",
      "npe2",
      "numpy",
      "napari>=0.4.19",
      "napari_ome_zarr",
      "ndjson",
      "qtpy",
      "superqt",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-mock; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-cryoet-data-portal\n\n[![MIT License](https://img.shields.io/pypi/l/napari-cryoet-data-portal.svg?color=green)](https://github.com/chanzuckerberg/napari-cryoet-data-portal/raw/main/LICENSE)\n[![Python package index](https://img.shields.io/pypi/v/napari-cryoet-data-portal.svg?color=green)](https://pypi.org/project/napari-cryoet-data-portal)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/napari-cryoet-data-portal.svg?color=green)](https://python.org)\n[![Test status](https://github.com/chanzuckerberg/napari-cryoet-data-portal/workflows/tests/badge.svg)](https://github.com/chanzuckerberg/napari-cryoet-data-portal/actions)\n[![Code coverage](https://codecov.io/gh/chanzuckerberg/napari-cryoet-data-portal/branch/main/graph/badge.svg)](https://codecov.io/gh/chanzuckerberg/napari-cryoet-data-portal)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cryoet-data-portal)](https://napari-hub.org/plugins/napari-cryoet-data-portal)\n\nList and open tomograms from the CZ Imaging Institute's [CryoET Data Portal] in [napari].\n\n![Plugin showing tomogram TS_043](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/61427a1f-df88-4e12-a680-32b8a10b6e6b)\n\n## Installation\n\nYou can install the latest stable version using [pip]:\n\n    pip install napari-cryoet-data-portal\n\nYou will also need to install napari separately as a Python package in the same environment.\nOne way to do that with Qt included is to run:\n\n    pip install \"napari[all]\"\n\nbut more generally you should follow the [latest napari installation instructions].\n\n## Usage\n\nSee the following video for a demonstration of basic usage of the plugin.\n\nhttps://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/51207e08-68af-446a-87bb-3de9c6756d35\n\nClick the *Connect* button to establish a connection to the data portal.\n\n![Connect button and URL to the portal](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/acefbbe8-855a-490b-be44-45a003069b08)\n\nYou can optionally query a subset of datasets, runs, voxel spacings, or tomograms using their corresponding IDs.\nThis can speed up the listing process as the portal grows.\nTo do so, select an ID type in the associated drop-down box from this panel, then enter the IDs of interest separated by commas in the text box next to it.\nFor example, if you only want to list datasets 10000 and 10001, select *Dataset IDs* from the drop-down box and enter the text *10000,10001* in the text box.\nBy default, all datasets are listed.\n\nAfter connecting to the portal, datasets are added below as they are found.\n\n![Datasets and tomograms in the portal shown as an interactive tree](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/7af78e00-bbba-4c5b-a286-fb865ca8cff0)\n\nDatasets and tomograms can be filtered by specifying a regular expression pattern.\n\n![Datasets and tomograms filtered by the text 26, so that only two are shown](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/96a57f4c-290e-4932-aa2d-95d13edd2d8c)\n\nSelecting a dataset displays its metadata, which can be similarly explored and filtered.\n\n![Metadata of dataset 10000 shown as an interactive tree of keys and values](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/b230720a-9083-4e35-a9db-44071c979fcc)\n\nSelecting a tomogram displays its metadata and also opens the lowest resolution tomogram and all of its associated point annotations in the napari viewer.\n\n![Metadata of tomogram TS_026 shown as an interactive tree of keys and values](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/386b3116-ba16-4f5d-840d-4eafa3dc62b0)\n\nHigher resolution tomograms can be loaded instead by selecting a different resolution and clicking the *Open* button.\n\n![Open button and resolution selector showing high resolution](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/d84c93b2-e6e7-43ee-aeb9-acd1a314637e)\n\nIn this case, napari only loads the data that needs to be displayed in the canvas.\nWhile this can reduce the amount of data loaded, it may also cause performance problems when initially opening and exploring the data.\nBy default, opening a new tomogram clears all the existing layers in napari.\nIf instead you want to keep those layers, uncheck the associated check-box in this panel.\n\nIn general, finding and fetching data from the portal can take a long time.\nAll plugin operations that fetch data from the portal try to run concurrently in order to keep interaction with napari and the plugin as responsive as possible.\nThese operations can also be cancelled by clicking the *Cancel* button.\n\n![Progress bar with loading status and cancel button](https://github.com/chanzuckerberg/napari-cryoet-data-portal/assets/2608297/2dc316ae-5231-4159-bc93-785548dbf6a5)\n\n## Contributing\n\nThis is still in early development, but contributions and ideas are welcome!\nDon't hesitate to [open an issue] or [open a pull request] to help improve this plugin.\n\nTo setup a development environment, fork this repository, clone your fork, change into its top level directory and run:\n\n    pip install -e \".[testing]\"\n\nThis project adheres to the [Contributor Covenant code of conduct].\nBy participating, you are expected to uphold this code.\nPlease report unacceptable behavior to opensource@chanzuckerberg.com.\n\n## Security\n\nIf you believe you have found a security issue, please see our [security policy] on how to report it.\n\n## License\n\nDistributed under the terms of the [MIT] license, \"napari-cryoet-data-portal\" is free and open source software. See the [license file] for more details.\n\n## Acknowledgements\n\nThis plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n[napari]: https://github.com/napari/napari\n[@napari]: https://github.com/napari\n[CryoET Data Portal]: https://chanzuckerberg.github.io/cryoet-data-portal\n[pip]: https://pypi.org/project/pip/\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[MIT]: http://opensource.org/licenses/MIT\n[security policy]: /SECURITY.md\n[license file]: /LICENSE\n[Contributor Covenant code of conduct]: https://github.com/chanzuckerberg/.github/tree/master/CODE_OF_CONDUCT.md\n[open an issue]: https://github.com/chanzuckerberg/napari-cryoet-data-portal/issues\n[open a pull request]: https://github.com/chanzuckerberg/napari-cryoet-data-portal/pulls\n[latest napari installation instructions]: https://napari.org/stable/tutorials/fundamentals/installation.html#install-as-python-package-recommended\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.ndjson",
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "CryoET Data Portal"
    ],
    "contributions_sample_data": [
      "Tomogram 10000-TS_026",
      "Tomogram 10000-TS_027"
    ]
  },
  {
    "normalized_name": "brainglobe-stitch",
    "name": "brainglobe-stitch",
    "display_name": "BrainGlobe Stitch",
    "version": "0.1.0a2",
    "created_at": "2024-10-16",
    "modified_at": "2024-11-19",
    "authors": [
      "Brainglobe Developers"
    ],
    "author_emails": [
      "Brainglobe Developers <hello@brainglobe.info>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/brainglobe-stitch/",
    "home_github": "https://github.com/brainglobe/brainglobe-stitch",
    "home_other": null,
    "summary": "A tool to stich large tiled datasets generated by the mesoSPIM.",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "napari>=0.4.18",
      "brainglobe-utils>=0.3.4",
      "h5py",
      "napari-ome-zarr",
      "ome-zarr",
      "zarr",
      "numpy",
      "qtpy",
      "tifffile",
      "pytest; extra == \"dev\"",
      "pytest-cov; extra == \"dev\"",
      "pytest-mock; extra == \"dev\"",
      "pytest-qt; extra == \"dev\"",
      "pyqt5; extra == \"dev\"",
      "coverage; extra == \"dev\"",
      "tox; extra == \"dev\"",
      "pooch; extra == \"dev\"",
      "black; extra == \"dev\"",
      "mypy; extra == \"dev\"",
      "pre-commit; extra == \"dev\"",
      "ruff; extra == \"dev\"",
      "setuptools-scm; extra == \"dev\""
    ],
    "package_metadata_description": "# brainglobe-stitch\n\nStitching tiled 3D light-sheet data in napari\n\n<p align=\"center\">\n  <img height=\"460\" src=\"https://github.com/user-attachments/assets/91f61f24-6fcf-4aa1-8a8f-de8c5e3db4a2\" alt=\"Stitching a mouse brain acquired at a resolution of 4.06 &micro;m/px, 4.06 &micro;m/px, 5 &micro;m/px using 4 tiles\">\n</p>\n\n----------------------------------\n\nA [napari] plugin for stitching tiled 3D acquisitions from a [mesoSPIM] light-sheet microscope.\nThe plugin utilises [BigStitcher] to align the tiles and napari to visualise the stitched data.\n\n## Installation\n\nWe strongly recommend to use a virtual environment manager (like `conda`). The installation instructions below\nwill not specify the Qt backend for napari, and you will therefore need to install that separately. Please see the\n[`napari` installation instructions](https://napari.org/stable/tutorials/fundamentals/installation.html) for further advice on this.\n\nTo install latest development version:\n\n    pip install git+https://github.com/brainglobe/brainglobe-stitch.git\n\nThis plugin requires Fiji to be installed on your system. You can download Fiji [here](https://imagej.net/Fiji/Downloads).\n\nThe BigStitcher plugin must be installed in Fiji. Please follow the instructions [here](https://imagej.net/plugins/bigstitcher/#download).\n\n## Seeking help or contributing\nWe are always happy to help users of our tools, and welcome any contributions. If you would like to get in contact with us for any reason, please see the [contact page of our website](https://brainglobe.info/contact.html).\n\n## Citation\nIf you find this package useful, please make sure to cite the original BigStitcher publication:\n> H√∂rl, D., Rojas Rusak, F., Preusser, F. *et al.* BigStitcher: reconstructing high-resolution image datasets of cleared and expanded samples. Nat Methods 16, 870‚Äì874 (2019). https://doi.org/10.1038/s41592-019-0501-0\n\n\n## License\nDistributed under the terms of the [BSD-3] license,\n\"brainglobe-stitch\" is free and open source software\n\n## Acknowledgements\nThis [napari] plugin was generated with [Cookiecutter] using napari's [cookiecutter-napari-plugin] template and the [Neuroinformatics Unit's template](https://github.com/neuroinformatics-unit/python-cookiecutter).\n\n[napari]: https://napari.org\n[mesoSPIM]: https://www.mesospim.org/\n[BigStitcher]: https://imagej.net/BigStitcher\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BrainGlobe Stitch"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nifti",
    "name": "napari-nifti",
    "display_name": "napari-nifti",
    "version": "0.0.17",
    "created_at": "2023-04-18",
    "modified_at": "2024-11-18",
    "authors": [
      "Karol Gotkowski"
    ],
    "author_emails": [
      "karol.gotkowski@dkfz.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-nifti/",
    "home_github": "https://github.com/MIC-DKFZ/napari-nifti",
    "home_other": null,
    "summary": "A napari plugin for reading and writing NIFTI files that have the extension .nii or .nii.gz.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "medvol",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-nifti\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-nifti.svg?color=green)](https://github.com/MIC-DKFZ/napari-nifti/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nifti.svg?color=green)](https://pypi.org/project/napari-nifti)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nifti.svg?color=green)](https://python.org)\n[![tests](https://github.com/MIC-DKFZ/napari-nifti/workflows/tests/badge.svg)](https://github.com/MIC-DKFZ/napari-nifti/actions)\n[![codecov](https://codecov.io/gh/MIC-DKFZ/napari-nifti/branch/main/graph/badge.svg)](https://codecov.io/gh/MIC-DKFZ/napari-nifti)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nifti)](https://napari-hub.org/plugins/napari-nifti)\n\nA napari plugin for reading and writing NIFTI files that have the extension .nii or .nii.gz.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-nifti` via [pip]:\n\n    pip install napari-nifti\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MIC-DKFZ/napari-nifti.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-nifti\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MIC-DKFZ/napari-nifti/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n# Acknowledgements\n<img src=\"HI_Logo.png\" height=\"100px\" />\n\n<img src=\"dkfz_logo.png\" height=\"100px\" />\n\nnapari-nifti is developed and maintained by the Applied Computer Vision Lab (ACVL) of [Helmholtz Imaging](http://helmholtz-imaging.de) \nand the [Division of Medical Image Computing](https://www.dkfz.de/en/mic/index.php) at the \n[German Cancer Research Center (DKFZ)](https://www.dkfz.de/en/index.html).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.nii.gz",
      "*.nrrd",
      "*.gz",
      "*.nii"
    ],
    "contributions_writers_filename_extensions": [
      ".nii",
      ".gz",
      ".nrrd",
      ".nii.gz"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-intensity-plotter",
    "name": "napari-intensity-plotter",
    "display_name": "Intensity Plotter",
    "version": "0.1.6",
    "created_at": "2024-10-20",
    "modified_at": "2024-11-15",
    "authors": [
      "Toranosuke Takagi"
    ],
    "author_emails": [
      "toranporin_1224@yahoo.co.jp"
    ],
    "license": "Copyright (c) 2024, Toranosuke...",
    "home_pypi": "https://pypi.org/project/napari-intensity-plotter/",
    "home_github": "https://github.com/Tbrn1103/napari-intensity-plotter",
    "home_other": null,
    "summary": "A plugin for plotting intensity profiles with control features in napari.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pyqtgraph; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-intensity-plotter\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-intensity-plotter.svg?color=green)](https://github.com/Tbrn1103/napari-intensity-plotter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-intensity-plotter.svg?color=green)](https://pypi.org/project/napari-intensity-plotter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-intensity-plotter.svg?color=green)](https://python.org)\n[![tests](https://github.com/Tbrn1103/napari-intensity-plotter/workflows/tests/badge.svg)](https://github.com/Tbrn1103/napari-intensity-plotter/actions)\n[![codecov](https://codecov.io/gh/Tbrn1103/napari-intensity-plotter/branch/main/graph/badge.svg)](https://codecov.io/gh/Tbrn1103/napari-intensity-plotter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-intensity-plotter)](https://napari-hub.org/plugins/napari-intensity-plotter)\n\n# napari-intensity-plotter\n\n**napari-intensity-plotter** is a plugin for **[napari](https://napari.org)** that provides tools to measure and plot intensity profiles in 2D time-series images.\n\n![Intensity Plot Widget](images/intensity_plot_widget_example.png)\n\n*Screenshot: Intensity profile of a region of interest in a 2D time-series image.*\n\n## Features\n\n- **Intensity Plot Widget**: Allows you to select a region of interest in a 2D time-series image and plot the intensity profile over time.\n- **Intensity Plot Control Widget**: Lets you fine-tune plot parameters and save the results as CSV or PNG files.\n\n## Installation\n\nYou can install `napari-intensity-plotter` via pip:\n\n```bash\npip install napari-intensity-plotter\n```\n\nAlternatively, you can install it directly from the napari plugin interface.\n\n### Usage\n\n1. **Load a 2D Time-Series Image**  \n   Load a 2D time-series image (e.g., fluorescence microscopy data) in napari.\n\n2. **Activate the Widgets**  \n   Open the `Intensity Plot Widget` and `Intensity Plot Control Widget` from the `Plugins` menu in napari.\n\n3. **Intensity Plot Widget**  \n   - Move your mouse over the image, or click on a specific location to plot the intensity profile of the selected region across slices (e.g., time).\n   - The region of interest (ROI) size can be adjusted using the square size setting in the control widget.\n\n4. **Intensity Plot Control Widget**  \n   - Configure the square size for the ROI (ensures that the region size remains odd).\n   - Set the directory to save plots and intensity data.\n   - Save the intensity profile as a `.csv` or `.png` file by clicking the corresponding buttons or using keyboard shortcuts (`Ctrl+S`).\n\n5. **Additional Controls**  \n   - Hide all layers using the `Hide All Layers` button or `Ctrl+D`.\n   - Use the rectangle to visualize the selected ROI.\n\n### Example Workflow\n\n**Step 1**: Load a 2D time-series image (e.g., `tif` or `nd2`) into napari. Ensure the layer is visible.\n\n**Step 2**: Open the `Intensity Plot Widget` to visualize intensity changes over time or slices for a specific ROI.\n\n**Step 3**: Use the `Intensity Plot Control Widget` to:\n- Adjust the square size for the ROI.\n- Specify a directory to save intensity data.\n- Enable saving in CSV or PNG formats.\n  \n**Step 4**: Save the plotted intensity data by clicking `Save to CSV/PNG` or pressing `Ctrl+S`.\n\n**Step 5**: Hide all layers if necessary using `Hide All Layers` or `Ctrl+D`.\n\n## Contributing\n\nContributions are welcome! If you encounter issues or have ideas for new features, please submit them via the [GitHub Issues](https://github.com/Tbrn1103/napari-intensity-plotter/issues).\n\n## Acknowledgements\n\nThis plugin was developed using the **napari plugin cookiecutter template**, which greatly streamlined the creation of this tool. See the [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) for more details.\n\nSpecial thanks to the napari community for their continuous support and resources.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License. See the [LICENSE](LICENSE) file for details.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Intensity Plot",
      "Intensity Plot Control"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-swc-reader",
    "name": "napari-swc-reader",
    "display_name": "napari-swc-reader",
    "version": "0.1.3",
    "created_at": "2024-11-14",
    "modified_at": "2024-11-14",
    "authors": [
      "Cl√©ment Caporal"
    ],
    "author_emails": [
      "caporal.clement@gmail.com"
    ],
    "license": "Copyright (c) 2024, Cl√©ment Ca...",
    "home_pypi": "https://pypi.org/project/napari-swc-reader/",
    "home_github": "https://github.com/LaboratoryOpticsBiosciences/napari-swc-reader",
    "home_other": null,
    "summary": "A simple napari plugin to load swc file to napari viewer",
    "categories": [
      "IO"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-swc-reader\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-swc-reader.svg?color=green)](https://github.com/LaboratoryOpticsBiosciences/napari-swc-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-swc-reader.svg?color=green)](https://pypi.org/project/napari-swc-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-swc-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/LaboratoryOpticsBiosciences/napari-swc-reader/workflows/tests/badge.svg)](https://github.com/LaboratoryOpticsBiosciences/napari-swc-reader/actions)\n[![codecov](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-swc-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/LaboratoryOpticsBiosciences/napari-swc-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-swc-reader)](https://napari-hub.org/plugins/napari-swc-reader)\n\nA minimal napari plugin to load swc file to napari viewer.\n\n![image](https://github.com/user-attachments/assets/1c9e5788-0e74-48ab-be0b-0fb74b35192c)\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [copier] using the [napari-plugin-template].\n\n## Features\n\n- Load swc file(s) to napari viewer\n- Display swc file(s) in napari viewer as points layers and lines layers\n- Size of points and lines are using the radius of the swc file\n- You can load an example swc from https://neuromorpho.org/dableFiles/jacobs/CNG%20version/204-2-6nj.CNG.swc or load it under `File` -> `Open Sample` -> `napari-swc-reader`\n\n**Limitations:**\n- Only support swc file(s) following specs http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html 7 columns\n- Cannot write swc file(s) to disk but you can access the raw swc data from the napari layers from `metadata` attribute with key `raw_swc`\n\n**Roadmap:**\n- Switch to use `napari.layers.Graph` [when it is available](https://github.com/napari/napari/issues/4274)\n\n## Installation\n\nYou can install `napari-swc-reader` via [pip]:\n\n    pip install napari-swc-reader\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/LaboratoryOpticsBiosciences/napari-swc-reader.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-swc-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[file an issue]: https://github.com/LaboratoryOpticsBiosciences/napari-swc-reader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.swc"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "napari-swc-reader"
    ]
  },
  {
    "normalized_name": "poser-napari",
    "name": "PoseR-napari",
    "display_name": "PoseR",
    "version": "0.0.1b4",
    "created_at": "2023-02-21",
    "modified_at": "2024-11-14",
    "authors": [
      "Pierce Mullen"
    ],
    "author_emails": [
      "pnm1@st-andrews.ac.uk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/poser-napari/",
    "home_github": "https://github.com/pnm4sfix/PoseR",
    "home_other": null,
    "summary": "A deep learning toolbox for decoding animal behaviour",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "napari[all]==0.4.14",
      "npe2==0.6.2",
      "pydantic==1.10.4",
      "numpy==1.23.5",
      "magicgui",
      "qtpy",
      "napari-video",
      "napari-plot==0.1.5",
      "tables",
      "imageio-ffmpeg==0.4.8",
      "pytorch-lightning",
      "test-tube",
      "scikit-learn",
      "matplotlib",
      "numba",
      "networkx",
      "seaborn",
      "ultralytics",
      "torcheval==0.0.7",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# PoseR\n\n[![License BSD-3](https://img.shields.io/pypi/l/PoseR.svg?color=green)](https://github.com/pnm4sfix/PoseR/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/PoseR-napari.svg?color=green)](https://pypi.org/project/PoseR-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/PoseR-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/pnm4sfix/PoseR/workflows/tests/badge.svg)](https://github.com/pnm4sfix/PoseR/actions)\n[![codecov](https://codecov.io/gh/pnm4sfix/PoseR/branch/main/graph/badge.svg)](https://codecov.io/gh/pnm4sfix/PoseR)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/PoseR)](https://napari-hub.org/plugins/PoseR)\n\nA deep learning toolbox for decoding animal behaviour\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n![alt text](https://github.com/pnm4sfix/PoseR/blob/add-functionality/docs/logo.png?raw=true)\n\n## Installation\n\nCreate an anaconda environment:\n\n    conda create -n PoseR python=3.10\n\nActivate PoseR environment:\n\n    conda activate PoseR\n\nInstall CUDA if using NVIDIA GPU:\n\n    conda install -c \"nvidia/label/cuda-11.7.0\" cuda\n\nInstall Pytorch:\nFor GPU:\n\n    conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n\nFor CPU only version:\n\n    conda install pytorch torchvision torchaudio cpuonly -c pytorch\n\nInstall napari:\n\n    pip install napari[all]==0.4.14 npe2==0.6.2 pydantic==1.10.4\n\n\nYou can install `PoseR` via [pip]:\n\n    pip install PoseR-napari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/pnm4sfix/PoseR.git\n\n\n## Quick start\n\nhttps://github.com/pnm4sfix/PoseR/blob/generalise-species/docs/QuickStart.md\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"PoseR\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/pnm4sfix/PoseR/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PoseR"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "midi-app-controller",
    "name": "midi-app-controller",
    "display_name": "midi-app-controller",
    "version": "0.1.1",
    "created_at": "2024-06-05",
    "modified_at": "2024-11-13",
    "authors": [],
    "author_emails": [],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/midi-app-controller/",
    "home_github": "https://github.com/midi-app-controller/midi-app-controller",
    "home_other": null,
    "summary": "Control napari with a MIDI controller.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari>=0.4.19",
      "python-rtmidi>=1.5.8",
      "pyyaml>=6.0.1",
      "pydantic>=2.7.1",
      "appdirs>=1.4.4",
      "qtpy>=2.4.1",
      "superqt>=0.6.5",
      "midi-app-controller[testing]; extra == \"dev\"",
      "pre-commit>=3.7.0; extra == \"dev\"",
      "pytest>=8.2.0; extra == \"testing\"",
      "pytest-cov>=5.0.0; extra == \"testing\"",
      "pytest-qt>=4.0.2; extra == \"testing\"",
      "napari[all]; extra == \"testing\"",
      "flexparser!=0.4.0; extra == \"testing\""
    ],
    "package_metadata_description": "# midi-app-controller\n\n[![codecov](https://codecov.io/gh/midi-app-controller/midi-app-controller/graph/badge.svg?token=YALMD0PQ80)](https://codecov.io/gh/midi-app-controller/midi-app-controller)\n[![Documentation Status](https://readthedocs.org/projects/midi-app-controller/badge/?version=latest)](https://midi-app-controller.readthedocs.io/en/latest/?badge=latest)\n[![PyPI version](https://badge.fury.io/py/midi-app-controller.svg)](https://badge.fury.io/py/midi-app-controller)\n\nmidi-app-controller is an app, that allows user to control all applications using 'pyapp-kit/app-model' with a USB MIDI controller.\n\n## Documentation\n\nDocumentation at https://midi-app-controller.readthedocs.io/en/latest/.\n\n## Usage (napari)\n\nMIDI App Controller is a package designed to integrate MIDI controllers with Python Qt apps using app-model. As of now, it is used most commonly with [napari](https://napari.org), a viewer for multi-dimensional images. We will show how to use MIDI App Controller with napari but getting started with other applications should look very similar.\n\n### Installation\n\nTo install MIDI App Controller in your environment (where Python and napari are already installed), use this command:\n\n```\npip install midi-app-controller\n```\n\nnapari will automatically detect the package and install the plugin next time it starts.\n\nTo install the newest development version, clone the GitHub repo and [install it as a local package](#installing).\n\n### Setup\n\nLaunch the plugin from the _Plugins_ menu.\n\n![](docs/img/plugins-menu.png)\n\nA panel will open to the side.\n\n![](docs/img/midi-status.png)\n\n#### Controller\n\nIf your MIDI controller is supported out of the box, you can simply select the appropriate model. If not, you will need to tell MIDI App Controller how to interact with this model of controller by creating a [controller schema](controllers.md).\n\nOnce you have selected the controller schema, you can select binds schema.\n\n#### MIDI ports\n\nIf they haven't been selected automatically, select MIDI input and output ports that correspond to your physical controller.\n\n### Start handling\n\nAfter a controller and bindings are selected, you can click \"Start handling\". This will start a thread that listens to all input from the controller and invokes appropriate commands. You can close the panel with the settings, the thread will work in the background until you click \"Stop handling\".\n\n### Edit binds\n\nClick \"Edit binds\" to open dialog where you can configure bindings by choosing which physical buttons and knobs on your controller correspond to which commands in the application. Think of it like configuring keyboard shortcuts.\n\n![](docs/img/edit-binds.png)\n\nAll configurations are simple YAML files which you can copy, share, or edit manually. You can click \"Reveal in explorer\" to see the exact location of the currently chosen config file. You shouldn't edit built-in presets stored in the package directory; when you edit a built-in preset in the graphical user interface, a copy will automatically be created.\n\nAfter you save changes, if you have already started handling, you need to click \"Restart handling\" to start a new server with the changes applied.\n\n## Usage without GUI\n\nThe library can be also controlled using the singleton of [`StateManager`](api_reference.md) class:\n```python\nfrom midi_app_controller.state.state_manager import get_state_manager\n\nstate = get_state_manager()\n# Now the library can be controlled using `state`.\n```\n\n## Development\n\n### Installing\n```sh\npython3 -m pip install -e .\n```\n\n### Testing\n```sh\npython3 -m pip install -e .[testing]\npython3 -m pytest --cov .\n```\n\n### Testing docs\n```sh\nmkdocs serve -a localhost:8080\n```\n\n### Using pre-commit\n```sh\npython3 -m pip install -e .[dev]\npre-commit install\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MIDI controller status and settings"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bacseg",
    "name": "napari-bacseg",
    "display_name": "BacSEG",
    "version": "1.1.2",
    "created_at": "2023-02-24",
    "modified_at": "2024-11-13",
    "authors": [
      "Piers Turner"
    ],
    "author_emails": [
      "Piers Turner <piers.turner@physics.ox.ac.uk>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-bacseg/",
    "home_github": "https://github.com/piedrro/napari-bacseg",
    "home_other": null,
    "summary": "Bacterial segmentation and analysis platform than can inport/export files in multiple formats. Integrating many tools such as Cellpose, ColiCoords, Oufti/MicrobeTracker.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari[all]==0.5.0",
      "torch",
      "torchvision",
      "cellpose==3.0.1",
      "opencv-python",
      "picassosr==0.7.3",
      "bactfit>=0.1.6",
      "numpy",
      "pyqt5",
      "pyqt6",
      "qtpy",
      "scipy",
      "natsort",
      "tqdm",
      "imagecodecs",
      "tifffile",
      "pandas",
      "mat4py",
      "glob2",
      "matplotlib",
      "scikit-image",
      "roifile",
      "openpyxl",
      "shapely",
      "colicoords",
      "psutil",
      "xmltodict",
      "astropy",
      "tiler",
      "imageio-ffmpeg",
      "aicspylibczi",
      "czifile",
      "omnipose",
      "h5py",
      "pyqtgraph",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-BacSEG\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bacseg.svg?color=green)](https://github.com/piedrro/napari-bacseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bacseg.svg?color=green)](https://pypi.org/project/napari-bacseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bacseg.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bacseg)](https://napari-hub.org/plugins/napari-bacseg)\n\nBacterial segmentation and analysis platform than can inport/export files in multiple formats. Integrating many tools such as Cellpose, ColiCoords, Oufti/MicrobeTracker.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installing BacSeg from PyPi\n\nCreate a new conda environment:\n\n    conda create --name napari-bacseg python==3.9\n    conda activate napari-bacseg\n    conda install -c anaconda git\n    conda update --all\n\nInstall `napari-bacseg` via [pip]:\n\n    pip install napari-bacseg\n\nLaunch Napari:\n\n    napari\n\nInstall latest version of 'napri-bacseg' from [pip]:\n\n    pip install napari-bacseg --upgrade\n\nSelect **napari-bacseg** from the **Plugins** dropdown menu\n\n## Installing BacSeg From GitHub\n\n    conda create ‚Äì-name napari-bacseg python==3.9\n    conda activate napari-bacseg\n    conda install -c anaconda git\n    conda update --all\n\n    pip install napari[all]\n\n    pip install git+https://github.com/piedrro/napari-bacseg.git\n\n## Updating BacSeg From Github\nOnce you have installed the plugin, you can update the plugin by running the following commands:\n\n    pip install git+https://github.com/piedrro/napari-bacseg.git\n\n## GPU Installation\n Once you have installed the plugin, you can install the GPU version of the plugin by running the following commands:\n\n    pip uninstall torch torchvision torchaudio -y\n    pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\nIf the latest CUDA versions don't work, try an older version like cuda 11.3:\n\n    pip uninstall torch torchvision torchaudio -y\n    pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu113\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bacseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/piedrro/napari-bacseg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BacSEG"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cosmos-ts",
    "name": "napari-cosmos-ts",
    "display_name": "napari-cosmos-ts",
    "version": "0.3.6",
    "created_at": "2024-04-12",
    "modified_at": "2024-11-13",
    "authors": [
      "Marcel Goldschen-Ohm"
    ],
    "author_emails": [
      "Marcel Goldschen-Ohm <goldschen-ohm@utexas.edu>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-cosmos-ts/",
    "home_github": "https://github.com/marcel-goldschen-ohm/napari-cosmos-ts",
    "home_other": null,
    "summary": "napari plugin for colocalization single-molecule spectroscopy (CoSMoS) time series (TS) analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy<2.1,>=2.0",
      "qtpy>=2.4.1",
      "pyqtgraph>=0.13.4",
      "pystackreg>=0.2.7",
      "pycpd>=2.0.0",
      "qtawesome>=1.3.1",
      "h5py>=3.11.0"
    ],
    "package_metadata_description": "# napari-cosmos-ts\n[napari](https://napari.org/stable/) plugin for colocalization single-molecule spectroscopy (CoSMoS) time series (TS) analysis.\n\n![GitHub Tag](https://img.shields.io/github/v/tag/marcel-goldschen-ohm/napari-cosmos-ts?cacheSeconds=1)\n![build-test](https://github.com/marcel-goldschen-ohm/napari-cosmos-ts/actions/workflows/build-test.yml/badge.svg)\n\n![GitHub Release](https://img.shields.io/github/v/release/marcel-goldschen-ohm/napari-cosmos-ts?include_prereleases&cacheSeconds=1)\n![publish](https://github.com/marcel-goldschen-ohm/napari-cosmos-ts/actions/workflows/publish.yml/badge.svg)\n\n- [Install](#install)\n- [Run](#run)\n- [Documentation](#documentation)\n\n## Install\nIf you are unfamiliar with how to create a python environment or pip install a package, see the [detailed installation instructions below](#install-for-beginners).\n\nRequires [napari](https://github.com/napari/napari).\n```shell\npip install \"napari[all]\"\n```\nInstall latest release version:\n```shell\npip install napari-cosmos-ts\n```\nOr install latest development version:\n```shell\npip install napari-cosmos-ts@git+https://github.com/marcel-goldschen-ohm/napari-cosmos-ts\n```\nTo upgrade a previously installed version, simply add the `--upgrade` flag to the above install commands.\n\n## Run\nLaunch the `napari` viewer. In a command shell or terminal *where you have activated the virtual environment where you installed napari-cosmos-ts* run the following command:\n```shell\nnapari\n```\nLaunch the `napari-cosmos-ts` plugin: From the napari viewer `Plugins menu` select `Colocalization Single-Molecule Time Series (napari-cosmos-ts)`. This should bring up the napari-cosmos-ts docked widget within the viewer.\n\n## Documentation\n:construction:\n\n## Install for Beginners\n1. Install the `conda` package manager. Simplest is to download [Miniconda](https://docs.conda.io/en/main/miniconda.html) and run the installer.\n2. Create a virtual python environment named `napari-env` (or name it whatever you want) in which to install [napari](https://napari.org/stable/) and this plugin. In a command shell or terminal run the following command:\n```shell\nconda create --name napari-env python\n```\n3. Activate your virtual environment. *!!! Note you will have to do this every time you open a new command shell or terminal.* In a command shell or terminal run the following command:\n```shell\nconda activate napari-env\n```\n4. Install `napari` into your virtual environment. In a command shell or terminal *where you have activated your virtual environment* run the following command:\n```shell\npip install \"napari[all]\"\n```\n5. Install `napari-cosmos-ts` into your virtual environment. In a command shell or terminal *where you have activated your virtual environment* run the following command:\n```shell\npip install napari-cosmos-ts\n```\nOr for the latest version of `napari-cosmos-ts`:\n```shell\npip install napari-cosmos-ts@git+https://github.com/marcel-goldschen-ohm/napari-cosmos-ts\n```\n*!!! Remember, every time you open a new command shell or terminal you will need to activate the virtual environment where you installed napari-cosmos-ts before [running the app](#run).*\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Colocalization Single-Molecule Time Series"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ctc-io",
    "name": "napari-ctc-io",
    "display_name": "Read CTC format",
    "version": "0.2.1",
    "created_at": "2024-06-04",
    "modified_at": "2024-11-13",
    "authors": [
      "Benjamin Gallusser"
    ],
    "author_emails": [
      "benjamin.gallusser@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-ctc-io/",
    "home_github": "https://github.com/bentaculum/napari-ctc-io",
    "home_other": null,
    "summary": "Drag and drop/write tracks from/to the Cell Tracking Challenge (CTC) format.",
    "categories": [
      "Visualization",
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "scikit-image",
      "tifffile",
      "pandas",
      "imagecodecs",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-ctc-io\n\n[![PyPI](https://img.shields.io/pypi/v/napari-ctc-io.svg?color=green)](https://pypi.org/project/napari-ctc-io)\n[![tests](https://github.com/bentaculum/napari-ctc-io/workflows/tests/badge.svg)](https://github.com/bentaculum/napari-ctc-io/actions)\n[![codecov](https://codecov.io/gh/bentaculum/napari-ctc-io/branch/main/graph/badge.svg)](https://codecov.io/gh/bentaculum/napari-ctc-io)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ctc-io)](https://napari-hub.org/plugins/napari-ctc-io)\n\n- Drag and drop annotations/results in the [Cell Tracking Challenge (CTC) format](https://celltrackingchallenge.net) into napari.\n\n  Works for `TRA`, `RES`, etc. folders, which contain a time sequence of segmentations in `tiff` format, and a corresponding tracklet file `*.txt`.\n- Write tracked cells (`labels` layer & corresponding `tracks` layer) to CTC format (see usage below).\n\n\nhttps://github.com/bentaculum/napari-ctc-io/assets/8866751/197c9ea2-4115-4829-851a-4b77eb843bf2\n\n\n## Installation\n\nYou can install `napari-ctc-io` via [pip]:\n\n    pip install napari-ctc-io\n\n\n\nTo install latest development version :\n\n\n    pip install git+https://github.com/bentaculum/napari-ctc-io.git\n\n## Usage of writer in widget\n\n```python\ndef _save(self, event=None):\n    pm = npe2.PluginManager.instance()\n\n    outdir = \"TRA\"\n    writer_contrib = pm.get_writer(\n        outdir,\n        [\"labels\", \"tracks\"],\n        \"napari-ctc-io\",\n    )[0]\n\n    save_layers(\n        path=outdir,\n        layers=[\n            self._viewer.layers[\"masks_tracked\"],\n            self._viewer.layers[\"tracks\"],\n        ],\n        plugin=\"napari-ctc-io\",\n        _writer=writer_contrib,\n    )\n```\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n`napari-ctc-io` is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bentaculum/napari-ctc-io/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*RES",
      "TRA"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-itk-io",
    "name": "napari-itk-io",
    "display_name": "napari-itk-io",
    "version": "0.4.1",
    "created_at": "2021-04-28",
    "modified_at": "2024-11-13",
    "authors": [
      "Matt McCormick"
    ],
    "author_emails": [
      "matt.mccormick@kitware.com"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-itk-io/",
    "home_github": "https://github.com/InsightSoftwareConsortium/napari-itk-io",
    "home_other": null,
    "summary": "File IO with itk for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari-plugin-engine>=0.2.0",
      "itk-io>=5.2.0",
      "itk-napari-conversion"
    ],
    "package_metadata_description": "# napari-itk-io\n\n[![License](https://img.shields.io/pypi/l/napari-itk-io.svg?color=green)](https://github.com/InsightSoftwareConsortium/napari-itk-io/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-itk-io.svg?color=green)](https://pypi.org/project/napari-itk-io)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-itk-io.svg?color=green)](https://python.org)\n[![tests](https://github.com/InsightSoftwareConsortium/napari-itk-io/workflows/tests/badge.svg)](https://github.com/InsightSoftwareConsortium/napari-itk-io/actions)\n[![codecov](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io/branch/master/graph/badge.svg)](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io)\n\nFile IO with [itk](https://itk.org) for [napari](https://napari.org).\n\nImage metadata, e.g. the pixel spacing, origin, and metadata tags, are preserved and passed into napari.\n\nSupported image file formats:\n\n- [BioRad](http://www.bio-rad.com/)\n- [BMP](https://en.wikipedia.org/wiki/BMP_file_format)\n- [DICOM](http://dicom.nema.org/)\n- [DICOM Series](http://dicom.nema.org/)\n- [ITK HDF5](https://support.hdfgroup.org/HDF5/)\n- [JPEG](https://en.wikipedia.org/wiki/JPEG_File_Interchange_Format)\n- [GE4,GE5,GEAdw](http://www3.gehealthcare.com)\n- [Gipl (Guys Image Processing Lab)](https://www.ncbi.nlm.nih.gov/pubmed/12956259)\n- [LSM](http://www.openwetware.org/wiki/Dissecting_LSM_files)\n- [MetaImage](https://itk.org/Wiki/ITK/MetaIO/Documentation)\n- [MINC 2.0](https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference)\n- [MGH](https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat)\n- [MRC](http://www.ccpem.ac.uk/mrc_format/mrc_format.php)\n- [NifTi](https://nifti.nimh.nih.gov/nifti-1)\n- [NRRD](http://teem.sourceforge.net/nrrd/format.html)\n- [Portable Network Graphics (PNG)](https://en.wikipedia.org/wiki/Portable_Network_Graphics)\n- [Tagged Image File Format (TIFF)](https://en.wikipedia.org/wiki/TIFF)\n- [VTK legacy file format for images](http://www.vtk.org/VTK/img/file-formats.pdf)\n\nFor DICOM Series, select the folder containing the series with *File -> Open\nFolder...*. The first series will be selected and sorted spatially.\n\n## Installation\n\nYou can install `napari-itk-io` via [pip]:\n\n    pip install napari-itk-io\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\nFollow the [itk contributing\nguidelines](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CONTRIBUTING.md)\nand the [itk code of\nconduct](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CODE_OF_CONDUCT.md).\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-itk-io\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/InsightSoftwareConsortium/napari-itk-io/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.bmp",
      "*.hdr",
      "*.jpeg",
      "*.mrc",
      "*.tiff",
      "*.dicom",
      "*.mnc2",
      "*.tif",
      "*.dcm",
      "*.nhdr",
      "*.pic",
      "*.mgh.gz",
      "*.nii.gz",
      "*.mgz",
      "*.nii",
      "*.h5",
      "*.mhd",
      "*.mha",
      "*.nrrd",
      "*.gipl",
      "*.gipl.gz",
      "*.png",
      "*.mgh",
      "*.jpg",
      "*.nia",
      "*.vtk",
      "*.lsm",
      "*.mnc.gz",
      "*.mnc",
      "*.hdf5"
    ],
    "contributions_writers_filename_extensions": [
      ".tiff",
      ".vtk",
      ".mgh.gz",
      ".hdr",
      ".mha",
      ".png",
      ".pic",
      ".mrc",
      ".mnc2",
      ".mhd",
      ".jpg",
      ".mnc",
      ".h5",
      ".hdf5",
      ".nii",
      ".nrrd",
      ".lsm",
      ".tif",
      ".gipl",
      ".bmp",
      ".nia",
      ".nii.gz",
      ".mgh",
      ".jpeg",
      ".dcm",
      ".gipl.gz",
      ".dicom",
      ".nhdr",
      ".mnc.gz",
      ".mgz"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-lf",
    "name": "napari-LF",
    "display_name": "napari LF",
    "version": "0.1.7",
    "created_at": "2022-07-21",
    "modified_at": "2024-11-13",
    "authors": [
      "Geneva Schlafly",
      "Amitabh Verma",
      "Rudolf Oldenbourg"
    ],
    "author_emails": [
      "gschlafly@uchicago.edu",
      "averma@mbl.edu",
      "rudolfo@mbl.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-lf/",
    "home_github": "https://github.com/PolarizedLightFieldMicroscopy/napari-LF",
    "home_other": null,
    "summary": "Light field imaging plugin for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "h5py",
      "pyopencl",
      "napari[all]",
      "opencv-python",
      "torch",
      "torchvision",
      "pytorch-lightning"
    ],
    "package_metadata_description": "# napari-LF\n\n[![License](https://img.shields.io/pypi/l/napari-LF.svg?color=green)](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-LF.svg?color=green)](https://pypi.org/project/napari-LF)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-LF.svg?color=green)](https://python.org)\n[![tests](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/workflows/tests/badge.svg)](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-LF)](https://napari-hub.org/plugins/napari-LF)\n[![Downloads](https://static.pepy.tech/badge/napari-lf)](https://pepy.tech/project/napari-lf)\n<!-- [![codecov](https://codecov.io/gh/PolarizedLightFieldMicroscopy/napari-LF/branch/main/graph/badge.svg)](https://codecov.io/gh/PolarizedLightFieldMicroscopy/napari-LF) -->\n\nLight field imaging plugin for napari\n\n----------------------------------\n\nDeconvolves a 4D light field image into a full 3D focus stack reconstruction\n\nhttps://user-images.githubusercontent.com/23206511/236919283-d53ca97a-9bdd-4598-b553-34996f688237.mp4\n\nnapari-LF contains an analytic and neural net analysis methods for light field images. To download example light field images, see our repository [napari-LF-docs-samples](https://github.com/PolarizedLightFieldMicroscopy/napari-LF-docs-samples).\n\n### LF Analyze\n**LF Analyze**, the analytic method, provides three basic processes to Calibrate, Rectify, and Deconvolve light field images:\n\nThe **Calibrate** process generates a calibration file that represents the optical setup that was used to record the light field images. The same calibration file can be used to rectify and deconvolve all light field images that were recorded with the same optical setup, usually the same microscope and light field camera. The Calibrate process requires as input the radiometry frame, dark frame, optical parameters, and volume parameters to generate the calibration file, which is subsequently used to rectify and deconvolve related light field images. The calibration file includes a point spread function (PSF) derived from the optical and volume parameters and is stored in HDF5 file format.\n\nThe **Rectify** process uses the calibration file for an affine transformation to scale and rotate experimental light field images that were recorded with a light field camera whose microlens array was (slightly) rotated with respect to the pixel array of the area detector and whose pixel pitch is not commensurate with the microlens pitch. After rectification, the rectified light field has the same integer number of pixels behind each microlens. When the Deconvolve process is called for an experimental light field image, rectifying the light field image is automatically applied before the iterative deconvolution does begin. However, the rectified light field image is not saved and is not available for viewing. Therefore, by pushing the Rectify button in the middle of the napari-LF widget, only the rectification step is invoked and the rectified light field image is saved to the project directory.\n\nThe **Deconvolve** process uses the PSF and a wave optics model to iteratively deconvolve a light field image into a stack of optical sections.\n\nThe **Parameter** panels, located in the lower half of the napari-LF widget, allows the user to specify settings for the reconstruction process. Once the appropriate parameters are selected, the Calibrate button followed by the Deconvolve button can be pushed to complete the reconstruction.\n\n### Neural Net\n**Neural Net** provides a method of applying a trained neural net model to deconvolve a light field image. Based on Pytorch Lightning and a provided [base class](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/blob/main/src/napari_lf/lfa/neural_nets/LFNeuralNetworkProto.py), you can either create your own network, or use the pre-shipped networks (LFMNet, VCDNet, ...).\n\n## Quickstart\n1. Install the napari-LF plugin into your napari environment, as described below under **Installation**.\n1. From the napari Plugins menu, select the napari-LF plugin to install its widget into the napari viewer.\n### LF Analyze\n1. Near the top of the widget, select your project folder containing the following images: light field, radiometry, and dark frame.\n1. Calibration\n    1. In the processing panel, navigate to **Calibrate, Required** (top tab **Calibrate**, bottom tab **Required**), which is the default selection.\n    1. Select **radiometry** and **dark frame** images from pull down menus.\n    1. Write the name of the **calibration file** you would like to produce, e.g. calibration.lfc.\n    1. Enter the appropriate **optical parameters** according to your microscope and sample material.\n    1. Enter the **volume parameters** you would like for your 3D reconstuction.\n    1. Push the `Calibrate` button.\n1. Deconvolution\n    1. In the processing panel, navigate to **Deconvolve, Required**.\n    1. Select **light field** image and **calibration file** from pull down menus.\n    1. Write the name of the **output image stack** you would like to produce, e.g. output_stack.tif.\n    1. Push the `Deconvolve` button.\nThe 3D focal stack reconstruction will display in the napari viewer and be saved in your original project folder.\n\n### Neural Net\n1. Click on the **LF Analyze** logo to toggle to the **Neural Net** mode.\n1. Near the top of the widget, select your project folder containing the light field image and the trained neural net. If you do not already have a trained model, you can train a model using this [Jupyter notebook](https://github.com/PolarizedLightFieldMicroscopy/napari-LF/blob/main/src/napari_lf/lfa/main_train_neural_net.ipynb).\n1. In the processing panel, select your **light field image** and **neural net model**.\n1. Write the name of the **output image stack** you would like to produce, e.g. output_stack.tif.\n1. Push the `Deconvolve` button.\nThe 3D focal stack reconstruction will display in the napari viewer and be saved in your original project folder.\n\n## Getting Help\nFor details about each parameter, hover over each parameter textbox to read the tooltip description.\nFor additional information about the reconstruction process, see our [User Guide](docs/napari-LF_UserGuide_12May2023.pdf).\n\n## Installation\n\nAfter you have [napari] installed, you can one of the methods below to install `napari-LF`.\n\nMethod 1: You can install `napari-LF` via [pip]:\n\n    pip install napari-LF\n\nMethod 2: Use the napari plugin menu.\n\n1. Open napari from the command line:\n\n        napari\n\n1. From the napari menu, select **Plugins > Install/uninstall Packages**.\n\n1. Either (a) scroll through the list of available plugins to find `napari-LF`, or (b) drag and drop a downloaded `napari-LF` directory into the bottom bar.\n\n1. Select **Install** to install the light field plugin.\n\nMethod 3: Install the latest development version from the command line.\n\n    pip install git+https://github.com/PolarizedLightFieldMicroscopy/napari-LF.git\n\nLastly, to access the installed plugin, open napari from the command line:\n\n    napari\n\nFrom the napari menu, select **Plugins > Main Menu (napari-LF)**. Note that you may need to close and reopen napari for the `napari-LF` to appear.\n\n### Installation for developers\n\nCreate a virtual environment from the command line for napari with the python libraries necessary for the light field plugin:\n\n    conda create -y -n napari-lf -c conda-forge python==3.9\n    conda activate napari-lf\n\nClone the github repository:\n\n    conda install git\n    git clone https://github.com/PolarizedLightFieldMicroscopy/napari-LF.git\n    cd napari-LF\n    pip install -e .\n\nThe necessary dependencies should be installed automatically with `napari-LF`. If for some reason `pyopencl` does not get installed properly, try installing with conda:\n\n    conda install -c conda-forge pyopencl\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-LF\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/PolarizedLightFieldMicroscopy/napari-LF/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Main Menu"
    ],
    "contributions_sample_data": [
      "napari LF"
    ]
  },
  {
    "normalized_name": "napari-smlmlab",
    "name": "napari-SMLMLAB",
    "display_name": "BP04 Practical",
    "version": "0.1.4",
    "created_at": "2024-04-25",
    "modified_at": "2024-11-13",
    "authors": [
      "Piers Turner"
    ],
    "author_emails": [
      "Piers Turner <piers.turner@physics.ox.ac.uk>"
    ],
    "license": "Copyright (c) 2024, Piers Turn...",
    "home_pypi": "https://pypi.org/project/napari-smlmlab/",
    "home_github": "https://github.com/piedrro/napari-bacseg",
    "home_other": null,
    "summary": "Napari widget for BP04 Practical.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari[all]==0.5.0",
      "numpy",
      "magicgui",
      "qtpy",
      "scipy",
      "pyqtgraph",
      "picassosr==0.7.3",
      "matplotlib",
      "tqdm",
      "pyqt5-tools",
      "trackpy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-SMLMLAB\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-SMLMLAB.svg?color=green)](https://github.com/piedrro/napari-SMLMLAB/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-SMLMLAB.svg?color=green)](https://pypi.org/project/napari-SMLMLAB)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-SMLMLAB.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-SMLMLAB)](https://napari-hub.org/plugins/napari-SMLMLAB)\n\nNapari widget for BP04 Practical\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-SMLMLAB` via [pip]:\n\n    pip install napari-SMLMLAB\n\nTo upgrade to the latest version of `napari-SMLMLAB` from [pip]:\n\n    pip install napari-SMLMLAB --upgrade\n\nTo install latest development version :\n\n    conda create ‚Äì-name napari-SMLMLAB python==3.9\n    conda activate napari-SMLMLAB\n    conda install -c anaconda git\n    conda update --all\n\n    pip install napari[all]\n\n    pip install git+https://github.com/piedrro/napari-SMLMLAB.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-SMLMLAB\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/piedrro/napari-SMLMLAB/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SMLM LAB"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "fish-scan",
    "name": "fish-scan",
    "display_name": "Fish Scan",
    "version": "1.1.0",
    "created_at": "2024-08-14",
    "modified_at": "2024-11-12",
    "authors": [
      "Arianna Ravera"
    ],
    "author_emails": [
      "ariannaravera22@gmail.com"
    ],
    "license": "Copyright (c) 2024, Arianna Ra...",
    "home_pypi": "https://pypi.org/project/fish-scan/",
    "home_github": "https://github.com/ariannaravera/fish-scan",
    "home_other": null,
    "summary": "Plugin to enhance fish detection and analysis for underwater research",
    "categories": [
      "Image Processing",
      "Segmentation",
      "Measurement"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "opencv-python",
      "matplotlib",
      "scikit-learn",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# fish-scan\n\n[![License BSD-3](https://img.shields.io/pypi/l/fish-scan.svg?color=green)](https://github.com/ariannaravera/fish-scan/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/fish-scan.svg?color=green)](https://pypi.org/project/fish-scan)\n[![Python Version](https://img.shields.io/pypi/pyversions/fish-scan.svg?color=green)](https://python.org)\n[![tests](https://github.com/ariannaravera/fish-scan/workflows/tests/badge.svg)](https://github.com/ariannaravera/fish-scan/actions)\n[![codecov](https://codecov.io/gh/ariannaravera/fish-scan/branch/main/graph/badge.svg)](https://codecov.io/gh/ariannaravera/fish-scan)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/fish-scan)](https://napari-hub.org/plugins/fish-scan)\n\nPlugin to enhance fish detection and analysis for underwater research\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `fish-scan` via [pip]:\n\n    pip install fish-scan\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/ariannaravera/fish-scan.git\n\n\n## Tutorial\n\n### 1. Color correction\n\nWith this tool you can correct colors in your image.\n\n1.\tOpen your image in napari by grabbing and dropping it or opening from the menu\n2.\tClick on ‚ÄúSelect the area‚Äù to select a rectagular area of the color you wanna correct\n3.\tDraw the rectangle in the area in which you have that color, eg. white area, as in the example below\n4.\tSelect the color name in the dropdown box, eg. in this case white\n5.\tClick on ‚ÄúCorrect color‚Äù\n6.\tIf you‚Äôre satisfied of the correction made, it is IMPORTANT to select as input ‚ÄúImage‚Äù(first box) the name of the new generated image (usually, ‚ÄúCurrect_corrected_...)*\n7.\tCuntinue your colors corrections re-starting from step 2\n\n#### Nomenclature:\n- you MUSH have your original image with its original name (in this case ‚ÄúGroup2_FishID_25April‚Ä¶‚Äù)\n- then, once you start correcting the image, the new generated images will be called as ‚ÄúCurrent_corrected_*original-name*‚Äù\n- if you perform the correction more times, your previous corrected image will be named ‚ÄúPrevious_corrected_*original-name*‚Äù and the new generated always ‚ÄúCurrent_corrected_*original-name*‚Äù.\n\nThis allows you to keep as ‚Äúprevious‚Äù the result that you already liked and approved, while the ‚Äúcurrent‚Äù you can play and experiment new corrections. \nBe aware that if you select as input image of the correction ‚Äúprevious‚Äù this means that you didn‚Äôt like the ‚Äúcurrent‚Äù one and it will be overwrite (while the ‚Äúprevious‚Äù remain the same), otherwise if you liked the ‚Äúcurrent‚Äù and you use it as input image of the new correction, then the ‚Äúcurrent‚Äù will become ‚Äúprevious‚Äù and the new result ‚Äúcurrent‚Äù.\nAt anytime if you want you can re-start from the original image by selecting it as input image (original image is NEVER changed).\n‚ÄÉ\n### 2. Set Scale\nWith this tool you can automatically measure your fish if a scale is in the image.\n\n1.\tClick on ‚ÄúSelect 1 cm‚Äù and draw a line on the scale represeting 1cm, as in this example:\n \n2.\tClick on ‚ÄúSet scale‚Äù and your fish will be automaticall measure in your final analysis with this scale\n‚ÄÉ\n### 3. Segmentation & Analysis\nWith this tool you can segment your fish.\n\nSteps:\n1.\tSelect the ‚ÄúImage‚Äù you want to analyse (be aware of the nomenclature*)\n2.\tClick ‚ÄúSelect fish area‚Äù button\n3.\tStart drawing your mask with the brush (yellow arrow below). I suggest to start drwaing the fish contours and then with the paint bucket (red arrow below) fill the inside. Adjust it with the eraser (orange arrow below) if needed\n    \n4.\tSelect the output folder you want the results saved in by clicking ‚ÄúBrowse‚Äù\n5.\tFinally, click on ‚ÄúAnalyse the fish‚Äù to generate your analysis\n\nHere is an example of the results you obtain from it:\n‚Ä¢\ttwo graphs per image,\no\tone representing the percentage of the 3 dominant color you want to study (black, white and orange) \n \no\tone with the composition of the real RGB values found in the image (the ones that are categorized in the 3 main classes you have in graph 1)\n \n\n‚Ä¢\tone csv file in which there are the number of black, white and orange pixels, their percentages (same values of graph 1), the length of the fish in pixels (always provided) and the length converted in cm (if a scale was provided).\no\tIf in the output folder chosen there were already a previously created csv we will append to that the new info, otherwise a new csv is created.\n \n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"fish-scan\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/ariannaravera/fish-scan/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "FishAnalysis"
    ],
    "contributions_sample_data": [
      "Fish Scan"
    ]
  },
  {
    "normalized_name": "organelle-segmenter-plugin",
    "name": "organelle-segmenter-plugin",
    "display_name": "Infer sub-Cellular Object Npe2 plugin",
    "version": "0.0.6",
    "created_at": "2023-05-13",
    "modified_at": "2024-11-12",
    "authors": [
      "Andy Henrie"
    ],
    "author_emails": [
      "ergonyc@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/organelle-segmenter-plugin/",
    "home_github": "https://github.com/ndcn/organelle-segmenter-plugin",
    "home_other": null,
    "summary": "A plugin that enables organelle segmentation, forked from tools from Allen Institute for Cell Science",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine>=0.1.4",
      "aicssegmentation",
      "aicsimageio",
      "numpy",
      "scikit-image",
      "aicsimageio>=4.7.0",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# organelle-segmenter-plugin\n\n[![License BSD-3](https://img.shields.io/pypi/l/organelle-segmenter-plugin.svg?color=green)](https://github.com/ndcn/organelle-segmenter-plugin/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/organelle-segmenter-plugin.svg?color=green)](https://pypi.org/project/organelle-segmenter-plugin)\n[![Python Version](https://img.shields.io/pypi/pyversions/organelle-segmenter-plugin.svg?color=green)](https://python.org)\n[![tests](https://github.com/ndcn/organelle-segmenter-plugin/workflows/tests/badge.svg)](https://github.com/ndcn/organelle-segmenter-plugin/actions)\n[![codecov](https://codecov.io/gh/ndcn/organelle-segmenter-plugin/branch/main/graph/badge.svg)](https://codecov.io/gh/ndcn/organelle-segmenter-plugin)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/organelle-segmenter-plugin)](https://napari-hub.org/plugins/organelle-segmenter-plugin)\n\n\n### A [napari](https://napari.org/stable/) plugin implementing [`infer-subc`](https://github.com/ndcn/infer-subc) segmentation workflows. \nSee [`infer-subc`](https://github.com/ndcn/infer-subc) for more information.\n\n# FRAMEWORK, RESOURCES & CONTRIBUTIONS\n \n## Forked from Allen Institute for Cell Science project\nThe Allen Cell & Structure Segmenter plugin for napari, from which this projects is forked, provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). ‚Äã[The Allen Cell & Structure Segmenter](https://allencell.org/segmenter) is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes.\n\nMore details about Segmenter can be found at https://allencell.org/segmenter\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Contributing\n\nContributions are very welcome. \n\n## License\n\nDistributed under the terms of the [BSD-3] license\n\n\"organelle-segmenter-plugin\" is free and open source software.\n\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/ndcn/organelle-segmenter-plugin/issues\n[napari]: https://github.com/napari/napari\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.czi",
      "*.tif",
      "*.xyz"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Batch processing",
      "Workflow Editor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cochlea-synapseg",
    "name": "cochlea-synapseg",
    "display_name": "Cochlea SynapSeg",
    "version": "0.0.1",
    "created_at": "2024-11-07",
    "modified_at": "2024-11-07",
    "authors": [
      "Cayla Miller"
    ],
    "author_emails": [
      "cayla@ucsd.edu"
    ],
    "license": "Copyright (c) 2024, Cayla Mill...",
    "home_pypi": "https://pypi.org/project/cochlea-synapseg/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin to segment cochlear ribbon synapses automatically, as well as edit and adjust",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "scikit-image",
      "scipy",
      "zarr",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# cochlea-synapseg\n\n[![License BSD-3](https://img.shields.io/pypi/l/cochlea-synapseg.svg?color=green)](https://github.com/ucsdmanorlab/cochlea-synapseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/cochlea-synapseg.svg?color=green)](https://pypi.org/project/cochlea-synapseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/cochlea-synapseg.svg?color=green)](https://python.org)\n[![tests](https://github.com/ucsdmanorlab/cochlea-synapseg/workflows/tests/badge.svg)](https://github.com/ucsdmanorlab/cochlea-synapseg/actions)\n[![codecov](https://codecov.io/gh/ucsdmanorlab/cochlea-synapseg/branch/main/graph/badge.svg)](https://codecov.io/gh/ucsdmanorlab/cochlea-synapseg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/cochlea-synapseg)](https://napari-hub.org/plugins/cochlea-synapseg)\n\nA plugin to segment cochlear ribbon synapses. \n\nMore is in the works, but for now it includes tools to more quickly generate ground truth ribbon segmentation (Plugins > SynapSeg - Ground Truth Widget).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `cochlea-synapseg` (receommended: in a new conda environment with up-to-date napari) via [pip]:\n\n    python -m pip install cochlea-synapseg\n\n## Usage\n\nAfter successfull installation, you can find the plugin next time you launch napari (Plugins > SynapSeg - Ground Truth Widget).\n\nThe ground truth widget is divided into multiple sections, for \"quick use\", be sure to check the settings denoted with asterisks:\n\n### Image Tools\n\n![image_tools](https://github.com/user-attachments/assets/323984ad-2cd3-4816-8ee5-e8b3f5063bc0)\n\n\\***1. Image Layer Selection** - use the dropdown to select the name of your image layer (here, the layer that contains the ribbon stain)\n\n(First load a 3D image using one of napari's native readers, or using the Cochlea-Synapseg .zarr reader (reads a .zarr with '3d/raw' and '3d/labeled').)\n\n\\***2. Refresh Image Layer Selection** - update the list of available image layers in #1\n\n**3. Pixel size information** - (in microns), used for some point readers to successfully convert real units to pixel coordinates. Can be left as 1 if this functionality is not needed.\n\n**4. Split channels** - splits multiple channels into separate image layers (useful for FIJI-saved .tif images)\n\n### Points Tools & Points to Labels\n\n![points_tools](https://github.com/user-attachments/assets/6b271d5c-51c1-4ca4-b3c0-683dddd69dc8)\n\n**5. Points Layer Selection and Refresh** - use the dropdown to select an existing points layer, use the refresh button to update the list (or skip to #8 if not loading in existing points)\n\n**6. Real -> Pixel Units** - if you've loaded some points were saved in real units, make sure the pixel size information above in #3 is correct, then click \"real -> pixel units\"\n\n**7. Channel Adjustment** - some points (like ImageJ/FIJI rois or CellCounter points), show up in the wrong z plane because their \"slice\" coordinates are a combination of both slice and channel info. If this happens, set the number of channels (in the original image, where the ROIs were created!), and then click \"chan -> z convert\". Z coordinates of the points layer will be divided by the number of channels specified. \n\n\\***8. New Points Layer** - if starting annotation from scratch, click to create a new points layer. #5 should automatically select this new layer. \n\n\\***9. Rotate to XY and \\*10. Auto-adjust Z** - these convenient functions allow you to quickly annotate points in Napari's 3D view. Simply click \"Rotate to XY before adding new points. These points will now have the correct XY position but will have missing Z information. (Rotate out of XY to confirm.) Click \"Auto-adjust z\" and the z will automatically adjust to the brightest point. \n\n**11. Manually Edit Z** - useful for overlapping points, can be used to manually edit the z position of ONLY selected points. Use the +/- arrow keys for single z steps type in a number to move a larger amount. \n\n**12. Snap too Max** - will automatically adjust all points to their local max (search radius defined in pixels in Advanced Settings -> snap to max rad). Useful for adjusting quickly dropped points, but proceed with caution if you have close-together points. \n\n\\***13. Points to Labels** - the key functionality of the module, creates a label layer by performing a local segmentation on all points.\n\n**14. Advanced Settings** - adjust settings for the points to labels function to optimize local segmentation and watershed separation of points. \n\n### Labels Tools\n\n![labels_tools](https://github.com/user-attachments/assets/6ef20ff6-61e2-4337-a177-8f957a67fb39)\n\n**15. Lables Layer Selection and Refresh** - use the dropdown to select an existing labels layer, use the refresh button to update the list\n\n**16. Make Labels Editable** - zarrs and other file formats tend to load in as dask arrays, which don't allow editing. Checking this box will make the labels layer editable by converting to a numpy array (will load the layer into memory, so be careful if dealing with large images!). This will allow you to edit the labels layer with tools like the paintbrush and eraser. Automatically enables if merging or removing labels is requested (see #17 and #19)\n\n\\***17. Remove a Label** - use the labels layer eyedropper tool to identify the ID of an unwanted label, then type in the box and click \"Remove label\"\n\n**18. Max Label Display** - shows the current maximum label ID. If you're painting labels by hand and need a new label ID, increment above this number. Use the refresh button to the right to get an up-to-date number if you've made changes to your label layer.\n\n\\***19. Merge labels** - if you have an existing labels layer, and then create new labels (e.g. from the points to labels function), select the layer you'd like to merge into your existing labels layer (i.e. box 15 and 19a should be different from one another!), and then click \"merge labels\". This function will automatically ensure overlapping label IDs are not used. \n\n**20. Labels to Points** - if wanted, you can take all your existing labels and convert them to a points layer based on their centroids. This may be helpful for quickly generating better labels using the points tools above. \n\n### Save to Zarr\n\n![save_zarr](https://github.com/user-attachments/assets/1d824f49-012f-4fac-8fa1-64d7d319cd34)\n\nFunctionality to save to .zarr format. Saves image as '3d/raw' and labels as '3d/labeled'. Used for later auto-segmentation of ribbons (not live in this plugin yet, but coming soon!). \n\n\\***21. File Path** - the directory in which to save the zarr; use the folder icon to search for an existing directory\n\n\\***22. File Name** - the zarr name to save to; use the magnifying glass icon to select an existing .zarr\n\n**23. From Source** - set the file path and name to where the image layer was loaded from. (Caution: if you loaded a zarr, this will result in the zarr being overwritten!)\n\n\\***24. Save 3D Only (recommended)** - saves the image and labels layers (selected in #1 and #15) in the specified .zarr, as 3d/raw and 3d/labeled, respectively. A reader is included with this plugin for this format as well. \n\n**25. Save 2D and 3D** - (not recommended) to be used in the future if 2D models are to be run on the data, saves both the 3D stacks as in 24, and then individual 2D slices for each z in 2d/raw/[z] and 2d/labeled/[z]\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"cochlea-synapseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.XLS",
      "*.xml",
      "*.xls",
      "*.csv",
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "SynapSeg - Ground Truth Widget"
    ],
    "contributions_sample_data": [
      "Cochlea SynapSeg"
    ]
  },
  {
    "normalized_name": "motile-plugin",
    "name": "motile-plugin",
    "display_name": "Motile",
    "version": "2.0.1",
    "created_at": "2024-05-23",
    "modified_at": "2024-11-04",
    "authors": [
      "Caroline Malin-Mayor"
    ],
    "author_emails": [
      "Caroline Malin-Mayor <malinmayorc@janelia.hhmi.org>"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/motile-plugin/",
    "home_github": null,
    "home_other": null,
    "summary": "Tracking with motile",
    "categories": [
      "Utilities"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "napari[all]",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "motile >=0.3",
      "motile-toolbox >=0.3.5",
      "pydantic",
      "tifffile[all]",
      "fonticon-fontawesome6",
      "pyqtgraph",
      "lxml-html-clean",
      "myst-parser ; extra == 'docs'",
      "sphinx ; extra == 'docs'",
      "sphinx-autoapi ; extra == 'docs'",
      "sphinx-rtd-theme ; extra == 'docs'",
      "sphinxcontrib-video ; extra == 'docs'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "package_metadata_description": "# Motile Napari Plugin\n\n[![tests](https://github.com/funkelab/motile-napari-plugin/workflows/tests/badge.svg)](https://github.com/funkelab/motile-napari-plugin/actions)\n[![codecov](https://codecov.io/gh/funkelab/motile-napari-plugin/branch/main/graph/badge.svg)](https://codecov.io/gh/funkelab/motile-napari-plugin)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/motile-plugin)](https://napari-hub.org/plugins/motile-plugin)\n\nThe full documentation of the plugin can be found [here](https://funkelab.github.io/motile_napari_plugin/).\n\nA plugin for tracking with [motile](https://github.com/funkelab/motile) in napari.\nMotile is a library that makes it easy to solve tracking problems using optimization\nby framing the task as an Integer Linear Program (ILP).\nSee the motile [documentation](https://funkelab.github.io/motile)\nfor more details on the concepts and method.\n\n----------------------------------\n\n## Installation\n\nThis plugin depends on [motile](https://github.com/funkelab/motile), which in\nturn depends on gurobi and ilpy. These dependencies must be installed with\nconda before installing the plugin with pip.\n\n    conda create -n motile-plugin python=3.10\n    conda activate motile-plugin\n    conda install -c conda-forge -c funkelab -c gurobi ilpy\n    pip install motile-plugin\n\n## Issues\n\nIf you encounter any problems, please\n[file an issue](https://github.com/funkelab/motile-napari-plugin/issues)\nalong with a detailed description.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Motile Main Widget",
      "Motile Menus Widget",
      "Motile Lineage View"
    ],
    "contributions_sample_data": [
      "Fluo-N2DL-HeLa (2D)",
      "Fluo-N2DL-HeLa crop (2D)",
      "Mouse Embryo Membranes (3D)"
    ]
  },
  {
    "normalized_name": "nellie",
    "name": "nellie",
    "display_name": "Nellie",
    "version": "0.4.1",
    "created_at": "2024-03-21",
    "modified_at": "2024-10-31",
    "authors": [
      "Austin E. Y. T. Lefebvre"
    ],
    "author_emails": [
      "austin.e.lefebvre+nellie@gmail.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/nellie/",
    "home_github": "https://github.com/aelefebv/nellie",
    "home_other": null,
    "summary": "Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy==1.26.4",
      "scipy==1.12.0",
      "scikit-image==0.22.0",
      "nd2==0.9.0",
      "ome-types==0.5.2",
      "pandas==2.2.1",
      "matplotlib==3.8.3",
      "napari[all]",
      "imagecodecs",
      "pydantic==2.9.2",
      "pydantic-core==2.23.4"
    ],
    "package_metadata_description": "# Nellie\n## Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy\n\n<img src=\"https://github.com/aelefebv/nellie/assets/26515909/96b7a113-be60-4028-bcd9-b444bdb943f6\" width=\"200px\" align=\"left\" /> *arXiv* \n\n  [Preprint Link](https://arxiv.org/abs/2403.13214) | [Cite](#reference)\n\n**Abstract:** The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes. We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures. Nellie adapts to image metadata, eliminating user input. Nellie‚Äôs preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions. Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis. Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users. \n\n**Nellie's pipeline and Napari plugin are both very much in early stages,** therefore [I highly encourage any and all feedback](#getting-help).\n\n## Example output intermediates\n\nhttps://github.com/aelefebv/nellie/assets/26515909/1df8bf1b-7116-4d19-b5fb-9658f744675b\n\n## Installation (~ 1 minute)\n\n**Notes:** \n- It is recommended (but usually not required) to [create a new environment](https://docs.python.org/3/library/venv.html) for Nellie to avoid conflicts with other packages.\n- May take several minutes to install.\n- Choose one of the following methods, and only one!\n- If you do not already have Python 3.9 or higher installed, download it via the [python website]([url](https://www.python.org/downloads/)).\n\nhttps://github.com/user-attachments/assets/50b1cd4b-6df7-4f19-8db3-4dcc03388513\n\n\n### Option 1. Via Napari plugin manager:\nIf not already installed, install Napari: https://napari.org/stable/tutorials/fundamentals/installation\n\nhttps://github.com/user-attachments/assets/0d44abe5-f575-4bd4-962a-2c102faf737c\n\n\n1. Open Napari\n2. Go to ```Plugins > Install/Uninstall Plugins...```\n3. Search for Nellie and click ```Install```\n4. Make sure Nellie is updated to the latest version.\n5. Restart Napari.\n### Option 2. Via PIP:\n\n\nhttps://github.com/user-attachments/assets/b63df093-e3e1-49cb-925b-7efce36b9015\n\n\n```bash\npython3 -m pip install nellie\n```\n#### Option 2a for NVIDIA GPU acceleration, optional (Windows, Linux):\nTo use GPU acceleration via NVIDIA GPUs, you also need to install cupy:\n```bash\npip install cupy-cudaXXx\n```\n- replace ```cupy-cudaXXx``` with the [appropriate version](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for your CUDA version.\n  - i.e. ```cupy-cuda11x``` for CUDA 11.x or ```cupy-cuda12x``` for CUDA 12.x\n- if you don't have CUDA installed, [go here](https://docs.cupy.dev/en/stable/install.html).\n- Mac Metal GPU-acceleration coming... eventually.\n\n## Usage\nThe sample dataset shown below is in the repo if you want to play around without, and can be downloaded [here](https://github.com/aelefebv/nellie/tree/main/sample_data).\n\n### General data preparation\n- It is strongly recommended to have your data in a parsable format, such as .ome.tif, .nd2, or other raw data files from microscopes.\n  - Importing into ImageJ/FIJI and saving via BioFormats with the proper image dimensions should do the trick.\n  - If the metadata cannot be parsed, you will have to manually enter it.\n- It is also recommended to crop your image as much as possible to reduce processing time and memory usage. But really, unless you have massive lightsheet data, it should be pretty fast (seconds to minutes on a typical modern desktop computer).\n\n### 3D + Timeseries dataset\n\nhttps://github.com/user-attachments/assets/531f76ee-f58e-4058-b5dc-4fdf09af3660\n\n### 3D (no Timeseries) dataset\n\nhttps://github.com/user-attachments/assets/30d55bfa-bade-4987-88f0-255bb36cb7e8\n\n### 2D + Timeseries dataset\n\nhttps://github.com/user-attachments/assets/d534c6e1-df31-4964-9c12-edff56228be3\n\n### Running Nellie's processing pipeline\n1. Start Napari (open a Terminal and type napari)\n    ```bash\n    napari\n    ```\n2. Go to \n```Plugins > Nellie (nellie)``` then to the ```File select``` tab.\n3. Click ```Select File``` of ```Select Folder``` to select your image(s).\n   - If the metadata boxes do not fill in automatically and turn red, this means Nellie did not detect that metadata portion from your image, and you must manually enter it or reformat your image and try again.\n     - The metadata slot will appear green if it is in the correct format.\n   - *Note, if you are batch processing, the metadata must be the same for all images if any of them are in an incorrect format (this will be fixed eventually). If they are different, but all pass validation, then it will process fine.\n   - You can preview your image via the ```Open preview``` button once the metadata is filled in to ensure it looks correct.\n   - From this tab, you can also choose what time points and channel you want to analyze, if your file contains more than one slice in those dimensions.\n4. Click the ```Process``` tab.\n5. You can run the full pipeline with ```Run Nellie```, or run individual steps below.\n    - Steps can only be run once its previous step has been run.\n    - Likewise, visualizations in the ```Visualization``` tab can only be opened once its respective step has been run.\n6. All intermediate files and output csvs will be saved to ```[image_directory]/nellie_output/```, which can be accessed via the ```Open output directory``` button.\n   - A separate .csv is created for each level of the organellar hierarchy.\n7. Once features have been exported, Nellie will automatically detect this, and allow analysis via the ```Analyze``` tab.\n   - Analysis at this point is optional, but can be helpful for visualizing, and selectively exporting data.\n\n### Using Nellie's visualization plugin\n1. Follow the previous processing steps, you only need to do this once per file as long as you don't move or delete the files.\n2. Open the ```Visualization``` tab\n3. Select a visualization from the list.\n   1. ```Raw```: Visualize the raw data for the processed channel.\n   2. ```Preprocessed```: Visualize the contrast-enhanced data.\n   3. ```Segmentation```: Visualize the organelle and branch instance segmentation masks.\n   4. ```Mocap Markers```: Visualize the mocap markers used for waypoints.\n   5. ```Reassigned Labels```: Visualize the organelle and branch instance segmentation masks where voxels are reassigned based on the first timepoint.\n4. To visualize tracks, open and select one of the segmentation layers.\n5. To visualize all tracks of all organelles/branches, click the ```Visualize all frame labels' tracks``` button.\n6. To visualize all tracks of a specific organelle/branch:\n   1. Click on the layer, and use the eyedropper tool at the top to select an organelle/branch to track.\n   2. Click the ```Visualize selected label's tracks```.\n\n### Using Nellie's analysis plugin\n1. Follow the previous processing steps, you only need to do this once per file as long as you don't move or delete the files.\n2. Open the ```Analyze``` tab, select the hierarchy level you want to visualize from the dropdown.\n3. Select the level-specific feature you want to visualize from the new dropdown.\n4. A histogram of all the data will be displayed.\n   - This histogram can be directly exported via the ```Save graph``` button. A .png will be saved to ```[image_directory]/nellie_output/graphs/``` with the current datetime.\n   - The values of the histogram can be exported via the ```Export graph data``` button. A .csv will be saved to ```[image_directory]/nellie_output/graphs/``` with the current datetime.\n   - The histogram's x-axis can be viewed in log10 scale via the ```Log scale``` checkbox.\n   - By default, the histogram shows lines at the mean +/- 1 standard deviation. This can instead be switched to median and quartiles via the ```Median view``` checkbox.\n5. Press the ```Overlay mask``` button to colormap the organelle mask based on your selected feature.\n   - Once overlaid, toggle the ```Timepoint data``` checkbox to allow you to select a specific timepoint to visualize via the slider.\n\n## Other features\n- Nellie's plugin offers an ```Easy screenshot``` feature:\n  - Press the button under ```Easy screenshot``` or hit Ctrl/Cmd-Shift-E after clicking your image.\n  - The .png will be saved to ```[image_directory]/nellie_output/screenshots/``` with the current datetime.\n\n## Feedback / Getting Help\nA few options are available for providing feedback or getting help with Nellie:\n\n[Github Issues](https://github.com/aelefebv/nellie/issues/new) | [email](mailto:austin.e.lefebvre+nellie@gmail.com) | [X](https://twitter.com/Austin_Lefebvre) | wherever else you can find me!\n\nTo avoid any unnecessary back-and-forth, please include any/all (if possible) of the following information in your bug report:\n- What kind of computer do you have, and what are its specs?\n- Send me screenshots of what is not working.\n- Send me any error logs in your terminal.\n- Send me the file you ran (if possible).\n- Any other information that might be helpful\n\n## Other Info\nFor a 16bit dataset, the output:input ratio is ~15x. There is an option in the GUI to automatically delete intermediates after processing, keeping only the CSV files containing the extracted features.\n\n## Requirements\nNellie has been tested on the following configurations:\n- Mac, Linux, and Windows operating systems\n- Python >= 3.9\n\n## License\nNellie ¬© 2024 by [Austin E. Y. T. Lefebvre](https://github.com/aelefebv) is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n\n## Reference\nIf you used Nelly or found this work useful in your own research, please cite our [arXiv preprint](https://arxiv.org/abs/2403.13214):\n\nLefebvre, A. E. Y. T., Sturm, G., et. al. Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy, arXiv, 2024, https://arxiv.org/abs/2403.13214\n\n```\n@misc{lefebvre2024nellie,\n      title={Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy}, \n      author={Austin E. Y. T. Lefebvre and Gabriel Sturm and Ting-Yu Lin and Emily Stoops and Magdalena Preciado Lopez and Benjamin Kaufmann-Malaga and Kayley Hake},\n      year={2024},\n      eprint={2403.13214},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## More fun examples\n### Microtubule growing ends:\n\nhttps://github.com/aelefebv/nellie/assets/26515909/88578dc9-f5c5-4188-a0e2-4e37037a44a9\n\n### Endoplasmic reticulum:\n\nhttps://github.com/aelefebv/nellie/assets/26515909/db76d388-a9cc-4650-b93d-69d357ace418\n\n### Peroxisomes:\n\nhttps://github.com/aelefebv/nellie/assets/26515909/58bda3cb-6489-4620-8584-a3728cd6b2ec\n\n## Code contents:\nFull documentation can be found within the code, and compiled by Sphinx in the file docs/_build/html/index.html\n\n### Nellie pipeline\nAll the Nellie pipeline code is found within the nellie folder\n- File and metadata loading, and file preparation is found at nellie/im_info/verifier.py\n- Preprocessing is found at nellie/segmentation/filtering.py\n- Segmentation of organelles is found at nellie/segmentation/labelling.py\n- Skeletonization and segmentation of branches is found at nellie/segmentation/networking.py\n- Mocap marker detection is found at nellie/segmentation/mocap_marking.py\n- Mocap marker tracking is found at nellie/tracking/hu_tracking.py\n- Voxel reassignment via flow interpolation is found at nellie/tracking/voxel_reassignment.py\n- Hierarchical feature extraction is found at nellie/feature_extraction/hierarchical.py\n\n### Nellie Napari plugin\nAll the Napari plugin code is found with the nellie_napari folder\n- The home tab is found at nellie_napari/nellie_home.py\n- The file selection tab is found at nellie_napari/nellie_fileselect.py\n- The processing tab is found at nellie_napari/nellie_processor.py\n- The visualization tab is found at nellie_napari/nellie_visualizer.py\n- The analysis tab is found at nellie_napari/nellie_analysis.py\n- The settings tab is found at nellie_napari/nellie_settings.py\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Nellie"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-boxmanager",
    "name": "napari-boxmanager",
    "display_name": "Box Manager",
    "version": "0.4.14",
    "created_at": "2022-09-26",
    "modified_at": "2024-10-30",
    "authors": [
      "Markus Stabrin"
    ],
    "author_emails": [
      "markus.stabrin@mpi-dortmund.mpg.de"
    ],
    "license": "MPL-2.0",
    "home_pypi": "https://pypi.org/project/napari-boxmanager/",
    "home_github": "https://github.com/MPI-Dortmund/napari-boxmanager",
    "home_other": null,
    "summary": "Particle selection tool for cryo-em",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "matplotlib",
      "mrcfile",
      "numpy<=1.23.5",
      "pystardb>=0.4.2",
      "napari>=0.4.17",
      "pandas",
      "scipy",
      "tifffile",
      "tqdm",
      "mrcfile; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "tox; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-boxmanager\n\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-boxmanager.svg?color=green)](https://github.com/mstabrin/napari-boxmanager/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-boxmanager.svg?color=green)](https://pypi.org/project/napari-boxmanager)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-boxmanager.svg?color=green)](https://python.org)\n[![tests](https://github.com/mstabrin/napari-boxmanager/workflows/tests/badge.svg)](https://github.com/mstabrin/napari-boxmanager/actions)\n[![codecov](https://codecov.io/gh/mstabrin/napari-boxmanager/branch/main/graph/badge.svg)](https://codecov.io/gh/mstabrin/napari-boxmanager)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boxmanager)](https://napari-hub.org/plugins/napari-boxmanager)\n\nParticle selection tool for cryo-em\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nHere is how to install napari together with the boxmanager plugin:\n\n    mamba env create -n napari -f https://raw.githubusercontent.com/MPI-Dortmund/napari-boxmanager/main/conda_env.yml\n    conda activate napari\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Mozilla Public License 2.0] license,\n\"napari-boxmanager\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tloc",
      "*.cs",
      "*.box",
      "*.rec",
      "*.tif",
      "*.temb",
      "*.star",
      "*.mrci",
      "*.cbox",
      "*.mrcs",
      "*.tmap",
      "*.coords",
      "*.mrc",
      "*.st"
    ],
    "contributions_writers_filename_extensions": [
      ".cbox",
      ".rec",
      ".mrcs",
      ".tmap",
      ".tloc",
      ".star",
      ".mrc",
      ".st",
      ".temb",
      ".mrci",
      ".coords",
      ".box"
    ],
    "contributions_widgets": [
      "boxmanager",
      "organize_layer",
      "bandpass_filter",
      "match_mics"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-signal-selector",
    "name": "napari-signal-selector",
    "display_name": "Napari Signal Selector",
    "version": "0.0.6",
    "created_at": "2023-10-17",
    "modified_at": "2024-10-29",
    "authors": [
      "Marcelo Leomil Zoccoler"
    ],
    "author_emails": [
      "marzoccoler@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-signal-selector/",
    "home_github": "https://github.com/zoccoler/napari-signal-selector",
    "home_other": null,
    "summary": "An interactive signal selector for napari, based on matplotlib.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "cmap",
      "nap-plot-tools>=0.1.2",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari>=0.4.19; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-signal-selector\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-signal-selector.svg?color=green)](https://github.com/zoccoler/napari-signal-selector/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-signal-selector.svg?color=green)](https://pypi.org/project/napari-signal-selector)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-signal-selector.svg?color=green)](https://python.org)\n[![tests](https://github.com/zoccoler/napari-signal-selector/workflows/tests/badge.svg)](https://github.com/zoccoler/napari-signal-selector/actions)\n[![codecov](https://codecov.io/gh/zoccoler/napari-signal-selector/branch/main/graph/badge.svg)](https://codecov.io/gh/zoccoler/napari-signal-selector)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-signal-selector)](https://napari-hub.org/plugins/napari-signal-selector)\n[![DOI](https://zenodo.org/badge/661588266.svg)](https://zenodo.org/doi/10.5281/zenodo.10041219)\n\nAn interactive signal selector and annotator for napari, based on [matplotlib](https://matplotlib.org/stable/).\n\n[Jump to Intallation](#installation)\n\n----------------------------------\n\n## Usage\n\nThis plugin opens an embedded plotter in napari capable of plotting and interacting (selecting/annotating) with individual object signals (typically temporal features).\n\n![plotting](https://github.com/zoccoler/napari-signal-selector/raw/main/images/plotting.gif)\n\n### Input Data\n\nnapari-signal-selector works with a [Labels layer](https://napari.org/stable/howtos/layers/labels.html) containing segmented objects and whose `features` attribute contains a table that follows the example structure shown below:\n\n| `label` | `frame` | `feature` | ...  |\n|-------|-------|---------|---|\n| 1     | 0     | 1.0     | ...  |\n| 2     | 0     | 1.0     | ...  |\n| 3     | 0     | 0.5     | ...  |\n| 4     | 0     | 0.5     | ...  |\n| 1     | 1     | 2.0     | ...  |\n| 2     | 1     | 1.0     | ...  |\n| 3     | 1     | 1.0     | ...  |\n| 4     | 1     | 1.0     | ...  |\n| 1     | 2     | 3.0     | ...  |\n| 2     | 2     | 1.0     | ...  |\n| 3     | 2     | 0.5     | ...  |\n| 4     | 2     | 1.5     | ...  |\n| ‚ãÆ     | ‚ãÆ     | ‚ãÆ     |   |\n\nBasically, it needs an object identifier (in this case, the `label` column) that matches the labels in the Labels layer, and other columns containing x- and y-axis numbers to plot. Typically, x-axis is some temporal-related property.\n\nHere is how one could add such a layer to a napari viewer via code (check [this example notebook](./examples/synthetic_example.ipynb) for more details):\n\n```python\nviewer.add_labels(labels_image, features = table)\n```\n\nIf a layer like this is selected, you can choose what to plot by means of dropdown fields in the bottom of the plotter.\n\nBelow is a basic example using the \"Flashing Polygons\" synthetic data:\n\n![intro](https://github.com/zoccoler/napari-signal-selector/raw/main/images/intro.gif)\n\n## Tools\n\n### Selection Tool\n\nThe selection tool (arrowhead icon) is a toggle button which enables you to select individual signals. Once activated, the icon gets highlighted and you can click over individual signals to select them. Right-clicking unselects everything.\n\n![select](https://github.com/zoccoler/napari-signal-selector/raw/main/images/select.gif)\n\nIf the region you want to click is too crowded, consider zooming in first and then selecting.\n\n![zoom-select](https://github.com/zoccoler/napari-signal-selector/raw/main/images/zoom_select.gif)\n\nAnd if you know which label you want to select, you can enable `'show selected'` from the Labels layer options to solely display one label at a time. The Lables layer picker tool may help you get the right label.\n\n![show-selected](https://github.com/zoccoler/napari-signal-selector/raw/main/images/show_selected.gif)\n\n### Annotation Tool\n\nOnce one or multiple signals are selected, you can annotate them with the annotation tool (brush with a 'plus' icon). You need to choose a signal class first.\n*Remember to right-click to remove previous selections when annotating different signal classes!*\n\n![annotation](https://github.com/zoccoler/napari-signal-selector/raw/main/images/annotation.gif)\n\nAnnotations are saved back in the table in a new column called 'Annotations'.\n*Currently multiple annotations is not possible, i.e., more than one class assigned to the same part of the signal.*\n\n### Span-Selection Tool\n\nYou can use the span-selection tool (bounded horizontal arrows icon) to sub-select one or multiple parts of signals. Right-click to unselect regions. Hold 'SHIFT' while dragging the mouse to select multiple sub-regions.\n\n![span-select](https://github.com/zoccoler/napari-signal-selector/raw/main/images/span_select.gif)\n\nYou can use this in conjunction with the annotation tool to have sub-regions from the same signal with different annotations.\n\n![](https://github.com/zoccoler/napari-signal-selector/raw/main/images/span_annotation.gif)\n\n### Deletion Tool\n\nIf you made a mistake, you can remove previous annotations by selecting signal(s) and clicking on the trash icon at the right of the toolbar (or just annotate them with class 0).\n\n![delete](https://github.com/zoccoler/napari-signal-selector/raw/main/images/delete.gif)\n\nAlso, with the selection tool enbaled, by holding 'SHIFT' and left-clicking, you can select all signals. This may be useful to delete all previous annotations.\n\n![select-delete-all](https://github.com/zoccoler/napari-signal-selector/raw/main/images/select_delete_all.gif)\n\n### Exporting Annotations\n\nThe table with annotations can be displayed in napari using the 'Show table' widget from [napari-skimage-regionprops plugin](https://github.com/haesleinhuepf/napari-skimage-regionprops#napari-skimage-regionprops-nsr), which is available under `Tools > Measurements > Show Table (nsr)`. This plugin may require a specific napari version, so check its documentation for more details.\n\n![](https://github.com/zoccoler/napari-signal-selector/raw/main/images/table_view.gif)\n\nBy the way, with `'show selected'` checked, you can click on a label row in the table and see the corresponding label in the image **...and** in the plotter!\n\nTo export the table, click on `'Save as csv...'`.\n\nAnother option is to run the following code in the napari console (replace `'Labels'` with the name of your Labels layer and `'annotations.csv'` with the desired file name or file path):\n\n```python\nimport pandas as pd\ndf = viewer.layers['Labels'].data.features\ndf.to_csv('annotations.csv')\n```\n\n## Installation\n\nYou can install `napari-signal-selector` via [pip]. Follow these steps from a terminal.\n\nWe recommend using `mamba-forge` whenever possible. Click [here](https://github.com/conda-forge/miniforge#mambaforge) to choose the right download option for your OS.\n**If you do not use `mamba-forge`, replace the `mamba` term whenever you see it below with `conda`.**\n\nCreate a conda environment :\n\n    mamba create -n napari-ss-env napari pyqt python=3.9\n    \nActivate the environment :\n\n    mamba activate napari-ss-env\n\nInstall `napari-signal-selector` via [pip] :\n\n    pip install napari-signal-selector\n\nAlternatively, install latest development version with :\n\n    pip install git+https://github.com/zoccoler/napari-signal-selector.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-signal-selector\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/zoccoler/napari-signal-selector/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Signal Selector and Annotator"
    ],
    "contributions_sample_data": [
      "Flashing Polygons (2D+t)",
      "Blinking Polygons (2D+t)"
    ]
  },
  {
    "normalized_name": "napari-simple-orthoviewer",
    "name": "napari-simple-orthoviewer",
    "display_name": "Orthogonal Viewer",
    "version": "0.0.5",
    "created_at": "2024-10-08",
    "modified_at": "2024-10-28",
    "authors": [
      "Krishnan Venkataraman"
    ],
    "author_emails": [
      "krishvraman95@gmail.com"
    ],
    "license": "Copyright (c) 2024, Krishnan V...",
    "home_pypi": "https://pypi.org/project/napari-simple-orthoviewer/",
    "home_github": "https://github.com/Krishvraman/napari-simple-orthoviewer",
    "home_other": null,
    "summary": "A simple orthogonal viewer in napari",
    "categories": [
      "Utilities",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "tifffile",
      "napari",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-simple-orthoviewer\n\nA simple orthogonal viewer in napari\n\n----------------------------------\n## Installation\n\nYou can install `napari-simple-orthoviewer` via [pip]:\n\n    pip install napari-simple-orthoviewer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Krishvraman/napari-simple-orthoviewer.git\n\n\n## Usage\n\n![ortho_views_with_overlay](https://github.com/user-attachments/assets/33c00852-13b8-42ca-aa37-cbd28743297c)\n\n\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-simple-orthoviewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Krishvraman/napari-simple-orthoviewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Orthoview"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-segment-blobs-and-things-with-membranes",
    "name": "napari-segment-blobs-and-things-with-membranes",
    "display_name": "napari-segment-blobs-and-things-with-membranes",
    "version": "0.3.12",
    "created_at": "2021-09-25",
    "modified_at": "2024-10-25",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-segment-blobs-and-things-with-membranes/",
    "home_github": "https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes",
    "home_other": null,
    "summary": "A plugin based on scikit-image for segmenting nuclei and cells based on fluorescent microscopy images with high intensity in nuclei and/or membranes",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "numpy",
      "scikit-image",
      "scipy",
      "napari-tools-menu>=0.1.17",
      "napari-time-slicer>=0.4.8",
      "napari-assistant",
      "stackview>=0.9.1"
    ],
    "package_metadata_description": "# napari-segment-blobs-and-things-with-membranes (nsbatwm)\n\n[![License](https://img.shields.io/pypi/l/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://pypi.org/project/napari-segment-blobs-and-things-with-membranes)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment-blobs-and-things-with-membranes.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-segment-blobs-and-things-with-membranes)\n[![Development Status](https://img.shields.io/pypi/status/napari-segment-blobs-and-things-with-membranes.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment-blobs-and-things-with-membranes)](https://napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7027634.svg)](https://doi.org/10.5281/zenodo.7027634)\n\nThis napari-plugin is based on scikit-image and allows segmenting nuclei and cells based on fluorescence microscopy images with high intensity in nuclei and/or membranes.\n\n## Usage\n\nThis plugin populates image processing operations to the `Tools` menu in napari.\nYou can recognize them with their suffix `(nsbatwm)` in brackets.\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/tools_menu_screenshot.png)\n\nYou can also call these functions as shown in [the demo notebook](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/blob/main/docs/demo.ipynb).\n\n### Voronoi-Otsu-Labeling\n\nThis algorithm uses [Otsu's thresholding method](https://ieeexplore.ieee.org/document/4310076) in combination with \n[Gaussian blur](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian) and a \n[Voronoi-Tesselation](https://en.wikipedia.org/wiki/Voronoi_diagram) \napproach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters which allow\nyou to fine-tune where objects should be cut (`spot_sigma`) and how smooth outlines should be (`outline_sigma`).\nThis implementation aims to be similar to [Voronoi-Otsu-Labeling in clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb).\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/voronoi_otsu_labeling.png)\n\n### Seeded Watershed\n\nStarting from an image showing high-intensity membranes and a seed-image where objects have been labeled (e.g. using Voronoi-Otsu-Labeling),\nobjects are labeled that are constrained by the membranes.\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/seeded_watershed.png)\n\n### Seeded Watershed with mask\n\nIf there is additionally a mask image available, one can use the `Seeded Watershed with mask`, to constraint the flooding \non a membrane image (1), starting from nuclei (2), limited by a mask image (3) to produce a cell segmentation within the mask (4).\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/seeded_watershed_with_mask.png)\n\n### Seeded Watershed using local minima as starting points\n\nSimilar to the Seeded Watershed and Voronoi-Otsu-Labeling explained above, you can use this tool to segment an image\nshowing membranes without an additional image showing nuclei. The two sigma parameters allow to fine tune how close \nobjects can be and how precise their boundaries are detected.\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/local_minima_seeded_watershed.png)\n\n### Gaussian blur\n\nApplies a [Gaussian blur](https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gaussian) to an\nimage. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/gaussian_blur.png)\n\n### Subtract background\n\nSubtracts background using [scikit-image's rolling-ball algorithm](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_rolling_ball.html). \nThis might be useful, for example to make intensity of membranes more similar in different regions of an image.\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/subtract_background.png)\n\n### Threshold Otsu\n\nBinarizes an image using [scikit-image's threshold Otsu algorithm](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_thresholding.html), also known as \n[Otsu's method](https://ieeexplore.ieee.org/document/4310076).\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/threshold_otsu.png)\n\n### Split touching objects (formerly known as binary watershed).\n\nIn case objects stick together after thresholding, this tool might help.\nIt aims to deliver similar results as [ImageJ's watershed implementation](https://imagej.nih.gov/ij/docs/menus/process.html#watershed).\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/binary_watershed.png)\n\n### Connected component labeling\n\nTakes a binary image and produces a label image with all separated objects labeled differently. Under the hood, it uses\n[scikit-image's label function](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html).\n\n![img.png](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/connected_component_labeling.png)\n\n### Manual split and merge labels\n\nSplit and merge labels in napari manually via the `Tools > Utilities menu`:\n\n![](https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/raw/main/docs/split_and_merge_demo.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nThis plugin is part of devbio-napari. To install it, please follow its [installation instructions](https://github.com/haesleinhuepf/devbio-napari#installation).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-segment-blobs-and-things-with-membranes\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-segment-blobs-and-things-with-membranes/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[image.sc]: https://image.sc\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "gaussian_blur",
      "subtract_background",
      "threshold_otsu",
      "threshold_yen",
      "threshold_isodata",
      "threshold_li",
      "threshold_mean",
      "threshold_minimum",
      "threshold_triangle",
      "binary_invert",
      "split_touching_objects",
      "connected_component_labeling",
      "seeded_watershed",
      "voronoi_otsu_labeling",
      "gauss_otsu_labeling",
      "gaussian_laplace",
      "median_filter",
      "mode_filter",
      "maximum_filter",
      "minimum_filter",
      "percentile_filter",
      "black_tophat",
      "white_tophat",
      "morphological_gradient",
      "local_minima_seeded_watershed",
      "thresholded_local_minima_seeded_watershed",
      "sum_images",
      "multiply_images",
      "divide_images",
      "invert_image",
      "skeletonize",
      "Manually_merge_labels",
      "Manually_split_labels",
      "rescale",
      "resize",
      "extract_slice",
      "grayscale_erosion",
      "binary_erosion",
      "grayscale_dilation",
      "binary_dilation",
      "grayscale_closing",
      "binary_closing",
      "grayscale_opening",
      "binary_opening"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "mmv-h4cells",
    "name": "mmv_h4cells",
    "display_name": "Cell Analyzer",
    "version": "1.1.0",
    "created_at": "2024-10-24",
    "modified_at": "2024-10-24",
    "authors": [
      "Lennart Kowitz"
    ],
    "author_emails": [
      "lennart.kowitz@isas.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/mmv-h4cells/",
    "home_github": "https://github.com/MMV-Lab/mmv_h4cells",
    "home_other": null,
    "summary": "A simple plugin to help with analyzing cells in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "scikit-image",
      "scipy",
      "aicsimageio",
      "opencv-python",
      "pandas",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# MMV_H4Cells\n\n[![License BSD-3](https://img.shields.io/pypi/l/mmv_h4cells.svg?color=green)](https://github.com/MMV-Lab/mmv_h4cells/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/mmv_h4cells.svg?color=green)](https://pypi.org/project/mmv_h4cells)\n[![Python Version](https://img.shields.io/pypi/pyversions/mmv_h4cells.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/mmv_h4cells/workflows/tests/badge.svg)](https://github.com/MMV-Lab/mmv_h4cells/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/mmv_h4cells/branch/main/graph/badge.svg)](https://codecov.io/gh/MMV-Lab/mmv_h4cells)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/mmv_h4cells)](https://napari-hub.org/plugins/mmv_h4cells)\n\nA simple plugin to help with analyzing cells in napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `mmv_h4cells` via [pip]:\n\n    pip install mmv_h4cells\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MMV-Lab/mmv_h4cells.git\n\n\n## Documentation\n\nThis plugin was developed for semi-automatic cell analysis to determine cell sizes of individual cells.\n\nThe core functionality includes the option to include or exclude individual (cell) instances in the evaluation via the include/exclude button. After a decision has been made, the plugin automatically centers on the next instance and a new decision can be made. In addition, you can include multiple cells at the same time using the \"select multiple\" function. It is also possible to analyze entire ROIs at once.\n \n### Get started\n\nTo get started, an instance segmentation must be loaded. This can be done simply via drag & drop. A raw image of the original data is optional, but certainly helps when deciding whether to include or exclude.\nOnce the layers have been loaded into napari, the plugin can be started.\nIf you have only interrupted the evaluation and exported the previous results, you can now import them again (the segmentation must be reloaded into napari). \n\n### Analysis\n\nThe analysis can be started by clicking on the \"Start analysis\" button. The next instance ID to be evaluated is shown next to \"Start analysis at\". To change the region of interest to be evaluated, a different ID can be entered there and the plugin will center on this within the next 2 decisions. Decisions are made by clicking the Include/Exclude button. If an instance is not completely recognized correctly, you can use the paint function of napari to correct this manually and then include the instance as usual using the button. The undo function can be used to undo the last decision and the \"Draw own cell\" button allows you to add unrecognized cells manually. This must be done cell by cell and confirmed each time using the button. The plugin does not allow other existing instances to be painted over. If this happens by mistake, a warning is displayed, oberlapping pixels are highlighted and users can either cancel via the cancel button within the warning or close the warning and correct this manually. \n\nWhen an instance is included, the respective instance is written to a segmentation layer, which can be exported using the export function. In addition, the ID, the size and the centroid are exported as a .csv file. We also export a .zarr file, which makes it possible to re-import previously exported results, for example to pause the analysis. To enable a smooth re-import, the .csv and the .zarr file must have the same name stem, so please either do not rename the files or rename them in the same way. \n\nFor a better overview, the included/excluded/remaining instances can be viewed using the buttons at the bottom.\n\n#### Select multiple cells\n\nWe also support the option of including several cells at once. To do so, the respective IDs must be entered at the bottom next to \"Include\" and then selected using the \"Select multiple\". This works by entering comma-separated IDs, so *1,5,100,17* would be a valid entry.\n\n#### Select ROI\n\nEntire ROIs can also be analyzed. To do this, simply enter the corner pixels in the \"Range x\" and \"Range y\" fields. All cells > the threshold are included; if, for example, cells that lie exactly at the edge of the ROI and are partially cut off are to be excluded, a corresponding threshold must be set.\n\nNote: Exported ROIs cannot be re-imported.\n\n### Hotkeys\n\n- `k` - Include\n- `g` - Exclude\n- `j` - Change visibility of all label layers for better inspection\n- `h` - Undo\n\n### Don'ts\n\nThis is a tool for analyzing cells. However, we do not catch every possible error and in order for the tool to run stable, it is important to avoid some operations:\n\n- Do not create new layers during the analysis.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"mmv_h4cells\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MMV-Lab/mmv_h4cells/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "MMV_H4Cells"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-exodeepfinder",
    "name": "napari-exodeepfinder",
    "display_name": "Napari ExoDeepFinder",
    "version": "0.0.11",
    "created_at": "2024-06-28",
    "modified_at": "2024-10-23",
    "authors": [
      "Constantin Aronssohn",
      "Arthur Masson"
    ],
    "author_emails": [
      "cnstt@tutanota.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-exodeepfinder/",
    "home_github": "https://github.com/deep-finder/napari-exodeepfinder",
    "home_other": null,
    "summary": "A napari plugin for the ExpDeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities. An orthoslice view has been added for an easier visualisation and annotation process.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "exodeepfinder>=0.3.11",
      "typing>=3.7.4.3",
      "pandas>=2.2.2",
      "pillow>=10.3.0",
      "napari[all]>=0.4.19; extra == \"all\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari[all]>=0.4.19; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-exodeepfinder\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-exodeepfinder.svg?color=green)](https://github.com/deep-finder/napari-exodeepfinder/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-exodeepfinder.svg?color=green)](https://pypi.org/project/napari-exodeepfinder)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-exodeepfinder.svg?color=green)](https://python.org)\n[![tests](https://github.com/deep-finder/napari-exodeepfinder/workflows/tests/badge.svg)](https://github.com/deep-finder/napari-exodeepfinder/actions)\n[![codecov](https://codecov.io/gh/deep-finder/napari-exodeepfinder/branch/main/graph/badge.svg)](https://codecov.io/gh/deep-finder/napari-exodeepfinder)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-exodeepfinder)](https://napari-hub.org/plugins/napari-exodeepfinder)\n\nA napari plugin for the ExoDeepFinder library which includes display, annotation, target generation, segmentation and clustering functionalities.\nAn orthoslice view has been added for an easier visualisation and annotation process.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\n1. Create a conda environment with python 3.10: `conda create -n napari-exodeepfinder python=3.10`\n1. Activate the environment: `conda activate napari-exodeepfinder`\n1. Install napari: `pip install napari-exodeepfinder`\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-exodeepfinder\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/deep-finder/napari-exodeepfinder/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.xlsx",
      "*.map",
      "*.h5",
      "*.xml",
      "*.rec",
      "*.tif",
      "*.xls",
      "*.mrc",
      "*.TIF",
      "*.ods"
    ],
    "contributions_writers_filename_extensions": [
      ".mrc",
      ".xml"
    ],
    "contributions_widgets": [
      "Reorder layers automatically",
      "Denoise tomogram",
      "Annotation",
      "Orthoslice view",
      "Segmentation",
      "Clustering"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pyclesperanto-assistant",
    "name": "napari_pyclesperanto_assistant",
    "display_name": "napari_pyclesperanto_assistant",
    "version": "0.25.0",
    "created_at": "2020-12-19",
    "modified_at": "2024-10-22",
    "authors": [
      "Robert Haase",
      "Talley Lambert"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-pyclesperanto-assistant/",
    "home_github": "https://github.com/clEsperanto/napari_pyclesperanto_assistant",
    "home_other": null,
    "summary": "GPU-accelerated image processing in napari using OpenCL",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "pyopencl",
      "toolz",
      "scikit-image",
      "napari>=0.4.15",
      "pyclesperanto-prototype>=0.24.5",
      "pyclesperanto==0.14.2",
      "magicgui",
      "numpy!=1.19.4",
      "pyperclip",
      "loguru",
      "jupytext",
      "jupyter",
      "pandas",
      "napari-tools-menu>=0.1.8",
      "napari-time-slicer>=0.4.0",
      "napari-skimage-regionprops>=0.2.0",
      "napari-workflows>=0.1.1",
      "napari-assistant>=0.2.0"
    ],
    "package_metadata_description": "# napari-pyclesperanto-assistant\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fclesperanto.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/clesperanto)\n[![website](https://img.shields.io/website?url=http%3A%2F%2Fclesperanto.net)](http://clesperanto.net)\n[![License](https://img.shields.io/pypi/l/napari-pyclesperanto-assistant.svg?color=green)](https://github.com/clesperanto/napari-pyclesperanto-assistant/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pyclesperanto-assistant.svg?color=green)](https://pypi.org/project/napari-pyclesperanto-assistant)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pyclesperanto-assistant.svg?color=green)](https://python.org)\n[![tests](https://github.com/clesperanto/napari_pyclesperanto_assistant/workflows/tests/badge.svg)](https://github.com/clesperanto/napari_pyclesperanto_assistant/actions)\n[![codecov](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant)\n[![Development Status](https://img.shields.io/pypi/status/napari_pyclesperanto_assistant.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pyclesperanto-assistant)](https://napari-hub.org/plugins/napari-pyclesperanto-assistant)\n[![DOI](https://zenodo.org/badge/322312181.svg)](https://zenodo.org/badge/latestdoi/322312181)\n\nThe py-clEsperanto-assistant is a yet experimental [napari](https://github.com/napari/napari) plugin for building GPU-accelerated image processing workflows. \nIt is part of the [clEsperanto](http://clesperanto.net) project and thus, aims at removing programming language related barriers between image processing ecosystems in the life sciences. \nIt uses [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) and with that [pyopencl](https://documen.tician.de/pyopencl/) as backend for processing images.\n\nThis napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix `(clEsperanto)` in brackets.\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/virtual_4d_support1.gif)\n\n## Usage\n\n### Start up the assistant\nStart up napari, e.g. from the command line:\n```\nnapari\n```\n\nLoad example data, e.g. from the menu `File > Open Samples > clEsperanto > CalibZAPWfixed` and \nstart the assistant from the menu `Tools > Utilities > Assistant (na)`.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1.png)\n\nIn case of two dimensional timelapse data, an initial conversion step might be necessary depending on your data source. \nClick the menu `Tools > Utilities > Convert to 2d timelapse`. In the dialog, select the dataset and click ok. \nYou can delete the original dataset afterwards:\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1a.png)\n\n### Set up a workflow\n\nChoose categories of operations in the top right panel, for example start with denoising using a Gaussian Blur with sigma 1 in x and y.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2.png)\n\nContinue with background removal using the top-hat filter with radius 5 in x and y.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2a.png)\n\nFor labeling the objects, use [Voronoi-Otsu-Labeling](https://nbviewer.jupyter.org/github/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb) with both sigma parameters set to 2.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2b.png)\n\nThe labeled objects can be extended using a Voronoi diagram to derive a estimations of cell boundaries.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2c.png)\n\nYou can then configure napari to show the label boundaries on top of the original image:\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2d.png)\n\nWhen your workflow is set up, click the play button below your dataset:\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/timelapse_2d.gif)\n\n### Neighbor statistics\n\nWhen working with 2D or 3D data you can analyze measurements in relationship with their neighbors. \nFor example, you can measure the area of blobs as shown in the example shown below using the menu \n`Tools > Measurements > Statistics of labeled pixels (clesperant)` and visualize it as `area` image by double-clicking on the table column (1).\nAdditionally, you can measure the maximum area of the 6 nearest neighbors using the menu `Tools > Measurments > Neighborhood statistics of measurements`.\nThe new column will then be called \"max_nn6_area...\" (2). When visualizing such parametric images next by each other, it is recommended to use\n[napari-brightness-contrast](https://www.napari-hub.org/plugins/napari-brightness-contrast) and visualize the same intensity range to see differences correctly.\n\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/neighbor_statistics.png)\n\n### Code generation\nYou can also export your workflow as Python/Jython code or as notebook. See the [napari-assistant documentation](https://www.napari-hub.org/plugins/napari-assistant) for details.\n\n## Features\n[pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) offers various possibilities for processing images. It comes from developers who work in life sciences and thus, it may be focused towards processing two- and three-dimensional microscopy image data showing cells and tissues. A selection of pyclesperanto's functionality is available via the assistant user interface. Typical workflows which can be built with this assistant include\n* image filtering\n  * denoising / noise reduction (mean, median, Gaussian blur)\n  * background subtraction for uneven illumination or out-of-focus light (bottom-hat, top-hat, subtract Gaussian background)\n  * grey value morphology (local minimum, maximum. variance)\n  * gamma correction\n  * Laplace operator\n  * Sobel operator\n* combining images\n  * masking\n  * image math (adding, subtracting, multiplying, dividing images) \n  * absolute / squared difference\n* image transformations\n  * translation\n  * rotation\n  * scale\n  * reduce stack  \n  * sub-stacks\n* image projections\n  * minimum / mean / maximum / sum / standard deviation projections\n* image segmentation\n  * binarization (thresholding, local maxima detection)\n  * labeling\n  * regionalization\n  * instance segmentation\n  * semantic segmentation\n  * detect label edges\n  * label spots\n  * connected component labeling\n  * Voronoi-Otsu-labeling\n* post-processing of binary images\n  * dilation\n  * erosion\n  * binary opening\n  * binary closing \n  * binary and / or / xor\n* post-processing of label images\n  * dilation (expansion) of labels\n  * extend labels via Voronoi\n  * exclude labels on edges\n  * exclude labels within / out of size / value range\n  * merge touching labels\n* parametric maps\n  * proximal / touching neighbor count\n  * distance measurements to touching / proximal / n-nearest neighbors\n  * pixel count map\n  * mean / maximum / extension ratio map\n* label measurements / post processing of parametric maps\n  * minimum / mean / maximum / standard deviation intensity maps\n  * minimum / mean / maximum / standard deviation of touching / n-nearest / neighbors\n* neighbor meshes\n  * touching neighbors\n  * n-nearest neighbors\n  * proximal neighbors\n  * distance meshes\n* measurements based on label images\n  * bounding box 2D / 3D\n  * minimum / mean / maximum / sum / standard deviation intensity\n  * center of mass\n  * centroid\n  * mean / maximum distance to centroid (and extension ratio shape descriptor)\n  * mean / maximum distance to center of mass (and extension ratio shape descriptor)\n  * statistics of neighbors (See related [publication](https://www.frontiersin.org/articles/10.3389/fcomp.2021.774396/full))\n* code export\n  * python / Fiji-compatible jython\n  * python jupyter notebooks\n* pyclesperanto scripting\n  * cell segmentation\n  * cell counting\n  * cell differentiation\n  * tissue classification\n\n## Installation\n\nIt is recommended to install the assistant using mamba. If you have never used mamba before, it is recommended to read \n[this blog post](https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html) first. \n\n```shell\nmamba create --name cle_39 python=3.9 napari-pyclesperanto-assistant -c conda-forge\nmamba activate cle_39\n```\n\nMac-users please also install this:\n\n    mamba install -c conda-forge ocl_icd_wrapper_apple\n    \nLinux users please also install this:\n    \n    mamba install -c conda-forge ocl-icd-system\n\nYou can then start the napari-assistant using this command:\n\n```\nnaparia\n```\n\n\n## Feedback and contributions welcome!\nclEsperanto is developed in the open because we believe in the open source community. See our [community guidelines](https://clij.github.io/clij2-docs/community_guidelines). Feel free to drop feedback as [github issue](https://github.com/clEsperanto/pyclesperanto_prototype/issues) or via [image.sc](https://image.sc)\n\n## Acknowledgements\nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany‚Äôs Excellence Strategy ‚Äì EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden.\nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\n\n[Imprint](https://clesperanto.github.io/imprint)\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Assistant",
      "statistics_of_labeled_pixels",
      "advanced_statistics",
      "neighborhood_statistics",
      "make_labels_editable",
      "auto_brightness_contrast",
      "auto_brightness_contrast_all_images",
      "reset_brightness_contrast",
      "split_stack",
      "set_voxel_size",
      "set_voxel_size_of_all_layers",
      "convert_image_to_labels",
      "convert_labels_to_image",
      "convert_to_numpy",
      "label",
      "voronoi_otsu_labeling"
    ],
    "contributions_sample_data": [
      "Lund",
      "Lund timelapse (100 MB)",
      "CalibZAPWfixed",
      "Sonneberg",
      "Haase_MRT_tfl3d1",
      "Pixel (cat)",
      "Artificial perfect tissue",
      "Artificial orderly tissue",
      "Artificial chaotic tissue",
      "Blobs (from ImageJ)"
    ]
  },
  {
    "normalized_name": "napari-trackastra",
    "name": "napari-trackastra",
    "display_name": "trackastra",
    "version": "0.1.6",
    "created_at": "2024-05-30",
    "modified_at": "2024-10-22",
    "authors": [
      "Benjamin Gallusser",
      "Martin Weigert"
    ],
    "author_emails": [
      "benjamin.gallusser@epfl.ch",
      "martin.weigert@epfl.ch"
    ],
    "license": "Copyright (c) 2024, Martin Wei...",
    "home_pypi": "https://pypi.org/project/napari-trackastra/",
    "home_github": "https://github.com/weigertlab/napari-trackastra",
    "home_other": null,
    "summary": "Napari plugin for cell tracking with trackastra.",
    "categories": [
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "scikit-image",
      "trackastra",
      "napari-ctc-io",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-trackastra\n\n[![PyPI](https://img.shields.io/pypi/v/napari-trackastra.svg?color=green)](https://pypi.org/project/napari-trackastra)\n[![tests](https://github.com/weigertlab/napari-trackastra/workflows/tests/badge.svg)](https://github.com/weigertlab/napari-trackastra/actions)\n[![codecov](https://codecov.io/gh/weigertlab/napari-trackastra/branch/main/graph/badge.svg)](https://codecov.io/gh/weigertlab/napari-trackastra)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-trackastra)](https://napari-hub.org/plugins/napari-trackastra)\n\nA napari plugin for cell tracking with [`trackastra`](https://github.com/weigertlab/trackastra).\n\n![demo](https://github.com/weigertlab/napari-trackastra/assets/8866751/097eb82d-0fef-423e-9275-3fb528c20f7d)\n\n\n## Installation\n1. Please install napari as outlined [here](https://napari.org/stable/tutorials/fundamentals/installation.html).\n\n2. After that, install the latest version of this plugin from PyPI with:\n    ```\n    pip install napari-trackastra\n    ```\n\nNotes:\n- For tracking with an integer linear program (ILP, which is optional), follow the [installation instructions of the main `trackastra` package](https://github.com/weigertlab/trackastra/blob/main/README.md#installation).\n- On Windows currently only supported for Python 3.10.\n\n## Usage\n\n- `trackastra` expects a timeseries of raw images and corresponding segmentations masks as input.\n- We provide some demo data at `File > Open Sample > trackastra`.\n- Tracked cells can be directly saved to [Cell Tracking Challenge format](https://celltrackingchallenge.net/datasets/).\n- Results can be drag-and-dropped back into napari for inspection.\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Trackastra tracking"
    ],
    "contributions_sample_data": [
      "bacteria",
      "hela"
    ]
  },
  {
    "normalized_name": "devbio-napari",
    "name": "devbio-napari",
    "display_name": "devbio-napari",
    "version": "0.11.0",
    "created_at": "2021-06-08",
    "modified_at": "2024-10-18",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@uni-leipzig.de"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/devbio-napari/",
    "home_github": "https://github.com/haesleinhuepf/devbio-napari",
    "home_other": null,
    "summary": "A bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "npe2",
      "numpy>=1.21.4",
      "napari-assistant>=0.4.9",
      "napari-pyclesperanto-assistant>=0.23.0",
      "napari-skimage-regionprops",
      "napari-animation",
      "PlatyMatch",
      "napari-plot-profile",
      "napari-accelerated-pixel-and-object-classification",
      "napari-brightness-contrast",
      "napari-plugin-search",
      "napari-segment-blobs-and-things-with-membranes",
      "napari-simpleitk-image-processing",
      "napari-stl-exporter",
      "napari-folder-browser",
      "napari-crop",
      "napari-clusters-plotter>=0.7.1",
      "napari-tabu",
      "napari-workflow-optimizer",
      "napari-workflow-inspector",
      "napari-curtain",
      "napari-layer-details-display",
      "napari",
      "vispy",
      "napari-mouse-controls",
      "the-segmentation-game",
      "napari-blob-detection",
      "jupyterlab",
      "napari-czifile2",
      "napari-roi",
      "pydantic!=1.10.0",
      "napari-pystackreg",
      "imageio!=2.22.1",
      "redlionfish",
      "jupyter-server",
      "seaborn"
    ],
    "package_metadata_description": "# devbio-napari\n\n[![License](https://img.shields.io/pypi/l/devbio-napari.svg?color=green)](https://github.com/haesleinhuepf/devbio-napari/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/devbio-napari.svg?color=green)](https://pypi.org/project/devbio-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/devbio-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/devbio-napari/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/devbio-napari/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/devbio-napari)\n[![Development Status](https://img.shields.io/pypi/status/devbio-napari.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/devbio-napari)](https://napari-hub.org/plugins/devbio-napari)\n\n \nA bundle of napari plugins useful for 3D+t image processing and analysis for studying developmental biology.\n\n* [accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\n  * Instance segmentation\n  * Semantic segmentation\n  * Object classification\n  * Random Forest Classifier training\n* [animation](https://www.napari-hub.org/plugins/napari-animation) \n  * Visualization\n* [blob-detection](https://www.napari-hub.org/plugins/napari-blob-detection)\n  * Detection\n* [brightness-contrast](https://www.napari-hub.org/plugins/napari-brightness-contrast)\n  * Visualization\n* [clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\n  * Visualization\n  * Plotting\n  * Semantic object segmentation\n  * Dimensionality reduction\n  * Unsupervised machine learning\n* [crop](https://www.napari-hub.org/plugins/napari-crop)\n  * Transformation\n* [curtain](https://www.napari-hub.org/plugins/napari-curtain)\n  * Visualization \n* [czifile2](https://www.napari-hub.org/plugins/napari-czifile2)\n  * File input/output\n* [folder-browser](https://www.napari-hub.org/plugins/napari-folder-browser)\n  * File input/output\n* [layer-details-display](https://www.napari-hub.org/plugins/napari-layer-details-display)\n  * Visualization\n* [mouse-controls](https://www.napari-hub.org/plugins/napari-mouse-controls)\n  * Interaction\n* [PlatyMatch](https://www.napari-hub.org/plugins/PlatyMatch)\n  * Image registration\n* [plot-profile](https://www.napari-hub.org/plugins/napari-plot-profile)\n  * Visualization\n  * Quantification\n* [plugin-search](https://www.napari-hub.org/plugins/napari-plugin-search)\n  * Interaction\n* [pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\n  * Filtering\n  * Instance segmentation\n  * Semantic segmentation\n  * Quantification\n* [pystackreg](https://www.napari-hub.org/plugins/napari-pystackreg)\n  * Image registration\n  * Motion correction\n* [RedLionfish](https://www.napari-hub.org/plugins/RedLionfish)\n  * Deconvolution\n  * Processing\n* [roi](https://www.napari-hub.org/plugins/napari-roi)\n  * Manual segmentation\n* [segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\n  * Filtering\n  * Instance segmentation\n  * Semantic segmentation\n* [simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\n  * Filtering\n  * Instance segmentation\n  * Semantic segmentation\n  * Quantification\n* [skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\n  * Quantification\n* [stl-exporter](https://www.napari-hub.org/plugins/napari-stl-exporter)\n  * File input/output\n* [tabu](https://www.napari-hub.org/plugins/napari-tabu)\n  * Interaction\n* [the-segmentation-game](https://www.napari-hub.org/plugins/the-segmentation-game)\n  * Quantification\n  * Segmentation quality assurance\n* [workflow-inspector](https://www.napari-hub.org/plugins/napari-workflow-inspector)\n  * Visualization\n* [workflow-optimizer](https://www.napari-hub.org/plugins/napari-workflow-optimizer)\n  * Interaction\n  * Optimization\n\n----------------------------------\n\n## Installation\n\nYou can install `devbio-napari` via conda/mamba. If you have never used conda before, please [read this guide first](https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html).  \nStart by creating an environment using mamba.\n\n```\nmamba create --name devbio-napari-env napari=0.4.19 python=3.9 devbio-napari pyqt -c conda-forge -c pytorch\n```\n\nAfterwards, activate the environment like this:\n\n```\nmamba activate devbio-napari-env\n```\n\nAfterwards, run this command from the command line\n\n```\nnaparia\n```\n\nThis window should open. It shows the [Assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \nRead more about how to use it in its [documentation](https://www.napari-hub.org/plugins/napari-assistant).\n\n![img.png](https://github.com/haesleinhuepf/devbio-napari/raw/master/docs/screenshot.png)\n\n## Troubleshooting: Graphics cards drivers\n\nIn case error messages contains \"ImportError: DLL load failed while importing cl: The specified procedure could not be found\" [see also](https://github.com/clEsperanto/pyclesperanto_prototype/issues/55) or \"\"clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR\", please install recent drivers for your graphics card and/or OpenCL device. Select the right driver source depending on your hardware from this list:\n\n* [AMD drivers](https://www.amd.com/en/support)\n* [NVidia drivers](https://www.nvidia.com/download/index.aspx)\n* [Intel GPU drivers](https://www.intel.com/content/www/us/en/download/726609/intel-arc-graphics-windows-dch-driver.html)\n* [Intel CPU OpenCL drivers](https://www.intel.com/content/www/us/en/developer/articles/tool/opencl-drivers.html#latest_CPU_runtime)\n* [Microsoft Windows OpenCL support](https://www.microsoft.com/en-us/p/opencl-and-opengl-compatibility-pack/9nqpsl29bfff)\n\nSometimes, mac-users need to install this:\n\n    conda install -c conda-forge ocl_icd_wrapper_apple\n\nSometimes, linux users need to install this:\n\n    conda install -c conda-forge ocl-icd-system\n\n\nIn case installation didn't work in the first attempt, you may have to call this command line to reset the napari configuration:\n\n```\nnapari --reset\n```\n\n## Troubleshooting: pytorch\n\nIn case pytorch-related plugins fail, install pytorch as explained on [its website](https://pytorch.org/get-started/locally/). Consider replacing `conda` with `mamba` in given instructions.\n\nFor example if you have an NVidia GPU at hand, install pytorch like this:\n```\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n```\nOr if not:\n```\nmamba install pytorch torchvision torchaudio cpuonly -c pytorch\n```\n\n## Contributing\n\nContributions are very welcome. \nIf you want to [suggest a new napari plugin](https://github.com/haesleinhuepf/devbio-napari/pulls) to become part of this distribution, please make sure it interoperates nicely with the other plugins. \nFor example, if the plugin you suggest provided cell segmentation algorithms, please check if the resulting segmented cells can be analysed using napari-skimage-regionprops.\nFurthermore, please make sure the README of the plugin you are proposing comes with user documentation, e.g. a step-by-step guide with screenshots explaining what users can do with the plugin and how to use it. \nIt is recommended to provide example data as well so that end-users can try out the plugin under conditions it was developed for.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"devbio-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/haesleinhuepf/devbio/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "microscope-napari",
    "name": "microscope-napari",
    "display_name": "microscope-napari",
    "version": "0.0.5",
    "created_at": "2024-06-21",
    "modified_at": "2024-10-15",
    "authors": [
      "Nanobiosensorics"
    ],
    "author_emails": [
      "horvath.robert@energia.mta.hu"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/microscope-napari/",
    "home_github": "https://github.com/Nanobiosensorics/microscope-napari",
    "home_other": null,
    "summary": "Nanobiosensorics microscopic napari plugin.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine>=0.1.4",
      "cellpose>0.6.3",
      "imagecodecs",
      "sphinx>=3.0; extra == \"docs\"",
      "sphinxcontrib-apidoc; extra == \"docs\"",
      "sphinx-rtd-theme; extra == \"docs\"",
      "sphinx-prompt; extra == \"docs\"",
      "sphinx-autodoc-typehints; extra == \"docs\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# Installation\n\nNapari needs to be set up on your machine in order to install this plugin.\n\nIf you do not have napari installed it can be done following [this article](https://napari.org/stable/tutorials/fundamentals/installation.html).\n\n## Napari plugin manager (recommended)\n\nSearch for `microscope-napari` and click install. \nAfter completed napari needs to be restarted to activate the plugin.\n\n![k√©p](https://github.com/Nanobiosensorics/microscope-napari/assets/65455148/5438235d-522e-458e-806d-89eaaa027be2)\n\n## Pip package manager\n\nYou can install the plugin in the environment where napari is set up with command.\n```\npip install microscope-napari\n```\nIf you have a conda environment use anaconda prompt.\n\n# Usage\n\nYou can access plugin's functionalities in the upper menu.\n\n![k√©p](https://github.com/Nanobiosensorics/microscope-napari/assets/65455148/dace1014-6ac0-4797-b0b5-00a56cbc6b61)\n\n## Cellpose\n\nImages can be segmented with custom and built-in cellpose models.\n\n![k√©p](https://github.com/Nanobiosensorics/microscope-napari/assets/65455148/82d300b9-c523-4b0a-bf44-0f3bfdadae07)\n\nFor further information make sure to check out [cellpose](https://github.com/MouseLand/cellpose) and [cellpose-napari](https://github.com/MouseLand/cellpose-napari) plugin.\n\n## Cell counting\n\nCell counts in pictures can be obtained with cellpose models and average intensity regression models.\nAny number of napari image layers can be selected to be evaluated.\n\n### Cellpose model\n\nWithout enabling regression model counting the default used method is cellpose segmenting.\nThe lower settings are for cellpose only.\n\nCell masks can be output to verify the accuracy of results.\n\nOur custom cellpose models can be accessed [there](https://drive.google.com/drive/folders/1-2SRK_AIlcSODebPoigKA7kbn5cb5s2o?usp=sharing).\n\n![k√©p](https://github.com/user-attachments/assets/00d1336f-eeb4-4074-87a6-70d9cd866c07)\n\n### Average intensity regression model\n\nFor these models we should enable the regression model counting feature.\nThe lower settings are irrelevant now, cell masks will not be output.\nIt is only used for counting cells in images.\n\nOur regression models can be accessed [there](https://drive.google.com/drive/folders/1-5uAXN1W5lbE2Pw6Tsa1lR5BYqmPgdEP?usp=sharing).\n\n![k√©p](https://github.com/user-attachments/assets/7d733347-ceed-4780-9bea-154c8faf3d4d)\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "cellpose",
      "cell counting"
    ],
    "contributions_sample_data": [
      "Cells (3D+2Ch)",
      "Cells 2D"
    ]
  },
  {
    "normalized_name": "napari-tomocube-data-viewer",
    "name": "napari-tomocube-data-viewer",
    "display_name": "Tomocube data viewer",
    "version": "2024.10.0",
    "created_at": "2023-03-06",
    "modified_at": "2024-10-15",
    "authors": [
      "Dohyeon Lee"
    ],
    "author_emails": [
      "Dohyeon Lee <dleh428@kaist.ac.kr>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tomocube-data-viewer/",
    "home_github": "https://github.com/ehgus/napari-tomocube-data-viewer",
    "home_other": null,
    "summary": "A plugin to visualize three-dimensional data from Tomocube's microscopy",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "TCFile>=2024.10.1",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-tomocube-data-viewer\n\n[![License MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/ehgus/napari-tomocube-data-viewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tomocube-data-viewer.svg?color=green)](https://pypi.org/project/napari-tomocube-data-viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tomocube-data-viewer.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tomocube-data-viewer)](https://napari-hub.org/plugins/napari-tomocube-data-viewer)\n\nA plugin to visualize three-dimensional data from [Tomocube](https://www.tomocube.com/)'s holotomography\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-tomocube-data-viewer` via [pip] or [conda]:\n\n    pip install napari-tomocube-data-viewer\n    conda install napari-tomocube-data-viewer\n\nTo install latest development version :\n\n    pip install git+https://github.com/ehgus/napari-tomocube-data-viewer.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-tomocube-data-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[conda]: https://docs.anaconda.com/free/miniconda/index.html\n[file an issue]: https://github.com/ehgus/napari-tomocube-data-viewer/issues\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.TCF"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-simpleitk-image-processing",
    "name": "napari-simpleitk-image-processing",
    "display_name": "napari-simpleitk-image-processing",
    "version": "0.4.9",
    "created_at": "2021-11-28",
    "modified_at": "2024-10-10",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-simpleitk-image-processing/",
    "home_github": "https://github.com/haesleinhuepf/napari-simpleitk-image-processing",
    "home_other": null,
    "summary": "Process and analyze images using SimpleITK in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine>=0.1.4",
      "numpy",
      "simpleitk",
      "napari-tools-menu>=0.1.17",
      "napari-time-slicer",
      "napari-skimage-regionprops>=0.5.1",
      "napari-assistant>=0.3.10",
      "pandas",
      "stackview>=0.3.2"
    ],
    "package_metadata_description": "# napari-simpleitk-image-processing (n-SimpleITK)\n\n[![License](https://img.shields.io/pypi/l/napari-simpleitk-image-processing.svg?color=green)](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-simpleitk-image-processing.svg?color=green)](https://pypi.org/project/napari-simpleitk-image-processing)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-simpleitk-image-processing.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-simpleitk-image-processing/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-simpleitk-image-processing)\n[![Development Status](https://img.shields.io/pypi/status/napari-simpleitk-image-processing.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-simpleitk-image-processing)](https://napari-hub.org/plugins/napari-simpleitk-image-processing)\n[![DOI](https://zenodo.org/badge/432729955.svg)](https://zenodo.org/badge/latestdoi/432729955)\n\nProcess images using [SimpleITK](https://simpleitk.org/) in [napari]\n\n## Usage\n\nFilters, segmentation algorithms and measurements provided by this napari plugin can be found in the `Tools` menu. \nYou can recognize them with their suffix `(n-SimpleITK)` in brackets.\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/screenshot_with_assistant.png)\n\nAll filters implemented in this napari plugin are also demonstrated in [this notebook](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/blob/main/docs/demo.ipynb).\n\n### Gaussian blur\n\nApplies a [Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur)\nto an image. This might be useful for denoising, e.g. before applying the Threshold-Otsu method.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/gaussian_blur.png)\n\n### Median filter\n\nApplies a [median filter](https://en.wikipedia.org/wiki/Median_filter) to an image. \nCompared to the Gaussian blur this method preserves edges in the image better. \nIt also performs slower.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/median_filter.png)\n\n### Bilateral filter\n\nThe [bilateral filter](https://en.wikipedia.org/wiki/Bilateral_filter) allows denoising an image\nwhile preserving edges.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/bilateral.png)\n\n### Threshold Otsu\n\nBinarizes an image using [Otsu's method](https://ieeexplore.ieee.org/document/4310076).\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/threshold_otsu.png)\n\n### Connected Component Labeling\n\nTakes a binary image and labels all objects with individual numbers to produce a label image.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/connected_component_labeling.png)\n\n### Measurements\n\nThis function allows determining intensity and shape statistics from labeled images. I can be found in the `Tools > Measurement tables` menu.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/measurements.png)\n\n### Signed Maurer distance map\n\nA distance map (more precise: [Signed Maurer Distance Map](https://itk.org/ITKExamples/src/Filtering/DistanceMap/MaurerDistanceMapOfBinary/Documentation.html)) can be useful for visualizing distances within binary images between black/white borders. \nPositive values in this image correspond to a white (value=1) pixel's distance to the next black pixel.\nBlack pixel's (value=0) distance to the next white pixel are represented in this map with negative values.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/signed_maured_distance_map.png)\n\n### Binary fill holes\n\nFills holes in a binary image.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/binary_fill_holes.png)\n\n### Touching objects labeling\n\nStarting from a binary image, touching objects can be splits into multiple regions, similar to the [Watershed segmentation in ImageJ](https://imagej.net/plugins/classic-watershed).\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/Touching_object_labeling.png)\n\n### Morphological Watershed\n\nThe [morhological watershed](http://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/32_Watersheds_Segmentation.html)\nallows to segment images showing membranes. Before segmentation, a filter such as the Gaussian blur or a median filter\nshould be used to eliminate noise. It also makes sense to increase the thickness of membranes using a maximum filter. \nSee [this notebook](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/segmentation_2d_membranes.ipynb) for details.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/morphological_watershed.png)\n\n### Watershed-Otsu-Labeling\n\nThis algorithm uses [Otsu's thresholding method](https://ieeexplore.ieee.org/document/4310076) in combination with \n[Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur) and the \n[Watershed-algorithm](https://en.wikipedia.org/wiki/Watershed_(image_processing)) \napproach to label bright objects such as nuclei in an intensity image. The alogrithm has two sigma parameters and a \nlevel parameter which allow you to fine-tune where objects should be cut (`spot_sigma`) and how smooth outlines \nshould be (`outline_sigma`). The `watershed_level` parameter determines how deep an intensity valley between two maxima \nhas to be to differentiate the two maxima. \nThis implementation is similar to [Voronoi-Otsu-Labeling in clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb).\n\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/watershed_otsu_labeling.png)\n\n### Richardson-Lucy Deconvolution\n\n[Richardson-Lucy deconvolution](https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution)\nallows to restore image quality if the point-spread-function of the optical system used \nfor acquisition is known or can be approximated.\n\n![img.png](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/docs/Richardson-Lucy-Deconvolution.png)\n\n\n## Installation\n\nYou can install `napari-simpleitk-image-processing` via using `conda` and `pip`.\nIf you have never used `conda` before, please go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.\n\n    conda install -c conda-forge napari\n    pip install napari-simpleitk-image-processing\n\n## Features\n\nThe user can select categories of features for feature extraction in the user interface. These categories contain the following measurements:\n* size:\n    * equivalent_ellipsoid_diameter\n    * equivalent_spherical_perimeter\n    * equivalent_spherical_radius\n    * number_of_pixels\n    * number_of_pixels_on_border\n* intensity:\n   * maximum \n   * mean \n   * median\n   * minimum \n   * sigma\n   * sum\n   * variance\n* perimeter:\n   * perimeter\n   * perimeter_on_border\n   * perimeter_on_border_ratio\n* shape:\n   * elongation\n   * feret_diameter\n   * flatness\n   * roundness\n* position:\n   * centroid \n   * bbox\n* moments:\n   * principal_axes\n   * principal_moments\n\n## See also\n\nThere are other napari plugins with similar functionality for processing images and extracting features:\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\n* [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\n* [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\n* [napari-allencell-segmenter](https://napari-hub.org/plugins/napari-allencell-segmenter)\n* [RedLionfish](https://www.napari-hub.org/plugins/RedLionfish)\n* [bbii-decon](https://www.napari-hub.org/plugins/bbii-decon)  \n* [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\n\nFurthermore, there are plugins for postprocessing extracted measurements\n* [napari-feature-classifier](https://www.napari-hub.org/plugins/napari-feature-classifier)\n* [napari-clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\n\n## Contributing\n\nContributions are very welcome. There are many useful algorithms available in \n[SimpleITK](https://simpleitk.org/). If you want another one available here in this napari\nplugin, don't hesitate to send a [pull-request](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/pulls).\nThis repository just holds wrappers for SimpleITK-functions, see [this file](https://github.com/haesleinhuepf/napari-simpleitk-image-processing/raw/main/src/napari_simpleitk_image_processing/_simpleitk_image_processing.py#L51) for how those wrappers\ncan be written.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-simpleitk-image-processing\" is free and open source software\n\n## Citation\n\nFor implementing this napari plugin, the \n[SimpleITK python notebooks](https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/) were very helpful. \nThus, if you find the plugin useful, consider citing the SimpleITK notebooks:\n\nZ. Yaniv, B. C. Lowekamp, H. J. Johnson, R. Beare, \n\"SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research\", \\\nJ Digit Imaging., 31(3): 290-303, 2018, [https://doi.org/10.1007/s10278-017-0037-8](https://doi.org/10.1007/s10278-017-0037-8).\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-simpleitk-image-processing/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "median_filter",
      "gaussian_blur",
      "threshold_otsu",
      "threshold_intermodes",
      "threshold_kittler_illingworth",
      "threshold_li",
      "threshold_moments",
      "threshold_renyi_entropy",
      "threshold_shanbhag",
      "threshold_yen",
      "threshold_isodata",
      "threshold_triangle",
      "threshold_huang",
      "threshold_maximum_entropy",
      "signed_maurer_distance_map",
      "morphological_watershed",
      "morphological_gradient",
      "standard_deviation_filter",
      "simple_linear_iterative_clustering",
      "scalar_image_k_means_clustering",
      "connected_component_labeling",
      "touching_objects_labeling",
      "watershed_otsu_labeling",
      "binary_fill_holes",
      "invert_intensity",
      "bilateral_filter",
      "laplacian_filter",
      "laplacian_of_gaussian_filter",
      "binominal_blur_filter",
      "canny_edge_detection",
      "gradient_magnitude",
      "h_maxima",
      "h_minima",
      "otsu_multiple_thresholds",
      "regional_maxima",
      "regional_minima",
      "richardson_lucy_deconvolution",
      "wiener_deconvolution",
      "tikhonov_deconvolution",
      "rescale_intensity",
      "sobel",
      "black_top_hat",
      "white_top_hat",
      "adaptive_histogram_equalization",
      "curvature_flow_denoise",
      "relabel_component",
      "label_contour",
      "label_statistics",
      "pixel_count_map",
      "elongation_map",
      "feret_diameter_map",
      "roundness_map"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-grid-cropping",
    "name": "napari-grid-cropping",
    "display_name": "Grid cropping napari plugin",
    "version": "0.0.1",
    "created_at": "2024-10-08",
    "modified_at": "2024-10-08",
    "authors": [
      "Niklas Breitenbach-Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "Copyright (c) 2024, Niklas Bre...",
    "home_pypi": "https://pypi.org/project/napari-grid-cropping/",
    "home_github": "https://github.com/gatoniel/napari-grid-cropping",
    "home_other": null,
    "summary": "Create multiple crops of an image in a grid like fashion.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "tifffile",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "magicgui; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-grid-cropping\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-grid-cropping.svg?color=green)](https://github.com/gatoniel/napari-grid-cropping/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-grid-cropping.svg?color=green)](https://pypi.org/project/napari-grid-cropping)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-grid-cropping.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-grid-cropping/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-grid-cropping/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-grid-cropping/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-grid-cropping)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-grid-cropping)](https://napari-hub.org/plugins/napari-grid-cropping)\n\nCreate multiple crops of an image in a grid like fashion.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-grid-cropping` via [pip]:\n\n    pip install napari-grid-cropping\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-grid-cropping.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-grid-cropping\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-grid-cropping/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Grid cropping",
      "Create label annotations"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-matplotlib",
    "name": "napari-matplotlib",
    "display_name": "napari Matplotlib",
    "version": "3.0.0",
    "created_at": "2022-05-02",
    "modified_at": "2024-10-07",
    "authors": [
      "David Stansby"
    ],
    "author_emails": [
      "d.stansby@ucl.ac.uk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-matplotlib/",
    "home_github": "https://github.com/matplotlib/napari-matplotlib",
    "home_other": null,
    "summary": "A plugin to use Matplotlib with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "matplotlib",
      "napari >=0.5",
      "numpy >=1.23",
      "tinycss2",
      "napari[all] ; extra == 'docs'",
      "numpydoc ; extra == 'docs'",
      "pydantic <2 ; extra == 'docs'",
      "pydata-sphinx-theme ; extra == 'docs'",
      "sphinx ; extra == 'docs'",
      "sphinx-automodapi ; extra == 'docs'",
      "sphinx-gallery ; extra == 'docs'",
      "napari[pyqt6_experimental] >=0.4.18 ; extra == 'testing'",
      "pooch ; extra == 'testing'",
      "pyqt6 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-mock ; extra == 'testing'",
      "pytest-mpl ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "pytest-xvfb ; (sys_platform == \"linux\") and extra == 'testing'"
    ],
    "package_metadata_description": "# napari-matplotlib\n\n[![License](https://img.shields.io/pypi/l/napari-matplotlib.svg?color=green)](https://github.com/matplotlib/napari-matplotlib/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-matplotlib.svg?color=green)](https://pypi.org/project/napari-matplotlib)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-matplotlib.svg?color=green)](https://python.org)\n[![tests](https://github.com/matplotlib/napari-matplotlib/workflows/tests/badge.svg)](https://github.com/matplotlib/napari-matplotlib/actions)\n[![codecov](https://codecov.io/gh/matplotlib/napari-matplotlib/branch/main/graph/badge.svg)](https://codecov.io/gh/matplotlib/napari-matplotlib)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/matplotlib/pytest-mpl/master.svg)](https://results.pre-commit.ci/latest/github/matplotlib/pytest-mpl/master)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-matplotlib)](https://napari-hub.org/plugins/napari-matplotlib)\n\nA plugin to create Matplotlib plots from napari layers\n\n----------------------------------\n\n## Introduction\n`napari-matplotlib` is a bridge between `napari` and `matplotlib`, making it easy to create publication quality `Matplotlib` plots based on the data loaded in `napari` layers.\n\nDocumentation can be found at https://napari-matplotlib.github.io/\n\n## Contributing\n\nContributions are very welcome! Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n`napari-matplotlib` is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n[file an issue]: https://github.com/dstansby/napari-matplotlib/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Histogram",
      "Scatter",
      "FeaturesScatter",
      "FeaturesHistogram",
      "1D slice"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-psf-analysis",
    "name": "napari_psf_analysis",
    "display_name": "napari_psf_analysis",
    "version": "1.1.4",
    "created_at": "2022-03-30",
    "modified_at": "2024-10-02",
    "authors": [
      "Tim-Oliver Buchholz"
    ],
    "author_emails": [
      "tim-oliver.buchholz@fmi.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-psf-analysis/",
    "home_github": "https://github.com/fmi-faim/napari-psf-analysis",
    "home_other": null,
    "summary": "A plugin to analyse point spread functions (PSFs).",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "bfio",
      "matplotlib<3.9",
      "matplotlib-scalebar",
      "napari",
      "numpy",
      "pandas",
      "scikit-image",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "tox; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-psf-analysis\n\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![PyPI](https://img.shields.io/pypi/v/napari-psf-analysis.svg?color=green)](https://pypi.org/project/napari-psf-analysis)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-psf-analysis.svg?color=green)](https://python.org)\n[![tests](https://github.com/fmi-faim/napari-psf-analysis/workflows/tests/badge.svg)](https://github.com/fmi-faim/napari-psf-analysis/actions)\n[![codecov](https://codecov.io/gh/fmi-faim/napari-psf-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/fmi-faim/napari-psf-analysis)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psf-analysis)](https://napari-hub.org/plugins/napari-psf-analysis)\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n---\n![application_screenshot](figs/napari-psf-analysis_demo.gif)\n<!-- start abstract -->\nA plugin to analyse point spread functions (PSFs) of optical systems.\n<!-- end abstract -->\n## Usage\n### Starting Point\nTo run a PSF analysis open an image of acquired beads. Add a point-layer\nand indicate the beads you want to measure by adding a point.\n\n### Run Analysis\nOpen the plugin (Plugins > napari-psf-analysis > PSF Analysis) and ensure\nthat your bead image and point layers are select in the `Basic` tab under\n`Image` and `Points` respectively.\nIn the `Advanced` tab further information can be provided. Only the filled\nin fields of the `Advanced` tab are saved in the output.\n\nAfter verifying all input fields click `Extract PSFs`.\n\n### Discard and Save Measurement\nOnce the PSF extraction has finished a new layer (`Analyzed Beads`) appears,\nholding a summary\nimage for every selected bead.\nIndividual summaries can be discarded by clicking the `Delete Displayed\nMeasurement` button.\n\nResults are saved to the selected `Save Dir` by clicking the `Save\nMeasurements` button.\n\nNote: Beads for which the bounding box does not fit within the image are\nautomatically excluded from the analysis and no output is generated.\n\n\n### Saved Data\nEvery image of the `Analyzed Beads` layer is saved as `{source_image_name}_X\n{bead-centroid-x}_Y{bead-centroid-y}_Z{bead-centroid-z}.png` file.\nAdditionally a `PSFMeasurement_{source_image_acquisition_date}_\n{source_image_name}_{microscope_name}_{magnification}_{NA}.csv` file is\nstored containing the measured values and all filled in fields.\n\n---\n<!-- start install -->\n## Installation\nWe recommend installation into a fresh conda environment.\n\n### 1. Install napari\n```shell\nconda create -y -n psf-analysis -c conda-forge python=3.9\n\nconda activate psf-analysis\n\nconda install -c conda-forge napari pyqt\n```\n\n### 2. Install napari-aicsimageio and bioformats\nRequired if you want to open other files than `.tif` e.g. `.stk. `.\n\n__Note:__ See [napari-aicsimageio](https://www.napari-hub.org/plugins/napari-aicsimageio) for more information about opening images.\n```shell\nconda install -c conda-forge openjdk bioformats_jar \"aicsimageio[all]\" napari-aicsimageio\n\nconda deactivate\nconda activate psf-analysis\n```\n\n### 3. Install napari-psf-analysis\nYou can install `napari-psf-analysis` via [pip]:\n\n```shell\npython -m pip install xmlschema\npython -m pip install napari-psf-analysis\n```\n\n### 4. Optional `Set Config`\nYou can provide a config yaml file with the available microscope names and a default save directory.\nThis will change the `Microscope` text field to a drop down menu and change the default save directory.\n\n`example_config.yaml`\n```yaml\nmicroscopes:\n  - TIRF\n  - Zeiss Z1\noutput_path: \"D:\\\\psf_analysis\\\\measurements\"\n```\n\nTo use this config navigate to `Plugins > napari-psf-analysis > Set Config` and select the config file.\n\n__Note:__ The save path is OS specific.\n\n<!-- end install -->\n### 5. Desktop Icon for Windows\nFollow [these instructions](https://twitter.com/haesleinhuepf/status/1537030855843094529) by Robert Haase.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-psf-analysis\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/fmi-faim/napari-psf-analysis/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Set Config",
      "PSF Analysis"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tomotwin",
    "name": "napari-tomotwin",
    "display_name": "TomoTwin",
    "version": "0.4.1",
    "created_at": "2023-03-24",
    "modified_at": "2024-10-02",
    "authors": [
      "Thorsten Wagner"
    ],
    "author_emails": [
      "twa1@posteo.de"
    ],
    "license": "MPL-2.0",
    "home_pypi": "https://pypi.org/project/napari-tomotwin/",
    "home_github": "https://github.com/MPI-Dortmund/napari-tomotwin",
    "home_other": null,
    "summary": "Several tools for the work with TomoTwin",
    "categories": [],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "matplotlib",
      "scipy",
      "napari-clusters-plotter>=0.7.2",
      "magicgui",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-tomotwin\n\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-tomotwin.svg?color=green)](https://github.com/MPI-Dortmund/napari-tomotwin/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tomotwin.svg?color=green)](https://pypi.org/project/napari-tomotwin)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tomotwin.svg?color=green)](https://python.org)\n\nSeveral tools for the work with TomoTwin :-)\n\n\n## Installation\n\nYou can install `napari-tomotwin` via [pip]:\n\n    pip install napari-tomotwin\n\n\n## License\n\nDistributed under the terms of the [Mozilla Public License 2.0] license,\n\"napari-tomotwin\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TomoTwin clustering workflow"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-micromanager",
    "name": "napari-micromanager",
    "display_name": "napari-micromanager",
    "version": "0.1.3",
    "created_at": "2021-08-15",
    "modified_at": "2024-09-26",
    "authors": [
      "Federico Gasparoli",
      "Talley Lambert"
    ],
    "author_emails": [
      "Federico Gasparoli <federico.gasparoli@gmail.com>",
      "Talley Lambert <talley.lambert@gmail.com>"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-micromanager/",
    "home_github": "https://github.com/pymmcore-plus/napari-micromanager",
    "home_other": null,
    "summary": "Micro-Manager GUI interface in napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "fonticon-materialdesignicons6",
      "napari>=0.4.13",
      "pymmcore-plus>=0.9.3",
      "pymmcore-widgets>=0.7.0rc1",
      "superqt>=0.5.1",
      "tifffile",
      "useq-schema>=0.4.1",
      "zarr",
      "mda-simulator; extra == 'dev'",
      "mypy; extra == 'dev'",
      "pre-commit; extra == 'dev'",
      "ruff; extra == 'dev'",
      "mkdocs-material; extra == 'docs'",
      "mkdocstrings-python; extra == 'docs'",
      "pyqt5; extra == 'pyqt5'",
      "pyqt6; extra == 'pyqt6'",
      "pyside2; extra == 'pyside2'",
      "pyside6; extra == 'pyside6'",
      "pytest; extra == 'test'",
      "pytest-cov; extra == 'test'",
      "pytest-qt; extra == 'test'"
    ],
    "package_metadata_description": "# napari-micromanager\n\n[![License](https://img.shields.io/pypi/l/napari-micromanager.svg?color=green)](https://github.com/napari/napari-micromanager/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-micromanager.svg?color=green)](https://pypi.org/project/napari-micromanager)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-micromanager.svg?color=green)](https://python.org)\n[![Tests](https://github.com/pymmcore-plus/napari-micromanager/actions/workflows/ci.yml/badge.svg)](https://github.com/pymmcore-plus/napari-micromanager/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/pymmcore-plus/napari-micromanager/branch/main/graph/badge.svg?token=tf6lYDWV1s)](https://codecov.io/gh/pymmcore-plus/napari-micromanager)\n\nGUI interface between napari and micromanager powered by [pymmcore-plus](https://pymmcore-plus.github.io/pymmcore-plus/) and [pymmcore-widgets](https://pymmcore-plus.github.io/pymmcore-widgets/)\n\n----------------------------------\n<img width=\"1840\" alt=\"napari-micromanager\" src=\"https://github.com/pymmcore-plus/napari-micromanager/assets/1609449/e1f395cd-2d57-488e-89e2-b1923310fc2a\">\n\n## Installation\n\nYou can install `napari-micromanager` via [pip]:\n\n    pip install napari-micromanager\n\nYou will also need a Qt backend such as PySide2/6, or PyQt5/6.  **PyQt is\npreferred and receives more testing**. If you've previously installed napari\ninto this environment with `pip install napari[all]`, then you will likely\nalready have it. If not, you will also need to install a Qt backend of your\nchoice:\n\n    pip install pyqt5  # or any of {pyqt5, pyqt6, pyside2, pyside6}\n\n### Getting micromanager adapters\n\nThe easiest way to get the micromanager adapters is to use:\n\n```\nmmcore install\n```\n\nthis will install micromanager to the pymmcore_plus folder in your site-package; use this to see where:\n\n```\npython -c \"from pymmcore_plus import find_micromanager; print(find_micromanager())\"\n```\n\nalternatively, you can direct pymmcore_plus to your own micromanager installation with the `MICROMANAGER_PATH`\nenvironment variable:\n\n```\nexport MICROMANAGER_PATH='/path/to/Micro-Manager-...'\n```\n\n## Contributing\n\nContributions are very welcome.\n\n### Launching napari with plugin\n\nYou can launch napari and automatically load this plugin using the `launch-dev.py` script:\n\n```bash\npython launch-dev.py\n```\n\nAlternatively you can run:\n\n```bash\nnapari -w napari-micromanager\n```\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-micromanager\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[file an issue]: https://github.com/pymmcore-plus/napari-micromanager/issues\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Main Window"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spatial-correlation-plotter",
    "name": "napari-spatial-correlation-plotter",
    "display_name": "Spatial Correlation Plotter",
    "version": "0.0.3",
    "created_at": "2024-08-13",
    "modified_at": "2024-09-26",
    "authors": [
      "Jules Vanaret"
    ],
    "author_emails": [
      "jules.vanaret@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-spatial-correlation-plotter/",
    "home_github": "https://github.com/jules-vanaret/napari-spatial-correlation-plotter",
    "home_other": null,
    "summary": "A plugin to compute and display spatial correlation histograms in Napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "matplotlib",
      "scikit-image",
      "qtpy",
      "pyclesperanto-prototype",
      "tapenade",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# :herb: napari-spatial-correlation-plotter\n\n[![License MIT](https://img.shields.io/pypi/l/napari-spatial-correlation-plotter.svg?color=green)](https://github.com/jules-vanaret/napari-spatial-correlation-plotter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spatial-correlation-plotter.svg?color=green)](https://pypi.org/project/napari-spatial-correlation-plotter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spatial-correlation-plotter.svg?color=green)](https://python.org)\n[![tests](https://github.com/jules-vanaret/napari-spatial-correlation-plotter/workflows/tests/badge.svg)](https://github.com/jules-vanaret/napari-spatial-correlation-plotter/actions)\n[![codecov](https://codecov.io/gh/jules-vanaret/napari-spatial-correlation-plotter/branch/main/graph/badge.svg)](https://codecov.io/gh/jules-vanaret/napari-spatial-correlation-plotter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatial-correlation-plotter)](https://napari-hub.org/plugins/napari-spatial-correlation-plotter)\n\n<img src=\"https://github.com/GuignardLab/tapenade/blob/Packaging/imgs/tapenade3.png\" width=\"100\">\n\nA plugin to dynamically interact with the spatial correlation heatmap obtained by comparing two continuous fields of biophysical properties in 3D tissues.\n\nIf you use this plugin for your research, please [cite us](https://github.com/GuignardLab/tapenade/blob/main/README.md#how-to-cite).\n\n`napari-spatial-correlation-plotter` is a [napari] plugin that is part of the [Tapenade](https://github.com/GuignardLab/tapenade) project. Tapenade is a tool for the analysis of dense 3D tissues acquired with deep imaging microscopy. It is designed to be user-friendly and to provide a comprehensive analysis of the data.\n\n## Overview\n\nWhile working with large and dense 3D and 3D+time gastruloid datasets, we found that being able to visualise and interact with the data dynamically greatly helped processing it.\nDuring the pre-processing stage, dynamical exploration and interaction led to faster tuning of the parameters by allowing direct visual feedback, and gave key biophysical insight during the analysis stage.\n\nThis plugins allows the user to analyse the spatial correlations of two 3D fields loaded in Napari (e.g two fluorescent markers). The user can dynamically vary the analysis length scale, which corresponds to the standard deviation of the Gaussian kernel used for smoothing the 3D fields. \nIf a layer of segmented nuclei instances is additionally specified, the histogram is constructed by binning values at the nuclei level (each point corresponds to an individual nucleus). Otherwise, individual voxel values are used.\nThe user can dynamically interact with the correlation heatmap by manually selecting a region in the plot. The corresponding cells (or voxels) that contributed to the region's statistics will be displayed in 3D on an independant Napari layer for the user to interact with and gain biological insight.\n\n<img src=\"imgs/Fig_Napari_correlation.png\">\n\n## Installation\n\nThe plugin obviously requires [napari] to run. If you don't have it yet, follow the instructions [here](https://napari.org/stable/tutorials/fundamentals/installation.html).\n\nThe simplest way to install `napari-spatial-correlation-plotter` is via the [napari] plugin manager. Open Napari, go to `Plugins > Install/Uninstall Packages...` and search for `napari-spatial-correlation-plotter`. Click on the install button and you are ready to go!\n\nYou can also install `napari-spatial-correlation-plotter` via [pip]:\n\n    pip install napari-spatial-correlation-plotter\n\nTo install latest development version :\n\n    pip install git+https://github.com/jules-vanaret/napari-spatial-correlation-plotter.git\n\n## Usage\n\n<img src=\"imgs/corr_0.png\">\n\nSteps:\n1. First, load your images (and optionally mask and labels) in Napari. You can drag and drop them from your file explorer to the Napari viewer, or open them using the `File > Open files...` menu.\n2. Click on the `Plugins > Spatial Correlation Plotter` menu to open the plugin.\n3. Select the first layer you want to study from the combo box `Quantity X`.\n4. Select the second layer you want to study from the combo box `Quantity Y`. In this example, labels have loaded in step 1. Labels layers can be chosen as `Quantity X` or `Quantity Y` so that the quantity to study is the object instance density (in this example, the labels come from nuclei segmentation, so this leads to studying the nuclei density, or equivalently the cell density) or instance volume fraction.\n5. Optionally, a mask layer (with boolean values, 0 for outside, 1 for inside) can be selected to restrict the analysis to a specific region of the image.\n6. Optionally, a labels layer can be selected so that the histogram is constructed by binning values obtained by averaging the two fields in the segegmented instances (in this case, the quantitites will be averaged inside nuclei).\n7. Use the `Blur sigma` slider to vary the length scale of the analysis. This corresponds to the standard deviation of the Gaussian kernel used for masked gaussian smoothing the 3D fields. If set to 0, no smoothing is applied, which can be useful to study the raw data or if the quantities are already coming from smoothed data.\n8. Click on the `Compute correlation heatmap` button to compute and plot the correlation heatmap.\n9. If the image does not properly fit in the window (e.g if the borders are cut), you can use the `Configure subplots > Tight layout` button to adjust the plot size.\n10. You can adjust the histogram binning by changing the `Heatmap bins` sliders. If the histogram range is too large (e.g due to outliers), you can adjust the `Percentiles` sliders to focus on the most relevant part of the histogram. You can also check options to (i) `Show individual cells` to display the individual points that compose the histogram as an additional scatter, (ii) `Show linear fit` to display the linear fit of the histogram, (iii) `Display quadrants` to display vertical and horizontal lines that divide the histogram in 4 quadrants. The lines are placed at the median of the histogram values in X and Y. For instance, once checked, these options lead to the following plot:\n\n<img src=\"imgs/corr_1.png\" width=280>\n\n11. You can click on the `Save the figure` button to save the current plot (many formats are available, including .png, .svg, .jpeg).\n12. You can interact with the plot by manually drawing a region of interest directly on the plot to automatically select and display the corresponding cells in 3D on an independant Napari Labels layer `clustered labels`. By using a left click, you can draw an arbitrary shape. By using a right click, you can draw a rectangle. If `Shift` is pressed while drawing, several groups of cells can appear on the `clustered labels` layer, each with a different color.\n13. To trigger the grid view like in the example image, you can click on the `Toggle grid mode` button. This will display all layers in a grid view. By right clicking the button, you can parametrize the grid view (e.g number of columns, number of rows, etc).\n14. You can switch between 2D and 3D view at all time by clicking on the `Toggle 2D/3D view` button (it resembles a square when in 2D mode, or a cube when in 3D mode).\n\n## Acknowledgements\n\nThe \"napari-clusters-plotter\" plugin [1] heavily inspired this plugin, most notably the `SelectFromCollection`, `MplCanvas` and `FigureToolbar` classes. The `PlotterWidget` class has been modified for the specific use case of this plugin, but the core functionalites have been adapted directly.\nnapari-clusters-plotter source code is available [here](https://github.com/BiAPoL/napari-clusters-plotter/tree/main).\n\n\n[1] Zigutyte, L., Savill, R., M√ºller, J., Zoccoler, M., Wagner, T., & Haase, R. (2023). napari-clusters-plotter. Zenodo. https://doi.org/10.5281/zenodo.5884657\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-spatial-correlation-plotter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jules-vanaret/napari-spatial-correlation-plotter/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Spatial Correlation Plotter"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cardio-bio-eval",
    "name": "napari-cardio-bio-eval",
    "display_name": "Cardio Biosensor Evaluation",
    "version": "0.1.5",
    "created_at": "2024-08-15",
    "modified_at": "2024-09-24",
    "authors": [
      "Nanobiosensorics"
    ],
    "author_emails": [
      "horvath.robert@energia.mta.hu"
    ],
    "license": "Copyright (c) 2024, Nanobiosen...",
    "home_pypi": "https://pypi.org/project/napari-cardio-bio-eval/",
    "home_github": null,
    "home_other": "None",
    "summary": "The evaluation of the epic cardio biosensor integrated into napari",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "matplotlib",
      "opencv-python-headless",
      "openpyxl",
      "napari[pyqt5]",
      "torch",
      "numpy==1.26",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# Cardio biosensor evaluaton in Napari\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-cardio-bio-eval.svg?color=green)](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cardio-bio-eval.svg?color=green)](https://pypi.org/project/napari-cardio-bio-eval)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cardio-bio-eval.svg?color=green)](https://python.org)\n[![tests](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/workflows/tests/badge.svg)](https://github.com//Nanobiosensorics/napari-cardio-bio-eval/actions)\n<!--[![codecov](https://codecov.io/gh/Nanobiosensorics/napari-cardio-bio-eval/branch/main/graph/badge.svg)](https://codecov.io/gh/Nanobiosensorics/napari-cardio-bio-eval)-->\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cardio-bio-eval)](https://napari-hub.org/plugins/napari-cardio-bio-eval)\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n----------------------------------\n\nThe plugin provides a widget which can load, preprocess, annotate and export cardio biosensor data.  \n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-cardio-bio-eval` via [pip]:\n\n    pip install napari-cardio-bio-eval\n\nOr use the Napari plugin manager and search for `napari-cardio-bio-eval`.\n\n<!--\nFirst install a fresh conda enviroment (or other python enviroment) and activate it:\n\n    conda create -y -n napari-env -c conda-forge python=3.10\n    conda activate napari-env\n\nThen you can pip install the plugin from the github repository and it will also downloads the necessary packages:\n\n    pip install git+https://github.com/Nanobiosensorics/napari-cardio-bio-eval\n\nThen you can start napari with a simple command:\n\n    napari\n-->\n# Usage\n\nYou can open the plugin's widgets from the **Plugins** menu after the installation of the plugin.\n\n![image](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/assets/78443646/5d209fb5-c921-45d6-bb63-c5e3ff1fb1f8)\n\n## Data loading and preprocessing\n\nAt the top of the widget, you need to select the directory, which contains the data you want to examine and process. To successfully load the data the directory have to contain the following files:\n- *_wl_power.file: which contains the starting values of the measurement\n- DRM directory: which contains the difference from the previous measurement point \n- *_avg.file: which contains additional biosensor data\n\n#### Import parameters:  \n- Flipping: horizontal and vertical mirroring of the biosensor recording\n- Signal range type: with this you can choose how do you want to select a smaller range of the measurement in the next field *Ranges*\n    - measurement phase: you can give the index of the phases you want to see, for example with 0-1 you can view the measurement from the start to the first pause\n    - individual point: you can select any given frames in an interval, for example with selecting 34 and 275 you can view the measurement from frame 34 to frame 275\n- Ranges: if you choose measurement phase then give the range of the phases you want to see and if you choose individual point then select the starting and end frames. The label above helps as it shows the phrases (except the last one) and the full time of the measurement. The minimum frame or phase must be smaller than the maximum.\n- Drift correction threshold: Ranges between 25 and 500.\n- Filter method: mean or median\n\n![image](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/assets/78443646/28b5f563-1c5e-4591-bdf6-2ece936becac)\n\n![image](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/assets/78443646/a6004667-deac-4ff8-8729-0fcb8bc35f7f)\n\nAfter selecting the source directory and the optional fliping you can load in the data with the ***Load Data*** button. After the raw data is loaded you can select the slice of the measurement you want to work with and some other parameters. Then by clicking the ***Preprocess Data*** button you start the processing and after a few seconds the well images will appear on the viewer.  \n\n![image](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/assets/78443646/23e38c70-d058-41cc-9e9e-f2d34ea553e0)\n\nEach well has its own layer. You can turn the layers visible or invisible by clicking the small eye icon on each layer.\n\nIf the selected range is not what you wanted then you can change the parameters and preprocess again. But if you moved on to the next step (manual background selection or peak detection) then you need to restart Napari to load other data or preprocess with different parameters.\n\nAfter you see the wells you can proceed to the next step or if the automatic background correction is not good enough you can click the ***Select Background Points Manually*** button and it will show the automatically selected background points for each well, which you can move to better background coordinates and in the next peak detection step these points will be used by the background correction algorithm. After the first export these points will be saved so if the same directory is loaded a second time the preprocessing will use these points.\n\nDuring the background selection do NOT delete any layers.\n\n![image](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/assets/78443646/133fd74a-b53b-4540-bdf9-209c325e2b4b)\n\n## Selecting the cells\n\nIn this step you can also set some parameters for the peak detection algorithm and then click the ***Detect Signal Peaks*** button to start the process. After a few seconds the wells with the potential cells will show on the window.\n\n#### Detection parameters:  \n- Threshold range: 25-5000\n- Neighbourhood size: 1-10\n- Error mask filtering:\n\n![image](https://github.com/Nanobiosensorics/napari-cardio-bio-eval/assets/78443646/842ad2e3-32dd-4d6c-8e98-47c137a7029b)\n\nHere you can delete, add or move the points on each points layer. There are keyboard shortcuts for easier use! \nAdditionally by double clicking on any point of the image you can examine the time-signal diagram of the selected point under the widget. Be sure to select the correct layer!  \n\nIf you do not need any of the wells, then you can delete the layer and it won't be exported. If you either delete an image or peak points layer belonging to a well, the well will not be included in the export!  \nAfter selecting the needed cells and wells (and deleting the unnecessary ones) you can export plots and additional information about them.\n\n## Exporting\n\nYou can select what kind of data do you want to export and click the ***Export Data*** button. The data will be exported to the source directory into a *result* sub-directory.\n\n#### Export options:\n- Coordinates: the coordinates of the selected cells\n- Preprocessed signals: \n- Raw signals:\n- Average signal: \n- Breakdown signal: \n- Max well: \n- Plot signals with well:  \n- Plot well with coordinates: \n- Plot cells individually: \n- Signal parts by phases: \n- Max centered signals: \n\n## Segmentation widget\n\nThe data loading and preprocessing is the same but it uses a deep learning model to segment the cells.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-cardio-bio-eval\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please file an issue along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Cardio Bio Peak Detection Widget",
      "Cardio Bio Segmentation Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-generic-simulator",
    "name": "napari-generic-SIMulator",
    "display_name": "napari generic SIMulator",
    "version": "0.1.3",
    "created_at": "2022-06-30",
    "modified_at": "2024-09-23",
    "authors": [
      "Meizhu Liang"
    ],
    "author_emails": [
      "ml2618@ic.ac.uk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-generic-simulator/",
    "home_github": "https://github.com/Meizhu-Liang/napari-generic-SIMulator",
    "home_other": null,
    "summary": "A napari plugin to simulate raw-image stacks of Structured illumination microscopy (SIM).",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tifffile",
      "opt-einsum",
      "matplotlib",
      "pypcd-imp",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "tifffile; extra == \"testing\"",
      "pypcd-imp; extra == \"testing\"",
      "opt-einsum; extra == \"testing\"",
      "matplotlib; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-generic-SIMulator\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-generic-SIMulator.svg?color=green)](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-generic-SIMulator.svg?color=green)](https://pypi.org/project/napari-generic-SIMulator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-generic-SIMulator.svg?color=green)](https://python.org)\n[![tests](https://github.com/Meizhu-Liang/napari-generic-SIMulator/workflows/tests/badge.svg)](https://github.com/Meizhu-Liang/napari-generic-SIMulator/actions)\n[![codecov](https://codecov.io/gh/Meizhu-Liang/napari-generic-SIMulator/branch/main/graph/badge.svg)](https://codecov.io/gh/Meizhu-Liang/napari-generic-SIMulator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-generic-SIMulator)](https://napari-hub.org/plugins/napari-generic-SIMulator)\n\nA napari plugin to simulate raw-image stacks of Structured illumination microscopy (SIM). \n\nThe simulation is originally based on the paper <strong>GPU-accelerated real-time reconstruction in Python of three-dimensional datasets from structured illumination microscopy with hexagonal patterns</strong> by\nHai Gong, Wenjun Guo and Mark A. A. Neil (https://doi.org/10.1098/rsta.2020.0162). \n\nThe calculation can be GPU-accelerated if the CUPY (tested with cupy-cuda11x) is installed. In addition, the TORCH package can complete the acceleration both on CPU if TORCH is installed, and on GPU if TORCH is compiled with the CUDA (tested with torch v1.13.1+cu117) enabled.\n\nCurrently applies to:\n- conventional 2-beam SIM data with 3 angles and 3 phases\n- 3-beam hexagonal SIM data with 7 phases, as described in the paper\n- 3-beam hexagonal SIM data with 5 phases at right-angles\n- conventional 3-beam 3-D data with 3 angles and 5 phases\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-generic-SIMulator` via [pip]:\n\n    pip install napari-generic-SIMulator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Meizhu-Liang/napari-generic-SIMulator.git\n\nThis plugin is compatible with **napari 0.4.17** or above, older versions of napari would show errors in _interpolation_.\n\n## Usage\n\n1) Open napari and create the viewer.\n\n\n2) Launch two widgets: **Point cloud generator** and **SIM data generator** in ***Plugin***.\n\n   ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/lauch.png)\n\n   The two widgets can be tabbed together.\n   ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/2tabs.png)\n\n\n3) Choose the type and other parameters of point cloud as a sample in **Point cloud generator**.\n\n    ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/pc.png)\n\n    The point cloud can be displayed in three dimensions, and be saved and loaded as .pcd files.\n  \n    https://user-images.githubusercontent.com/74197598/227589232-9006842b-6706-48b7-9f2b-fe93c6698503.mp4\n\n\n4) Adjust parameters in SIM data generator to simulate a raw image stack.\n\n   Apart from basic parameters such as the refractive index, the wavelengths and so on, the z scanning can be either \n   **z drift**: the conventional SIM (imaging a raw stack at the same z-position) or **z step**: the drifting case in \n   the papaer mentioned above (imaging only one raw image at a z-position).\n\n\n   ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/raw_stack.png)\n\n   The parameters used in the simulation can be saved with the image stack by clicking **save tif with tags**. Tags (of current or of one stack dragged into napari viewer) can be printed in Python by **print tags**. \n\n\n5) Three-dimensional point spread function (**PSF**), optical transfer function (**OTF**) and **illumination** patterns applied in the simulation can be showed by buttons. Note the all of these correspond the generated raw-image stack, so keep the parameters the same before showing the **PSF** (or **OTF** and **illumination**).\n\n    https://user-images.githubusercontent.com/74197598/227588321-ad3c8f17-1c61-4079-9e34-9b1f990714c1.mp4\n    \n    https://user-images.githubusercontent.com/74197598/227586957-b76ad56e-44d5-4d9b-a1cc-2cfd08ca5400.mp4\n    \n    https://user-images.githubusercontent.com/74197598/227585827-64531265-b4fb-48a9-9698-7f263f22d718.mp4 \n   \n6) The raw image stacks can be then processed by napari-sim-processor (https://www.napari-hub.org/plugins/napari-sim-processor).\n   \n   ![raw](https://github.com/Meizhu-Liang/napari-generic-SIMulator/raw/main/images/processor.png)\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-generic-SIMulator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Meizhu-Liang/napari-generic-SIMulator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Point cloud generator",
      "SIM data generator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "manini",
    "name": "manini",
    "display_name": "Manini",
    "version": "0.0.11",
    "created_at": "2023-11-04",
    "modified_at": "2024-09-18",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/manini/",
    "home_github": "https://github.com/hereariim/manini",
    "home_other": null,
    "summary": "An user-friendly plugin that enables to annotate images from a pre-trained model (segmentation, classification, detection) given by an user.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "scikit-image",
      "pandas",
      "opencv-python-headless",
      "tensorflow",
      "PyQt5",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\"",
      "pytest-xvfb; extra == \"testing\"",
      "numpy; extra == \"testing\"",
      "magicgui; extra == \"testing\"",
      "qtpy; extra == \"testing\"",
      "scikit-image; extra == \"testing\"",
      "pandas; extra == \"testing\"",
      "opencv-python-headless; extra == \"testing\"",
      "tensorflow; extra == \"testing\""
    ],
    "package_metadata_description": "# manini\n\n[![License BSD-3](https://img.shields.io/pypi/l/manini.svg?color=green)](https://github.com/hereariim/manini/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/manini.svg?color=green)](https://pypi.org/project/manini)\n[![Python Version](https://img.shields.io/pypi/pyversions/manini.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/manini/workflows/tests/badge.svg)](https://github.com/hereariim/manini/actions)\n[![codecov](https://codecov.io/gh/hereariim/manini/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/manini)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/manini)](https://napari-hub.org/plugins/manini)\n\nManini (**MA**chi**N**e **IN**ference  & Correct**I**on) is thought as a tool to boost the collaborative contribution of end-users to the assessment of deep learning model during their testing phase.\nIt is a user-Friendly plugin that enables to manually correct the result of an inference of deep learning model by an end-user. The plugin covers the following informational tasks: segmentation, classification and object detection.\n\n## White paper\n\nHerearii Metuarea, David Rousseau. [Toward more collaborative deep learning project management in plant phenotyping. ](https://essopenarchive.org/doi/full/10.22541/essoar.169876925.51005273/v1)\n\nESS Open Archive . October 31, 2023.\nDOI: 10.22541/essoar.169876925.51005273/v1\n\n----------------------------------\n\nThis plugin was written by Herearii Metuarea, PHENET engineer at LARIS (French laboratory located in Angers, France) in Imhorphen team (bioimaging research group lead) under the supervision by David Rousseau (Full professor). This plugin was designed in the context of the european project INVITE and PHENET.\n\n![Screenshot from 2023-11-13 00-13-13](https://github.com/hereariim/manini/assets/93375163/c602e802-71b9-48ec-a9f2-cec3e4fa8220)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html!\n\n-->\n\n## Installation\n\nYou can install `manini` via [pip]:\n\n    pip install manini\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/manini.git\n\n\n## Description\n\nThis plugin is a tool to perform image inference. The inference is open to the model for image segmentation (binary or multiclass), image classification and object detection. The dimension of image should be the same size with the input of model.\nCurrently compatible with tensorflow h5 models and torch torchscript models. In this format, the model file must contain all the elements of the model (architecture, weights, etc). Several ongoing developments, feel free to contact us if you have some request.\n\n## Contact\n\nImhorphen team, bioimaging research group\n\n42 rue George Morel, Angers, France\n\n- Pr David Rousseau, david.rousseau@univ-angers.fr\n- Herearii Metuarea, herearii.metuarea@univ-angers.fr \n\n### Scheme\n\n![manini](https://github.com/hereariim/manini/assets/93375163/636a5e15-da0f-4387-8f37-b8ca89b4482b)\n\n#### Input\n\nThe user must deposit two items (+1 optional item). \n\n- A compressed file (.zip) containing the images in RGB\n\n```\n.\n‚îî‚îÄ‚îÄ input.zip\n    ‚îú‚îÄ‚îÄ im_1.JPG\n    ‚îú‚îÄ‚îÄ im_2.JPG \n    ‚îú‚îÄ‚îÄ im_3.JPG\n    ...\n    ‚îî‚îÄ‚îÄ im_n.JPG\n```\n\n- A model file (.h5 , pt or torchscript) which is the segmentation model\n- A text file (.txt) containing the names of the classes (optional)\n\nThe Ok button is used to validate the imported elements. The Run button is used to launch the segmentation.\n\n#### Process\n\nCorrection is made by selecting some classes displayed in a widget :\n\n- Paint panel for image segmentation\n\n- Table for image classification\n\n- Bounding box panel for object detection\n\n#### Output\n\n##### Segmentation + Detection\n\nThe plugin suggest 'Export' widget. When user select image and mask, the Save button allows you to obtain data in a compressed file. This file contains folders containing the images and their mask.\n\n##### Classification\n\nThe Save button allows you to obtain a csv file. This file is the table on which the user had made his modifications.\n\n#### Tutorial\n\nPlease, you can learn better if you watch a video tutorial below.\n\nPresentation video of the context where the plugin was developped : [MANINI Napari Plugin Part 1](https://www.youtube.com/watch?v=ltbMIhApwRk)\n\nTutorial video to get started : [MANINI Napari Plugin Part 2](https://www.youtube.com/watch?v=HU21VQpvRAM)\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"manini\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/manini/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Manini",
      "Export"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-save-transformed",
    "name": "napari-save-transformed",
    "display_name": "napari-save-transformed",
    "version": "0.0.2",
    "created_at": "2024-07-02",
    "modified_at": "2024-09-11",
    "authors": [
      "Jan Eglinger"
    ],
    "author_emails": [
      "Jan Eglinger <jan.eglinger@fmi.ch>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-save-transformed/",
    "home_github": "https://github.com/fmi-faim/napari-save-transformed",
    "home_other": null,
    "summary": "Napari plugin to save layers with their transforms applied.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari"
    ],
    "package_metadata_description": "# napari-save-transformed\n\n[![PyPI - Version](https://img.shields.io/pypi/v/napari-save-transformed.svg)](https://pypi.org/project/napari-save-transformed)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/napari-save-transformed.svg)](https://pypi.org/project/napari-save-transformed)\n\n-----\n\n## Table of Contents\n\n- [Installation](#installation)\n- [License](#license)\n\n## Installation\n\n```console\npip install napari-save-transformed\n```\n\n## License\n\n`napari-save-transformed` is distributed under the terms of the [BSD-3-Clause](https://spdx.org/licenses/BSD-3-Clause.html) license.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".tif"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cellpose-napari",
    "name": "cellpose-napari",
    "display_name": "cellpose-napari",
    "version": "0.2.0",
    "created_at": "2021-04-28",
    "modified_at": "2024-08-26",
    "authors": [
      "Carsen Stringer"
    ],
    "author_emails": [
      "stringerc@janelia.hhmi.org"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/cellpose-napari/",
    "home_github": "https://github.com/Mouseland/cellpose-napari",
    "home_other": null,
    "summary": "a generalist algorithm for anatomical segmentation",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine>=0.1.4",
      "cellpose>0.6.3",
      "imagecodecs",
      "sphinx>=3.0; extra == \"docs\"",
      "sphinxcontrib-apidoc; extra == \"docs\"",
      "sphinx-rtd-theme; extra == \"docs\"",
      "sphinx-prompt; extra == \"docs\"",
      "sphinx-autodoc-typehints; extra == \"docs\"",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# cellpose-napari <img src=\"docs/_static/favicon.ico\" width=\"50\" title=\"cellpose\" alt=\"cellpose\" align=\"right\" vspace = \"50\">\n\n[![Documentation Status](https://readthedocs.org/projects/cellpose-napari/badge/?version=latest)](https://cellpose-napari.readthedocs.io/en/latest/?badge=latest)\n[![tests](https://github.com/mouseland/cellpose-napari/workflows/tests/badge.svg)](https://github.com/mouseland/cellpose-napari/actions)\n[![codecov](https://codecov.io/gh/Mouseland/cellpose-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/MouseLand/cellpose-napari)\n[![PyPI version](https://badge.fury.io/py/cellpose-napari.svg)](https://badge.fury.io/py/cellpose-napari)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/cellpose-napari)](https://pypistats.org/packages/cellpose-napari)\n[![Python version](https://img.shields.io/pypi/pyversions/cellpose-napari)](https://pypistats.org/packages/cellpose-napari)\n[![License](https://img.shields.io/pypi/l/cellpose-napari.svg?color=green)](https://github.com/mouseland/cellpose-napari/raw/master/LICENSE)\n[![Contributors](https://img.shields.io/github/contributors-anon/MouseLand/cellpose-napari)](https://github.com/MouseLand/cellpose-napari/graphs/contributors)\n[![website](https://img.shields.io/website?url=https%3A%2F%2Fwww.cellpose.org)](https://www.cellpose.org)\n[![GitHub stars](https://img.shields.io/github/stars/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)\n[![GitHub forks](https://img.shields.io/github/forks/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)\n\na napari plugin for anatomical segmentation of general cellular images\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\nThe plugin code was written by Carsen Stringer, and the cellpose code was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose, read the [**paper**](https://t.co/kBMXmPp3Yn?amp=1) or watch this [**talk**](https://t.co/JChCsTD0SK?amp=1). \n\nFor support with the plugin, please open an [issue](https://github.com/MouseLand/cellpose-napari/issues). For support with cellpose, please open an [issue](https://github.com/MouseLand/cellpose/issues) on the cellpose repo. \n\n\nIf you use this plugin please cite the [paper](https://www.nature.com/articles/s41592-020-01018-x):\n::\n    \n      @article{stringer2021cellpose,\n      title={Cellpose: a generalist algorithm for cellular segmentation},\n      author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},\n      journal={Nature Methods},\n      volume={18},\n      number={1},\n      pages={100--106},\n      year={2021},\n      publisher={Nature Publishing Group}\n      }\n\n\n![cellpose-napari_plugin](https://cellpose-napari.readthedocs.io/en/latest/_images/napari_main_demo_fast_small.gif?raw=true \"cellpose-napari\")\n\n## Installation\n\nInstall an [Anaconda](https://www.anaconda.com/download/) distribution of Python -- Choose **Python 3** and your operating system. Note you might need to use an anaconda prompt if you did not add anaconda to the path.\n\nInstall `napari` with pip: `pip install napari[all]`. Then install `cellpose-napari` via [pip]:\n\n    pip install cellpose-napari\n    \n Or install the plugin inside napari in the plugin window.\n\nIf install fails in your base environment, create a new environment:\n1. Download the [`environment.yml`](https://github.com/MouseLand/cellpose-napari/blob/master/environment.yml?raw=true) file from the repository. You can do this by cloning the repository, or copy-pasting the text from the file into a text document on your local computer.\n2. Open an anaconda prompt / command prompt with `conda` for **python 3** in the path\n3. Change directories to where the `environment.yml` is and run `conda env create -f environment.yml`\n4. To activate this new environment, run `conda activate cellpose_napari`\n5. You should see `(cellpose_napari)` on the left side of the terminal line. \n\nIf you have **issues** with cellpose installation, see the [cellpose docs](https://cellpose.readthedocs.io/en/latest/installation.html) for more details, and then if the suggestions fail, open an issue.\n\n### Upgrading software\n\nYou can upgrade the plugin with\n~~~\npip install cellpose-napari --upgrade\n~~~\n\nand you can upgrade cellpose with\n~~~\npip install cellpose --upgrade\n~~~\n\n### GPU version (CUDA) on Windows or Linux\n\nIf you plan on running many images, you may want to install a GPU version of *torch* (if it isn't already installed).\n\nBefore installing the GPU version, remove the CPU version:\n~~~\npip uninstall torch\n~~~\n\nFollow the instructions [here](https://pytorch.org/get-started/locally/) to determine what version to install. The Anaconda install is recommended along with CUDA version 10.2. For instance this command will install the 10.2 version on Linux and Windows (note the `torchvision` and `torchaudio` commands are removed because cellpose doesn't require them):\n\n~~~\nconda install pytorch cudatoolkit=10.2 -c pytorch\n~~~~\n\nWhen upgrading GPU Cellpose in the future, you will want to ignore dependencies (to ensure that the pip version of torch does not install):\n~~~\npip install --no-deps cellpose --upgrade\n~~~\n\n### Installation of github version\n\nFollow steps from above to install the dependencies. In the github repository, run `pip install -e .` and the github version will be installed. If you want to go back to the pip version of cellpose-napari, then say `pip install cellpose-napari`.\n\n\n## Running the software\n\n\nOpen napari with the cellpose-napari dock widget open\n```\nnapari -w cellpose-napari\n```\n\nThere is sample data in the File menu, or get started with your own images!\n\n### Detailed usage [documentation](https://cellpose-napari.readthedocs.io/).\n\n## Contributing\n\nContributions are very welcome. Tests are run with pytest.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"cellpose-napari\" is free and open source software.\n\n## Dependencies\ncellpose-napari relies on the following excellent packages (which are automatically installed with conda/pip if missing):\n- [napari](https://napari.org)\n- [magicgui](https://napari.org/magicgui/)\n\ncellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing):\n- [torch](https://pytorch.org/)\n- [numpy](http://www.numpy.org/) (>=1.16.0)\n- [numba](http://numba.pydata.org/numba-doc/latest/user/5minguide.html)\n- [scipy](https://www.scipy.org/)\n- [natsort](https://natsort.readthedocs.io/en/master/)\n- [tifffile](https://pypi.org/project/tifffile/)\n- [opencv](https://opencv.org/)\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "cellpose"
    ],
    "contributions_sample_data": [
      "Cells (3D+2Ch)",
      "Cells 2D"
    ]
  },
  {
    "normalized_name": "napari-allencell-annotator",
    "name": "napari-allencell-annotator",
    "display_name": "napari-allencell-annotator",
    "version": "2.0.1",
    "created_at": "2022-07-27",
    "modified_at": "2024-08-21",
    "authors": [
      "Allen Institute for Cell Science"
    ],
    "author_emails": [],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-allencell-annotator/",
    "home_github": "https://github.com/aics-int/napari-allencell-annotator/",
    "home_other": null,
    "summary": "A plugin that enables annotations provided by Allen Institute for Cell Science",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari >=0.4.9",
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "xarray >=2022.6.0",
      "magicgui >=0.3.7",
      "aicspylibczi >=3.0.5",
      "fsspec >=2022.8.2",
      "bioformats-jar",
      "bfio",
      "qtpy",
      "bioio",
      "bioio-ome-tiff",
      "bioio-czi",
      "bioio-ome-zarr",
      "tifffile >=2021.8.30",
      "bioio-imageio",
      "napari >=0.4.9 ; extra == 'all'",
      "napari-plugin-engine >=0.1.4 ; extra == 'all'",
      "numpy ; extra == 'all'",
      "xarray >=2022.6.0 ; extra == 'all'",
      "magicgui >=0.3.7 ; extra == 'all'",
      "aicspylibczi >=3.0.5 ; extra == 'all'",
      "fsspec >=2022.8.2 ; extra == 'all'",
      "bioformats-jar ; extra == 'all'",
      "bfio ; extra == 'all'",
      "qtpy ; extra == 'all'",
      "bioio ; extra == 'all'",
      "bioio-ome-tiff ; extra == 'all'",
      "bioio-czi ; extra == 'all'",
      "bioio-ome-zarr ; extra == 'all'",
      "tifffile >=2021.8.30 ; extra == 'all'",
      "bioio-imageio ; extra == 'all'",
      "black >=19.10b0 ; extra == 'all'",
      "codecov >=2.0.22 ; extra == 'all'",
      "docutils <0.16,>=0.10 ; extra == 'all'",
      "flake8 >=3.7.7 ; extra == 'all'",
      "psutil >=5.7.0 ; extra == 'all'",
      "pytest >=4.3.0 ; extra == 'all'",
      "pytest-cov ==2.6.1 ; extra == 'all'",
      "pytest-raises >=0.10 ; extra == 'all'",
      "pytest-qt >=3.3.0 ; extra == 'all'",
      "quilt3 >=3.1.12 ; extra == 'all'",
      "pyqt5 ; extra == 'all'",
      "pytest-runner ; extra == 'all'",
      "bumpversion >=0.5.3 ; extra == 'all'",
      "gitchangelog >=3.0.4 ; extra == 'all'",
      "ipython >=7.5.0 ; extra == 'all'",
      "m2r >=0.2.1 ; extra == 'all'",
      "pytest-runner >=4.4 ; extra == 'all'",
      "Sphinx <3,>=2.0.0b1 ; extra == 'all'",
      "sphinx-rtd-theme >=0.1.2 ; extra == 'all'",
      "tox >=3.5.2 ; extra == 'all'",
      "twine >=1.13.0 ; extra == 'all'",
      "wheel >=0.33.1 ; extra == 'all'",
      "black >=19.10b0 ; extra == 'dev'",
      "bumpversion >=0.5.3 ; extra == 'dev'",
      "docutils <0.16,>=0.10 ; extra == 'dev'",
      "flake8 >=3.7.7 ; extra == 'dev'",
      "gitchangelog >=3.0.4 ; extra == 'dev'",
      "ipython >=7.5.0 ; extra == 'dev'",
      "m2r >=0.2.1 ; extra == 'dev'",
      "pytest >=4.3.0 ; extra == 'dev'",
      "pytest-cov ==2.6.1 ; extra == 'dev'",
      "pytest-raises >=0.10 ; extra == 'dev'",
      "pytest-runner >=4.4 ; extra == 'dev'",
      "pytest-qt >=3.3.0 ; extra == 'dev'",
      "quilt3 >=3.1.12 ; extra == 'dev'",
      "Sphinx <3,>=2.0.0b1 ; extra == 'dev'",
      "sphinx-rtd-theme >=0.1.2 ; extra == 'dev'",
      "tox >=3.5.2 ; extra == 'dev'",
      "twine >=1.13.0 ; extra == 'dev'",
      "wheel >=0.33.1 ; extra == 'dev'",
      "pytest-runner ; extra == 'setup'",
      "black >=19.10b0 ; extra == 'test'",
      "codecov >=2.0.22 ; extra == 'test'",
      "docutils <0.16,>=0.10 ; extra == 'test'",
      "flake8 >=3.7.7 ; extra == 'test'",
      "psutil >=5.7.0 ; extra == 'test'",
      "pytest >=4.3.0 ; extra == 'test'",
      "pytest-cov ==2.6.1 ; extra == 'test'",
      "pytest-raises >=0.10 ; extra == 'test'",
      "pytest-qt >=3.3.0 ; extra == 'test'",
      "quilt3 >=3.1.12 ; extra == 'test'",
      "pyqt5 ; extra == 'test'"
    ],
    "package_metadata_description": "# napari-allencell-annotator\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-allencell-annotator.svg?color=green)](https://github.com/bbridge0200/napari-allencell-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-allencell-annotator.svg?color=green)](https://pypi.org/project/napari-allencell-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/bbridge0200/napari-allencell-annotator/workflows/tests/badge.svg)](https://github.com/bbridge0200/napari-allencell-annotator/actions)\n[![codecov](https://codecov.io/gh/bbridge0200/napari-allencell-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/bbridge0200/napari-allencell-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-allencell-annotator)](https://napari-hub.org/plugins/napari-allencell-annotator)\n\nA plugin that enables image annotation/scoring and writes annotations to a .csv file. \nPlugin provided by the Allen Institute for Cell Science.\n\nThe Allen Cell Image Annotator plugin for napari provides an intuitive\ngraphical user interface to create annotation templates, annotate large \nimage sets using these templates, and save image annotations to a csv file. \nThe Allen Cell Image Annotator is a Python-based open source toolkit \ndeveloped at the Allen Institute for Cell Science for both blind, unbiased and un-blind \nmicroscope image annotating. This toolkit supports easy image set selection\nfrom a file finder and creation of annotation templates (text, checkbox, drop-down, spinbox, and point).\nWith napari's multi-dimensional image viewing capabilities, the plugin seamlessly allows users to\nview each image and write annotations into the custom template.\nAnnotation templates can be written to a json file for sharing or re-using. After annotating,\nthe annotation template, image file list, and the annotation values \nare conveniently saved to csv file, which can be re-opened for further annotating. \n\n-   Supports the following image types:\n    - `OME-TIFF`\n    - `TIFF`\n    - `CZI` \n    - `PNG` \n    - `JPEG`\n    - `OME-ZARR`\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to files up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation using Command Line\n### 1. Prerequisites\n\nThe plugin requires [Conda](https://docs.anaconda.com/anaconda/install/).\n- [Installing on Windows ](https://docs.anaconda.com/anaconda/install/windows/) \n  - Follow the steps linked above except\n  - On step 8, check top the box to add to PATH\n  - ![Alt text](napari_allencell_annotator/assets/windowsstep8.png)\n- [Installing on Mac ](https://docs.anaconda.com/anaconda/install/mac-os/) \n\n### 2. Install the plugin\nClick the link corresponding to your OS.\n#### [Windows](https://alleninstitute-my.sharepoint.com/:u:/g/personal/r_dhamrongsirivadh_alleninstitute_org/EexXIxeeIbNEs4KMjimcXOcBMn2J2QwxJhNEkOcRHC1eVg?e=JKa5WI)\n- From the link above, click the three dots on the top menu bar and select download. \n- Open a file explorer and go to the Downloads folder. Use **Option 1** below. A prompt window should open and start installing. If this fails use **Option 2**. \n  - **Option 1**: Double-click the file _install_napari.sh_\n  - **Option 2**: Search the file finder for Anaconda Prompt. Open version 3. Run the following commands one line at a time. \n    - conda create -n napari_annotator python=3.10 anaconda\n    - conda activate napari_annotator\n    - python -m pip install --upgrade pip\n    - python -m pip install \"napari[all]\"\n    - python -m pip install napari-allencell-annotator\n    - napari\n  - **Still not working?** Try using conda forge instead of pip. \n    - Ex: conda install -c conda-forge napari instead of python -m pip install \"napari[all]\"\n#### [MacOS/Unix](https://alleninstitute-my.sharepoint.com/:u:/g/personal/r_dhamrongsirivadh_alleninstitute_org/ESeAYWwWFuRFhgpqgbiKQ6QBXdU8Dg8OU9ilpJ5VmoY-cA?e=BHpReg)\n- From the link above, download the file. \n- Open terminal. \n- Run _chmod +x ./Downloads/install_napari.command_ \n  - If you get a file not found error try adjusting the path to match where install_napari.command was downloaded.\n- Open finder, navigate to the file, double-click _install_napari.command_ . \n  - A terminal window should open and start installing. \n  \n\n### 3. Launch the Plugin\n\nOnce the napari window opens, go to **Plugins**.\n- If **napari-allencell-annotator** is listed click it to launch. \n- If it is not listed \n- **Install/Uninstall Plugins** ‚á® check the box next to **napari-allencell-annotator** ‚á® **close** ‚á® **Plugins** ‚á® **napari-allencell-annotator** .\n\n### 4. Re-opening the Plugin After Installing\n- Windows\n  - Search for anaconda navigator in file finder\n  - Click on navigator version 3\n  - Once the navigator opens, click **Environments** on the left side\n  - Click on the annotator environment and wait for it to load\n  - Press the play button\n  - Type _napari_ in the prompt that opens\n  - Click **Plugins** ‚á® **napari-allencell-annotator**\n- MacOS\n  - Open terminal\n  - Run these commands one line at a time\n    - conda activate napari_annotator\n    - napari\n  - Click **Plugins** ‚á® **napari-allencell-annotator**\n\n## Installation from Napari Hub\nIf you have previously installed Napari on your machine, you can follow these steps to install the plugin from Napari Hub.\n\n### 1. Install the Plugin\n- Open Napari\n- Go to **Plugins** ‚á® **Install/Uninstall Plugins...**\n- Find **napari-allencell-annotator** in **Available Plugins**\n- Click **Install**\n- Close the window after the installation finishes\n\n### 2. Launch the Plugin\n- Click **Plugins** ‚á® **napari-allencell-annotator**\n  - You might have to restart Napari for the annotator to appear in the plugin list.\n  - If you still can't see the plugin, go to **Install/Uninstall Plugins** ‚á® check the box next to **napari-allencell-annotator**.\n\n## Quick Start\n\n1. Open napari\n2. Start the plugin \n   - Open napari, go to **Plugins** ‚á® **napari-allencell-annotator**.\n3. Create or import annotations and add images to annotate.\n\nFor more detailed usage instructions, check out this [document](napari_allencell_annotator/assets/AnnotatorInstructions.pdf) \n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-allencell-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bbridge0200/napari-allencell-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MainView"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "pycudadecon",
    "name": "pycudadecon",
    "display_name": "pyCUDAdecon",
    "version": "0.5.1",
    "created_at": "2022-08-10",
    "modified_at": "2024-08-15",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "Talley Lambert <talley.lambert@gmail.com>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/pycudadecon/",
    "home_github": "https://github.com/tlambert03/pycudadecon",
    "home_other": null,
    "summary": "Python wrapper for CUDA-accelerated 3D deconvolution",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "tifffile",
      "typing-extensions",
      "ipython; extra == 'dev'",
      "mypy; extra == 'dev'",
      "pdbpp; extra == 'dev'",
      "pre-commit; extra == 'dev'",
      "rich; extra == 'dev'",
      "ruff; extra == 'dev'",
      "furo==2022.9.29; extra == 'docs'",
      "ghp-import==2.1.0; extra == 'docs'",
      "jupyter-book==0.13.1; extra == 'docs'",
      "sphinx-autodoc-typehints==1.19.1; extra == 'docs'",
      "pytest-cov; extra == 'test'",
      "pytest>=6.0; extra == 'test'"
    ],
    "package_metadata_description": "# pyCUDAdecon\n\nThis package provides a python wrapper and convenience functions for\n[cudaDecon](https://github.com/scopetools/cudaDecon), which is a CUDA/C++\nimplementation of an accelerated Richardson Lucy Deconvolution\nalgorithm<sup>1</sup>.\n\n* CUDA accelerated deconvolution with a handful of artifact-reducing features.\n* radially averaged OTF generation with interpolation for voxel size\n  independence between PSF and data volumes\n* 3D deskew, rotation, general affine transformations\n* CUDA-based camera-correction for [sCMOS artifact correction](https://llspy.readthedocs.io/en/latest/camera.html)\n\n\n### Install\n\nThe conda package includes the required pre-compiled libraries for Windows and Linux. See GPU driver requirements [below](#gpu-requirements)\n\n```sh\nconda install -c conda-forge pycudadecon\n```\n\n*macOS is not supported*\n\n### üìñ   &nbsp; [Documentation](http://www.talleylambert.com/pycudadecon)\n\n\n### GPU requirements\n\nThis software requires a CUDA-compatible NVIDIA GPU. The underlying cudadecon\nlibraries have been compiled against different versions of the CUDA toolkit.\nThe required CUDA libraries are bundled in the conda distributions so you don't\nneed to install the CUDA toolkit separately.  If desired, you can pick which\nversion of CUDA you'd like based on your needs, but please note that different\nversions of the CUDA toolkit have different GPU driver requirements:\n\nTo specify a specific cudatoolkit version, install as follows (for instance, to\nuse `cudatoolkit=10.2`)\n\n```sh\nconda install -c conda-forge pycudadecon cudatoolkit=10.2\n```\n\n| CUDA | Linux driver | Win driver |\n| ---- | ------------ | ---------- |\n| 10.2 | ‚â• 440.33     | ‚â• 441.22   |\n| 11.0 | ‚â• 450.36.06  | ‚â• 451.22   |\n| 11.1 | ‚â• 455.23     | ‚â• 456.38   |\n| 11.2 | ‚â• 460.27.03  | ‚â• 460.82   |\n\n\nIf you run into trouble, feel free to [open an\nissue](https://github.com/tlambert03/pycudadecon/issues) and describe your\nsetup.\n\n\n## Usage\n\n\nThe [`pycudadecon.decon()`](https://www.talleylambert.com/pycudadecon/deconvolution.html#pycudadecon.decon) function is designed be able to handle most basic applications:\n\n```python\nfrom pycudadecon import decon\n\n# pass filenames of an image and a PSF\nresult = decon('/path/to/3D_image.tif', '/path/to/3D_psf.tif')\n\n# decon also accepts numpy arrays\nresult = decon(img_array, psf_array)\n\n# the image source can also be a sequence of arrays or paths\nresult = decon([img_array, '/path/to/3D_image.tif'], psf_array)\n\n# see docstrings for additional parameter options\n```\n\nFor finer-tuned control, you may wish to make an OTF file from your PSF using [`pycudadecon.make_otf()`](https://www.talleylambert.com/pycudadecon/otf.html#pycudadecon.make_otf), and then use the [`pycudadecon.RLContext`](https://www.talleylambert.com/pycudadecon/deconvolution.html#pycudadecon.RLContext) context manager to setup the GPU for use with the [`pycudadecon.rl_decon()`](https://www.talleylambert.com/pycudadecon/deconvolution.html#pycudadecon.rl_decon) function.  (Note all images processed in the same context must have the same input shape).\n\n```python\nfrom pycudadecon import RLContext, rl_decon\nfrom glob import glob\nimport tifffile\n\nimage_folder = '/path/to/some_images/'\nimlist = glob(image_folder + '*488*.tif')\notf_path = '/path/to/pregenerated_otf.tif'\n\nwith tifffile.TiffFile(imlist[0]) as tf:\n    imshape = tf.series[0].shape\n\nwith RLContext(imshape, otf_path, dz) as ctx:\n    for impath in imlist:\n        image = tifffile.imread(impath)\n        result = rl_decon(image, ctx.out_shape)\n        # do something with result...\n```\n\nIf you have a 3D PSF volume, the [`pycudadecon.TemporaryOTF`](https://www.talleylambert.com/pycudadecon/otf.html#pycudadecon.TemporaryOTF) context manager facilitates temporary OTF generation...\n\n```python\n # continuing with the variables from the previous example...\n psf_path = \"/path/to/psf_3D.tif\"\n with TemporaryOTF(psf) as otf:\n     with RLContext(imshape, otf.path, dz) as ctx:\n         for impath in imlist:\n             image = tifffile.imread(impath)\n             result = rl_decon(image, ctx.out_shape)\n             # do something with result...\n```\n\n... and that bit of code is essentially what the [`pycudadecon.decon()`](https://www.talleylambert.com/pycudadecon/deconvolution.html#pycudadecon.decon) function is doing, with a little bit of additional conveniences added in.\n\n*Each of these functions has many options and accepts multiple keyword arguments. See the [documentation](https://www.talleylambert.com/pycudadecon/index.html) for further information on the respective functions.*\n\nFor examples and information on affine transforms, volume rotations, and deskewing (typical of light sheet volumes acquired with stage-scanning), see the [documentation on Affine Transformations](https://www.talleylambert.com/pycudadecon/affine.html)\n___\n\n<sup>1</sup> D.S.C. Biggs and M. Andrews, Acceleration of iterative image restoration algorithms, Applied Optics, Vol. 36, No. 8, 1997. https://doi.org/10.1364/AO.36.001766\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "CUDA-Deconvolution"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "stardist-napari",
    "name": "stardist-napari",
    "display_name": "stardist-napari",
    "version": "2024.8.6.1",
    "created_at": "2021-06-01",
    "modified_at": "2024-08-06",
    "authors": [
      "Uwe Schmidt",
      "Martin Weigert"
    ],
    "author_emails": [
      "research@uweschmidt.org",
      "martin.weigert@epfl.ch"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/stardist-napari/",
    "home_github": "https://github.com/stardist/stardist-napari",
    "home_other": null,
    "summary": "Object Detection with Star-convex Shapes",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "stardist>=0.8.3",
      "napari>=0.4.13",
      "magicgui>=0.4.0",
      "tensorflow; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "tensorflow-metal; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "pytest; extra == \"test\"",
      "pytest-qt; extra == \"test\"",
      "napari[pyqt]>=0.4.13; extra == \"test\""
    ],
    "package_metadata_description": "# StarDist Napari Plugin\n\n[![PyPI version](https://img.shields.io/pypi/v/stardist-napari.svg)](https://pypi.org/project/stardist-napari)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/stardist-napari/badges/version.svg)](https://anaconda.org/conda-forge/stardist-napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/stardist-napari)](https://napari-hub.org/plugins/stardist-napari)\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fstardist.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/stardist)\n\nThis project provides the [napari](https://napari.org/) plugin for [StarDist](https://github.com/stardist/stardist), a deep learning based 2D and 3D object detection method with star-convex shapes. StarDist has originally been developed (see [papers](https://github.com/stardist/stardist#stardist---object-detection-with-star-convex-shapes)) for the segmentation of densely packed cell nuclei in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\n\nIf you use this plugin for your research, please [cite us](https://github.com/stardist/stardist#how-to-cite).\n\n![Screenshot](https://github.com/stardist/stardist-napari/raw/main/images/stardist_napari_screenshot_small.png)\n\n\n## Installation\n\nInstall the plugin with `pip install stardist-napari` or from within napari via `Plugins > Install/Uninstall Plugins‚Ä¶`. If you want GPU-accelerated prediction, please read the more detailed [installation instructions](https://github.com/stardist/stardist#installation) for StarDist.\n\n- You can activate the plugin in napari via `Plugins > stardist-napari: StarDist`.\n- Example images for testing are provided via `File > Open Sample > stardist-napari`.\n\n\n## Documentation\n\nThe two main buttons at the bottom of the plugin are (see right side of screenshot above):\n\n**Restore Defaults**: Restore default values for [inputs](#inputs) (exceptions: *Input Image*, *Image Axes*, *Custom Model*).\n\n**Run**: Start the prediction with the selected inputs and create the [outputs](#outputs) when done.\n\nAll plugin activity is shown in the napari *activity dock*, which can be shown/hidden by clicking on the word `activity` next to the little arrow at the bottom right of the napari window.\n\n### Inputs\n\nThe plugin does perform input validation, i.e. it will disable the `Run` button if it detects a problem with the selected inputs. Problematic input fields are highlighted with a \"lightcoral\" background color ![](https://via.placeholder.com/15/f08080/f08080.png), and their [*tooltips*](https://en.wikipedia.org/wiki/Tooltip) typically explain what the problem is. Some error messages are shown at the bottom in napari's status bar, such as for incompatibilities between multiple input fields. Input fields with warnings (also explained via tooltips) are highlighted with an orange background color ![](https://via.placeholder.com/15/ffa500/ffa500.png).\n\n**Input Image**: Select a napari layer of type `Image` as the input.  \n*Tooltip:* Shows the shape of the image.\n\n**Image Axes**: String that describes the semantic image axes and their order, e.g. `YX` for a 2D image. This parameter is automatically chosen (i.e. guessed) when a new input image is selected and should work in most cases. Permissible axis values are: `X` (width/columns), `Y` (height/rows), `Z` (depth/planes), `C` (channels), `T` (frames/time).  \n*Tooltip:* Shows the mapping of semantic axes to the shape of the selected input image.\n\n**Predict on field of view (only for 2D models in 2D view)**: If enabled, the StarDist prediction is only applied to the current field of view of the napari viewer. As the name of this checkbox indicates, this only works for 2D StarDist models and when the napari viewer is in 2D viewing mode. The checkbox is not even shown if those conditions are not met.\n\n#### *Neural Network Prediction*\n\n**Model Type**: Choice whether to use registered pre-trained models (`2D`, `3D`) or provide a path to a model folder (`Custom 2D/3D`). Based on this choice, either the input for *Pre-trained Model* or *Custom Model* is shown below.  \n(Further information regarding pre-trained models: [how to register your own model](https://nbviewer.org/github/CSBDeep/CSBDeep/blob/master/examples/other/technical.ipynb#Registry-for-pretrained-models), [model registration in StarDist](https://github.com/stardist/stardist/blob/f73cdc44f718d36844b38c1f1662dbb66d157182/stardist/models/__init__.py#L17-L29).)\n\n**Pre-trained Model**: Select a registered pre-trained model from a list. The first time a model is selected, it is downloaded and cached locally.\n\n**Custom Model**: Provide a path to a StarDist model folder, containing at least `config.json` and a compatible neural network weights file (with suffix `.h5` or `.hdf5`). If present, `thresholds.json` is also loaded and its values can be used via the button *Set optimized postprocessing thresholds (for selected model)*. If you want to use [a model from bioimage.io](https://bioimage.io/#/?tags=stardist&type=model), you first need to convert it to the regular StarDist model folder format (see how to do this [here](https://nbviewer.org/github/stardist/stardist/blob/main/examples/other2D/bioimageio.ipynb#Import-bioimage.io-model-as-StarDist-model)).\n\n**Model Axes**: A read-only text field that shows the semantic axes that the currently selected model expects as input. Additionally, we show the number of expected input channels, e.g. `YXC[2]` to indicate that the model expects a 2D input image with 2 channels. Seeing the model axes is helpful to understand whether the axes of the input image are compatible or not.\n\n**Normalize Image**: A checkbox to indicate whether to perform [percentile-based input image normalization](https://forum.image.sc/t/normalization-in-stardist/41696/2) or not. This should be checked if the input image wasn't [manually normalized](https://forum.image.sc/t/stardist-extension/37696/7) such that most pixel values are in the range 0 to 1. If unchecked, inputs *Percentile low* and *Percentile high* are hidden.\n\n**Percentile low**: Percentile value of input pixel distribution that is mapped to 0 (~min value). If there aren't any outlier pixels in your image, you may use percentile `0` to do a standard [min-max image normalization](https://www.codecademy.com/article/normalization).\n\n**Percentile high**: Percentile value of input pixel distribution that is mapped to 1 (~max value). If there aren't any outlier pixels in your image, you may use percentile `100` to do a standard [min-max image normalization](https://www.codecademy.com/article/normalization).\n\n**Input image scaling**: Number or list of numbers (one per input axis) to scale the input image before prediction and rescale the output accordingly. For example, a value of `0.5` indicates that all spatial axes are downscaled to half their size before prediction, and that the outputs are scaled to double their size. This is useful to adapt to different object sizes in the input image.  \n*Tooltip:* Shows the mapping of scale values to the semantic axes of the selected input image.\n\n#### *NMS Postprocessing*\n\n**Probability/Score Threshold**: Determine the number of object candidates to enter non-maximum suppression. Higher values lead to fewer segmented objects, but will likely avoid false positives. The selected model may have an associated threshold value, which can be loaded via the *Set optimized postprocessing thresholds (for selected model)* button.\n\n**Overlap Threshold**: Determine when two objects are considered the same during non-maximum suppression. Higher values allow segmented objects to overlap substantially. The selected model may have an associated threshold value, which can be loaded via the *Set optimized postprocessing thresholds (for selected model)* button.\n\n**Output Type**: Choose format of [outputs](#outputs) (see below for details). Selecting `Label Image` will create the outputs *StarDist labels* and *StarDist class labels* (for multi-class models only) as napari `Labels` layers. Selecting `Polygons / Polyhedra` will instead return the output *StarDist polygons* as a napari `Shapes` layer for a 2D model, or *StarDist polyhedra* as a napari `Surface` layer for a 3D model. Selecting `Both` will return both types of outputs.\n\n#### *Advanced Options*\n\n**Number of Tiles**: String `None` (to disable tiling) or list of integer numbers (one per axis of input image) to determine how the input image is tiled before the CNN prediction is computed on each tile individually. This is needed to avoid (GPU) memory issues that can occur for large input images. Note that the NMS postprocessing is still run only once with candidates from the predictions of all image tiles.  \n*Tooltip:* Shows the mapping of tile values to the semantic axes of the selected input image.\n\n**Normalization Axes**: String of semantic axes which are jointly normalized (if they are present in the input image). For example, the default value `ZYX` indicates that all spatial axes are always normalized together; if an image has multiple channels, the pixels will be normalized separately per channel (e.g. this is what typically makes sense for fluorescence microscopy where channels are independent). On the other hand, the channels in RGB color images typically need to be normalized jointly, hence using `ZYXC` makes sense in this case. Note: if an image is explicitly opened with `rgb=True` in napari, the channels are automatically normalized together.  \n*Tooltip:* Shows a brief explanation.\n\n**Time-lapse Labels**: If the input is a time-lapse/movie, each frame is first independently processed by StarDist. If `Separate per frame (no processing)` is chosen, the object ids in the label images of each frame are not modified, i.e. they are consecutive integers that always start at 1. Selecting `Unique through time` will cause object ids to be unique over time, i.e. the smallest object id in a given frame is larger than the largest object id of the previous frame. Finally, choosing `Match to previous frame (via overlap)` will perform a simple form of [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm) matching/tracking, where object ids are propagated from one frame to the next based on object overlap.\n\n**Show CNN Output**: Create additional [outputs](#outputs) (see below for details) *StarDist probability* and *StarDist distances* that show the direct results of the CNN prediction which are the inputs to the NMS postprocessing. Additionally, *StarDist class probabilities* is created for multi-class models.\n\n**Set optimized postprocessing thresholds (for selected model)**: Button to set *Probability/Score Threshold* and *Overlap Threshold* to the values provided by the selected model. Nothing is changed if the model does not provide threshold values.\n\n### Outputs\n\n**StarDist polygons**: The detected/segmented 2D objects as polygons (napari `Shapes` layer).\n\n**StarDist polyhedra**: The detected/segmented 3D objects as surfaces (napari `Surface` layer).\n\n**StarDist labels**: The detected/segmented 2D/3D objects as a *label image* (napari `Labels` layer). In an integer-valued label image, the value of a given pixel denotes the id of the object that it belongs to. For example, all pixels with value 5 belong to the object with id 5. All background pixels (that don't belong to any object) have value 0.\n\n**StarDist class labels** ([multi-class models](https://nbviewer.org/github/stardist/stardist/blob/master/examples/other2D/multiclass.ipynb) only): The classes of detected/segmented 2D/3D objects as a *semantic segmentation labeling* (napari `Labels` layer). The integer value of a given pixel denotes the class id of the object that it belongs to. For example, all pixels with value 3 belong to the object class 3. Note that all pixels that belong to a specific object instance (as returned by *StarDist labels*) do have the same object class here. All background pixels (that don't belong to an object class) have value 0.\n\n**StarDist probability**: The object probabilities predicted by the neural network as a single-channel image (napari `Image` layer).\n\n**StarDist distances**: The radial distances predicted by the neural network as a multi-channel image (napari `Image` layer).\n\n**StarDist class probabilities** ([multi-class models](https://nbviewer.org/github/stardist/stardist/blob/master/examples/other2D/multiclass.ipynb) only): The object class probabilities predicted by the neural network as a multi-channel image (napari `Image` layer).\n\n\n## Troubleshooting & Support\n\n- The [image.sc forum](https://forum.image.sc/tag/stardist) is the best place to start getting help and support. Make sure to use the tag `stardist`, since we are monitoring all questions with this tag.\n- For general questions about StarDist, it's worth taking a look at the [frequently asked questions (FAQ)]( https://stardist.net/docs/faq.html).\n- If you have technical questions or found a bug, feel free to [open an issue](https://github.com/stardist/stardist-napari/issues).\n\n\n## Other resources\n\nA demonstration of an earlier version of the plugin is shown in [this video](https://www.youtube.com/watch?v=Km1_TnUQ4FM&list=PLilvrWT8aLuZCaOkjucLjvDu2YRtCS-JT&index=5).\n\nMany of the parameters are identical to those of our [StarDist ImageJ/Fiji plugin](https://github.com/stardist/stardist-imagej), which are documented [here](https://imagej.net/plugins/stardist#usage).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "StarDist"
    ],
    "contributions_sample_data": [
      "Nuclei (2D)",
      "Nuclei (3D)",
      "H&E Nuclei (2D RGB)"
    ]
  },
  {
    "normalized_name": "napari-kld",
    "name": "napari-kld",
    "display_name": "Kernel Learning Deconvolution",
    "version": "1.1.0",
    "created_at": "2024-07-31",
    "modified_at": "2024-08-05",
    "authors": [
      "Qiqi Lu"
    ],
    "author_emails": [
      "136303971@qq.com"
    ],
    "license": "Copyright (c) 2024, Qiqi Lu\nAl...",
    "home_pypi": "https://pypi.org/project/napari-kld/",
    "home_github": null,
    "home_other": "None",
    "summary": "Kernel learning deconvolution (KLD) is a rapid deconvolution algorithm for fluorescence microscopic image, which learns the forward and backward kernels in Richardson-Lucy Deconvolution (KLD) from paired low-/high-resolution images. ",
    "categories": [
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy==1.26.4",
      "magicgui",
      "qtpy",
      "scikit-image",
      "torch==2.0",
      "torchvision",
      "fft-conv-pytorch",
      "pytorch-msssim",
      "tensorboard",
      "pydicom",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-kld\n\n[![License](https://img.shields.io/pypi/l/napari-kld.svg?color=green)](https://github.com/qiqi-lu/napari-kld/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-kld.svg?color=green)](https://pypi.org/project/napari-kld)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-kld.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-kld)](https://napari-hub.org/plugins/napari-kld)\n\n`napari-kld` is a `napari` plugin that implements kernel learning deconvolution algrotihm.\n\n## **Kernel Learning Deconvolution (KLD)**\n\nKLD is a rapid deconvolution algorithm for fluorescence microscopic image, which learns the forward and backward kernels in Richardson-Lucy Deconvolution (RLD) from paired low-resolution (LR) and high-resolution (HR) images.\n\nIt only requires **one sample** to training the model, and **two iterations** to achieve a superior deconvolution performance compared to RLD and its variants using unmatched backward projection.\n\n**This [napari] plugin was generated with [copier] using the [napari-plugin-template].*\n\n## **Installation**\n\nYou must install `napari` firstly and then install `napari-kld`.\n\n### **Install `napari`**\n\nYou can download the `napari` bundled app for a simple installation via https://napari.org/stable/tutorials/fundamentals/quick_start.html#installation.\n\nOr, you can install `napari` with Python using pip:\n\n```\nconda create -y -n napari-env -c conda-forge python=3.10\nconda activate napari-env\npython -m pip install 'napari[all]'\n```\n\nRefer to https://napari.org/stable/tutorials/fundamentals/quick_start.html#installation.\n\n### **Install `napari-kld`**\n\nYou can install `napari-kld` plugin with `napari`:\n\n`Plugins` > `Install/Uninstall Plugins‚Ä¶` > [search napari-kld] > `install`.\n\n\nOr you can install `napari-kld` via [pip]:\n\n    pip install napari-kld\n\n## **Instruction**\nThis plugin includes two part:\n\n- `RL Deconvolution` : Conventional RLD algorithm using different type of backward kernels (including matched backward kernel [`Traditional`] and unmatched backward kernels [`Guassian`, `Butterworth`, `Wiener-Butterworth (WB)`]). The forward kernel, i.e., point spread function (PSF), is required.\n\n- `KL Deconvolution` : KLD using learned forward/backward kernels.\n\n**You can download the `\"test\"` folder  at https://github.com/qiqi-lu/kernel-learning-deconvolution for testing, which save some 2D/3D images used for training and testing.**\n\n## **RL Deconvolution**\n\nThe conventional RLD using different type of backward kernels.\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `RL Deconvolution`\n\n2. Load input low-resolution (LR) image: `File` > `Open File(s)` > `[choose the image to be deconvolved]` > `[the image will appear in the layer list of napari]`, such as the simulated image `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/train/raw/0.tif\"`.\n\n3. Choose the name of loaded image in `Input RAW data`, such as `\"0\"`.\n\n4. Press `Choose` to choose a `PSF` correspongding to the loaded image, such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/train/psf.tif\"`.\n\n5. Choose the type of backward kernel in `Method` combo box:\n\n    - `Traditional` : the backward kernel is just the flip of forward kernel (i.e., PSF).\n    - `Guassian` : Guassian-shaped backward kernel, thw FWHM of which is same as the forward kernel.\n    - `Butterworth` : Butterworth-shaped backward kernel, which is constructed using Butterworth filter.\n    - `WB` : WB-shaped backward kernel, which is constructed by combining Wiener and Butterworth filter.\n\n6. Set the number of RL iterations `Iterations` and parameters of backward kernel*.\n\n7. Press `run` button to do deconvolution.\n\n8. Wait the `progress bar` to reach 100%.\n\n9. The output deconved image will appear in the layer list named as `{name of input image}_deconv_{Method}_iter_{Iterations}`, such as `\"0_deconv_traditional_iter_30\"`.\n\n**The adjustment of parameters of backward kernels should refer to the paper : Guo, M. et al. Rapid image deconvolution and multiview fusion for optical microscopy. Nat Biotechnol 38, 1337‚Äì1346 (2020).*\n\n## **KL Deconvolution**\n\n### **Training data preparation**\n\nThe data used for training must be prepared in a folder consisting of:\n\n- A folder named `\"gt\"` (optional) , such as `\"test/data/real/2D/train/gt\"`, which saves all the GT images (only support .tif file).\n- A folder named `\"raw\"`, such as `\"test/data/real/2D/train/raw\"`, which saves all the LR images (only support .tif file). The file names must be the same as those in `\"gt\"` folder.\n- A file named `\"train.txt\"`, such as `\"test/data/real/2D/train/train.txt\"`, which saves the name of each image in `\"gt\"/\"raw\"` filder in each line.\n\n### **When yuo have paired LR image and HR image**\n\nWhen we have paired LR image and HR image, we can treat LR image as **raw input** and HR image as **ground truth** (GT). We can first learn the forward kernel and then learn the backward kernel in a **supervised strategy**.\n\n#### **Training of Forward Projection**\n\nTrain the forward projection to learn forward kernel.\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `KL Deconvolution`\n\n2. Choose `Training` tab.\n\n3. Choose `Data Directory`, such as `\"test/data/real/2D/train\"`. Then the dimention of training data will show in the `Dimension` box.\n\n4. Choose `Output Directory`, such as `\"test/data/real/2D\"`.\n\n5. `PSF Directory` is not required as the PSF is unknown.\n\n6. If the raw input and GT image have different intensity, please check the `Preprocess` check box, which will rescale the input and GT images to have the same intensity. Here, do not check.\n\n7. In the `Forward Projection` box, set the parameters of training:\n    - `Epoch` : number of epochs of training.\n    - `Batch Size` : batch size of training data used during training.\n    - `Kernel Size (z, xy)` : the size of forward kernel to learned.\n    - `Optimizer` : the optimization algorithm. Default: Adam.\n    - `Learning Rate` : learning rate of training.\n    - `Decay Step` Ôºö the decay step of learning rate. Note: `0` for no decay.\n    - `Decay Rate` : the decay rate of learning rate.\n\n8. Press `run` button. You can press the `stop` button to end the training.\n\n9. Wait the `progress bar` to reach 100% and training finished.\n\nAfter the training of forward projection, the results will be save in the `/checkpoints` folder in `Output Directory`, the model was named as `forward_bs_{batch size}_lr_{learning rate}_ks_{kernel size (z)}_{kernel size (xy)}`, such as `\"test/data/real/2D/checkpoints/forward_bs_1_lr_0.001_ks_1_31\"`, which consists of:\n- a `log` folder saved the `Tensorboard` log, which can be opened with `Tensorboard`.\n- many model checkpoints, named as `epoch_{epoch}.pt`.\n- a `parameters.json` file saving the parameters used to training the model.\n\n#### **Training of Backward Projection**\n\nAfter training of forward projeciton, we can freeze the forward projeciton and then train the backward projeciton.\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `KL Deconvolution`\n\n2. Choose `Training` tab.\n\n3. Choose `Data Directory`, such as `\"test/data/2D/real/train\"`. Then the dimention of training data will show in the `Dimension` box.\n\n4. Choose `Output Directory`, such as `\"test/data/2D/real\"`.\n\n5. `PSF Directory` is no required as the PSF is unknown.\n\n6. If the raw input and GT image have different intensity, please check the `Preprocess` check box, which will rescale the input and GT images to have the same intensity. Here, do not check.\n\n7. In the `Backward Projeciton` box, set parameters for the trianing of backward projeciton.\n\n    - `Training strategy` : `supervised` training or `self-supervised` training. Here, set as `supervised`, as we have the GT images. When choosing the `self-supervised` mode, a PSF is required.\n    - `Iterations (RL)` : The number of iterations of RL iterative procedure. Default: 2.\n    - `Epoch` : The number fo epochs used to traing the model.\n    - `Batch Size` : The batch size used to training the model.\n    - `Kernel Size (z, xy)`: The size of backward kernel, `x` and `y` have the same size.\n    - `FP directory` : the directory of the pre-trained forward projeciton model, such as `\"test/data/real/2D/checkpoints/forward_bs_1_lr_0.001_ks_1_31/epoch_500_final.pt\"` (commonly the model labeled with `\"_final\"` is used).\n    - `Optimizer` : Optimization algorithm. Default: Adam.\n    - `Learning Rate` : The learning rate used to trianing the model.\n    - `Decay Step` : the decay step of learning rate.\n    - `Decay Rate` : the decay rate of learning rate.\n\n8. Press `run` button. You can press the `stop` button to end the training.\n\n9. Wait the `progress bar` to reach 100% and then the training finishes.\n\nWhen the training finishes, the results will be save in the `/checkpoints` folder in `Output Directory`, the model was named as `backward_bs_{batch size}_lr_{learning rate}_iter_{num of RL iterations}_ks_{kernel size (z)}_{kernel size (xy)}`, such as `\"test/data/real/2D/checkpoints/backward_bs_1_lr_1e-05_iter_2_ks_1_31\"`, which consists of:\n\n- a `log` folder saved the `Tensorboard` log, which can be opened with `Tensorboard`.\n- many model checkpoints, named as `epoch_{epoch}.pt`.\n- a `parameters.json` file saving the parameters used to training the model.\n\nNow we get the learned forward projection and backward projection.\n\nNext, we can use them to do `Prediction`.\n\n### **When you only have a PSF**\n\nWhen you only have a PSF to do deconvolution, you can train the model using simulated data following the below steps:\n\n1. Generate simulaiton data.\n2. Train the model under supervised mode.\n3. Apply the trained model on real data.\n\n#### **Simulation data generation**\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `KL Deconvolution`\n\n2. Choose `Simulation` tab.\n\n3. Choose the `Output Directory` of the generated simulation data, such as `\"test\\data\\simulation\"`.\n\n4. Choose the `PSF Directory` (only support 2D/3D PSF file save as .tif, axes = (y, x) or (z, y, x)), such as `\"test\\data\\simulation\\psf.tif\"`.\n\n5. Adjust the parameters as needed.\n    - `Image Shape` : the shape of simulated image, when `z=1`, 2D image will be generated.\n    - `PSF Crop` : when the input PSF is too large, you can crop the PSF to acuqire a smaller PSF. All the PSF will be converted to have an odd shape and normalized.\n    - `Num of Simulations` : number of generated images.\n    - `Gaussian (std)` : the standard deviation (std) of Gaussian noise added in the generated LR images. The mean of Gaussian noise = 0. Default: 0 (i.e., without gaussian noise).\n    - `Poisson` : whether to add Poisson noise, if `True`, make the `Enable` checked.\n    - `Ratio` : a ratio factor multiplied on GT image to control the level of Poisson noise, thus the simulated raw input LR image RAW can be expressed as:\n\n    $$ RAW = Possion((GT \\cdot Ratio)\\times PSF) + Gaussian $$\n\n    - `Scale Factor` : downsampling scale factor. Default: 1.\n\n6. Press `run` button.\n\n7. Wait the `progress bar` to reach 100%.\n\nThe generated simulation data will be save in `Output directory`, named as `\"data_{shape_z}_{shape_y}_{shape_x}_gauss_{std of Gaussian noise}_poiss_{whether to add Poisson noise}_ratio_{Ratio}\"`, such as: `\"test\\data\\simulation\\data_128_128_128_gauss_0.0_poiss_0_ratio_1.0\\train\"`\n\n- `\"data\\train\\gt\"` saves the GT images which consist of structures with various shapes*.\n- `\"data\\train\\raw\"` saves the RAW images with blur and noise.\n- `\"data\\train\\parameters.json\"` is a dictionary of parameters used to generate the simulation data.\n- `\"data\\train\\psf.tif\"` is the PSF used in the simulation data generation (as the original PSF may be cropped).\n- `\"data\\train\\train.txt` save all the image used to train the model.\n\nAfter you generate simulation data, you can use them to train the model.\n\n**the code was refered to the paper: Li, Y. et al. Incorporating the image formation process into deep learning improves network performance. Nat Methods 19, 1427‚Äì1437 (2022).*\n\n*You may need to adjust the noise level in the image accordding to the real acuqired data.*\n\n#### **Training with known PSF and simulated data**\n\nThe simulated data should be those generated using the known PSF.\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `KL Deconvolution`\n\n2. Choose `Training` tab.\n\n3. Choose `Data Directory`, such as `test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/train\"`, which saves the data used to train the model in should include:\n    - A `gt` folder saves the GT images\n    - A `raw` folder save the raw input LR images with the same file name as GT images\n    - A `train.txt` file saves all the file names used to train the model (does not need to list all the files in `gt`/`raw` folder but at least one).\n\n4. Choose a `Output Directory` to save the model checkpoints, such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0\"`.\n\n5. Choose `PSF Directory` of the PSF corresponding to the data, such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/train/psf.tif\"`. Then the `Forward Projection` group box will be invisible as we do not need to learn the forward kernel when we know the PSF. Just use the PSF as the forward kernel.\n\n6. If the raw input and GT image have different intensity, please check the `Preprocess` check box, which will rescale the input and GT images to have the same intensity. Here, do not check.\n\n7. Then set parameters to learn the backward kernel.\n\n    - `Training Strategy` : `supervised` training or `self-supervised` training. Here, set as `supervised`, as we have the GT images.\n    - `Iterations (RL)` : the number of iterations of RL iteration procedure. Default: 2.\n    - `Epoch` : the number fo epochs used to traing the model.\n    - `Batch Size` : the batch size used to training the model.\n    - `Kernel Size (z, xy)`: the size of backward kernel, `x` and `y` directions have the same size.\n    - `FP Directory` : the directory of the forward projeciton model. Here, it is disabled as the PSF is known.\n    - `Optimizer` : Optimization algorithm. Default: Adam.\n    - `Learning Rate` : the learning rate used to trianing the model.\n    - `Decay Step` : the decay step of learning rate.\n    - `Decay Rate` : the decay rate of learning rate.\n\n8. Press `run` button. You can press the `stop` button to end the training.\n\n9. Wait the `progress bar` to reach 100% and the training finishes.\n\nWhen the training finishes, a checkpoints folder will be created in `Output Directory` such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/checkpoints\"`.\n\nThe models is save in `/checkpoints` folder, which is named as `\"backward_bs_{batch size}_lr_{learning rate}_iter_{num of RL iterations}_ks_{kernel size (z)}_{kernel size (xy)}\"`, such as `\"/checkpoints/backward_bs_1_lr_1e-06_iter_2_ks_1_31\"`, consists of:\n\n- A `log` folder saved the `Tensorboard` log, which can be open with `Tensorboard`.\n- Many model checkpoints, named as `epoch_{epoch}.pt`.\n- A `parameters.json` file saving the parameters used to training the model.\n\n### **When you only have LR image and corresponding PSF**\nWhen we only have LR image and its PSF, we can traing the backward projection through supervised training using simulation data as introduced above. The plugin also provide an alternative self-supervised training stratergy to learn the backward kernel.\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `KL Deconvolution`\n\n2. Choose `Training` tab.\n\n3. Choose `Data Directory`, such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/train\"`.\n\n4. choose `Output Directory`, such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0\"`.\n\n5. Choose `PSF Directory`, such as `\"test/data/simulation/data_128_128_128_gauss_0.0_poiss_0_ratio_1.0/train/psf.tif\"` then the `Forward Projection` box will be invisiable.\n\n6. As there is no GT image, preprocessing is not needed. Do not check `Preprocess` check box.\n\n7. In the `Backward Projeciton` box, set parameters for the trianing of backward projeciton.\n\n    - `Training strategy` : `supervised` training or `self-supervised` training. Set as `self-supervised`, as we do not have the GT images.\n    - `Iterations (RL)` : the number of iterations of RL iteration procedure. Default: 2.\n    - `Epoch` : the number fo epochs used to traing the model.\n    - `Batch Size` : the batch size used to training the model.\n    - `Kernel Size (z, xy)`: the size of backward kernel, `x` and `y` directions have the same size.\n    - `FP Directory` : the directory of the forward projeciton model. Here, it is disabled as the PSF is known.\n    - `Optimizer` : Optimization algorithm. Default: Adam.\n    - `Learning Rate` : the learning rate used to trianing the model.\n    - `Decay Step` : the decay step of learning rate.\n    - `Decay Rate` : the decay rate of learning rate.\n\n8. Press `run` button. You can press the `stop` button to end the training.\n\n9. Wait the `progress bar` to reach 100% and training finishes.\n\nWhen the training finishes, the results will be save in the `/checkpoints` folder in `Output Directory`, the model was named as `backward_bs_{batch size}_lr_{learning rate}_iter_{num of RL iterations}_ks_{kernel size (z)}_{kernel size (xy)}_ss`, such as `\"/checkpoints/backward_bs_1_lr_1e-05_iter_2_ks_31_31_ss\"`, which consists of:\n\n- a `log` folder saved the `Tensorboard` log, which can be opened with `Tensorboard`.\n- many model checkpoints, named as `epoch_{epoch}.pt`.\n- a `parameters.json` file saving the parameters used to training the model.\n\n*The performance of self-supervised learning may be inferior to supervised learning according to our experiments.*\n\n### **Prediction**\nUse the learned forward/backward kernel to do deconvolution.\n\n1. Open `napari` and load `napari-kld` plugin: `Plugins` > `Kernel Learning Deconvolution` > `KL Deconvolution`\n\n2. Choose `Prediction` tab.\n\n3. Load raw input low-resolution image through `napari`: `File` > `Open File(s)` > `[choose the image to be deconvolved]` > `[the image will appear in the layer list of napari]`, such as `\"test/data/real/2D/test/raw/2.tif\"`.\n\n4. Choose the loaded image in `Input RAW data` box, e.g., `2`.\n\n5. If the PSF is known, choose the `PSF directory`.\n\n6. If the PSF is unknown, choose the `Forward Projection` directory, such as `\"test/data/real/2D/checkpoints/forward_bs_1_lr_0.001_ks_1_31/epoch_500_final.pt\"` (commonly the model labeled with `\"_final\"` is used). If both the directories of PSF and Forward Projeciton is choosen, KLD will directly use the PSF selected.\n\n7. Choose the `Backward Projeciton` directory, such as `\"test/data/real/2D/checkpoints/backward_bs_1_lr_1e-05_iter_2_ks_1_31/epoch_1000_final.pt\"`  (commonly the model labeled with `\"_final\"` is used).\n\n8. Set the number of RL iterations at `Iterations (RL)`. Default: 2.\n\n9. Press run to do deconvolution.\n\n10. Wait the progress bar to reach 100%.\n\nThe deconvolved image will be directly shown in the `layer list` of `napari`, named as `\"{input data name}_deconvo_iter_{number of RL iterations}\"`, e.g., `\"16_deconv_iter_2\"`. You can save it as needed.\n\n### **Others**\nThe `log` tab print the message during running.\nPress `clean` button will clean all the text in the `log` box.\n\n### **Notice**\n\n- *Currently, the plugin is runned on CPU. We have tried to run the training on GPU, but the training time did not decrease (maybe it is because the FFT-based covnlution was not optimized on GPU). We are trying to make improvements.*\n\n- *The training time may be very long if we set the kernel size or the number of epoches too large, especially for 3D images. Besides, it also depends on the  computation capability of your device.*\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nMIT LICENSE\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[copier]: https://copier.readthedocs.io/en/stable/\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[napari-plugin-template]: https://github.com/napari/napari-plugin-template\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "RL Deconvolution",
      "KL Deconvolution"
    ],
    "contributions_sample_data": [
      "Kernel Learning Deconvolution"
    ]
  },
  {
    "normalized_name": "napari-pymeshlab",
    "name": "napari-pymeshlab",
    "display_name": "napari pymeshlab",
    "version": "0.0.6",
    "created_at": "2022-01-14",
    "modified_at": "2024-07-31",
    "authors": [
      "Zach Marin",
      "Robert Haase"
    ],
    "author_emails": [
      "zach.marin@yale.edu"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-pymeshlab/",
    "home_github": "https://github.com/zacsimile/napari-pymeshlab",
    "home_other": null,
    "summary": "Interfaces between napari and pymeshlab library to allow import, export and construction of surfaces.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "npe2",
      "numpy",
      "pymeshlab"
    ],
    "package_metadata_description": "# napari-pymeshlab\n\n[![License](https://img.shields.io/pypi/l/napari-pymeshlab.svg?color=green)](https://github.com/zacsimile/napari-pymeshlab/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pymeshlab.svg?color=green)](https://pypi.org/project/napari-pymeshlab)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pymeshlab.svg?color=green)](https://python.org)\n[![tests](https://github.com/zacsimile/napari-pymeshlab/workflows/tests/badge.svg)](https://github.com/zacsimile/napari-pymeshlab/actions)\n[![codecov](https://codecov.io/gh/zacsimile/napari-pymeshlab/branch/main/graph/badge.svg)](https://codecov.io/gh/zacsimile/napari-pymeshlab)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pymeshlab)](https://napari-hub.org/plugins/napari-pymeshlab)\n\nInterfaces between `napari` and the `pymeshlab` library to allow import, export, construction and processing of surfaces. \n\nThis is a WIP and feature requests are welcome. Please check [PyMeshLab](https://pymeshlab.readthedocs.io/en/latest/)\nfor possible features.\n\n![img.png](https://github.com/zacsimile/napari-pymeshlab/raw/main/docs/screenshot.png)\n\n## Feature list\n\n- Read/write .3ds, .apts, .asc, .bre, .ctm, .dae, .e57, .es, .fbx, .glb, .gltf, .obj, .off, .pdb, .ply,\n                  .ptx, .qobj, .stl, .vmi, .wrl, .x3d, .x3dv\n- [Screened Poisson Surface Reconstruction](https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf)\n- [Convex hull of a surface](https://pymeshlab.readthedocs.io/en/0.1.9/tutorials/apply_filter.html)\n- [Laplacian smoothing of surfaces](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#laplacian_smooth)\n- [Smoothing surfaces using Taubin's method](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#taubin_smooth)\n- [Surface simplification using clustering decimation](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#simplification_clustering_decimation)\n- [colorize_curvature_apss](https://pymeshlab.readthedocs.io/en/0.1.9/filter_list.html#colorize_curvature_apss)\n\nSome functions are shown in the [demo notebook](docs/demo.ipynb).\n\n----------------------------------\n\n<!--\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation \n\nYou can install `napari-pymeshlab` via [pip]:\n\n    pip install napari-pymeshlab\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-pymeshlab\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/zacsimile/napari-pymeshlab/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.e57",
      "*.obj",
      "*.wrl",
      "*.gltf",
      "*.ply",
      "*.glb",
      "*.fbx",
      ".x3dv",
      "*.bre",
      "*.3ds",
      "*.es",
      "*.pdb",
      "*.stl",
      "*.dae",
      "*.vmi",
      "*.qobj",
      "*.x3d",
      "*.ptx",
      "*.apts",
      "*.ctm",
      "*.off",
      "*.asc"
    ],
    "contributions_writers_filename_extensions": [
      ".asc",
      ".apts",
      ".glb",
      ".off",
      ".ply",
      ".stl",
      ".3ds",
      ".x3dv",
      ".es",
      ".vmi",
      ".wrl",
      ".obj",
      ".dae",
      ".qobj",
      ".x3d",
      ".e57",
      ".ptx",
      ".pdb",
      ".ctm",
      ".gltf",
      ".bre",
      ".fbx"
    ],
    "contributions_widgets": [
      "Screened Poisson Reconstruction"
    ],
    "contributions_sample_data": [
      "sphere",
      "shell",
      "bunny"
    ]
  },
  {
    "normalized_name": "napari-segment-anything-2",
    "name": "napari-segment-anything-2",
    "display_name": "Segment Anything 2",
    "version": "0.0.1",
    "created_at": "2024-07-30",
    "modified_at": "2024-07-30",
    "authors": [
      "Jord√£o Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "Copyright (c) 2024, Jord√£o Bra...",
    "home_pypi": "https://pypi.org/project/napari-segment-anything-2/",
    "home_github": "https://github.com/JoOkuma/napari-segment-anything-2",
    "home_other": null,
    "summary": "A napari plugin for Meta's Segment Anything 2 in Images and Videos",
    "categories": [
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "magicgui",
      "numpy",
      "qtpy",
      "scikit-image",
      "napari; extra == 'testing'",
      "pyqt5; extra == 'testing'",
      "pytest; extra == 'testing'",
      "pytest-cov; extra == 'testing'",
      "pytest-qt; extra == 'testing'",
      "tox; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-segment-anything-2\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-segment-anything-2.svg?color=green)](https://github.com/JoOkuma/napari-segment-anything-2/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-segment-anything-2.svg?color=green)](https://pypi.org/project/napari-segment-anything-2)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment-anything-2.svg?color=green)](https://python.org)\n[![tests](https://github.com/JoOkuma/napari-segment-anything-2/workflows/tests/badge.svg)](https://github.com/JoOkuma/napari-segment-anything-2/actions)\n[![codecov](https://codecov.io/gh/JoOkuma/napari-segment-anything-2/branch/main/graph/badge.svg)](https://codecov.io/gh/JoOkuma/napari-segment-anything-2)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment-anything-2)](https://napari-hub.org/plugins/napari-segment-anything-2)\n\nA napari plugin for Meta's Segment Anything 2 in Images and Videos\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-segment-anything-2` via [pip]:\n\n    pip install git+https://github.com/jookuma/segment-anything-2@no-cc\n    pip install napari[all] napari-segment-anything-2\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jookuma/segment-anything-2@no-cc\n    pip install napari[all] git+https://github.com/JoOkuma/napari-segment-anything-2.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-segment-anything-2\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/JoOkuma/napari-segment-anything-2/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segment Anything 2"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "hipocount-napari",
    "name": "hipocount-napari",
    "display_name": "hipocount-napari",
    "version": "0.0.1",
    "created_at": "2024-07-23",
    "modified_at": "2024-07-23",
    "authors": [
      "Borys Olifirov"
    ],
    "author_emails": [
      "omnia.fatum@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/hipocount-napari/",
    "home_github": "https://github.com/wisstock/hipocount-napari",
    "home_other": null,
    "summary": "Quantitative analysis of immunofluorescence images of hippocampal slices",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari"
    ],
    "package_metadata_description": "hipocount-napari\n================\n\nQuantitative analysis of immunofluorescence images of hippocampal slices\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Preprocessing",
      "Pyramid layer masking",
      "Astrocytes masking",
      "GLT dots masking",
      "GLT conut in pyramid",
      "GLT conut in astrocytes"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "anchor-droplet-chip",
    "name": "anchor_droplet_chip",
    "display_name": "anchor_droplet_chip",
    "version": "0.4.6",
    "created_at": "2023-03-13",
    "modified_at": "2024-07-19",
    "authors": [
      "Andrey Aristov"
    ],
    "author_emails": [
      "aaristov@pasteur.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/anchor-droplet-chip/",
    "home_github": "https://github.com/BaroudLab/anchor-droplet-chip",
    "home_other": null,
    "summary": "Segment organoids and measure intensities",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "dask",
      "fire",
      "h5py",
      "jupyterlab",
      "matplotlib",
      "napari",
      "nd2",
      "numpy",
      "pandas",
      "pyqt6",
      "pytest-qt",
      "pyyaml",
      "scikit-image",
      "scipy",
      "seaborn",
      "tifffile",
      "zarr-tools",
      "zenodo-get"
    ],
    "package_metadata_description": "# ‚öì anchor-droplet-chip\n## Measuring single-cell susceptibility to antibiotics within monoclonal fluorescent bacteria.\n\nWe are imaging the entire chip using 20x 0.7NA objective lens using automatic stitching in NIS.\nBright-field image 2D and TRITC-3D acquired. The 3D stack is converted to 2D using maximum projection in NIS or Fiji. Both channels are then merged together and saved as a tif stack. After that this package can be applied to detect the individual droplets and count the fluorescent cells.\n\nAs the chips are bonded to the coverslip manually, they contain a randon tilt and shift, so detecting individual droplets proved to be unreliable. The current approach consisnts of preparing a well-lebelled template bright-field image and a labelled mask and matching the experimental brightfield image to the template.\n![Paper outline(1)](https://user-images.githubusercontent.com/11408456/178001287-513e6398-c4e0-4946-b38f-6cb98dc0ee6c.svg)\n\n## Installation\n```bash\npip install anchor-droplet-chip\n```\n## Usage\n\n1. Notebook: `jupyter lab example.ipynb`\n2. Napari plugin: see the menu `Plugins / andhor-droplet-chips / ...\n3. Command line:\n\n    `python -m adc.align --help`\n\n    `python -m adc.count --help`\n\n### Dowloading the raw data\nHead to release page https://github.com/BaroudLab/anchor-droplet-chip/releases/tag/v0.0.1 and download files one by one.\n\nOr\n\nExecute the notebook example.ipynb - the data will be fetched automatically.\n\n### Aligning the chips with the template and the mask\n\nDay 1:\n```bash\npython -m adc.align day1/00ng_BF_TRITC_bin2.tif template_bin16_bf.tif labels_bin2.tif\n```\nThis command will create the stack day1/00ng_BF_TRITC_bin2-aligned.tif, which can be viewed in Fiji.\n![Screenshot of 00ng_BF_TRITC_bin2-aligned.tif](https://user-images.githubusercontent.com/11408456/176169270-3d494fc3-a771-4bf0-859e-c9cc853ce2d9.png)\n\nDay 2:\n```bash\npython -m adc.align day2/00ng_BF_TRITC_bin2_24h.tif template_bin16_bf.tif labels_bin2.tif\n```\n\n### Counting the cells day 1 and day2\n```\npython -m adc.count day1/00ng_BF_TRITC_bin2-aligned.tif day1/counts.csv\npython -m adc.count day2/00ng_BF_TRITC_bin2_24h-aligned.tif day2/counts.csv\n```\n\n### Combining the tables from 2 days\n```\npython adc.merge day1/counts.csv day2/counts.csv table.csv\n```\n\n### Plotting and fitting the probabilities\n\n\n## Sample data\n\n### Batch processing:\n\nFirst you'll need to clone the repo locally and install it to have the scripts at hand.\n\n```bash\ngit clone https://github.com/BaroudLab/anchor-droplet-chip.git\n\ncd anchor-droplet-chip\n\npip install .\n```\nMake a data folder\n```bash\nmkdir data\n\n```\nDownload the dataset from Zenodo https://zenodo.org/record/6940212\n```bash\nzenodo_get 6940212 -o data\n```\nProceed with Snakemake pipeline to get tha table and plots. Be careful with the number of threads `-c` as a single thread can consume over 8 GBs of RAM.\n```bash\nsnakemake -c4 -d data table.csv\n```\n\n# Napari plugin functionaluties\n\n## nd2 reader\n\nOpen large nd2 file by drag-n-drop and select anchor-droplet-chip as a reader.\nThe reader plugin will aotimatically detect the subchannels and split them in different layers.\nThe reader will also extract the pixel size from metadata and save it as Layer.metadata[\"pixel_size_um\"]\nThe data itself is opened ad dask array using nd2 python library.\n\n## Substack\n\nSome datasets are so big, it's hard to even to open them, let alone doing processing in them.\n`anchor-droplet-chip / Make a sub stack ` addresses this problem.\nUpon opening the plugin you'll see all  dimensions of you dataset, and the axes will become named accordingly.\nSimply choose the subset of data you need, and click \"Crop it!\". This will create a new layer with the subset of data.\nNote that no new files are created in the process and in the background nd2 library lazy loading chunks of data from the original nd2 file.\n\n## Populate ROIs along the line\nDraw a line in the new shapes layer and call the widget. It will populate square ROIs along the line. Adjust the number of columns and rows. This way you can manually map the 2D wells on your chip.\n\n## Crop ROIs\nUse this widget to crop the mapped previously ROIs. The extracted crops can be saved as tifs.\n\n## Split along axis\n\nAllows to split any dataset along a selected axis and save the pieces as separate tifs (imagej format, so only TZCYX axes supported)\n* Select the axis name\n* Click Split it! and check the table with the names, shapes and paths.\n* To change the prefix, set the folder by clicking at \"Choose folder\"\n* Once the table looks right, click \"Save tifs\" and wait. The colunm \"saved\" will be updated along the way.\n![image](https://user-images.githubusercontent.com/11408456/214313498-5b1f8408-1fa3-4e24-810a-b9394e936c8e.png)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.nd2",
      "*.tiff",
      "*P=*.tif",
      "*.h5",
      "*.tif",
      "*.zarr",
      "*.csv",
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segment cells using cellpose",
      "Detect wells",
      "Combine stack",
      "Count Cells",
      "Crop ROIs",
      "Populate ROIs along the line",
      "Make a sub stack",
      "Split the stack along one dimension",
      "Make projection along one dimension"
    ],
    "contributions_sample_data": [
      "Show template",
      "Show centers"
    ]
  },
  {
    "normalized_name": "recorder-napari",
    "name": "recOrder-napari",
    "display_name": "recOrder-napari",
    "version": "0.4.2rc1",
    "created_at": "2022-04-19",
    "modified_at": "2024-07-16",
    "authors": [
      "Computational Microscopy Platform",
      "CZ Biohub"
    ],
    "author_emails": [
      "shalin.mehta@czbiohub.org"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/recorder-napari/",
    "home_github": null,
    "home_other": null,
    "summary": "Computational microscopy toolkit for label-free imaging",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "waveorder ==2.0.0rc3",
      "pycromanager ==0.27.2",
      "click >=8.0.1",
      "natsort >=7.1.1",
      "colorspacious >=1.1.2",
      "pyqtgraph >=0.12.3",
      "napari-ome-zarr >=0.3.2",
      "napari[pyqt6_experimental]",
      "importlib-metadata",
      "iohub ==0.1.0.dev5",
      "wget >=3.2",
      "pytest >=5.0.0 ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "black ; extra == 'dev'",
      "hypothesis ; extra == 'dev'"
    ],
    "package_metadata_description": "# recOrder\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/recOrder-napari)\n[![Python package index download statistics](https://img.shields.io/pypi/dm/recOrder-napari.svg)](https://pypistats.org/packages/recOrder-napari)\n[![Python package index](https://img.shields.io/pypi/v/recOrder-napari.svg)](https://pypi.org/project/recOrder-napari)\n[![Development Status](https://img.shields.io/pypi/status/napari.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n\n`recOrder` is a collection of computational imaging methods. It currently provides QLIPP (quantitative label-free imaging with phase and polarization), phase from defocus, and fluorescence deconvolution. \n\n[![Unveiling the invisible](https://github.com/mehta-lab/recOrder/blob/main/docs/images/comms_video_screenshot.png?raw=true)](https://www.youtube.com/watch?v=JEZAaPeZhck)\n\nAcquisition, calibration, background correction, reconstruction, and applications of QLIPP are described in the following [E-Life Paper](https://elifesciences.org/articles/55502):\n\n```bibtex\nSyuan-Ming Guo, Li-Hao Yeh, Jenny Folkesson, Ivan E Ivanov, Anitha P Krishnan, Matthew G Keefe, Ezzat Hashemi, David Shin, Bryant B Chhun, Nathan H Cho, Manuel D Leonetti, May H Han, Tomasz J Nowakowski, Shalin B Mehta, \"Revealing architectural order with quantitative label-free imaging and deep learning,\" eLife 2020;9:e55502 DOI: 10.7554/eLife.55502 (2020).\n```\n\nThese are the kinds of data you can acquire with `recOrder` and QLIPP:\n\nhttps://user-images.githubusercontent.com/9554101/271128301-cc71da57-df6f-401b-a955-796750a96d88.mov\n\nhttps://user-images.githubusercontent.com/9554101/271128510-aa2180af-607f-4c0c-912c-c18dc4f29432.mp4\n\n## What do I need to use `recOrder`\n`recOrder` is to be used alongside a conventional widefield microscope. For QLIPP, the microscope must be fitted with an analyzer and a universal polarizer: \n\nhttps://user-images.githubusercontent.com/9554101/273073475-70afb05a-1eb7-4019-9c42-af3e07bef723.mp4\n\nFor phase-from-defocus or fluorescence deconvolution methods, the universal polarizer is optional.\n\nThe overall structure of `recOrder` is shown in Panel B, highlighting the structure of the graphical user interface (GUI) through a napari plugin and the command-line interface (CLI) that allows users to perform reconstructions.\n\n![Flow Chart](https://github.com/mehta-lab/recOrder/blob/main/docs/images/recOrder_Fig1_Overview.png?raw=true)\n\n\n\n## Software Quick Start\n\n(Optional but recommended) install [anaconda](https://www.anaconda.com/products/distribution) and create a virtual environment:\n\n```sh\nconda create -y -n recOrder python=3.9\nconda activate recOrder\n```\n\nInstall `recOrder-napari`:\n\n```sh\npip install recOrder-napari\n```\n\nOpen `napari` with `recOrder-napari`:\n\n```sh\nnapari -w recOrder-napari\n```\n\nFor more help, see [`recOrder`'s documentation](https://github.com/mehta-lab/recOrder/tree/main/docs). To install `recOrder` \non a microscope, see the [microscope installation guide](https://github.com/mehta-lab/recOrder/blob/main/docs/microscope-installation-guide.md).\n\n## Dataset\n\n[Slides](https://doi.org/10.5281/zenodo.5135889) and a [dataset](https://doi.org/10.5281/zenodo.5178487) shared during a workshop on QLIPP and recOrder can be found on Zenodo, and the napari plugin's sample contributions (`File > Open Sample > recOrder-napari` in napari).\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5178487.svg)](https://doi.org/10.5281/zenodo.5178487)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5135889.svg)](https://doi.org/10.5281/zenodo.5135889)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.zarr",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Main Menu"
    ],
    "contributions_sample_data": [
      "Polarization Target Data (10 MB)",
      "Polarization Target Reconstruction (10 MB)",
      "Zebrafish Embryo Reconstruction (92 MB)"
    ]
  },
  {
    "normalized_name": "napari-ome-zarr",
    "name": "napari-ome-zarr",
    "display_name": "napari-ome-zarr",
    "version": "0.6.1",
    "created_at": "2021-06-14",
    "modified_at": "2024-07-15",
    "authors": [
      "OME Team"
    ],
    "author_emails": [
      "ome-team@openmicroscopy.org"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-ome-zarr/",
    "home_github": "https://github.com/ome/napari-ome-zarr",
    "home_other": null,
    "summary": "A reader for zarr backed OME-NGFF images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "ome-zarr >=0.3.0",
      "numpy",
      "vispy",
      "napari >=0.4.13"
    ],
    "package_metadata_description": "# napari-ome-zarr\n\n[![License](https://img.shields.io/pypi/l/napari-ome-zarr.svg?color=green)](https://github.com/ome/napari-ome-zarr/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ome-zarr.svg?color=green)](https://pypi.org/project/napari-ome-zarr)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ome-zarr.svg?color=green)](https://python.org)\n[![tests](https://github.com/ome/napari-ome-zarr/workflows/tests/badge.svg)](https://github.com/ome/napari-ome-zarr/actions)\n[![codecov](https://codecov.io/gh/ome/napari-ome-zarr/branch/master/graph/badge.svg)](https://codecov.io/gh/ome/napari-ome-zarr)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/ome/napari-ome-zarr/main.svg)](https://results.pre-commit.ci/latest/github/ome/napari-ome-zarr/main)\n\n\nA reader for zarr backed [OME-NGFF](https://ngff.openmicroscopy.org/) images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\n[Install napari] if not already installed.\n\nYou can install `napari-ome-zarr` via [pip]. Activate the same environment as you installed napari into, then:\n\n    pip install napari-ome-zarr\n\n## Usage\n\nNapari will use `napari-ome-zarr` plugin to open images that the plugin recognises as ome-zarr.\nThe image metadata from OMERO will be used to set channel names and rendering settings\nin napari::\n\n    napari \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.3/9836842.zarr/\"\n\n\nIf a dialog in napari pops up, encouraging you to choose a reader, choose ``napari-ome-zarr`` and click OK. You can stop it happening with addition of ``--plugin napari-ome-zarr`` as in the example below.\n\nTo open a local file::\n\n    napari --plugin napari-ome-zarr 13457227.zarr\n\nOR in python::\n\n    import napari\n\n    viewer = napari.Viewer()\n    viewer.open(\"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0101A/13457537.zarr\", plugin=\"napari-ome-zarr\")\n\n    napari.run()\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-ome-zarr\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[Install napari]: https://napari.org/stable/tutorials/fundamentals/installation.html\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/ome/napari-ome-zarr/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.zarr",
      "*.zarr*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mesofield",
    "name": "napari-mesofield",
    "display_name": "MesoField",
    "version": "0.1.0",
    "created_at": "2024-07-13",
    "modified_at": "2024-07-13",
    "authors": [
      "Jacob Gronemeyer"
    ],
    "author_emails": [
      "jgronemeyer@psu.edu"
    ],
    "license": "Mozilla Public License Version...",
    "home_pypi": "https://pypi.org/project/napari-mesofield/",
    "home_github": "https://github.com/Gronemeyer/napari-mesofield",
    "home_other": null,
    "summary": "A plugin that extends napari functionality as an image acquisition software to automate multi-modal experimental control, setup, and analysis for mesoscopic widefield acquistion of 1P epiflourescent signals in the mouse cortex.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-mesofield\n\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-mesofield.svg?color=green)](https://github.com/Gronemeyer/napari-mesofield/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mesofield.svg?color=green)](https://pypi.org/project/napari-mesofield)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mesofield.svg?color=green)](https://python.org)\n[![tests](https://github.com/Gronemeyer/napari-mesofield/workflows/tests/badge.svg)](https://github.com/Gronemeyer/napari-mesofield/actions)\n[![codecov](https://codecov.io/gh/Gronemeyer/napari-mesofield/branch/main/graph/badge.svg)](https://codecov.io/gh/Gronemeyer/napari-mesofield)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mesofield)](https://napari-hub.org/plugins/napari-mesofield)\n\nA plugin that extends napari functionality as an image acquisition software to automate multi-modal experimental control, setup, and analysis for mesoscopic widefield acquistion of 1P epiflourescent signals in the mouse cortex.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-mesofield` via [pip]:\n\n    pip install napari-mesofield\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Gronemeyer/napari-mesofield.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Mozilla Public License 2.0] license,\n\"napari-mesofield\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Gronemeyer/napari-mesofield/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Container Threshold",
      "Magic Threshold",
      "Autogenerate Threshold",
      "Example QWidget"
    ],
    "contributions_sample_data": [
      "MesoField"
    ]
  },
  {
    "normalized_name": "napari-blosc2",
    "name": "napari-blosc2",
    "display_name": "Blosc2 Reader & Writer",
    "version": "0.0.2",
    "created_at": "2024-07-11",
    "modified_at": "2024-07-11",
    "authors": [
      "Karol Gotkowski"
    ],
    "author_emails": [
      "karol.gotkowski@dkfz.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-blosc2/",
    "home_github": null,
    "home_other": "None",
    "summary": "An image reader & writer for blosc2 images.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "blosc2",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-blosc2\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-blosc2.svg?color=green)](https://github.com/Karol-G/napari-blosc2/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-blosc2.svg?color=green)](https://pypi.org/project/napari-blosc2)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-blosc2.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blosc2)](https://napari-hub.org/plugins/napari-blosc2)\n\nAn image reader & writer for blosc2 images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-blosc2` via [pip]:\n\n    pip install napari-blosc2\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-blosc2\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.b2nd"
    ],
    "contributions_writers_filename_extensions": [
      ".b2nd"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-xgboost",
    "name": "napari-xgboost",
    "display_name": "Pixel Classification XGBoost",
    "version": "0.1.0",
    "created_at": "2024-07-05",
    "modified_at": "2024-07-05",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@uni-leipzig.de"
    ],
    "license": "Copyright (c) 2024, Robert Haa...",
    "home_pypi": "https://pypi.org/project/napari-xgboost/",
    "home_github": "https://github.com/haesleinhuepf/napari-xgboost",
    "home_other": null,
    "summary": "A plugin for pixel classification using XGBoost",
    "categories": [
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "xgboost",
      "apoc",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-xgboost\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-xgboost.svg?color=green)](https://github.com/haesleinhuepf/napari-xgboost/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-xgboost.svg?color=green)](https://pypi.org/project/napari-xgboost)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-xgboost.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-xgboost/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-xgboost/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-xgboost/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-xgboost)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-xgboost)](https://napari-hub.org/plugins/napari-xgboost)\n\nA plugin for pixel classification using [XGBoost](https://xgboost.readthedocs.io/en/stable/), inspired by [Digital Sreeni's Youtube video](https://www.youtube.com/watch?v=yqkNslkzLk4).\n\nNote: This plugin is work-in-progress. Check out the [github issues](https://github.com/haesleinhuepf/napari-xgboost/issues) to see what's currently being worked on.\n\n## Usage\n\nLoad an example image into napari. Add a Labels layer by clicking on this button:\n\n![img.png](https://github.com/haesleinhuepf/napari-xgboost/raw/main/docs/images/img.png)\n\nThen, draw a sparse annotation on the image. Try to draw thin lines on background and foreground, e.g. like this:\n\n![img_1.png](https://github.com/haesleinhuepf/napari-xgboost/raw/main/docs/images/img_1.png)\n\nThen click the menu `Layers > Segment > Train Pixel Classifier (XGBoost)`.\n\n![img_2.png](https://github.com/haesleinhuepf/napari-xgboost/raw/main/docs/images/img_2.png)\n\nIn the dialog, select the original image and the labels layer. Also enter a filename where the model should be saved. \nAfterwards, click on `Run` to explore the result.\n\n![img_3.png](https://github.com/haesleinhuepf/napari-xgboost/raw/main/docs/images/img_3.png)\n\n## Installation\n\nYou can install `napari-xgboost` via [pip]:\n\n    pip install napari-xgboost\n\nTo install latest development version :\n\n    pip install git+https://github.com/haesleinhuepf/napari-xgboost.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-xgboost\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-xgboost/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Train Pixel Classifier (XGBoost)"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-blender",
    "name": "napari-blender",
    "display_name": "Blender Visualization",
    "version": "1.0.1",
    "created_at": "2024-07-03",
    "modified_at": "2024-07-03",
    "authors": [
      "Krijn H. van der Steen"
    ],
    "author_emails": [
      "\"Krijn H. van der Steen\" <k.h.vandersteen@gmail.com>"
    ],
    "license": "Mozilla Public License Version...",
    "home_pypi": "https://pypi.org/project/napari-blender/",
    "home_github": "https://github.com/Living-Technologies/napari-blender",
    "home_other": null,
    "summary": "A plug-in to help visualize and analyze organoid segmentation using Blender",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.10",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari-video",
      "tifffile",
      "bpy >=4.0.0",
      "trimesh",
      "opencv-python",
      "scipy",
      "pathlib",
      "mathutils",
      "pandas"
    ],
    "package_metadata_description": "# napari-blender\n\nnapari-blender is a plugin for napari that allows you to render 3D scenes using Blender.\n\nThis is a system that combines optical validation of model predictions by having different 3D visualizations and quantitative evaluation. Utilizing Blender‚Äôs rendering capabilities for deepening the understanding of nuclei segmentation for users with different levels of expertise. Examples are time-lapse animations (aimed to be generated from label data), opaque ground truth visualizations with solid prediction objects to compare prediction quality optically and 3D images where nuclei are coloured according to their prediction quality. To facilitate meaningful quantitative evaluation, different metrics are calculated for these predictions and displayed within the animation, such as the Jaccard index, Intersection over Union and the F1-score. The emphasis in this system is on operating it with limited technical knowledge, allowing for bridging between researchers and developers; but includes metrics that allow for a deeper understanding of performance for expert users.\n\nDocumentation can be found at https://living-technologies.github.io/napari-blender/\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nThe installation of `napari-blender` is made easier by the inclusion of a virtual environment containing specified versions of each library. These versions should be followed, since dependencies could otherwise break the build. \n\nDownload the [environment.yml], and navigate to the directory of the file. Install by:\n\n    conda env create -f environment.yml\n    conda activate napari-blender-env\n\nNow you can install the plug-in using:\n\n    pip install napari-blender\n\nNote:\n\nIn Windows, an error might occur in the installation of the `mathutils` package, stating 'Microsoft Visual C++ 14.0 or greater is required'. A fix can be found on[StackOverflow].\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Living-Technologies/napari-blender.git\n\n## License\n\nDistributed under the terms of the [Mozilla Public License 2.0] license,\n\"napari-blender\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[environment.yml]: https://github.com/Living-Technologies/napari-blender/blob/main/environment.yml\n[StackOverflow]: https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Living-Technologies/napari-blender/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Transparent Overlay",
      "Microscopy Overlay",
      "Gradient Overlay",
      "Tracked Nuclei",
      "Timelapse Mode",
      "Video Loader"
    ],
    "contributions_sample_data": [
      "Blender Visualization"
    ]
  },
  {
    "normalized_name": "napari-fast4dreg",
    "name": "napari-fast4dreg",
    "display_name": "Fast4DReg",
    "version": "0.0.1",
    "created_at": "2024-07-03",
    "modified_at": "2024-07-03",
    "authors": [
      "Marcel Issler"
    ],
    "author_emails": [
      "marcel.issler@kuleuven.be",
      "marcel.issler@vib.be"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-fast4dreg/",
    "home_github": null,
    "home_other": "None",
    "summary": "Dask empowered multidim, rigid registration for volumetric measurements",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "matplotlib",
      "zarr",
      "tqdm",
      "scipy",
      "pandas",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-fast4dreg\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-fast4dreg.svg?color=green)](https://github.com/Macl-I/napari-fast4dreg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-fast4dreg.svg?color=green)](https://pypi.org/project/napari-fast4dreg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-fast4dreg.svg?color=green)](https://python.org)\n[![tests](https://github.com/Macl-I/napari-fast4dreg/workflows/tests/badge.svg)](https://github.com/Macl-I/napari-fast4dreg/actions)\n[![codecov](https://codecov.io/gh/Macl-I/napari-fast4dreg/branch/main/graph/badge.svg)](https://codecov.io/gh/Macl-I/napari-fast4dreg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-fast4dreg)](https://napari-hub.org/plugins/napari-fast4dreg)\n\nDask empowered multi-dimensional, registration for volumetric measurements.\nThis is a python port of the original Fast4DReg Fiji Plugin, with added rotation correction in lateral direction and support for out of memory processing.\nThe original paper can be found here:\nhttps://journals.biologists.com/jcs/article/136/4/jcs260728/287682/Fast4DReg-fast-registration-of-4D-microscopy\n\n----------------------------------\n\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-fast4dreg` via [pip]:\n\n    pip install napari-fast4dreg\n\n## Usage \n\nIt's easy! \n1) Just drag and drop your image, or the test image from this repository, into napari and open it normally. \nDon't worry if your file is big, napari already internally uses dask to open even the biggest images (although it might hurt the performance).\n2) Open the napari-fast4dreg plugin from the plugin menu.\n3) In the image row, make sure your image is selected in the image drop down menu.\n4) In the axes row, choose the structure of your input image. If your axis orientation is correct in ImageJ choose the standard TZCYX (ImageJ) orientation. If you are using python to process the image you probabbly are using the alternatively availabe CTZYX orientation. In this case just select CTZYX in the drop down menu instead.\n5) Select the reference channel used for the registration. The drift will be determined for this reference channel and applied to all other channels. Counting begins by 0. In case for the test image we select the nuclear signal in channel 1.\n6) Select the corrections that you want to apply on your image. Note that the crop function reduces only in xy, according to the previously determined drift. (e.g. drift = -5 in x --> drop 5 pixels from the left hand side of the registered stack.)\n7) Wait for output (this may take a while, so go and get a coffe or tea).\n8) Enjoy your registered image.\n\n\n## Example Outcome\nThe output will consist of the following (if chosen): \n- registered.tif: The registered file, output of this image registration pipeline.\n- tmp_data: This folder was used for temporary data saving and stores at the end the registered image in a chunked manner (can be deleted or dragged into napari for a greater data versatility)\n- drifts.csv: csv table, home to the drift of all corrected variables, if you prefer your own plotting style, here is where you find the pure drift table.\n- XY-Drift.svg: Vector based graphic, visualising the drift in lateral direction. The svg format can be opened by your web browser or directly imported to powerpoint. Key advantage of .svg instead of .png: You can resize any way you like without loss of image quality.\n- Z-Drift.svg: Vector based graphic, visualising the drift in axial direction.\n- Rotation-Drift.svg: Showing rotation correction of the image in lateral direction.\n  \n![3D_MIP_registration](./media/3D_registration.gif)\n![3D_plane](./media/3D_plane_registration.gif)\n![XY-Drift](./media/XY-Drift.svg)\n![Z-Drift](./media/Z-Drift.svg)\n![Rotation-Drift](./media/Rotation-Drift.svg)\n\n## Contributing\n\nContributions are very welcome. Just send me an E-mail: marcel.issler@kuleuven.be or directly submit a pull request.\n\n## Credit \nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-fast4dreg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "fast4dreg"
    ],
    "contributions_sample_data": [
      "Fast4DReg"
    ]
  },
  {
    "normalized_name": "napari-opt-handler",
    "name": "napari-opt-handler",
    "display_name": "OPT Preprocessing",
    "version": "0.0.3",
    "created_at": "2024-07-03",
    "modified_at": "2024-07-03",
    "authors": [
      "David Palecek",
      "Giorgia Tortora"
    ],
    "author_emails": [
      "David Palecek <david@stanka.de>",
      "Giorgia Tortora <giorgia.tortora@polimi.it>"
    ],
    "license": "Copyright (c) 2024, David Pale...",
    "home_pypi": "https://pypi.org/project/napari-opt-handler/",
    "home_github": "https://github.com/QBioImaging/napari-opt-handler",
    "home_other": null,
    "summary": "Optical Projection Tomography preprocessing plugin for napari",
    "categories": [
      "Image Processing",
      "Transformations"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "scikit-image",
      "matplotlib",
      "magicgui ; extra == 'napari'",
      "napari[pyqt5] ; extra == 'napari'",
      "pooch >=1 ; extra == 'napari'",
      "qtpy ; extra == 'napari'",
      "pre-commit ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pytest-mock ; extra == 'testing'",
      "pytest-timeout ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-opt-handler\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-opt-handler.svg?color=green)](https://raw.githubusercontent.com/QBioImaging/napari-opt-handler/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-opt-handler.svg?color=green)](https://pypi.org/project/napari-opt-handler)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-opt-handler.svg?color=green)](https://python.org)\n[![tests](https://github.com/QBioImaging/napari-opt-handler/workflows/tests/badge.svg)](https://github.com/QBioImaging/napari-opt-handler/actions)\n[![codecov](https://codecov.io/gh/QBioImaging/napari-opt-handler/branch/main/graph/badge.svg)](https://codecov.io/gh/QBioImaging/napari-opt-handler)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-opt-handler)](https://napari-hub.org/plugins/napari-opt-handler)\n\nOptical Projection Tomography preprocessing plugin for napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\nplace for the gif\n<img src=\"\" width=\"700\"/>\n\nJump to:\n- [Usage](#usage)\n  - [Starting point](#starting-point)\n  - [Global settings](#settings)\n  - [Corrections](#corrections)\n  - [Other](#other)\n- [Installation](#installation)\n- [Troubleshooting installation](#troubleshooting-installation)\n- [Contributing](#contributing)\n- [License](#license)\n- [Acknowledgements](#acknowledgements)\n\n## üõÄ Usage\n\n### Starting point\n1. Data streamed from ImSwitch OPT widget (see details here[LINK])\n2. Loaded images as data stack\n3. Other stack 3D volume data formats\n\n### Global settings\nTracking\n\nInplace operations\n\n### Corrections\nDark-field correction\nBright-field correction\nBad-pixel correction\nIntensity correction\n\n### Other\nBinning\nROI\n-Log\n\n## üíª Installation\n\nYou can install `napari-opt-handler` via [pip]:\n\n    pip install napari-opt-handler\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/QBioImaging/napari-opt-handler.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## üöì License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-opt-handler\" is free and open source software\n\n## üî® Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/QBioImaging/napari-opt-handler/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "OPT Preprocessing"
    ],
    "contributions_sample_data": [
      "OPT Preprocessing"
    ]
  },
  {
    "normalized_name": "zarpaint",
    "name": "zarpaint",
    "display_name": "zarpaint",
    "version": "0.4.0",
    "created_at": "2021-06-17",
    "modified_at": "2024-06-26",
    "authors": [
      "Abigail S McGovern and Juan Nunez-Iglesias"
    ],
    "author_emails": [
      "juan.nunez-iglesias@monash.edu"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/zarpaint/",
    "home_github": "https://github.com/jni/zarpaint",
    "home_other": null,
    "summary": "Paint segmentations directly to on-disk/remote zarr arrays",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "magicgui",
      "napari >=0.4.19",
      "numpy",
      "pyyaml",
      "qtpy",
      "scipy",
      "scikit-image >=0.21",
      "toolz",
      "zarr <3,>=2.11",
      "tensorstore ; extra == 'all'",
      "coverage ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari[pyqt5] ; extra == 'testing'"
    ],
    "package_metadata_description": "# zarpaint\n\n[![License](https://img.shields.io/pypi/l/zarpaint.svg?color=green)](https://github.com/jni/zarpaint/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/zarpaint.svg?color=green)](https://pypi.org/project/zarpaint)\n[![Python Version](https://img.shields.io/pypi/pyversions/zarpaint.svg?color=green)](https://python.org)\n[![tests](https://github.com/jni/zarpaint/workflows/tests/badge.svg)](https://github.com/jni/zarpaint/actions)\n[![codecov](https://codecov.io/gh/jni/zarpaint/branch/main/graph/badge.svg)](https://codecov.io/gh/jni/zarpaint)\n\nPaint segmentations directly to on-disk/remote zarr arrays\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `zarpaint` via [pip]:\n\n    pip install zarpaint\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"zarpaint\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/jni/zarpaint/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Add Points With Alt-Click in 3D",
      "Set Axis Labels",
      "Reorder Dims",
      "Create Labels Layer",
      "Split With Watershed",
      "Copy Data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-segmentation-overlap-filter",
    "name": "napari-segmentation-overlap-filter",
    "display_name": "Segmentation Filter Overlap",
    "version": "0.0.1",
    "created_at": "2024-06-24",
    "modified_at": "2024-06-24",
    "authors": [
      "Vanessa Dao"
    ],
    "author_emails": [
      "vanessadao31@yahoo.co.uk"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-segmentation-overlap-filter/",
    "home_github": "https://github.com/FrancisCrickInstitute/PARSEG",
    "home_other": null,
    "summary": "A simple plugin to remove overlapping segmentations from images",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# PARSEG\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/FrancisCrickInstitute/CALM_Template/HEAD?labpath=blob%2Fmain%2Fsegment_image.ipynb)\n[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3115/)\n![Commit activity](https://img.shields.io/github/commit-activity/y/FrancisCrickInstitute/CALM_Template?style=plastic)\n![GitHub](https://img.shields.io/github/license/FrancisCrickInstitute/CALM_Template?color=green&style=plastic)\n\nPARSEG (PAralellised Refinement of SEGmentations) combines segmentation masks and filters overlapping objects based on colocalization statistics, such as percent overlap. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Overview\nBy leveraging [Dask], PARSEG filters overlapping segmentations masks in a computationally efficient manner by processing individual 2D slices in parallel. \n\nThere are two different ways to interact with PARSEG and use it for different objectives:\n\n* As a napari plugin for graphical user interaction\n* As a Python API to allow users to integrate PARSEG tools into their own custom workflows\n\n## Installation\n\nYou can install `napari-segmentation-overlap-filter` via [pip]:\n\n    pip install napari-segmentation-overlap-filter\n\n## Getting Started\n\n### Napari Plugin\n1. Download the example dataset images\n2. Start napari and open both images as separate layers\n3. Convert the layers from an `Image Layer` to a `Labels Layer` by right-clicking on the layer\n4. Open the plugin with `Plugins > Segmentation Overlap Filter` and the widget will appear on the right\n5. Select the two segmentation masks you'd like to combine using the drop down and menu\n6. Drag the slider to set percent overlap allowed\n7. Click `Run`\n8. Optionally, export the overlap dataframe as a csv file\n\n### Python API\nThis [example notebook] shows how you can integrate the Python API into your own workflow for filtering and combining overlapping segmentation masks\n\n## Issues\n\nIf you encounter any problems, please file an issue along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[Dask]: https://www.dask.org/\n[pip]: https://pypi.org/project/pip/\n[example notebook]: https://github.com/FrancisCrickInstitute/PARSEG/blob/main/Notebooks/Combine_Segmentations_And_Filter_Overlaps.ipynb\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segmentation Overlap Filter"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-deeplabcut",
    "name": "napari-deeplabcut",
    "display_name": "napari DeepLabCut",
    "version": "0.2.1.7",
    "created_at": "2022-06-03",
    "modified_at": "2024-06-12",
    "authors": [
      "Team DeepLabCut",
      "Lead by Jessy Lauer"
    ],
    "author_emails": [
      "admin@deeplabcut.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-deeplabcut/",
    "home_github": "https://github.com/DeepLabCut/napari-deeplabcut",
    "home_other": null,
    "summary": "napari + DeepLabCut annotation tool",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "dask-image",
      "matplotlib >=3.3",
      "napari ==0.4.18",
      "natsort",
      "numpy",
      "opencv-python-headless",
      "pandas",
      "pyyaml",
      "qtpy >=2.4",
      "scikit-image",
      "scipy",
      "tables",
      "pyside6 ==6.4.2 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-deeplabcut: keypoint annotation for pose estimation\n\n\n\n<img src=\"https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1d409ffe-c9f4-47e1-bde2-3010c1c40455/naparidlc.png?format=750w\" width=\"450\" title=\"napari-deeplabcut\" alt=\"napari+deeplabcut\" align=\"right\" vspace = \"80\">\n\n[üìöDocumentation](https://deeplabcut.github.io/DeepLabCut/README.html) |\n[üõ†Ô∏è DeepLabCut Installation](https://deeplabcut.github.io/DeepLabCut/docs/installation.html) |\n[üåé Home Page](https://www.deeplabcut.org) |\n\n[![License: BSD-3](https://img.shields.io/badge/License-BSD3-blue.svg)](https://www.gnu.org/licenses/bsd3)\n[![PyPI](https://img.shields.io/pypi/v/napari-deeplabcut.svg?color=green)](https://pypi.org/project/napari-deeplabcut)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-deeplabcut.svg?color=green)](https://python.org)\n[![tests](https://github.com/DeepLabCut/napari-deeplabcut/workflows/tests/badge.svg)](https://github.com/DeepLabCut/napari-deeplabcut/actions)\n[![codecov](https://codecov.io/gh/DeepLabCut/napari-deeplabcut/branch/main/graph/badge.svg)](https://codecov.io/gh/DeepLabCut/napari-deeplabcut)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deeplabcut)](https://napari-hub.org/plugins/napari-deeplabcut)\n\nA napari plugin for keypoint annotation, also used within DeepLabCut!\n\n\n## Installation\n\nIf you installed DeepLabCut[gui], this plugin is already installed. However, you can also use this as a stand-alone keypoint annotator without using DeepLabCut. Instructions below!\n\nStart by installing PySide6 with `pip install \"pyside6==6.4.2\"`; this is the library we now use to build GUIs.\n\nYou can then install `napari-deeplabcut` via [pip]:\n\n    pip install napari-deeplabcut\n\n\n\nAlternatively, to install the latest development version, run:\n\n    pip install git+https://github.com/DeepLabCut/napari-deeplabcut.git\n\n\n## Usage\n\nTo use the plugin, please run:\n\n    napari\n\nThen, activate the plugin in Plugins > napari-deeplabcut: Keypoint controls.\n\nAll accepted files (config.yaml, images, h5 data files) can be loaded\neither by dropping them directly onto the canvas or via the File menu.\n\nThe easiest way to get started is to drop a folder (typically a folder from within a DeepLabCut's `labeled-data` directory), and, if labeling from scratch, drop the corresponding `config.yaml` to automatically add a `Points layer` and populate the dropdown menus.\n\n[üé• DEMO\n](https://youtu.be/hsA9IB5r73E)\n\n**Tools & shortcuts are:**\n\n- `2` and `3`, to easily switch between labeling and selection mode\n- `4`, to enable pan & zoom (which is achieved using the mouse wheel or finger scrolling on the Trackpad)\n- `M`, to cycle through regular (sequential), quick, and cycle annotation mode (see the description [here](https://github.com/DeepLabCut/napari-deeplabcut/blob/5a5709dd38868341568d66eab548ae8abf37cd63/src/napari_deeplabcut/keypoints.py#L25-L34))\n- `E`, to enable edge coloring (by default, if using this in refinement GUI mode, points with a confidence lower than 0.6 are marked\nin red)\n- `F`, to toggle between animal and body part color scheme.\n- `V`, to toggle visibility of the selected layer.\n- `backspace` to delete a point.\n- Check the box \"display text\" to show the label names on the canvas.\n- To move to another folder, be sure to save (`Ctrl+S`), then delete the layers, and re-drag/drop the next folder.\n- One can jump to a specific image by double-clicking and editing the current frame number (located to the right of the slider).\n- Selected points can be copied with `Ctrl+C`, and pasted onto other images with `Ctrl+V`.\n\n\n### Save Layers\n\nAnnotations and segmentations are saved with `File > Save Selected Layer(s)...` (or its shortcut `Ctrl+S`).\nOnly when saving segmentation masks does a save file dialog pop up to name the destination folder;\nkeypoint annotations are otherwise automatically saved in the corresponding folder as `CollectedData_<ScorerName>.h5`.\n- As a reminder, DLC will only use the H5 file; so be sure if you open already labeled images you save/overwrite the H5.\n- Note, before saving a layer, make sure the points layer is selected. If the user clicked on the image(s) layer first, does `Save As`, then closes the window, any labeling work during that session will be lost!\n- Modifying and then saving points in a `machinelabels...` layer will add to or overwrite the existing `CollectedData` layer and will **not** save to the `machinelabels` file.\n\n\n### Video frame extraction and prediction refinement\n\nSince v0.0.4, videos can be viewed in the GUI.\n\nSince v0.0.5, trailing points can be visualized; e.g., helping in the identification\nof swaps or outlier, jittery predictions.\n\nLoading a video (and its corresponding output h5 file) will enable the video actions\nat the top of the dock widget: they offer the option to manually extract video\nframes from the GUI, or to define cropping coordinates.\nNote that keypoints can be displaced and saved, as when annotating individual frames.\n\n\n## Workflow\n\nSuggested workflows, depending on the image folder contents:\n\n1. **Labeling from scratch** ‚Äì the image folder does not contain `CollectedData_<ScorerName>.h5` file.\n\n    Open *napari* as described in [Usage](#usage) and open an image folder together with the DeepLabCut project's `config.yaml`.\n    The image folder creates an *image layer* with the images to label.\n    Supported image formats are: `jpg`, `jpeg`, `png`.\n    The `config.yaml` file creates a *Points layer*, which holds metadata (such as keypoints read from the config file) necessary for labeling.\n    Select the *Points layer* in the layer list (lower left pane on the GUI) and click on the *+*-symbol in the layer controls menu (upper left pane) to start labeling.\n    The current keypoint can be viewed/selected in the keypoints dropdown menu (right pane).\n    The slider below the displayed image (or the left/right arrow keys) allows selecting the image to label.\n\n    To save the labeling progress refer to [Save Layers](#save-layers).\n    `Data successfully saved` should be shown in the status bar, and the image folder should now contain a `CollectedData_<ScorerName>.h5` file.\n    (Note: For convenience, a CSV file with the same name is also saved.)\n\n2. **Resuming labeling** ‚Äì the image folder contains a `CollectedData_<ScorerName>.h5` file.\n\n    Open *napari* and open an image folder (which needs to contain a `CollectedData_<ScorerName>.h5` file).\n    In this case, it is not necessary to open the DLC project's `config.yaml` file, as all necessary metadata is read from the `h5` data file.\n\n    Saving works as described in *1*.\n\n    ***Note that if a new body part has been added to the `config.yaml` file after having started to label, loading the config in the GUI is necessary to update the dropdown menus and other metadata.***\n\n    ***As `viridis` is `napari-deeplabcut` default colormap, loading the config in the GUI is also needed to update the color scheme.***\n\n3. **Refining labels** ‚Äì the image folder contains a `machinelabels-iter<#>.h5` file.\n\n    The process is analog to *2*.\n    Open *napari* and open an image folder.\n    If the video was originally labeled, *and* had outliers extracted it will contain a `CollectedData_<ScorerName>.h5` file and a `machinelabels-iter<#>.h5` file. In this case, select the `machinelabels` layer in the GUI, and type `e` to show edges. Red indicates likelihood < 0.6. As you navigate through frames, images with labels with edges will need to be refined (moved, deleted, etc). Images with labels without edges will be on the `CollectedData` (previous manual annotations) layer and shouldn't need refining. However, you can switch to that layer and fix errors. You can also right-click on the `CollectedData` layer and select `toggle visibility` to hide that layer. Select the `machinelabels` layer before saving which will append your refined annotations to `CollectedData`.\n\n    If the folder only had outliers extracted and wasn't originally labeled, it will not have a `CollectedData` layer. Work with the `machinelabels` layer selected to refine annotation positions, then save.\n\n    In this case, it is not necessary to open the DLC project's `config.yaml` file, as all necessary metadata is read from the `h5` data file.\n\n    Saving works as described in *1*.\n\n4. **Drawing segmentation masks**\n\n    Drop an image folder as in *1*, manually add a *shapes layer*. Then select the *rectangle* in the layer controls (top left pane),\n    and start drawing rectangles over the images. Masks and rectangle vertices are saved as described in [Save Layers](#save-layers).\n    Note that masks can be reloaded and edited at a later stage by dropping the `vertices.csv` file onto the canvas.\n\n### Workflow flowchart\n\n```mermaid\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n  id1[What stage of labeling?]\n  id2[deeplabcut.label_frames]\n  id3[deeplabcut.refine_labels]\n  id4[Add labels to, or modify in, \\n `CollectedData...` layer and save that layer]\n  id5[Modify labels in `machinelabels` layer and save \\n which will create a `CollectedData...` file]\n  id6[Have you refined some labels from the most recent iteration and saved already?]\n  id7[\"All extracted frames are already saved in `CollectedData...`.\n1. Hide or trash all `machinelabels` layers.\n2. Then modify in and save `CollectedData`\"]\n  id8[\"\n1. hide or trash all `machinelabels` layers except for the most recent.\n2. Select most recent `machinelabels` and hit `e` to show edges.\n3. Modify only in `machinelabels` and skip frames with labels without edges shown.\n4. Save `machinelabels` layer, which will add data to `CollectedData`.\n\t- If you need to revisit this video later, ignore `machinelabels` and work only in `CollectedData`\"]\n\n  id1 -->|I need to manually label new frames \\n or fix my labels|id2\n  id1 ---->|I need to refine outlier frames \\nfrom analyzed videos|id3\n  id2 -->id4\n  id3 -->|I only have a `machinelabels...` file|id5\n  id3 ---->|I have both `machinelabels` and `CollectedData` files|id6\n  id6 -->|yes|id7\n  id6 ---->|no, I just extracted outliers|id8\n```\n\n### Labeling multiple image folders\n\nLabeling multiple image folders has to be done in sequence; i.e., only one image folder can be opened at a time.\nAfter labeling the images of a particular folder is done and the associated *Points layer* has been saved, *all* layers should be removed from the layers list (lower left pane on the GUI) by selecting them and clicking on the trashcan icon.\nNow, another image folder can be labeled, following the process described in *1*, *2*, or *3*, depending on the particular image folder.\n\n\n### Defining cropping coordinates\n\nPrior to defining cropping coordinates, two elements should be loaded in the GUI:\na video and the DLC project's `config.yaml` file (into which the crop dimensions will be stored).\nThen it suffices to add a `Shapes layer`, draw a `rectangle` in it with the desired area,\nand hit the button `Store crop coordinates`; coordinates are automatically written to the configuration file.\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\nTo locally install the code, please git clone the repo and then run `pip install -e .`\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-deeplabcut\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[file an issue]: https://github.com/DeepLabCut/napari-deeplabcut/issues\n\n\n## Acknowledgements\n\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. We thank the Chan Zuckerberg Initiative (CZI) for funding the initial development of this work!\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mov",
      "*.png",
      "*.h5",
      "*.mp4",
      "*.jpeg",
      "*.jpg",
      "*",
      "*.avi",
      "*.yaml"
    ],
    "contributions_writers_filename_extensions": [
      ".csv",
      ".h5"
    ],
    "contributions_widgets": [
      "Keypoint controls"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-layer-table",
    "name": "napari-layer-table",
    "display_name": "Layer Table",
    "version": "0.0.13",
    "created_at": "2022-04-20",
    "modified_at": "2024-06-12",
    "authors": [
      "Robert Cudmore"
    ],
    "author_emails": [
      "rhcudmore@ucdavis.edu"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-layer-table/",
    "home_github": "https://github.com/mapmanager/napari-layer-table",
    "home_other": null,
    "summary": "A plugin to display a layer as a table.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy"
    ],
    "package_metadata_description": "# napari-layer-table\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![PyPI version](https://badge.fury.io/py/napari-layer-table.svg)](https://badge.fury.io/py/napari-layer-table)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-table)](https://napari-hub.org/plugins/napari-layer-table)\n[![Python](https://img.shields.io/badge/python-3.7|3.8|3.9|3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)\n[![OS](https://img.shields.io/badge/OS-Linux|Windows|macOS-blue.svg)]()\n[![tests](https://github.com/mapmanager/napari-layer-table/workflows/Tests/badge.svg)](https://github.com/mapmanager/napari-layer-table/actions)\n[![codecov](https://codecov.io/gh/mapmanager/napari-layer-table/branch/main/graph/badge.svg?token=8S8EFI8NBC)](https://codecov.io/gh/mapmanager/napari-layer-table)\n<!-- [![PyPI](https://img.shields.io/pypi/v/napari-layer-table.svg?color=green)](https://pypi.org/project/napari-layer-table) -->\n<!-- [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-table)](https://napari-hub.org/plugins/napari-layer-table) -->\n\nA plugin to display a layer as a table.\n\nThis will work well with point layers. We are debugging shapes and labeled layers, come back to check on that!\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-layer-table` via [pip]:\n\n    pip install napari-layer-table\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/mapmanager/napari-layer-table.git\n\n## Using the Plugin\n\nYou can use the napari-layer-table plugin to display points layer as a table.\n\n- Open a napari viewer with a Points layer\n- Add the plugin to the napari viewer from Plugins menu -> Add dock widget -> napari-layer-table: Points Table\n- The selected layer is displayed in the table.\n- The table has columns for:\n    - Point symbol with face color\n    - Point coordinates (x,y,z)\n    - If the layer has properties, they are also shown as columns\n\n![](plugin-2.gif)\n\n## Plugin Features\n\n- Bi-directional selection between layer and table.\n- Bi-directional deletion between layer and table.\n- Points added to the layer are added to the table.\n- Points moved in the layer are updated in the table.\n- Multiple points selected in the layer are also selected in the table\n- Changes to face color and symbol in the layer are updated in the table.\n- Ability to sort individual columns from low to high or high to low\n- `Refresh` button to manually refresh the table data\n- `btf` button to manually bring the layer whose table data is being shown to front\n\nRight-click for context menu to:\n\n- Toggle table columns on/off.\n- Toggle shift+click to add a point to the layer (no need to switch viewer mode)\n- Copy table to clipboard\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-layer-table\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/mapmanager/napari-layer-table/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Layer Table"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-stracking",
    "name": "napari-stracking",
    "display_name": "napari-stracking",
    "version": "0.1.10",
    "created_at": "2021-08-11",
    "modified_at": "2024-06-05",
    "authors": [
      "Sylvain Prigent"
    ],
    "author_emails": [
      "sylvain.prigent@inria.fr"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-stracking/",
    "home_github": "https://github.com/sylvainprigent/napari-stracking",
    "home_other": null,
    "summary": "Linking and tracks analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "stracking >=0.1.10"
    ],
    "package_metadata_description": "# napari-stracking\n\n[![License](https://img.shields.io/pypi/l/napari-stracking.svg?color=green)](https://github.com/sylvainprigent/napari-stracking/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-stracking.svg?color=green)](https://pypi.org/project/napari-stracking)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stracking.svg?color=green)](https://python.org)\n[![tests](https://github.com/sylvainprigent/napari-stracking/workflows/tests/badge.svg)](https://github.com/sylvainprigent/napari-stracking/actions)\n[![codecov](https://codecov.io/gh/sylvainprigent/napari-stracking/branch/master/graph/badge.svg)](https://codecov.io/gh/sylvainprigent/napari-stracking)\n\n`napari-stracking` is a suite of **Napari** plugins for the [stracking library](\nhttps://sylvainprigent.github.io/stracking/) dedicated to particles tracking in 2D+t and 3D+t images. Each step of\nthe tracking process is separated in independent plugins to ease the creation of pipelines.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-stracking` via [pip]:\n\n    pip install napari-stracking\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-stracking\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/sylvainprigent/napari-stracking/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SParticlesProperties",
      "SDetectorDog",
      "SDetectorDoh",
      "SDetectorLog",
      "SDetectorSeg",
      "STracksFeatures",
      "SLinkerShortestPath",
      "SFilterTrack",
      "SScale",
      "SLoad",
      "SExport",
      "SPipeline"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-open-ctc",
    "name": "napari-open-ctc",
    "display_name": "Open CTC data",
    "version": "0.1.2",
    "created_at": "2024-06-03",
    "modified_at": "2024-06-03",
    "authors": [
      "Benjamin Gallusser"
    ],
    "author_emails": [
      "benjamin.gallusser@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-open-ctc/",
    "home_github": "https://github.com/bentaculum/napari-open-ctc",
    "home_other": null,
    "summary": "\"Drag and drop annotations/results in the Cell Tracking Challenge (CTC) format into napari.\"",
    "categories": [
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "scikit-image",
      "tifffile",
      "pandas",
      "imagecodecs",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-open-ctc\n\n[![PyPI](https://img.shields.io/pypi/v/napari-open-ctc.svg?color=green)](https://pypi.org/project/napari-open-ctc)\n[![tests](https://github.com/bentaculum/napari-open-ctc/workflows/tests/badge.svg)](https://github.com/bentaculum/napari-open-ctc/actions)\n[![codecov](https://codecov.io/gh/bentaculum/napari-open-ctc/branch/main/graph/badge.svg)](https://codecov.io/gh/bentaculum/napari-open-ctc)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-open-ctc)](https://napari-hub.org/plugins/napari-open-ctc)\n\nDrag and drop annotations/results in the [Cell Tracking Challenge (CTC) format](https://celltrackingchallenge.net) into napari.\n\nWorks for `TRA`, `RES`, etc. folders, which contain a time sequence of segmentations in `tiff` format, and a corresponding tracklet file `*.txt`.\n\nhttps://github.com/bentaculum/napari-open-ctc/assets/8866751/197c9ea2-4115-4829-851a-4b77eb843bf2\n\n\n## Installation\n\nYou can install `napari-open-ctc` via [pip]:\n\n    pip install napari-open-ctc\n\n\n\nTo install latest development version :\n\n\n    pip install git+https://github.com/bentaculum/napari-open-ctc.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n`napari-open-ctc` is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bentaculum/napari-open-ctc/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*RES",
      "TRA"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-utrack-loader",
    "name": "napari-utrack-loader",
    "display_name": "Utrack Loader",
    "version": "0.0.1",
    "created_at": "2024-05-31",
    "modified_at": "2024-05-31",
    "authors": [
      "Jules Vanaret"
    ],
    "author_emails": [
      "jules.vanaret@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-utrack-loader/",
    "home_github": "https://github.com/jules-vanaret/napari-utrack-loader",
    "home_other": null,
    "summary": "A simple plugin to load images, detections and tracks from the u-track software into napari",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tifffile",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-utrack-loader\n\n[![License MIT](https://img.shields.io/pypi/l/napari-utrack-loader.svg?color=green)](https://github.com/jules-vanaret/napari-utrack-loader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-utrack-loader.svg?color=green)](https://pypi.org/project/napari-utrack-loader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-utrack-loader.svg?color=green)](https://python.org)\n[![tests](https://github.com/jules-vanaret/napari-utrack-loader/workflows/tests/badge.svg)](https://github.com/jules-vanaret/napari-utrack-loader/actions)\n[![codecov](https://codecov.io/gh/jules-vanaret/napari-utrack-loader/branch/main/graph/badge.svg)](https://codecov.io/gh/jules-vanaret/napari-utrack-loader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-utrack-loader)](https://napari-hub.org/plugins/napari-utrack-loader)\n\nA simple plugin to use FooBar segmentation within napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-utrack-loader` via [pip]:\n\n    pip install napari-utrack-loader\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jules-vanaret/napari-utrack-loader.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-utrack-loader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jules-vanaret/napari-utrack-loader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Utrack Loader"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "yt-napari",
    "name": "yt-napari",
    "display_name": "yt-napari",
    "version": "0.5.0",
    "created_at": "2022-05-02",
    "modified_at": "2024-05-23",
    "authors": [
      "Chris Havlin"
    ],
    "author_emails": [
      "chris.havlin@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/yt-napari/",
    "home_github": "https://github.com/data-exp-lab/yt-napari",
    "home_other": null,
    "summary": "A napari plugin for loading data from yt",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui >=0.6.1",
      "napari >=0.4.19",
      "numpy",
      "packaging",
      "pydantic >2.0",
      "qtpy",
      "unyt",
      "yt >=4.0.1",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "taskipy ; extra == 'dev'",
      "sphinx ; extra == 'docs'",
      "nbsphinx <0.8.8 ; extra == 'docs'",
      "sphinx-jsonschema <1.19.0 ; extra == 'docs'",
      "Jinja2 <3.1.0 ; extra == 'docs'",
      "dask[array,distributed] ; extra == 'full'"
    ],
    "package_metadata_description": "# yt-napari\n\n[![License](https://img.shields.io/pypi/l/yt-napari.svg?color=green)](https://github.com/data-exp-lab/yt-napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/yt-napari.svg?color=green)](https://pypi.org/project/yt-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/yt-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/data-exp-lab/yt-napari/workflows/tests/badge.svg)](https://github.com/data-exp-lab/yt-napari/actions)\n[![codecov](https://codecov.io/gh/data-exp-lab/yt-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/data-exp-lab/yt-napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/yt-napari)](https://napari-hub.org/plugins/yt-napari)\n[![Documentation Status](https://readthedocs.org/projects/yt-napari/badge/?version=latest)](https://yt-napari.readthedocs.io/en/latest/?badge=latest)\n\nA [napari] plugin for loading data from [yt].\n\nThis readme provides a brief overview including:\n\n1. [Installation](#Installation)\n2. [Quick Start](#Quick-Start)\n3. [Contributing](#Contributing)\n\nFull documentation is available at [yt-napari.readthedocs.io].\n\n## Installation\n\n### 1. (optional) install `yt` and `napari`\n\nIf you skip this step, the installation in the following section will only install the minimal package requirements for `yt` or `napari`, in which case you will likely need to manually install some packages. So if you are new to either package, or if you are installing in a clean environment, it may be simpler to  install these packages first.\n\nFor `napari`,\n\n    pip install napari[all]\n\nwill install `napari` with the default `Qt` backend (see [here](https://napari.org/tutorials/fundamentals/installation#choosing-a-different-qt-backend) for how to choose between `PyQt5` or `PySide2`).\n\nFor `yt`, you will need `yt>=4.0.1` and any of the optional dependencies for your particular workflow. If you know that you'll need more than the base `yt` install, you can install the full suite of dependent packages with\n\n    pip install yt[full]\n\nSee the [`yt` documentation](https://yt-project.org/doc/installing.html#leveraging-optional-yt-runtime-dependencies) for more information. If you're not sure which packages you'll need but don't want the full yt installation, you can proceed to the next step and then install any packages as needed (you will receive error messages when a required package is missing).\n\n### 2. install `yt-napari`\n\nYou can install the `yt-napari` plugin with:\n\n    pip install yt-napari\n\nIf you are missing either `yt` or `napari` (or they need to be updated), the above installation will fetch and run a minimal installation for both.\n\nTo install the latest development version of the plugin instead, use:\n\n    pip install git+https://github.com/data-exp-lab/yt-napari.git\n\nNote that if you are working off the development version, be sure to use the latest documentation\nfor reference: https://yt-napari.readthedocs.io/en/latest/\n\n## Quick Start\n\nAfter [installation](#Installation), there are three modes of using `yt-napari`:\n\n1. jupyter notebook interaction ([jump down](#jupyter-notebook-interaction))\n2. loading a json file from the napari gui ([jump down](#loading-a-json-file-from-the-napari-gui))\n3. napari widget plugins ([jump down](#napari-widget-plugins))\n\n### jupyter notebook interaction\n\n`yt-napari` provides a helper class, `yt_napari.viewer.Scene` that assists in properly aligning new yt selections in the napari viewer when working in a Jupyter notebook.\n\n```python\nimport napari\nimport yt\nfrom yt_napari.viewer import Scene\nfrom napari.utils import nbscreenshot\n\nviewer = napari.Viewer()\nds = yt.load(\"IsolatedGalaxy/galaxy0030/galaxy0030\")\nyt_scene = Scene()\n\nleft_edge = ds.domain_center - ds.arr([40, 40, 40], 'kpc')\nright_edge = ds.domain_center + ds.arr([40, 40, 40], 'kpc')\nres = (600, 600, 600)\n\nyt_scene.add_region(viewer,\n                    ds,\n                    (\"enzo\", \"Temperature\"),\n                    left_edge=left_edge,\n                    right_edge=right_edge,\n                    resolution=res)\n\nyt_scene.add_region(viewer,\n                    ds,\n                    (\"enzo\", \"Density\"),\n                    left_edge=left_edge,\n                    right_edge=right_edge,\n                    resolution=res)\n\nnbscreenshot(viewer)\n```\n\n ![Loading a subset of a yt dataset in napari from a Jupyter notebook](./assets/images/readme_ex_001.png)\n\n`yt_scene.add_to_viewer` accepts any of the keyword arguments allowed by `viewer.add_image`. See the full documentation ([yt-napari.readthedocs.io]) for more examples, including additional helper methods for linking layer appearance.\n\nAdditionally, with `yt_napari`>= v0.2.0, you can use the `yt_napari.timeseries` module to help sample and load in selections from across datasets.\n\n### loading a selection from a yt dataset interactively\n\n`yt-napari` provides two ways to sample a yt dataset and load in an image layer into a Napari viewer: the yt Reader plugin and json file specification.\n\n#### using the yt Reader plugin\n\nTo use the yt Reader plugin, click on `Plugins -> yt-napari: yt Reader`. From there, add a region or slice selector then specify a field type and field and bounds to sample  between and then simply click \"Load\":\n\n![Loading a subset of a yt dataset from the napari viewer](./assets/images/readme_ex_003_gui_reader.gif)\n\nYou can add multiple selections and load them all at once or adjust values and click \"Load\" again.\n\n#### using the yt Time Series Reader plugin\n\nTo use the yt Time Series Reader plugin, click on  `Plugins -> yt-napari: yt Time Series Reader`. Specify your file matching: use `file_pattern` to enter glob expressions or use `file_list` to enter a list of specific files.\nThen add a slice or region to sample for each matched dataset file (note: be careful of memory here!):\n\n![Loading timeseries selections from the napari viewer](./assets/images/readme_ex_004_gui_timeseries.gif)\n\n#### using a json file and schema\n\n`yt-napari` also provides the ability to load json that contain specifications for loading a file. Properly formatted files can be loaded from the napari GUI as you would load any image file (`File->Open`). The json file describes the selection process for a dataset as described by a json-schema. The following json file results in similar layers as the above examples:\n\n\n```json\n{\"$schema\": \"https://raw.githubusercontent.com/data-exp-lab/yt-napari/main/src/yt_napari/schemas/yt-napari_0.0.1.json\",\n \"datasets\": [{\"filename\": \"IsolatedGalaxy/galaxy0030/galaxy0030\",\n               \"selections\": {\"regions\": [{\n                                \"fields\": [{\"field_name\": \"Temperature\", \"field_type\": \"enzo\", \"take_log\": true},\n                                           {\"field_name\": \"Density\", \"field_type\": \"enzo\", \"take_log\": true}],\n                                \"left_edge\": [460.0, 460.0, 460.0],\n                                \"right_edge\": [560.0, 560.0, 560.0],\n                                \"resolution\": [600, 600, 600]\n                              }]},\n               \"edge_units\": \"kpc\"\n             }]\n}\n```\n\nTo help in filling out a json file, it is recommended that you use an editor capable of parsing a json schema and displaying hints. For example, in vscode, you will see field suggestions after specifying the `yt-napari` schema:\n\n![interactive json completion for yt-napari](./assets/images/readme_ex_002_json.png)\n\nSee the full documentation at [yt-napari.readthedocs.io] for a complete specification.\n\n\n## Contributing\n\nContributions are very welcome! Development follows a fork and pull request workflow. To get started, you'll need a development installation and a testing environment.\n\n### development environment\n\nTo start developing, fork the repository and clone your fork to get a local copy. You can then install in development mode with\n\n    pip install -e .\n\n### tests and style checks\n\nBoth bug fixes and new features will need to pass the existing test suite and style checks. While both will be run automatically when you submit a pull request, it is helpful to run the test suites locally and run style checks throughout development. For testing, you can use [tox] to test different python versions on your platform.\n\n    pip install tox\n\nAnd then from the top level of the `yt-napari` directory, run\n\n    tox\n\nTox will then run a series of tests in isolated environments. In addition to checking the terminal output for test results, the tox run will generate a test coverage report: a `coverage.xml` file and a `htmlcov` folder -- to view the results, open `htmlcov/index.html` in a browser.\n\nIf you prefer a lighter weight test, you can also use `pytest` directly and rely on the Github CI to test different python versions and systems. To do so, first install `pytest` and some related plugins:\n\n    pip install pytest pytest-qt pytest-cov\n\nNow, to run the tests:\n\n    pytest -v --cov=yt_napari --cov-report=html\n\nIn addition to telling you whether or not the tests pass, the above command will write out a code coverage report to the `htmlcov` directory. You can open up `htmlcov/index.html` in a browser and check out the lines of code that were missed by existing tests.\n\nFor style checks, you can use [pre-commit](https://pre-commit.com/) to run checks as you develop. To set up `pre-commit`:\n\n    pip install pre-commit\n    pre-commit install\n\nafter which, every time you run `git commit`, some automatic style adjustments and checks will run. The same style checks will run when you submit a pull request, but it's often easier to catch them early.\n\nAfter submitting a pull request, the `pre-commit.ci` bot will run the style checks. If style checks fail, you can have the bot attempt to auto-fix the failures by adding the following in a comment on it's own:\n\n    pre-commit.ci autofix\n\nThe bot will then commit changes to your pull request after which you will want to run `git pull` locally to update your local version of the branch before making further changes to the branch.\n\n### building documentation locally\n\nDocumentation can be built using `sphinx` in two steps. First, update the api mapping with\n\n    sphinx-apidoc -f -o docs/source src/yt_napari/\n\nThis will update the `rst` files in `docs/source/` with the latest docstrings in `yt_napari`. Next, build the html documentation with\n\n    make html\n\n\n### updating the pydantic models and schema\n\nThe schema versioning follows a `major.minor.micro` versioning pattern that matches the yt-napari versioning. Each yt-napari release should have an accompanying updated schema file, even if the  contents of the schema file have not changed. On-disk schema are stored in  `src/yt_napari/schemas/`, with copies in the documentation at `docs/_static`.\n\nThere are a number of utilities to help automate the management of schema in `repo_utilities/`. The easiest way to use these utitities is with `taskipy` from the command line. To list available scripts:\n\n```commandline\ntask --list\n```\n\nBefore a release, run\n\n```commandline\ntask validate_release vX.X.X\n```\n\nwhere `vX.X.X` is the version of the upcoming release. This script will run through some checks that ensure:\n* the on-disk schema matches the schema generated by the pydantic model\n* the schema files in the documentation match the schema files in the package\n\nIf any of the checks fail, you will be advised to update the schema using `update_schema_docs`. If you\nrun without providing a version:\n\n```commandline\ntask update_schema_docs\n```\n\nIt will simply copy over the existing on-disk schema files to the documentation. If you run with a version:\n\n```commandline\ntask update_schema_docs -v vX.X.X\n```\nIt will write a schema file for the current pydantic model, overwriting any on-disk schema files for\nthe provided version.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"yt-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Funding\n\nThe yt-napari plugin project was funded with support from the Chan Zuckerberg Initiative through the napari Plugin Accelerator Grants project, [Enabling Access To Multi-resolution Data](https://chanzuckerberg.com/science/programs-resources/imaging/napari/enabling-access-to-multi-resolution-data/).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[yt-napari.readthedocs.io]: https://yt-napari.readthedocs.io/en/stable/\n\n[file an issue]: https://github.com/data-exp-lab/yt-napari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[yt]: https://yt-project.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.json"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "yt Reader",
      "yt Time Series Reader",
      "yt Metadata Explorer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pixseq",
    "name": "napari-PixSeq",
    "display_name": "PixSeq",
    "version": "1.0.3",
    "created_at": "2024-02-29",
    "modified_at": "2024-05-21",
    "authors": [
      "Piers Turner"
    ],
    "author_emails": [
      "piers.turner@physics.ox.ac.uk"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-pixseq/",
    "home_github": "https://github.com/piedrro/napari-PixSeq",
    "home_other": null,
    "summary": "A Napari plugin for extracting time series traces from Single Molecule Localisation Microsocpy (SMLM) data.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari[all]",
      "numpy",
      "magicgui",
      "qtpy",
      "scipy",
      "pyqtgraph",
      "picassosr",
      "pandas",
      "matplotlib",
      "opencv-python",
      "tqdm",
      "originpro",
      "pyqt5-tools",
      "torch",
      "cellpose >=3.0.1",
      "omnipose",
      "trackpy",
      "shapely",
      "astropy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-PixSeq\n\n[![License MIT](https://img.shields.io/pypi/l/napari-GapSeq2.svg?color=green)](https://github.com/piedrro/napari-PixSeq/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-GapSeq2.svg?color=green)](https://pypi.org/project/napari-PixSeq/)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-GapSeq2.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-GapSeq2)](https://napari-hub.org/plugins/napari-PixSeq)\n\nA **Napari** plugin for extracting time series traces from single molecule FRET data.\n\nnapari-PixSeq uses **Picasso** (picassosr) as a backend and includes features for **aligning** image channels/datasets, **undrifting** images, **detecting/fitting** localisations and extracting **traces**, and supports both **ALEX** and **FRET** data. Traces can be exported in different formats for downstream analysis.\n\nnapari-PixSeq traces can be analysed with TraceAnalyser: https://github.com/piedrro/TraceAnalyser\n\nThis is still undergoing development, so some features may not work as expected.\n\nThis was built by Dr Piers Turner from the Kapanidis Lab, University of Oxford.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-PixSeq` via [pip]:\n\n    pip install napari-PixSeq\n\nYou can install `napari-PixSeq` via [GitHub]:\n\n    conda create ‚Äì-name napari-pixseq python==3.9\n    conda activate napari-pixseq\n    conda install -c anaconda git\n    conda update --all\n\n    pip install git+https://github.com/piedrro/napari-PixSeq.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-PixSeq\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/piedrro/napari-GapSeq2/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PixSeq"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-philow",
    "name": "napari-PHILOW",
    "display_name": "napari-PHILOW",
    "version": "0.2.0",
    "created_at": "2022-05-02",
    "modified_at": "2024-05-20",
    "authors": [
      "Hiroki Kawai"
    ],
    "author_emails": [
      "h.kawai888@gmail.com"
    ],
    "license": "GPLv3",
    "home_pypi": "https://pypi.org/project/napari-philow/",
    "home_github": "https://github.com/neurobiology-ut/PHILOW",
    "home_other": null,
    "summary": "PHILOW is an interactive deep learning-based platform for 3D datasets",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "scikit-image",
      "dask-image",
      "opencv-python",
      "matplotlib",
      "pandas",
      "torch",
      "torchvision",
      "segmentation-models-pytorch",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-PHILOW\n\n[![License](https://img.shields.io/pypi/l/napari-PHILOW.svg?color=green)](https://github.com/neurobiology-ut/PHILOW/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-PHILOW.svg?color=green)](https://pypi.org/project/napari-PHILOW)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-PHILOW.svg?color=green)](https://python.org)\n[![tests](https://github.com/neurobiology-ut/napari-PHILOW/workflows/tests/badge.svg)](https://github.com/neurobiology-ut/PHILOW/actions)\n[![codecov](https://codecov.io/gh/neurobiology-ut/napari-PHILOW/branch/main/graph/badge.svg)](https://codecov.io/gh/neurobiology-ut/PHILOW)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-PHILOW)](https://napari-hub.org/plugins/napari-PHILOW)\n\n# PHILOW <br>\n***P***ython-based platform for ***h***uman-***i***n-the-***lo***op (HITL)  ***w***orkflow (PHILOW) <br>\n\nPHILOW is an interactive deep learning-based platform for 3D datasets implemented on top of [napari](https://github.com/napari/napari)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nInstall napari and Pytorch first.   \nSee [napari] and [Pytorch](https://pytorch.org/) for more information.\n\nYou can install `napari-PHILOW` via [pip]:\n\n    pip install napari-PHILOW\n    \nor clone this repository   \nthen\n```angular2\ncd PHILOW\npip install -e .\n```\n    \n\n## Usage\n\nLaunch napari \n\n```angular2\nnapari\n```\n\n\n#### load dataset\n\n\n1) Plugins > napari-PHILOW > Annotation Mode\n\n2) Select original dir : all slices must be in separate 8bit PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)\n\n3) Select mask dir : To resume from the middle of the annotation, specify here the name of the directory containing the mask image. The directory must contain the same number of files with the same name as the original image.   \n If you are starting a completely new annotation, you do not need to specify a directory. The directory for mask is automatically created and blank images are generated and stored.\n\n4) Enter a name for the label or model you want to create (e.g. mito, cristae, ...)   \nThis name will be used as the directory name of the newly created mask dir if no mask dir is specified, \nand as the name of the csv file for training dataset management.\n\n5) Check if you want to create new dataset (new model)\nWhen checked, if there is already a csv file for training dataset management, a new csv file with one sequential number will be generated.\n\n6) Start tracing\n\n\n#### create labels\nCreate a label with the brush function.\nmore information ‚Üí https://napari.org/tutorials/fundamentals/labels.html\n\n#### Orthogonal view\nIf you want to see orthogonal view, click on the location you want to see while holding down the Shift button.    \nThe image from xy, yz, and zx will be displayed on the right side of the screen.\n\n#### Low confident layer\nIf you are in the second iteration and you are loading the prediction results, you will see a low confidence layer.    \nThis shows the area where the confidence of the prediction result is low.    \nUse this as a reference for correction.   \n\n#### Small object layer\nWe provide a small object layer to find small painted areas.   \nThis is a layer for displaying small objects.   \nThe slider widget on the left allows you to change the maximum object size to be displayed.   \n\n#### save labels\nIf you want to save your label, click the \"save\" button on the bottom right.\n\n#### select training dataset\nWe are providing a way to manage the dataset for use in training.   \nIf you want to use the currently displayed slice as your training data, click the 'Not Checked' button near the center left to display 'Checked'.\n\n\n### Train and pred with your gpu machine\n#### Train\nTo train on your GPU machine (or with CPU), \n\n1) Plugins > napari-PHILOW > Trainer\n   \n2) Select original dir : all slices must be in separate 8bit PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \n   \n3) Select labels dir : all label images should be named same as original images and contains data management csv file   \n   \n4) Select dir for save trained model   \n   \n5) Click on the \"start training\" button   \n\n6) Dice score and dice loss are displayed. For more detail, check the command line for the progress of training. If you want to stop in the middle, click stop button.   \n\n##### IF YOU WANT TO SEGMENT CRISTAE AREA IN THE EM DATASET\n\n1) Plugins > napari-PHILOW > Trainer\n\n2) Click on the \"Cristae segmentation mode\" button   \n\n3) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)  \n\n4) Select mito mask dir : all label images should be named same as original images\n\n5) Select dir for save trained model  \n\n6) Select cristae labels dir : all label images should be named same as original images and contains data management csv file  \n\n7) Click on the \"start training\" button   \n\n8) Dice score and dice loss are displayed. For more detail, check the command line for the progress of training. If you want to stop in the middle, click stop button.   \n   \n#### Predict\nTo predict labels on your machine,  \n\n1) Plugins > napari-PHILOW > Predicter\n   \n2) Select original dir : all slices must be in separate 8bit PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \n   \n3) (Optional) Select labels dir if you want to keep labels witch were used on training, and data management csv file   \n   \n4) Select model dir contains hdf5 file   \n   \n5) Select output dir for predicted labels   \n\n6) Uncheck the box if you DO NOT want to use TAP (Three-Axis-Prediction)   \n   \n7) Click on the \"predict\" button  \n\n8) Check the command line for the progress of prediction. If you want to stop in the middle, use ctrl+C.   \n\n9) You can start the next round of annotation by selecting the merged_prediction directory as the mask dir in Annotation mode.\n\n##### IF YOU WANT TO SEGMENT CRISTAE AREA IN THE EM DATASET\n\n1) Plugins > napari-PHILOW > Predicter\n\n2) Select original dir : all slices must be in separate PNG and must be sequentially numbered (e.g. 000.png, 001.png ...)   \n\n3) (Optional) Select cristae labels dir if you want to keep labels witch were used on training, and data management csv file  \n\n4) Select model dir contains hdf5 file   \n\n5) Select output dir for predicted labels   \n\n6) Uncheck the box if you DO NOT want to use TAP (Three-Axis-Prediction)   \n\n7) Click on the \"Use cristae inference mode\" button   \n\n8) Select mitochondria mask dir : all label images should be named same as original images\n\n9) Click on the \"predict\" button  \n\n10) Check the command line for the progress of prediction. If you want to stop in the middle, use ctrl+C.   \n\n11) You can start the next round of annotation by selecting the merged_prediction directory as the mask dir in Annotation mode.\n\n### Train and predict with Google Colab   \nIf you don't have a GPU machine, you can use Google Colab to perform GPU-based training and prediction for free.    \n\n1) Open [train and predict notebook](https://github.com/neurobiology-ut/PHILOW/blob/develop/notebooks/train_and_pred_using_PHILOW.ipynb) and click \"Open in Colab\" button\n\n2) You can upload your own dataset to train and predict, or try it on demo data   \n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-PHILOW\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n# Authors <br>\n\nShogo Suga <br>\nHiroki Kawai <br>\n<a href=\"http://park.itc.u-tokyo.ac.jp/Hirabayashi/WordPress/\">Yusuke Hirabayashi</a> \n\n\n# How to Cite <br>\nShogo Suga, Koki Nakamura, Yu Nakanishi, Bruno M Humbel, Hiroki Kawai, Yusuke Hirabayashi, An interactive deep learning-based approach reveals mitochondrial cristae topologies. PLoS Biol 21(8): e3002246.\n<a href=\"https://doi.org/10.1371/journal.pbio.3002246\">https://doi.org/10.1371/journal.pbio.3002246</a>\n\n\n```\n@article {Suga_Nakamura_Nakanishi_Humbel_Kawai_Hirabayashi_2023,\n\ttitle={An interactive deep learning-based approach reveals mitochondrial cristae topologies},\n\tvolume={21},\n\tISSN={1545-7885},\n\tDOI={10.1371/journal.pbio.3002246},\n\tnumber={8},\n\tjournal={PLOS Biology},\n\tpublisher={Public Library of Science},\n\tauthor={Suga, Shogo and Nakamura, Koki and Nakanishi, Yu and Humbel, Bruno M. and Kawai, Hiroki and Hirabayashi, Yusuke},\n\tyear={2023},\n\tmonth={Aug},\n\tpages={e3002246},\n\tlanguage={en}\n}\n```\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotation Mode",
      "Trainer",
      "Predicter",
      "Selector"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tomodl",
    "name": "napari-tomodl",
    "display_name": "ToMoDL Reconstruction",
    "version": "0.1.17",
    "created_at": "2023-03-15",
    "modified_at": "2024-05-20",
    "authors": [
      "Marcos Antonio Obando",
      "Germ√°n Mato",
      "Teresa Correia"
    ],
    "author_emails": [
      "marcos.obando@ib.edu.ar"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tomodl/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin for optical projection tomography reconstruction with model-based neural networks.",
    "categories": [],
    "package_metadata_requires_python": "<=3.9",
    "package_metadata_requires_dist": [
      "magicgui",
      "qtpy",
      "napari[all]",
      "pyqt5",
      "phantominator",
      "opencv-python",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-tomodl\n\n[![License MIT](https://img.shields.io/pypi/l/napari-tomodl.svg?color=green)](https://github.com/marcoso96/napari-tomodl/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tomodl.svg?color=green)](https://pypi.org/project/napari-tomodl)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tomodl.svg?color=green)](https://python.org)\n[![tests](https://github.com/marcoso96/napari-tomodl/workflows/tests/badge.svg)](https://github.com/marcoso96/napari-tomodl/actions)\n[![codecov](https://codecov.io/gh/marcoso96/napari-tomodl/branch/main/graph/badge.svg)](https://codecov.io/gh/marcoso96/napari-tomodl)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tomodl)](https://napari-hub.org/plugins/napari-tomodl)\n\nA plugin for optical projection tomography reconstruction with model-based neural networks.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n## Introduction and usage\n\nToMoDL allows users to reconstruct tomography images from its raw projections juts from uploading them as an ordered stack of files into the napari viewer.\n\n1 - Load ordered stack: Click File -> Open Files as Stack... and load the angular projections for parallel beam optical tomography reconstruction.\n\n![plot](https://raw.githubusercontent.com/marcoso96/ToMoDL/main/napari-tomodl/figures/fig3.png)\n\n2 - Select the current volume in the dropdown menu with the button 'Select image layer'. Notice that the projections should be in grayscale and more than one slide in the stack.\n\n![plot](https://raw.githubusercontent.com/marcoso96/ToMoDL/main/napari-tomodl/figures/fig4.png)\n\n3 - If the axis is not correctly aligned in acquisition time, we provide an algorithm to do so by clicking on 'Align axis'. This will align the sinogram respect to the center of the detector in order to maximise the variance of the reconstructions. See Walls et al. \n\n4 - Reshape the reconstructed volume to a desired size. This can be useful to prevent exhausting your computing capabilities.\n\n5 - Clip to circle should be False by default.\n\n6 - Choose if filtering should be used. By the moment we only allow using ramp filtering for FBP only (both CPU and GPU).\n\n7 - Choose the correct order of the axis of the projections (T -> Theta axis, Q -> Detector axis)\n\n8 - Reconstruct! A new Layer should be created on top of the projections stack containing the reconstructed volume.\n\n![plot](https://raw.githubusercontent.com/marcoso96/ToMoDL/main/napari-tomodl/figures/fig2.png)\n\n## Installation\n\nThis package requires [torch-radon] for optimized GPU tomographic reconstruction:\n\n    pip install 'torch-radon @ https://rosh-public.s3-eu-west-1.amazonaws.com/radon-v2/cuda-11.1/torch-1.8/torch_radon-2.0.0-cp38-cp38-linux_x86_64.whl'\n\nand `PyTorch == 1.8.0` via wheel, which can be downloaded and installed with: \n\n    pip install 'torch @ https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp38-cp38-linux_x86_64.whl'\n\nYou can install `napari-tomodl` via [pip]:\n\n    pip install napari-tomodl\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-tomodl\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[torch-radon]: https://github.com/matteo-ronchetti/torch-radon\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "OPT reconstruction"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-segment-everything",
    "name": "napari-segment-everything",
    "display_name": "Napari Segment Everything",
    "version": "0.1.6",
    "created_at": "2024-04-01",
    "modified_at": "2024-05-17",
    "authors": [
      "Brian Northan",
      "Ian Coccimiglio"
    ],
    "author_emails": [
      "bnorthan@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-segment-everything/",
    "home_github": "https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything",
    "home_other": null,
    "summary": "A Napari SAM plugin to segment everything in your image (not just some things)",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "torch",
      "torchvision",
      "segment-anything",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari",
      "gdown",
      "opencv-python",
      "timm",
      "torchpack",
      "onnx",
      "onnxsim",
      "matplotlib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-segment-everything\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-segment-everything.svg?color=green)](https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-segment-everything.svg?color=green)](https://pypi.org/project/napari-segment-everything)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment-everything.svg?color=green)](https://python.org)\n[![tests](https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything/workflows/tests/badge.svg)](https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything/actions)\n[![codecov](https://codecov.io/gh/True-North-Intelligent-Algorithms/napari-segment-everything/branch/main/graph/badge.svg)](https://codecov.io/gh/True-North-Intelligent-Algorithms/napari-segment-everything)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment-everything)](https://napari-hub.org/plugins/napari-segment-everything)\n\nA Napari SAM plugin to segment everything in your image (not just some things)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\nhttps://github.com/True-North-Intelligent-Algorithms/napari-segment-everything/assets/4366342/1f451e4a-bf66-4b77-a91d-4fa283270160\n\n## Instructions\n\n### 0. Select recipe (implementation)\n\nUse the 'select recipe' combo box to choose the implementation.   Currently 'Mobile SAM v2', 'Mobile SAM finetuned' and 'SAM Automatic Mask Generator' are available.  Not that the sub-options will change slightly depending on which recipe you choose.  'Mobile SAM v2' and 'Mobile SAM finetuned' (finetuned using Cellpose training data) first use a bounding box detector to locate objects then feed the bounding boxes to SAM.  'SAM Automatic Mask Generator' uses a grid of points as the prompt for SAM.  Our experiments indicate that the 'Mobile SAM' recipes work well in most cases.  'SAM Automatic Mask Generator' may be useful for cases where bounding box detection was sub-optimal.  \n\n### 1. Generate 3D labels\n\nIn the first step adjust SAM settings and generate a 3D representation of your labels.  The 3D view is needed to represent overlapping labels (labels that overlap in xy can be represented at different z).  After tweaking settings press 'Generate 3D labels'.  Be patient.  SAM with permissive settings can potentially find thousands of labels in a complicated image.  At least 6G of GPU memory is recommended to run SAM and to render to 3D label map (which can be large). \n\n### 2. Filter 3D labels\n\nIn the next step select a stat (solidity, hue, IOE, stability and other stats are available) then use the sliders and number boxes to filter out labels that do not represent structure of interest.  If you double click on a label a popup will appear containing the stats for that label.  Inpect stats for labels you want to keep, and labels you want to eliminate to help determine the filter settings. \n\n### 3. Generate 2D labels\n\nIn this step the 3D labels are projected to a 2D label map, use the dropdown to choose between projecting big labels in front or small labels in front.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-segment-everything\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Napari Segment Everything"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-hippo",
    "name": "napari-hippo",
    "display_name": "Hippo",
    "version": "0.2.0",
    "created_at": "2024-01-31",
    "modified_at": "2024-05-15",
    "authors": [
      "Sam Thiele"
    ],
    "author_emails": [
      "s.thiele@hzdr.de"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-hippo/",
    "home_github": null,
    "home_other": "None",
    "summary": "A fat and clumsy collection of hyperspectral tools",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "natsort",
      "hylite",
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "hylite ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-hippo\n\n\n| <a href=\"https://github.com/samthiele/napari-hippo/wiki\"><img src=\"https://github.com/samthiele/napari-hippo/blob/main/logo.png\" height=\"32\"/></a>| [![License MIT](https://img.shields.io/pypi/l/napari-hippo.svg?color=green)](https://github.com/samthiele/napari-hippo/blob/main/LICENSE) | [![PyPI](https://img.shields.io/pypi/v/napari-hippo.svg?color=green)](https://pypi.org/project/napari-hippo) | [![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-hippo)](https://napari-hub.org/plugins/napari-hippo)\n| -------------------------- | ------------------------------- |--------------------|-------------------|\n\n    \nA large and slightly clumsy plugin for viewing and analysing hyperspectral data in Napari.\n\n![Funky screenshot of the napari-hippo GUI](screenshot.png)\n\n\n----------------------------------\n\n## Installation\n\nFollow [these instructions](https://napari.org/stable/tutorials/fundamentals/installation) to first install napari. Then you can install `napari-hippo` via [pip](https://pypi.org/project/napari-hippo):\n\n    pip install napari-hippo\n\n## Documentation\n\nWe are in the process of building a documentation wiki for this plugin [here](https://github.com/samthiele/napari-hippo/wiki).\n\n## Contributing\n\nContributions are very welcome! Please feel free to submit pull requests or tell us about your ideas (or problems) on the [discussions](https://pypi.org/project/napari-hippo) page.\n\n## License\n\nDistributed under the terms of the [MIT](https://github.com/samthiele/napari-hippo/blob/main/LICENSE) license,\n`napari-hippo` is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/samthiele/napari-hippo/issues/new/choose) along with a detailed description.\n\n## Citation\n\nA citation for `napari-hippo` will be announced shortly.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.bmp",
      "*.png",
      "*.hdr",
      "*.jpeg",
      "*.jpg",
      "*",
      "*.dat"
    ],
    "contributions_writers_filename_extensions": [
      ".dat",
      ".hdr"
    ],
    "contributions_widgets": [
      "Input / Output",
      "Crunchy",
      "Hylite",
      "Field QAQC",
      "Caterpillar"
    ],
    "contributions_sample_data": [
      "Hippo HSI"
    ]
  },
  {
    "normalized_name": "napari-basicpy",
    "name": "napari-basicpy",
    "display_name": "BaSiCpy shadow correction in napari",
    "version": "0.0.3",
    "created_at": "2024-05-06",
    "modified_at": "2024-05-06",
    "authors": [
      "Tim Morello"
    ],
    "author_emails": [
      "tdmorello@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-basicpy/",
    "home_github": "https://github.com/peng-lab/napari-basicpy",
    "home_other": null,
    "summary": "BaSiCPy illumination correction for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "basicpy >=1.2.0",
      "numpy",
      "qtpy",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-black ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "flake8-isort ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pydocstyle ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "tox ; extra == 'tox-testing'",
      "pytest ; extra == 'tox-testing'",
      "pytest-cov ; extra == 'tox-testing'",
      "pytest-qt ; extra == 'tox-testing'",
      "napari ; extra == 'tox-testing'",
      "pyqt5 ; extra == 'tox-testing'"
    ],
    "package_metadata_description": "# napari-basicpy\n\n[![License](https://img.shields.io/pypi/l/napari-basicpy.svg?color=green)](https://github.com/tdmorello/napari-basicpy/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-basicpy.svg?color=green)](https://pypi.org/project/napari-basicpy)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-basicpy.svg?color=green)](https://python.org)\n[![tests](https://github.com/tdmorello/napari-basicpy/workflows/tests/badge.svg)](https://github.com/tdmorello/napari-basicpy/actions)\n[![codecov](https://codecov.io/gh/tdmorello/napari-basicpy/branch/main/graph/badge.svg)](https://codecov.io/gh/tdmorello/napari-basicpy)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-basicpy)](https://napari-hub.org/plugins/napari-basicpy)\n\nBaSiCPy illumination correction for [napari]\n\n## Example\n\n![example](resources/example.gif)\n\n----------------------------------\n\n## Installation\n\n### Recommended Installation Method\n\nWe highly recommend using a `conda` virtual environment to install and operate this plugin.\n\nTo use Python 3.9, for example:\n\n    conda create -n basicpy -c conda-forge python=3.9 napari pyqt && \\\n    conda activate basicpy && \\\n    pip install napari-basicpy\n\nFor further instructions on installing `napari`, visit their [install guide](https://napari.org/stable/tutorials/fundamentals/installation).\n\n---\n\n**IMPORTANT NOTE FOR APPLE SILICON AND WINDOWS USERS:**\n\nIf the above instructions fail with Apple silicon (e.g., M1/M2 chip) or Windows, you may need to install the `jax` and `jaxlib` following the instruction [here](https://github.com/peng-lab/BaSiCPy#installation).\n\n---\n\n### Other Installation Methods\n\nYou can also install `napari-basicpy` via [pip]:\n\n    pip install napari-basicpy\n\n\nTo install latest development version:\n\n    pip install git+https://github.com/peng-lab/napari-basicpy.git\n\nor\n\n    pip install git+https://github.com/tdmorello/napari-basicpy.git\n\n## Usage\n\n### General Usage\n\nThis plugin expects a stack of tiles as input. Mosaic images should be deconstructed into their tiled components before processing. Individual tiles should be two-dimensional.\n\nThere are many options to customize the performance of BaSiCPy. Please refer to the BaSiCPy documentation on parameters [here](https://basicpy.readthedocs.io/en/latest/api.html#basicpy.basicpy.BaSiC) for details.\n\n### Batch Processing\n\nComing soon...\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-basicpy\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/peng-lab/napari-basicpy/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BaSiC Shadow Correction"
    ],
    "contributions_sample_data": [
      "Random",
      "Cell Culture",
      "Timelapse Brightfield",
      "Timelapse Nanog",
      "Timelapse Pu1",
      "WSI Brain"
    ]
  },
  {
    "normalized_name": "napari-mm3",
    "name": "napari-mm3",
    "display_name": "napari-mm3",
    "version": "0.0.16",
    "created_at": "2022-06-02",
    "modified_at": "2024-05-06",
    "authors": [
      "Gursharan Ahir",
      "Michael Sandler",
      "Ryan Thiermann"
    ],
    "author_emails": [
      "ryan.thiermann@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-mm3/",
    "home_github": "https://github.com/junlabucsd/napari-mm3",
    "home_other": null,
    "summary": "a plugin for mother machine image analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "h5py",
      "tifffile ==2021.11.2",
      "scikit-learn",
      "scikit-image",
      "tensorflow",
      "nd2reader",
      "seaborn",
      "elasticdeform"
    ],
    "package_metadata_description": "# napari-mm3\n\n[![License](https://img.shields.io/pypi/l/napari-mm3.svg?color=green)](https://github.com/junlabucsd/napari-mm3/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mm3.svg?color=green)](https://pypi.org/project/napari-mm3)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mm3.svg?color=green)](https://python.org)\n[![tests](https://github.com/junlabucsd/napari-mm3/workflows/tests/badge.svg)](https://github.com/junlabucsd/napari-mm3/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mm3)](https://napari-hub.org/plugins/napari-mm3)\n\nA plugin for mother machine image analysis by the [Jun Lab](https://jun.ucsd.edu/).\n\nReference:\n[Tools and methods for high-throughput single-cell imaging with the mother machine. Ryan Thiermann, Michael Sandler, Gursharan Ahir, John T. Sauls, Jeremy W. Schroeder, Steven D. Brown, Guillaume Le Treut, Fangwei Si, Dongyang Li, Jue D. Wang, Suckjoon Jun. eLife12:RP88463\nhttps://doi.org/10.7554/eLife.88463.1](https://elifesciences.org/reviewed-preprints/88463)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n\nhttps://github.com/junlabucsd/napari-mm3/assets/40699438/1b3e6121-f5e1-475f-aca3-c6ed1b5bab3a\n\n\n\n## Installation\n\nWe describe installation with mamba, a faster version of conda which we recommend. Installation with conda is the exact same, except replace `mamba` with `conda` Run the following command:\n\n```\nmamba create -n napari-mm3 -c conda-forge conda-build tensorflow napari\n``` \nNow, you need to install our code (please let us know if this causes problems -- it has been a pain point in the past). To do so, clone the repository:\n\n```\ngit clone https://github.com/junlabucsd/napari-mm3.git\n```\n\nThen, run the following commands from within your conda environment:\n```\ncd napari-mm3\npip install -e .\n```\nThis supplies you with the latest, most recent version of our code.\n\nIf you would like to have a more stable version, simply run `pip install napari-mm3`. In general, we recommend going off of the github version.\n\nnapari-MM3 can use the [python-bioformats](https://pypi.org/project/python-bioformats/) library to import various image file formats. It can be installed with pip:\n```\npip install python-bioformats\n```\nIf your raw images are in the .nd2 format, they will be read in with the nd2reader package. In this case, Bio-Formats is not required.\n\nNOTES:\nNot running the conda command above and trying to install things in a different way may lead to difficult issues with PyQt5. We recommend following the above commands to simplify the situation.\nUsing `pip -e .` instead of `mamba develop .` is a deliberate choice, the former did not seem to register the plugin with napari.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Usage guide\n\n\nhttps://github.com/junlabucsd/napari-mm3/assets/8302475/68c726be-620e-4375-b1c9-3db56ac9a82a\n\nAdditional reference information is available below.\n### a. Preprocessing\n\n* [TIFFConverter](https://github.com/junlabucsd/napari-mm3/blob/main/docs/tiffconvert-widget.md) -- Turn your nd2 microscopy data, or other format via bioformats, into TIFFs. If your data is already exported as individual TIFF files, skip to the [Compile](https://github.com/junlabucsd/napari-mm3/blob/main/docs/compile-widget.md) widget. Take note of the [input image guidelines](https://github.com/junlabucsd/napari-mm3/blob/main/docs/Input-images-guidelines.md).\n\n* [Compile](https://github.com/junlabucsd/napari-mm3/blob/main/docs/compile-widget.md) -- Locate traps, separate their timelapses into their own TIFFs, and return metadata.\n\n* [PickChannels](https://github.com/junlabucsd/napari-mm3/blob/main/docs/pickchannels-widget.md) -- User guided selection of empty and full traps.\n\n### b. Segmentation\n\n___With Otsu's method:___\n\n* [Subtract](https://github.com/junlabucsd/napari-mm3/blob/main/docs/subtract-widget.md) -- Remove (via subtraction) empty traps from the background of traps that contain cells; run this on the phase contrast channel.\n\n* [SegmentOtsu](https://github.com/junlabucsd/napari-mm3/blob/main/docs/segmentotsu-widget.md) -- Use Otsu's method to segment cells.\n\n___With U-Net:___\n\n* [Annotate](https://github.com/junlabucsd/napari-mm3/blob/main/docs/annotate-widget.md) -- annotate images for ML (U-Net or similar) training purposes.\n\n* [Train U-Net](https://github.com/junlabucsd/napari-mm3/blob/main/docs/trainunet-widget.md) -- Train a U-Net model for cell segmentation.\n\n* [SegmentUnet](https://github.com/junlabucsd/napari-mm3/blob/main/docs/segmentunet-widget.md) -- Run U-Net segmentation.\n\n### c. Tracking\n\n* [Track](https://github.com/junlabucsd/napari-mm3/blob/main/docs/track-widget.md) -- Acquire individual cell properties and track lineages.\n\n### d. Fluorescence data analysis\n\n* [Subtract](https://github.com/junlabucsd/napari-mm3/blob/main/docs/subtract-widget.md) -- Remove (via subtraction) empty traps from the background of traps that contain cells. This time, run this on your fluorescence channels.\n\n* [Colors](https://github.com/junlabucsd/napari-mm3/blob/main/docs/colors-widget.md) -- Calculate fluorescence information.\n\n### e. Focus tracking\n\n* [Foci](https://github.com/junlabucsd/napari-mm3/blob/main/docs/foci-widget.md) -- We use this to track `foci` (bright fluorescent spots) inside of cells.\n\n### f. Extracting data and plotting\n\n* The notebook [here](https://github.com/junlabucsd/napari-mm3/blob/main/notebooks/napari_mm3_analysis_template.ipynb) demonstrates how to extract, filter and visualize the lineage data output by the [Track](https://github.com/junlabucsd/napari-mm3/blob/main/docs/track-widget.md) widget.\n\n\n### g. Outputs, inputs, and file structure\nFinally, to better understand the data formats, you may wish to refer to the following documents:\n\n* [Input image guidelines](https://github.com/junlabucsd/napari-mm3/blob/main/docs/Input-images-guidelines.md)\n\n* [File structure](https://github.com/junlabucsd/napari-mm3/blob/main/docs/file-structure.md)\n\n* [Output data structure](https://github.com/junlabucsd/napari-mm3/blob/main/docs/Cell-class-docs.md)\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mm3\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/junlabucsd/napari-mm3/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TIFFConverter",
      "Compile",
      "PickChannels",
      "Subtract",
      "SegmentOtsu",
      "SegmentUnet",
      "Track",
      "Annotate",
      "TrainUnet",
      "Colors",
      "Foci",
      "FociPicking"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-conidie",
    "name": "napari-conidie",
    "display_name": "Conidie",
    "version": "1.0.2",
    "created_at": "2022-12-15",
    "modified_at": "2024-04-29",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-conidie/",
    "home_github": "https://github.com/hereariim/napari-conidie",
    "home_other": null,
    "summary": "A segmentation tool to get conidie and hyphe",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "h5py",
      "scikit-image",
      "napari",
      "matplotlib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-conidie\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-conidie.svg?color=green)](https://github.com/hereariim/napari-conidie/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-conidie.svg?color=green)](https://pypi.org/project/napari-conidie)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-conidie.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/napari-conidie/workflows/tests/badge.svg)](https://github.com/hereariim/napari-conidie/actions)\n[![codecov](https://codecov.io/gh/hereariim/napari-conidie/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-conidie)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-conidie)](https://napari-hub.org/plugins/napari-conidie)\n\nA segmentation tool to get conidie and hyphe\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\nThis plugin is a use case for obtaining conidia and hyphae surface from images. This plugin is a private tool dedicated exclusively to the work of the QUASAV team.\n\n## Installation\n\nThis private tool cannot be found in the built-in napari. The installation therefore follows two steps:\n\n1 - Install latest development version :\n\n    git clone https://github.com/hereariim/napari-conidie.git\n\n## Getting started\n\nAs prerequisite, user must have installed ilastik in its computer.\n\nBefore using the plugin, you must have two data:\n\n- The ilastik model\n- The compressed file contained your images structured as followed :\n\n```\n‚îî‚îÄ‚îÄ Compressed file\n    ‚îú‚îÄ‚îÄ Folder1\n    ‚îÇ   ‚îú‚îÄ‚îÄ img0_1.jpg\n    ‚îÇ   ‚îú‚îÄ‚îÄ img0_2.jpg\n    ‚îÇ   ...\n    ‚îÇ   ‚îî‚îÄ‚îÄ img0_n.jpg\n    ‚îÇ \n    ‚îú‚îÄ‚îÄ Folder2\n    ‚îÇ   ‚îú‚îÄ‚îÄ img1_1.jpg\n    ‚îÇ   ‚îú‚îÄ‚îÄ img1_2.jpg\n    ‚îÇ   ...\n    ‚îÇ   ‚îî‚îÄ‚îÄ img1_n.jpg\n    ...\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ  Foldern\n        ‚îú‚îÄ‚îÄ imgn_1.jpg\n        ‚îú‚îÄ‚îÄ imgn_2.jpg\n        ...\n        ‚îî‚îÄ‚îÄ imgn_n.jpg\n```\n\n## Plugin\n\n![here](https://github.com/hereariim/napari-conidie/assets/93375163/07cf6bc3-3d55-4ae1-94ac-8e8b33193963)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-conidie\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/napari-conidie/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segmentation",
      "Show data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-apple",
    "name": "napari-apple",
    "display_name": "Apple",
    "version": "0.0.8",
    "created_at": "2022-06-23",
    "modified_at": "2024-04-24",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-apple/",
    "home_github": "https://github.com/hereariim/napari-apple",
    "home_other": null,
    "summary": "Detection of apple based on YOLOv4 model",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python-headless",
      "scikit-image",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-apple\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-apple.svg?color=green)](https://github.com/hereariim/napari-apple/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-apple.svg?color=green)](https://pypi.org/project/napari-apple)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-apple.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/napari-apple/workflows/tests/badge.svg)](https://github.com/hereariim/napari-apple/actions)\n[![codecov](https://codecov.io/gh/hereariim/napari-apple/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-apple)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-apple)](https://napari-hub.org/plugins/napari-apple)\n\nDetection of apple based on YOLOv4-tiny model\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nBefore you can operate the module, you must install the `napari-apple` module.\n\n### Instruction for napari-module\n\nYou can install `napari-apple` via [pip]:\n\n    pip install napari-apple\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/napari-apple.git\n\n## How does it works\n\nHere, user drop its images in the napari windows. The plugin shows two widgets : \n- Image detection\n- Export data\n\nIn Image detection, user select the interesting layer to detect apple. The \"Run\" button run the inference detection based on Yolov4-tiny model. At the end, the result is displayed on screen. User can correct freely the detection by removing or adding box in image.\n\nIn Export data, user export select the interesting shape layer and RGB image. A button \"Save to csv\" save bounding box coordinate in Yolo way into a text file.\n\n![Capture d'√©cran 2024-04-24 114340](https://github.com/hereariim/napari-apple/assets/93375163/d8873a6a-8ebb-4686-bfe9-e7e7729378b1)\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-apple\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/napari-apple/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Image detection",
      "Export data"
    ],
    "contributions_sample_data": [
      "Apple"
    ]
  },
  {
    "normalized_name": "napari-blossom",
    "name": "napari-blossom",
    "display_name": "Blossom",
    "version": "0.1.6",
    "created_at": "2022-06-20",
    "modified_at": "2024-04-24",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-blossom/",
    "home_github": "https://github.com/hereariim/napari-blossom",
    "home_other": null,
    "summary": "Segmentation of blossom apple tree images",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy >=1.23.0",
      "magicgui >=0.6.1",
      "qtpy",
      "opencv-python-headless >=4.7.0.68",
      "tensorflow >=2.11.0",
      "scikit-image >=0.19.3",
      "napari",
      "focal-loss >=0.0.7",
      "pillow >=9.3.0",
      "tqdm >=4.64.1",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-blossom\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-blossom.svg?color=green)](https://github.com/hereariim/napari-blossom/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-blossom.svg?color=green)](https://pypi.org/project/napari-blossom)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-blossom.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/napari-blossom/workflows/tests/badge.svg)](https://github.com/hereariim/napari-blossom/actions)\n[![codecov](https://codecov.io/gh/hereariim/napari-blossom/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-blossom)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blossom)](https://napari-hub.org/plugins/napari-blossom)\n\nSegmentation of blossom apple tree images\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\nThis plugin was written by Herearii Metuarea, student intern at LARIS (French laboratory located in Angers, France) in Imhorphen, french scientific team lead by David Rousseau (Full professor). This plugin was designed as part of the european project INVITE.\n\n<img width=\"86\" alt=\"Logo-IRHS-h_2022_png_large\" src=\"https://github.com/hereariim/napari-blossom/assets/93375163/750bbd60-ef3e-4148-9cbd-8a32e11252a4\"> ![Logo-INRAE](https://github.com/hereariim/napari-blossom/assets/93375163/d7cc95a1-f09c-4430-8ac8-8962c1046767) ![logo2](https://github.com/hereariim/napari-blossom/assets/93375163/3b41c838-acc8-49a3-81f8-46d1305f43d3) ![logolaris1](https://github.com/hereariim/napari-blossom/assets/93375163/bf92a903-5810-4c43-aa28-573f96f64ff9) ![logo1](https://github.com/hereariim/napari-blossom/assets/93375163/f9361560-dd4f-49f4-955d-ffe41b5c014d)\n\n## Installation\n\nYou can install `napari-blossom` via [pip]:\n\n    pip install napari-blossom\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/napari-blossom.git\n\n## How does it works\n\nThis module offers a plugin that allows you to segment the images of the apple tree flowers. As input, you can enter a **single image** with the image selection widget. Once the image is entered in the napari window, you can segment the apple blossoms with the image segmentation widget by running the run button. The segmented image will appear in the napari window.\n\n![Capture d'√©cran 2024-04-24 120758](https://github.com/hereariim/napari-blossom/assets/93375163/4f0c6ac7-b3a8-4849-9c5c-0ff5f35c8362)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-blossom\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/napari-blossom/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Segmentation",
      "Save zip"
    ],
    "contributions_sample_data": [
      "Blossom"
    ]
  },
  {
    "normalized_name": "napari-findaureus",
    "name": "napari-findaureus",
    "display_name": "napari-findaureus",
    "version": "0.0.4",
    "created_at": "2024-04-20",
    "modified_at": "2024-04-20",
    "authors": [
      "Shibarjun Mandal"
    ],
    "author_emails": [
      "shibarjunmandal@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-findaureus/",
    "home_github": null,
    "home_other": "None",
    "summary": "Locate bacteria in CLSM obtained infected bone tissue images",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "qtpy",
      "napari[all]",
      "aicsimageio ==4.11.0",
      "nd2 ==0.5.3",
      "aicspylibczi ==3.1.2",
      "fsspec ==2023.5.0",
      "readlif ==0.6.5",
      "czifile ==2019.7.2",
      "tifffile ==2023.7.10",
      "webcolors ==1.13",
      "opencv-python ==4.7.0.72",
      "numpy ==1.24.3",
      "scikit-image ==0.20.0",
      "xmltodict",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-findaureus\n\n\"Findaureus\" is now available to use in napari.\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/shibarjun/napari-findaureus/main/docs/napari-findaureus.png\" />\n</p>\n\nFindaureus is a tool designed to identify bacteria in infected bone tissue images obtained via Confocal Laser Scanning Microscopy (CLSM). This tool can be accessed independently [here](https://github.com/shibarjun/Findaureus). Findaureus has been integrated as a plugin for napari. In addition to its bacteria-locating algorithm, the napari viewer provides improved visualization features, in 2D and 3D perspectives.\n\n----------------------------------\n## Installation\n### Windows/Linux\nIf you don‚Äôt have conda installed, you can get miniconda or Anaconda from their websites.\n1. Open your command line tool and run these commands to create and activate a conda environment:\n```\nconda create -n napari-findaureus python=3.9\nconda activate napari-findaureus\n```\n2. Install napari and napari-findaureus with this command:\n```\npip install \"napari[all]\" napari-findaureus\n```\n### macOS\n1. Create an environment with napari and pyqt5\n```\nconda create -n napari-findaureus -c conda-forge python=3.9 pyqt imagecodecs napari\n```\n2. Install the napari-findaureus plugin\n```\npip install napari-findaureus\n```\n\n## Start napari-findaureus\nLaunch napari from the terminal while the napari-findaureus environment is running.\n```\nnapari\n```\nTo launch the napari plugin, go to ‚ÄúPlugins‚Äù and select ‚Äúnapari-findaureus‚Äù.\n## Quick demo\nTo use the `napari-findaureus` plugin, please follow the steps below:\n\n1. First, download some relevant fluorescence-labeled images of infected mouse bone tissues from [Zenodo](https://zenodo.org/doi/10.5281/zenodo.8411791).\n2. Next, load the image file through the `napari-findaureus` plugin.\n3. Navigate to the ‚ÄúPlugins‚Äù menu and select the `napari-findaureus` option to activate the widget.\n4. In the viewer, identify the bacteria channel from the \"layer list,\" which is specified in the image file name, and select it.\n5. Once the bacteria channel is selected, click on the `Find bacteria!` button.\n6. The widget will display the image-related data and bacteria count. If you need additional help, click on the `Instruction` button in the widget.\n7. Before you proceed to another image, reset the viewer by clicking on the `Reset` button provided in the widget.\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/shibarjun/napari-findaureus/main/docs/napari-findaureus.gif\" />\n</p>\n\nEnjoy exploring the fascinating world of bacteria in mouse bone tissues!\n\n----------------------------------\n## Contributing\nWe welcome and appreciate all contributions to the `napari-findaureus` project! Whether it's reporting bugs, suggesting new features, improving documentation, or writing code, your involvement is greatly valued.\nWhen using our dataset or referring to our work, we kindly ask that you acknowledge the dataset and cite the related articles. This helps support our work and allows us to continue improving this project.\n\nThank you for your interest and support!\n## Citations and Dataset\n### Findaureus\n Mandal S, Tannert A, L√∂ffler B, Neugebauer U, Silva LB (2024) [Findaureus: An open-source application for locating Staphylococcus aureus in fluorescence-labelled infected bone tissue slices.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0296854) PLoS ONE 19(1): e0296854.\n### Infected mouse bone tissue\nMandal S, Tannert A, Ebert C, Guliev RR, Ozegowski Y, Carvalho L, Wildemann B, Eiserloh S, Coldewey SM, L√∂ffler B, Basti√£o Silva L, Hoerr V, Tuchscherr L, Neugebauer U. (2023) [Insights into S. aureus-Induced Bone Deformation in a Mouse Model of Chronic Osteomyelitis Using Fluorescence and Raman Imaging.](https://www.mdpi.com/1422-0067/24/11/9762) International Journal of Molecular Sciences 24(11):9762.\n\n### [Dataset](https://zenodo.org/doi/10.5281/zenodo.8411791)\n## Acknowledgements\n\nThis project is a part of the European Union's Horizon 2020 research and innovation program under grant agreement No 861122 (ITN IMAGE-IN). We acknowledge support from the Jena Biophotonics and Imaging Laboratory (JBIL), from the European Union via EFRE funds within the Th√ºringer Innovationszentrum f√ºr Medizintechnik-L√∂sungen (ThIMEDOP, FKZ IZN 2018 0002), the BMBF via the funding program Photonics Research Germany (LPI, FKZ: 13N15713) and via the CSCC (FKZ 01EO1502) and the Institute of Anatomical and Molecular Pathology, University Coimbra, Portugal.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.nd2",
      "*.czi",
      "*.tiff",
      "*.lif"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari-findaureus"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "surforama",
    "name": "surforama",
    "display_name": "Surforama",
    "version": "0.0.9",
    "created_at": "2024-03-08",
    "modified_at": "2024-04-18",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "surforama@kyleharrington.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/surforama/",
    "home_github": "https://github.com/cellcanvas/surforama",
    "home_other": null,
    "summary": "a tool for using surfaces to explore volumetric data in napari",
    "categories": [
      "Annotation",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "mrcfile",
      "numpy",
      "pooch",
      "qtpy",
      "pyacvd",
      "pyvista",
      "rich",
      "scikit-image",
      "starfile",
      "trimesh",
      "typer",
      "napari ; extra == 'dev'",
      "pyqt5 ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "napari[all] ; extra == 'napari'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# surforama\na napari-based tool for using surfaces to explore volumetric data in napari\n\ninspired by [membranorama](https://github.com/dtegunov/membranorama)\n\n![Screenshot of surforama showing a surface in the slice of a tomogram](surforama_screenshot.png)\n\n## installation\n`surforama` requires the napari viewer. If you would like to install napari and surforama together in one line, you can use the following command:\n\n```bash\npip install \"surforama[napari]\"\n```\n\n\nIf you already have napari installed, you can directly install surforama in the same environment:\n\n```bash\npip install surforama\n```\n\n## usage\n### launch with demo data\nIf you'd like to test surforama out, you can launch surforama with demo data:\n\n```bash\nsurforama --demo\n```\n\n### launch without data\nYou can launch surforama using the command line interface. After you have installed surforama, you can launch it with the following command in your terminal:\n\n```bash\nsurforama\n```\nAfter surforama launches, you can load your image and mesh into napari and get surfing!\n\n### launch with data\nIf you have an MRC-formatted tomogram and an obj-formatted mesh, you can launch using the following command:\n\n```bash\nsurforama --image-path /path/to/image.mrc --mesh-path /path/to/mesh.obj\n```\n\n## developer installation\n\nIf you would like to make changes to the surforama source code, you can install surformama with the developer tools as follows:\n\n```bash\ncd /path/to/your/surforama/source/code/folder\npip install -e \".[dev]\"\n```\nWe use pre-commit to keep the code tidy. Install the pre-commit hooks to activate the checks:\n\n```bash\npre-commit install\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.obj",
      "*.star"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Surforama"
    ],
    "contributions_sample_data": [
      "thylakoid membrane",
      "covid virion membrane"
    ]
  },
  {
    "normalized_name": "napari-imodmodel",
    "name": "napari-imodmodel",
    "display_name": "Napari Imod Model",
    "version": "1.0.2",
    "created_at": "2024-04-16",
    "modified_at": "2024-04-16",
    "authors": [
      "Moritz Wachsmuth-Melm"
    ],
    "author_emails": [
      "mail@moritzwm.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-imodmodel/",
    "home_github": "https://github.com/MoritzWM/napari-imodmodel",
    "home_other": null,
    "summary": "Open IMOD model files in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "imodmodel",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-imodmodel\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-imodmodel.svg?color=green)](https://github.com/MoritzWM/napari-imodmodel/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-imodmodel.svg?color=green)](https://pypi.org/project/napari-imodmodel)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-imodmodel.svg?color=green)](https://python.org)\n[![tests](https://github.com/MoritzWM/napari-imodmodel/workflows/tests/badge.svg)](https://github.com/MoritzWM/napari-imodmodel/actions)\n[![codecov](https://codecov.io/gh/MoritzWM/napari-imodmodel/branch/main/graph/badge.svg)](https://codecov.io/gh/MoritzWM/napari-imodmodel)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-imodmodel)](https://napari-hub.org/plugins/napari-imodmodel)\n\nOpen IMOD model files in napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-imodmodel` via [pip]:\n\n    pip install napari-imodmodel\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MoritzWM/napari-imodmodel.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-imodmodel\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MoritzWM/napari-imodmodel/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mod"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "Napari Imod Model"
    ]
  },
  {
    "normalized_name": "napari-folder-browser",
    "name": "napari-folder-browser",
    "display_name": "napari-folder-browser",
    "version": "0.1.4",
    "created_at": "2021-10-03",
    "modified_at": "2024-03-26",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-folder-browser/",
    "home_github": "https://github.com/haesleinhuepf/napari-folder-browser",
    "home_other": null,
    "summary": "Browse folders of images and open them using double-click",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine >=0.1.4",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# napari-folder-browser\n\n[![License](https://img.shields.io/pypi/l/napari-folder-browser.svg?color=green)](https://github.com/haesleinhuepf/napari-folder-browser/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-folder-browser.svg?color=green)](https://pypi.org/project/napari-folder-browser)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-folder-browser.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-folder-browser/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-folder-browser/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-folder-browser/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-folder-browser)\n\nBrowse folders of images and open them using double-click or <ENTER>. You can also navigate through the list using arrow up/down keys.\n\n![](https://github.com/haesleinhuepf/napari-folder-browser/raw/main/docs/napari-folder-browser.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-folder-browser` from within napari by clicking menu `Plugins > Install/uninstall Plugins...` and entering here:\n![img.png](https://github.com/haesleinhuepf/napari-folder-browser/raw/main/docs/install.png)\n\nYou can install `napari-folder-browser` via [pip]:\n\n    pip install napari-folder-browser\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Development\n### Test the plugin in Napari\nSimply use pip install and run napari to test the plugin in the same environment:\n```bash\npip install -e .\nnapari\n```\n\n### Conda\nIf you prefer to use conda, you can create a new environment with the following command:\n```bash\nconda env create -f environment.yml\nconda activate napari-folder-browser\n```\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-folder-browser\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-folder-browser/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[image.sc]: https://image.sc\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "FolderBrowser"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "iterseg",
    "name": "iterseg",
    "display_name": "iterseg",
    "version": "0.3.0",
    "created_at": "2023-12-01",
    "modified_at": "2024-03-24",
    "authors": [
      "Abigail S McGovern & Juan Nunez-Iglesias"
    ],
    "author_emails": [
      "Abigail.McGovern1@monash.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/iterseg/",
    "home_github": "https://github.com/abigailmcgovern/iterseg",
    "home_other": null,
    "summary": "napari plugin for iteratively improving unet-watershed segmentation",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "dask",
      "torch",
      "scikit-image",
      "pandas",
      "ome-zarr",
      "zarr",
      "matplotlib",
      "napari",
      "umetrix",
      "numba",
      "scipy",
      "seaborn"
    ],
    "package_metadata_description": "# iterseg\n\n[![License](https://img.shields.io/pypi/l/iterseg.svg?color=green)](https://github.com/abigailmcgovern/iterseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/iterseg.svg?color=green)](https://pypi.org/project/iterseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/iterseg.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/iterseg)](https://napari-hub.org/plugins/iterseg)\n\nnapari plugin for iteratively improving a deep learning-based unet-watershed segmentation. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\nInstall iterseg using pip. Assuming you have python and pip installed (e.g., via miniconda), you can install iterseg with only one line, typed into terminal (MacOS/Linux) or annaconda prompt (Windows). We recomend installing into a [new environment](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#) as some of our dependencies may not play well in the sandpit with certain versions of packages that may exist in a prexisting one. \n\n```bash\npip install iterseg napari\n```\n\n\n## Opening iterseg\nOnce `iterseg` is installed, you can access it through the napari viewer, which you can open from the command line (e.g., terminal (MacOS), anaconda prompt (Windows), git bash (Windows), etc.). To open napari simply type into the command line:\n```bash\nnapari\n```\n\n## Loading data\nOnce you've opened napari, you can load image, labels, or shapes data through the `load_data` widget. to open the widget go to **plugins/iterseg/load_data** at the top left of your screen (MacOS) or viewer (Windows). \n\n ![find the widgets](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/load_data_find.png)\n\nOnce the widget appears at the right of the napari window, enter the name you want to give the data you are loading (this will appear in the layers pannel on the left of the window). Choose the type of layer you want to load (Image, Labels, or Shapes: segmentations are loaded as labels layer). You can load a folder of files or a zarr file using \"choose directory\" (zarrs are recognised as a folder of files) or you can load a tiff file using \"choose file\". You can tell the program what the scale of the 3D frames will be in (in the format (z, y, x)).\n\n ![load data](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/load_data.png)\n\nIf you are using a single image file (3D, 4D, 5D - ctzyx) or a directory of 3D images (zyx), for \"data type\" choose \"individual frames\". If you are using a directory of 4D images (tzyx) choose \"image stacks\". If you are loading a file that is 4D or 5D and want to load time points (4D: tzyx, czyx) or channels (5D: ctzyx) as individual layers, select \"split channels\". \n\n## Segmenting images\n\nYou can segment data using the \"segment_data\" widget, which can be found at **plugins/iterseg/segment_data**. Once the widget appears, you can choose (1) the image layer you want to segment, (2) the folder into which to save the data, (3) the name you want to give the output file, (4) the type of segmentation to use, (5: optionally) the path to a neural network or configuration file, (6: optionally) a layer produced during training which contains metadata pointing to the trained neual network, (7) chunk size (the size of the neural network input), (8) margin (the margin of overlap between chunks). There is also an optional tickbox for debugging. If this is selected, errors will be easier to identify but you won't be able to interact with the viewer until the segmentation is done. \n\n ![segmentation in progress](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/segmenting_in_progress.png)\n\n  ![segmented data](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/segmented_data.png)\n\nSegmented images can be used to more quickly generate ground truth for training, to assess segmentation quality, or for downstream analyses. \n\n### Segmentation algorithms\n#### Affinity U-Net Watershed\nThe affinity U-net watershed is a feature based instance segmentation algorithm. A trained U-net predicts an edge affinity graph (basically boundaries in the x, y, and z axes), a map of centre points, and a mask that specifies which pixesl belong to objects. The feature map is fed to a modified watershed algorithm. The object centres are used to find seeds for the watershed and the affinity graph is used to find bounaries between objects. If you train a network using `iterseg`, you can select the outputted network file to segment. Otherwise, if one is not selected, a network we have trained to detect platelets will be used. This might be appropriate for small objects with high anisotropy. \n\n#### DoG Blob Segmentation\nThe DoG blob segmentation uses a difference of Gaussian (DoG) filter to find blob shaped objects. The DoG filter is used to find object seeds, a foreground mask, and is fed to a watershed to label objects. This algorithm cannot be trained but can be configured with a configuration file. An example configuration file can be seen in the example folder in this repository. Please see the Segmentation_config.md file for more details. \n\n## Generating ground truth\nWe include two tools that are useful for generating ground truth: \"save frames\" and \"ground truth from ROI\". \n\n### Save frames\n\nThe first tool is \"save frames\" can be found at **plugins/iterseg/save_frames**. It enables you to save frames of interest from a  series of segmented images or timeseries. \n\n ![save frames](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/save_frames.png)\n\n### Ground truth from ROI\n\nThe \"ground truth from ROI\" tool can be found at **plugins/iterseg/ground_truth_from_ROI**. This tool enables you to take a small portion of corrected data and place it into a new frame, which can be used for training. The new data can be tiled in the new frame to overrepresent the data in the training data set. At present, the ROI must be selected by adding a shapes layer (added using the icon circled in orange), then adding a rectangle (blue circle).\n\n ![make an ROI](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/generate_ROI.png)\n\n The rectangle will be used to select a region of the xy-plane. This can be seen in 3D below. \n\n ![2D ROI in 3D](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/roi_before_3D.png)\n\n At present, the entire z stack above and below the rectangle will be used to generate ground truth. We aim to incorporate 3D bounding boxes in the future. If multiple ROIs are selected, multiple new image frames will be made, each with a single ROI. When you generate ground truth from the shapes layer, you are able to select the desired shapes layer, image layer, and labels layer. Additionally, you can choose how many times you want to tile the ROI and how much padding to leave between. Tiling will start at the top right and progress right before moving to the next row. You will also be able to choose the save name and the folder into which to save the data. \n\n ![ground truth from ROI](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/gt_from_ROI.png)\n\n## Training a network\n`iterseg` includes a widget for training a u-net for the u-net affinity watershed. The training widget can be found at **plugins/iterseg/train_from_viewer**. Before training, you will need to load the images and ground truth you want to train from. The images and ground truth should each be a series of 3D frames that are stacked into a layer (we suggest loading from a series of frames in a directory). Once loaded, you are able to select a layer as the ground truth and a layer as the image data. You can tell the program what the scale of the output frames will be (in the format (z, y, x)). You can select what type of center prediction to use (we suggest centredness), what type of prediction to use for the mask, and what extent of affinities you want to train (if n = 1, the network will predict only the direct boundaries between objects in each axis, if greater than 1 the network will still predict the direct boundaires but will also predict where there is a new object n steps away - can be used as collateral learning to enhance training). Affinities extent is developmental. Please submit an issue for any problems. \n\n ![train from viewer](https://github.com/AbigailMcGovern/iterseg/blob/main/docs/images/train_from_viewer.png)\n\nFor the U-net training, we allow you to choose the learning rate for the [ADAM optimiser](https://arxiv.org/abs/1412.6980) used to train the network. You can also choose between binary cross entropy loss ([BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)) and Dice loss ([DICELoss](https://arxiv.org/abs/1707.03237v3)). We have found in our data that BCE loss works better. You can also choose how many chunks of data are produced from each frame (n each) and how many epochs you want to train for the training will be done in n_each * n_frames batches with a minibatch size of 1. \n\nIn the future we hope to expand this training widget to enable training other types of networks. Please get involved if you feel you can help with this. \n\n## Assessing segmentations\n`iterseg` includes widgets for assessing and comparing segmentations. If you want to assess segmentation quality, you will need to load a ground truth and a segmentation to assess. Once loaded, you can select the ground truth and segmentation (model segmentation) using the widget found in **plugins/iterseg/assess_segmentation**. You can select which metrics you want to assess. The metrics we enable are:\n- **Variation of information (VI):** VI is a two part measure. It includes a measure of undersegmentation and oversegmentation. Undersegmentation is a measure of the amount of new information you get from looking at the ground truth if you have already seen the segmentation. It can be interpreted as the proportion of objects that are incorrectly merged. Oversegmentation is a measure of the amount of new information you get from looking at the segmentation if you have already seen the ground truth. It can be interpreted as the proportion of objects that are incorrectly split. For more info please see the [scikit-image documentation](https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.variation_of_information). \n- **Object count difference (OD):** Object count difference is simply the difference in number of objects between a ground truth and the assessed segmentation (card(ground truth) - card(segmentation)). \n- **Average precision (AP):** Average precision  Average precision is a combined measure of how accurate the model is at finding true positive (real) objects (we call this precision) and how many of ground truth real objects it found (this is called recall). The assessment of whether an object is TP, FP, and FN depends on the threashold of overlap between objects. Here we use the intersection of union (IoU), which is the proportion of overlap between the bounding boxes of ground truth and model segemented objects. AP is assessed using different IoU thresholds (from 0.35-0.95). The resultant data will be plotted as IoU by AP. \n\n  - Precision = TP / (TP + FP)Recall = TP / (TP + FN). \n  - Abbreviations: FN, false negative; TP, true positive; FP, false \n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"iterseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/abigailmcgovern/iterseg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.ome.zarr"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "train_from_viewer",
      "load_data",
      "assess_segmentation",
      "compare_segmentations",
      "segment_data",
      "save_frames",
      "ground_truth_from_ROI"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "redlionfish",
    "name": "RedLionfish",
    "display_name": "RedLionfish",
    "version": "0.10",
    "created_at": "2021-11-19",
    "modified_at": "2024-03-22",
    "authors": [
      "Luis Perdigao"
    ],
    "author_emails": [
      "luis.perdigao@rfi.ac.uk"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/redlionfish/",
    "home_github": "https://github.com/rosalindfranklininstitute/RedLionfish",
    "home_other": null,
    "summary": "Fast Richardson-Lucy deconvolution of 3D volume data using GPU or CPU with napari plugin.",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "pyopencl",
      "reikna"
    ],
    "package_metadata_description": "![RedLionfish Logo](./redlionfish_logo.svg)\n\n# RedLionfish (RL) deconvolution\n\n*Richardson-Lucy deconvolution for fishes, scientists and engineers.*\n\n\nThis software is for filtering 3D data using the Richardson-Lucy deconvolution algorithm.\n\nRichardson-Lucy is an iterative deconvolution algorithm that is used to remove\npoint spread function (PSF) or optical transfer function (OTF) artifacts from experimental images.\n\nThe method was originally developed for astronomy to remove optical effects and simultaneously reduce poisson noise in 2D images.\n\n[Lucy, L. B. An iterative technique for the rectification of observed distributions. The Astronomical Journal 79, 745 (1974). DOI: 10.1086/111605](https://ui.adsabs.harvard.edu/abs/1974AJ.....79..745L/abstract)\n\nThe method can also be applied to 3D data. Nowadays this filtering technique is also widely used by microscopists.\n\nThe Richardson-Lucy deconvolution algorigthm is iterative. Each iteration involves the calculation of 2 convolutions, one element-wise multiplication and one element-wise division.\n\nWhen dealing with 3D data, the Richardson-Lucy algorithm is quite computional intensive primarly due to the calculation of the convolution, and can take a while to complete depending on the resources available. Convolution is significantly sped up using FFT compared to raw convolution.\n\nThis software was developed with the aim to make the R-L computation faster by exploiting GPU resources, and with the use of FFT convolution.\n\nTo make RedLionfish easily accessible, it is available through PyPi and anaconda (conda-forge channel). A useful plugin for Napari is also available.\n\nPlease note that this software only works with 3D data. For 2D data there are many alternatives such as the DeconvolutionLab2 in Fiji (ImageJ) and sckikit-image.\n\n## Napari plugin\n\nYou can now use the Napari's plugin installation in *Menu -> Plugins -> Install/Uninstall Plugins...*.\nHowever, if you chose to use this method, GPU acceleration may not be available and it will use the CPU backend. Better check.\n\n![](resources\\imag1.jpg)\n\nAlternatively, if you follow the installation instructions below, and install the napari in the same python environment\nthen the plugin should be immediately available in the *Menu -> Plugins -> RedLionfish*.\n\n\n## Installation\n\nPreviously there was a problem in installing using `pip`, because no PyOpenCL wheels for windows were avaiable. It is now avaialble.\n\nThis package can be installed using pip or conda.\n\nNapari plugin installation engine can also be used to install this package.\n\n\n### Install from PyPi\n\n```\npip install redlionfish\n```\n\n\n### Conda install\n\nThis package is available in conda-forge channel.\nIt contains the precompiled libraries and it will install all the requirments for GPU-accelerated RL calculations.\n\n`conda install redlionfish -c conda-forge`\n\nIn Linux , the package `ocl-icd-system` may also be useful.\n\n```\nconda install reikna pyopencl ocl-icd-system -c conda-forge\n```\n\n\n#### Manual installation using the conda package file.\n\nDownload the appropriate conda package .bz2 at [https://github.com/rosalindfranklininstitute/RedLionfish/releases](https://github.com/rosalindfranklininstitute/RedLionfish/releases)\n\nIn the command line, successively run:\n```\nconda install <filename.bz2>\nconda update --all -c conda-forge\n```\nThe second line is needed because you are installing from a local file, conda installer will not install dependencies. Right after this you should run the update command given.\n\n\n### Manual installation (advanced and for developers)\n\nPlease note that in order to use OpenCL GPU accelerations, PyOpenCL must be installed.\nThe best way to get it working is to install it under a conda environment.\n\nThe installation is similar to the previously described for PyPi.\n\n`conda install reikna pyopencl`\n\nor\n\n`conda install reikna pyopencl ocl-icd-system -c conda-forge` (Linux)\n\nClone/download from source [https://github.com/rosalindfranklininstitute/RedLionfish/](https://github.com/rosalindfranklininstitute/RedLionfish/)\n\nand run\n\n`python setup.py install`\n\n\n### Debug installation\nIf you want to test and modify the code then you should probably install in debug mode using:\n\n`python setup.py develop`\n\nor\n\n`pip install -e .`\n\n\n## More information\n\nThe software has algorithms for Richardson-Lucy deconvolution that use either CPU and GPU.\n\nThe CPU version is very similar to the [skimage.restoration.richardson_lucy](https://scikit-image.org/docs/dev/api/skimage.restoration.html#skimage.restoration.richardson_lucy) code, with improvments in speed.\nmajor differences are:\n\n- the convolution steps use FFT only.\n- PSF and PSF-flipped FFTs are precalculated before starting iterations.\n\nThe GPU version, was written in to use Reikna package, which does FFT using OpenCL, via PyOpenCL.\n\nUnfortunately, a major limitation in RAM usage exists with PyOpenCL.\nLarge 3D data volumes with cause out-of-memory error when trying to upload data to the GPU for FFT calculations.\nAs such, to overcome this problem, a block algorithm is used, which splits data into blocks with padded data.\nThe results are then combined together to give the final result.\nThis affects the perfomance of the calculation rather significantly, but with the advantage of being possible to handle large data volumes.\n\nIf Richardson-Lucy deconvolution using the GPU method fails, RedLionfish will fallback to CPU calculation. Check console output for messages.\n\nIf you are using the RedLionfish in your code, note that, by default, `def doRLDeconvolutionFromNpArrays()` method it uses the GPU OpenCL version.\n\n## Testing\n\nUse pytest to test the package. Test files are in `/test` folder\n\nMany examples can be found in `/scripts' folder.\n\nA useful way to test and benchmark the package installation can be run from the proect root using the command:\n\n'python scripts/test_and_benchm.py'\n\nor in windows\n\n'python scripts\\test_and_benchm.py'\n\nThis will print out information about your GPU device (if available) and run some deconvolutions.\nIt initially creates some data programatically, convolutes with a gaussian PSF, and add Poisson noise.\nThen it executes executes the\nRichardson-Lucy deconvolution calculation using CPU and GPU methods, for 10 iterations.\nDuring the calculation it will print some information to the console/terminal, including the time it takes to run the calculation.\n\n\nComputer generated data and an experimental PSF can be found in `scripts\\testdata`\n\n### Testing Redlionfish in napari\n\nHere is an example testing the Redlionfish plugin in napari:\n\n1. load data `scripts/testdata/gendata_psfconv_poiss_large.tif` (can use draga and drop)\n2. load psf data `scripts/testdata/PSF_RFI_8bit.tif`\n3. In the RedLionfish side window ensure that 'gendata_psfconv_poiss_large' is selected in data dropdown widget, and `PSF_RFI_8bit` is selected in psfdata widget.\n4. Choose number of iterations (default=10)\n5. Click 'Go' button and wait until result shows as a new data layer.\n6. Use controls of the left panel to compare before and after RL deconvolution: select 'RL-deconvolution' layer and set colormap to red. Hide PSF_RFI_8bit. Make sure that both 'RL-deconvolution' and 'gendata-psfconv' are visible. Now, hide/unhide RL-deconvolution layer to see before and after deconvolution. Adjust contrast limits of each layer as desired.\n\n\n## GPU vs CPU\n\nYou may notice that choosing GPU does not make RL-calculation much faster compared with CPU, and sometimes is slower.\n\nWhich method runs the R-L deconvolution faster. That depends on the computer configuration/architecture.\n\nGPU calculations will be generally faster than CPU with bigger data volumes.\n\nGPU calculation will be significantly faster if using a dedicated GPU card.\n\nPlease see benchmark values that highlights significant variability in calculation speeds.\n\n\n[benchmark_results.md](benchmark_results.md)\n\n\n## Coding\n\nPlease feel free to browse the `/scripts` folder for examples.\n\nIn order to use the functions, add the follwoing import to your code,\n\n`import RedLionfishDeconv`\n\nThe most useful function is perhaps the following.\n\n`def doRLDeconvolutionFromNpArrays(data_np , psf_np ,*, niter=10, method='gpu', useBlockAlgorithm=False, callbkTickFunc=None, resAsUint8 = False) `\n\nThis will do the Richardson-Lucy deconvolution on the data_np (numpy, 3 dimensional data volume) using the provided PSF data volume, for 10 iterations. GPU method is generally faster but it may fail. If it does fail, the program will automatically use the CPU version that uses the scipy fft package.\n\n\n\n## Manually building the conda package\n\nFor this installation, ensure that the conda-build package is installed\n\n`conda install conda-build`\n\nIn windows, simply execute\n\n`conda-create-package.bat`\n\n\nOr, execute the following command-line to create the installation package.\n\n`conda-build --output-folder ./conda-built-packages -c conda-forge conda-recipe`\n\nand the conda package will be created in folder *conda-built-packages*.\n\nOtherwise, navigate to `conda-recipe`, and execute on the command-line `conda build .`\n\nIt will take a while to complete.\n\n## Contact\n\nReport issues and questions in project's github page, please. Please don't try to send emails as they may be igored or spam-filtered.\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "RedLionfish_widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "platetrack",
    "name": "platetrack",
    "display_name": "platetrack",
    "version": "0.0.7",
    "created_at": "2023-12-03",
    "modified_at": "2024-03-20",
    "authors": [
      "Abigail S McGovern"
    ],
    "author_emails": [
      "abigail_mcgovern@hotmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/platetrack/",
    "home_github": "https://github.com/abigailmcgovern/platelet-tracking",
    "home_other": null,
    "summary": "napari plugin for tracking platelets with trackpy",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "trackpy",
      "pandas",
      "plateletanalysis"
    ],
    "package_metadata_description": "# platetrack\nA small napari plugin for tracking platelets. Platetrack requires a segmentation and an image containing raw data. We recomend trying the napari plugin iterseg to generate these. Platetrack uses trackpy for tracking and outputs a dataframe with platelet coordinates, tracking information, and several other variables, which provide information about each platelet. \n\n\n## Installation\n\nThere are three main ways to install platetrack:\n\n### Install Using pip\n\nType the following into your terminal (MacOS or Ubuntu) or annaconda prompt (windows):\n\n```bash\npip install platetrack\n```\n\n### Install via napari hub\n\nType the following into your terminal (MacOS or Ubuntu) or annaconda prompt (windows):\n\n```bash\ninstall napari\nnapari\n```\n\nOnce napari has opened (this may take a second the first time you open it), go to the pannel at the top of the screen and select the 'plugins' dropdown. Then select install/uninstall plugins. A new window will open showing available plugins. Either scroll down to or search 'platetrack' and click 'install'. \n\n### Install from Source Code\n*please use this for now*\n\nType the following into your terminal (MacOS or Ubuntu) or annaconda prompt (windows):\n\n```bash\ngit clone <repository https or ssh>\ncd platetrack\npip install .\n```\n\n\n## Opening Platetrack\nOnce annotrack is properly installed you will be able to open platetrack by opening napari. You can open napari through the command line (terminal (MacOS or Ubuntu) or annaconda prompt (windows)) as follows:\n\n```bash\nnapari\n```\n\nYou can find the platetrack widgets by selecting the dropdown 'plugins' at the pannel at the top of the screen and selecting the platetrack widget 'track_platelets'.  \n\n\n## Tracking Platelets\nYou can track platelets and obtain a dataframe of information about platelet observation by providing an image/s (t, z, y, x) and a segmentation (t, z, y, x). There are no specific file format requirements, only that you first load the image and segmentation into napari. The napari plugin iterseg provides a widget that will help you load zarr format files. If you have an image with multiple channels (i.e., laser colours), load them into separate napari layers. Iterseg has an option for this called \"split channels\". Otherwise, refer to the napari website for instructions on using napari layers. \n\n### Parameters for widget\n\n- **labels_layer**: The napari layer containing the segmentation.\n- **image_layer**: The napari layer containing the image (you only need this if you don't want to use all image layers).\n- **use_all_image_layers**: If you have several image channels selecting this will obtain information about each channel. The info about image intensity will be stored in columns of the data frame named *[layer name]*_max, *[layer name]*_mean_, \n- **sample_name**: what is the name of the sample (i.e., an identifyer for the biological sample including, for example, the animal number, date, experimental conditions, etc.). This is important if you are planning to combine data frames with different treatment groups. \n- **treatment_name**: name of treatement group or experimental condition (will be added as a categorical variable). This is important if you are planning to combine data frames with different treatment groups. \n- **x_microns**: How big are pixels in the x axis (probably in microns). We need this so that physical rather than pixel coordinates can be computed. \n- **y_microns**: How big are pixels in the y axis (probably in microns). We need this so that physical rather than pixel coordinates can be computed. \n- **z_microns**: How big are pixels in the z axis (probably in microns). We need this so that physical rather than pixel coordinates can be computed. \n- **save_dir**: Directory into which you want to save output data\n- **save_file**: name to give the file, \n- **save_format**: There are two options for save format \"parquet\" or \"csv\".  \n- **search_range**: This is a parameter for the tracking. The search range is how far away (in physical units, e.g., microns) the tracking algorithm will look for the same platelet at the next time point. This can be reduced if trackpy is running out of computational resources due to a high number of observations (platelets)  \n- **xy_origin**: If you are rotating the data (e.g., you might want to align the blood flow with the y axis like we do) this parameter defines the centre of rotation. If you would like to use the geometric centre of the image just use \"centre\". Otherwise, provide a tuple (computer word ‚Äì basically a list of numbers between brackets) of coordinates in physical units in yx format (e.g., (126, 148)). \n- **rotation**: The number of degrees by which to rotate the data counterclockwise. \n\n\n\n## Platelet data outputted\nA number of variables are computed about the platelets alongside the tracking. Each variable is reported for every platelet observation (execpt veclocity, which is only reported for tracked platelets after the first observation). \n\n- Mean platelet intensity in each image channel\n- Max platelet pixel intensity in each channel\n- Platelet elongation (0-1, 0 being least elongated, 1 being most elongated)\n- Platelet flatness (0-1, 0 being least flat, 1 being most flat)\n- Platelet velocity (dv)\n- Platelet coordinate velocities (dvx, dvy, dvz)\n- Platelet local density (density of platlets in a 15 um radius around the platelet)\n- Lists of platelet neighbours within 15 um radius\n- Lists of distances of each platelet neighbours within 15 um radius\n\n\n## Contributing and User Support\n\n**User support:** If you have an issue with platetrack please add an issue (go to the Issues tab at the top of the GitHub page). If your issue is a bug, please include as much information as possible to help debug the problem. Examples of information include: details about the image and segmentation data (dimensions), number of images, number of samples you are trying to take. If you are requesting an improvement, try to be as clear as possible about what you need. \n\n**Contributing:** If you want to contribute to platetrack, please fork the repo and if you want to make changes make a pull request with as much detail about the change as possible. Please ensure any changes you want to make don't break the existing functions.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.parquet",
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "track_platelets"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "annotrack",
    "name": "annotrack",
    "display_name": "annotrack",
    "version": "0.0.3",
    "created_at": "2023-12-01",
    "modified_at": "2024-03-19",
    "authors": [
      "Abigail S McGovern"
    ],
    "author_emails": [
      "abigail_mcgovern@hotmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/annotrack/",
    "home_github": "https://github.com/abigailmcgovern/annotrack",
    "home_other": null,
    "summary": "napari plugin for annotating tracks to estimate error rates",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "dask",
      "napari",
      "numpy",
      "zarr",
      "pandas",
      "sphinx ; extra == 'docs'",
      "nd2 ; extra == 'io'",
      "pytest ; extra == 'testing'"
    ],
    "package_metadata_description": "# annotrack\nAnnotrack is a napari plugin for annotating errors in object trajectories. The plugin will help you take a sample of track segments along with a small section of corresponding image and segmentation. Annotrack allows you to annotate three types of errors: (1) ID swap errors (track jumps between objects), (2) false starts (track starts on a pre-existing object) and false terminations (track ends but object still exists). By looking at the combined rates of false starts and false terminations you can assess track discontinutation errors. \n\n**Please note:** Images and segmentations must be in zarr format. Tracks should be in parquet format.  \n\n## Installation \n\nThere are three main ways to install annotrack:\n\n### Install Using pip\n*Please note that this is planned/under development*\n\nType the following into your terminal (MacOS or Ubuntu) or annaconda prompt (windows):\n\n```bash\npip install annotrack\n```\n\n### Install\n*Please note that this is planned/under development*\n\nType the following into your terminal (MacOS or Ubuntu) or annaconda prompt (windows):\n\n```bash\ninstall napari\nnapari\n```\n\nOnce napari has opened (this may take a second the first time you open it), go to the pannel at the top of the screen and select the 'plugins' dropdown. Then select install/uninstall plugins. A new window will open showing available plugins. Either scroll down to or search 'annotrack' and click 'install'. \n\n### Install from Source Code\n*please use this for now*\n\nType the following into your terminal (MacOS or Ubuntu) or annaconda prompt (windows):\n\n```bash\ngit clone https://github.com/AbigailMcGovern/annotrack.git\ncd annotrack\npip install .\n```\n\n## Opening Annotrack\nOnce annotrack is properly installed you will be able to open annotrack by opening napari. You can open napari through the command line (terminal (MacOS or Ubuntu) or annaconda prompt (windows)) as follows:\n\n```bash\nnapari\n```\n\nYou can find the annotrack widgets by selecting the dropdown 'plugins' at the pannel at the top of the screen and hovering over 'annotrack'.  \n\n## Sample from CSV\n\nTo sample your tracks you will need to supply the file paths for the images, segmentations, and tracks. You supply this in a csv that is structured as shown below:\n\n ![csv_structure widget](https://github.com/AbigailMcGovern/annotrack/blob/main/media/csv_structure.png)\n\nIn this csv, you may also specify how many samples are to be taken from each file. If this is not provided, annotrack will use the value you supply to the `sample_from_csv` widget. The csv must contain a column that specifies a category to which each sample belongs (e.g., species, experimental condition, drug, etc.).  If this isnt important for your samples, just add a dummy category (e.g., sample_type : [A, A, A, A]). \n\nTo access the widget and sample track segments, go to the top of the screen, go to **plugins > annotrack > sample_from_csv**. When the widget is displayed, select the csv file, select a directory into which to save results, and proivide a name for the summary data file (i.e., where your annotations will be written). \n\n ![sample_from_csv widget](https://github.com/AbigailMcGovern/annotrack/blob/main/media/sample_from_csv.png)\n\n### Widget parameters\n- **path to csv**: \n        The path storing the info from which to generate the samples. \n        The CSV should have the columns: image_path, labels_path, tracks_path, <category_col>, \n        You can also add an optional n_samples column if you would like to \n        specify how many samples to take from each individual file. Otherwise, \n        the default \"n_samples\" you've supplied will be used.\n- **output dir**: \n        Where will the output be saved?\n- **output name**: \n        What will output summary files/directories be called?\n- **n samples**: \n        How many samples to be obtained from each file. Will be overwritten\n        if there is a valid integer number in the n_samples colum of the csv.\n- **tzyx cols**: \n        What are the names of the columns denoting time (in frames) and coordinate\n        positions (in pixels) in the file containing tracks? The order should be:\n        t, z, y, x. \n- **id col**: \n        What is the name of the column denoting the specific ID for each tracked\n        object?\n- **scale**: \n        size of pixels (e.g., in um) for the z, y, and x coordinates (in that\n        order)\n- **frames**: \n        Approximate maximum number of frames of track segment. \n        Max frames = frames (if even) or frames - 1 (if odd)\n- **box size**: \n        Approximate size of bounding box (in pixels). \n- **min track len**: \n        You can set a minimum track len to include in the search. \n        This can help to eliminate less useful data. This should be at least 1 to only include tracked objects. Set higher only if you are specifically interested in longer lived tracks. \n- **image channel**: \n        This denotes the index of the channel from which to get \n        image data (0: channel 1, 1: channel 2, 2: channel 3, 3: channel 4)\n\n### Annotate Now?\n\nIn the case that we are annotating multiple conditions to compare, we want to show them in the one session in randomised order with the annotator blinded to where the sample has originated from. We want to be able to annotated unannotated data from the sample without having the burden of having to do this all at once. The annotations are therefore saved into the saved sample. A selected number of samples saved from the various tracking experiments can be annotated using the following code. If you re-execute this code, you will only be shown not yet annotated data, unless you request otherwise.\n\nKeys to navagate and annotate samples\n- '2' - move to next sample\n- '1' - move to previous sample\n- 'y' - annotate as correct (will move to the next sample automatically)\n- 'n' - annotate as containing an error (will move to the next sample automatically)\n- 'i' - annotate the frame following a ID swap error\n- 't' - annotate the fame following an incorrect termination\n- 'Shift-t' - annotate the frame containing a false start error\n- 's' - annotate an error ('i', 't', or 'Shift-t') as being associated with a segmentation error (merge or split of objects)\n\nWhen an error is associated the specific frame ('i', 't', 'Shift-t', or 's'), the frame number (within the original image) will be added to a list of errors for the sample within the sample's (.smpl) info data frame. E.g., you may have a list of ID swaps for your sampled track segment (`[108, 111, 112]`) and a corresponding list of segmentation error associations (`[108, 112]`). \n\n## Annotate Existing Sample\nIf you have already saved a sample and want to annotate it, you can load the sample data using the `annotate_existing_sample` widget. This might be useful if you want to have several annotators annotate the same sample. To access this widget, open napari\n\n ![annotate_existing_sample widget](https://github.com/AbigailMcGovern/annotrack/blob/main/media/annotate_existing_sample.png)\n\n## Contributing and User Support\n\n**User support:** If you have an issue with annotrack please add an issue (go to the Issues tab at the top of the GitHub page). If your issue is a bug, please include as much information as possible to help debug the problem. Examples of information include: details about the image and segmentation data (dimensions), number of images, number of samples you are trying to take. If you are requesting an improvement, try to be as clear as possible about what you need. \n\n**Contributing:** If you want to contribute to annotrack, please fork the repo and if you want to make changes make a pull request with as much detail about the change as possible. Please ensure any changes you want to make don't break the existing functions.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "sample_from_csv",
      "annotate_existing_samples"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-live-recording",
    "name": "napari-live-recording",
    "display_name": "napari-live-recording",
    "version": "0.3.8",
    "created_at": "2021-10-05",
    "modified_at": "2024-03-16",
    "authors": [
      "\"Jacopo Abramo",
      "Pia Pritzke",
      "Felix Wanitschke\""
    ],
    "author_emails": [
      "jacopo.abramo@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-live-recording/",
    "home_github": "https://github.com/jacopoabramo/napari-live-recording",
    "home_other": null,
    "summary": "A napari plugin for live video recording with a generic camera device.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "superqt",
      "numpy",
      "opencv-python",
      "tifffile",
      "napari[all]",
      "qtpy",
      "microscope >=0.7.0",
      "pims",
      "pyqtgraph",
      "pymmcore-plus >=0.6.7",
      "pymmcore-widgets",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-live-recording\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/jacopoabramo/napari-live-recording/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-live-recording.svg?color=green)](https://pypi.org/project/napari-live-recording)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-live-recording.svg?color=green)](https://python.org)\n![tests](https://github.com/jacopoabramo/napari-live-recording/actions/workflows/test_and_deploy.yaml/badge.svg)\n[![codecov](https://codecov.io/github/jacopoabramo/napari-live-recording/graph/badge.svg?token=WhI2MO452Z)](https://codecov.io/github/jacopoabramo/napari-live-recording) \\\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-live-recording)](https://napari-hub.org/plugins/napari-live-recording)\n[![Chan-Zuckerberg Initiative](https://custom-icon-badges.demolab.com/badge/Chan--Zuckerberg_Initiative-red?logo=czi)](https://chanzuckerberg.com/)\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Description\n\n`napari-live-recording` (or `nlr`, if you like acronyms) is a <a href=\"#why-medium-weight\">medium-weight</a> plugin part of the napari ecosystem that provides an easy \naccess point for controlling area detector devices (most commonly reffered to as cameras) with a common interface.\nOther than that, the plugin also allows to create computation pipelines that can be executed real-time in a flow starting directly from the camera stream.\n\n> [!NOTE]\n> \n> ### Why medium weight?\n> `napari-live-recording` relies on multithreading to handle camera control,\n> image processing and data storage via a common pipelined infrastructure.\n> More details are provided in the documentation.\n\nThe plugin allows the following operations:\n\n- snapping: capture a single image\n- live view: continously acquiring from the currently active camera and show the collected data on the napari viewer;\n- recording: stream data to disk from the currently active cameras\n\nWhen recording, the plugin allows to store images according to the following formats:\n\n- ImageJ TIFF\n- OME-TIFF\n\n> [!NOTE]\n> Future releases will also add further file formats to the recording options, specifically:\n> - HDF5\n> - MP4\n>\n> We will also provide a method to add custom metadata to the recorded image files.\n\n## Supported cameras\n\n`napari-live-recording` aims to maintain itself agnostic for the type of cameras it controls. Via a common API (Application Programming Interface),\nit possible to define a controller for a specific camera. Instructions\non how to do so are provided in the documentation.\n\nBy default, the plugin is shipped with the following interfaces:\n\n- an [OpenCV](./src/napari_live_recording/control/devices/opencv.py) camera grabber;\n- a [Micro-Manager](./src/napari_live_recording/control/devices/micro_manager.py) interface via the package [`pymmcore-plus`](https://pypi.org/project/pymmcore-plus/);\n- an interface to the [microscope](./src/napari_live_recording/control/devices/pymicroscope.py) python package.\n\n## Documentation\n\nTo install and use the plugin you can review the documentation [here](./docs/documentation.md).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Acknowledgments\n\nThe developers would like to thank the [Chan-Zuckerberg Initiative (CZI)](https://chanzuckerberg.com/) for providing funding\nfor this project via the [napari Ecosystem Grants](https://chanzuckerberg.com/science/programs-resources/imaging/napari/napari-live-recording-camera-control-through-napari/).\n\n<p align=\"center\">\n  <img src=\"https://images.squarespace-cdn.com/content/v1/63a48a2d279afe2a328b2823/5830fddc-a02b-451a-827b-3d4446dcf57b/Chan_Zuckerberg_Initiative.png\" width=\"150\">\n</p>\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-live-recording\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jacopoabramo/napari-live-recording/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Live recording"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "msi-explorer",
    "name": "MSI-Explorer",
    "display_name": "MSI-Explorer",
    "version": "1.0.1",
    "created_at": "2023-07-20",
    "modified_at": "2024-03-13",
    "authors": [
      "lennart kowitz"
    ],
    "author_emails": [
      "lennart.kowitz@isas.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/msi-explorer/",
    "home_github": "https://github.com/MMV-Lab/MSI-Explorer",
    "home_other": null,
    "summary": "a napari plug-in for biochemical annotation of mass spectrometry imaging data",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "pyimzml",
      "matplotlib",
      "vaex",
      "opencv-python",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# MSI-Explorer\n\n[![License BSD-3](https://img.shields.io/pypi/l/MSI-Explorer.svg?color=green)](https://github.com/MMV-Lab/MSI-Explorer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/MSI-Explorer.svg?color=green)](https://pypi.org/project/MSI-Explorer)\n[![Python Version](https://img.shields.io/pypi/pyversions/MSI-Explorer.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/MSI-Explorer/workflows/tests/badge.svg)](https://github.com/MMV-Lab/MSI-Explorer/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/MSI-Explorer/branch/main/graph/badge.svg?token=LR8CU032ZD)](https://codecov.io/gh/MMV-Lab/MSI-Explorer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/MSI-Explorer)](https://napari-hub.org/plugins/MSI-Explorer)\n\n# User Manual\n\nThe MSI-Explorer napari plugin is a powerful tool designed for targeted biochemical annotations in MSI data. This user manual provides a comprehensive guide on how to install, use, and explore the functionalities of the plugin within the napari platform. It covers data import, visualization, mean intensity calculation, region of interest (ROI) analysis, annotation with selected databases and pre-processing such as noise reduction and normalization. \n\n[MSI-Explorer] \n \n## Installation\n\nInstall napari by using this command.\n   \n     pip install \"napari[all]\"\n\nYou can install `MSI-Explorer` via [pip]:\n   \n     pip install MSI-Explorer\n\n## Usage\nStart napari from the console with:\n\n    napari\n\nNavigate to `Plugins -> MSI-Explorer (MSI-Explorer)`\n![Plugin](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/104718fa-227e-4117-9b52-f674a265d218)\n\n### 1. Uploading and visualization of mass spectrometry imaging data\n- Select imzml file using `Load imzML`.\n- Metadata can be checked by `View Metadata`.\n![Uploading MSI data_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/a4783643-cf8e-4c68-af8e-03f264a48573)\n\n![Visualization of MSI data_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/5e37c375-d430-419a-9038-9980e858c482)\n\n\n####\nUpon uploading profile mode data, a pop-up appears prompting you to convert it to centroid mode.\nSelecting `Yes` converts the data, while `No` keeps it in its original profile format.\n\n![profile_centroid](https://github.com/nmmtsaw/MSI-Explorer-Manual/assets/127961719/5eecf5c2-e9b5-45da-a620-6dfaad058faf)\n\n### 2. Calculating mean (average) intensity\n- To calculate the mean spectrum, click on `Show true mean spectrum`.\n- Clicking `Show image` will create an image view of the currently plotted data\n- To export the plotted data as .csv file, click `Export spectrum data`.\n- To save the spectrum plot as image, click `Export spectrum plot`.\n\n![Calculating mean spectrum](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/2e921e00-75cf-4925-a9de-01d093277a06)\n\n![Calculating mean spectrum_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/19a713e3-a9ff-4e0c-be6b-545fb29991c6)\n\n\n#### 2.1. Calculating mean (average) intensity of selected m/z value\nTo focus on a specific m/z value, zoom in on the spectrum plot. The figure will be as\nshown as below.\n![Calculating mean spectrum specific mz_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/ba47080a-f439-4dc2-96b9-1f82ee5acbc3)\n\nIt is recommended to use `Multi` panel view.\nThe image can be displayed by `Show image` and the data can be exported as `.csv` file by using `Export spectrum data`.\n\n### 3. Pre-processing\nThe pre-processing capabilities of MSI-Explorer enhance data quality and prepare MSI data for downstream analysis. Pre-processing steps involve: \n\n\n#### (a) Noise reduction\nUsers can choose their desired level of noise reduction (shown as a percentage) for their experiment. \n\n![Noise reduction_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/9ce5e428-fe46-4f5f-a53f-7186c9f5ca8c)\n\n#### (b) Normalization\nThe normalization methods that the user can apply are \n- Total ion current (TIC)\n- Root mean square (RMS)\n- Medium\n- Reference peak (or internal standard)\n\n![normalization_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/972b30af-8425-46e4-bb54-705df52c725a)\n\n#### (c) Hotspot removal\nHotspot removal can also be applied using a default threshold of 99.99%.\n![hotspot removal_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/c9d279fa-d03b-499d-857d-6953ba7ea253)\n\n\nAfter pre-processing steps are chosen, click `Execute` and `Show true mean spectrum` to calculate the mean intensity.\n\nThe figure shows the spectrum and image of the TIC normalization with 3% noise reduction and hotspot removal for the 99.9% quantile.\n![pre-processed_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/d1068382-f6e2-4af9-9c5b-949fb87ac90c)\n\n\n### 4. Database\nTo use the database search, click on `Select` and a pop-up window will appear. There,\nselect `Metabolite_database_ver2`, which is a built-in database, and click `Confirm`.\n\n![Database](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/928fa260-196e-4034-8ddd-0944c751c77e)\n\nThe features of the database function are\n1. Charge (neutral, positive or negative)\n2. Adduct (based on the charge chosen)\n3. Range of the m/z value for the image display\n4. custom search with molecule name or m/z value\n\n![Database_search](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/ca7d943a-1b6b-4cba-bf4d-934ee574cc61)\n\nUsers can customize the database with exact mass, molecule name, or molecular formula. The format should be as shown in the table and the headers are not needed in the database.\n\nExact mass | Molecule name | Molecula formula\n------- | -------- | --------\n176.0950 | Cotinine | C10H12N2O\n174.1117 | Arginine | C6H14N4O2\n244.0881 | Biotin | C10H16N2O3S\n\n### 5. Region of interest (ROI) selection\n- To select the ROI, click on `Select ROI for mean spectrum`. Adjust the brush size and label color. You can fill the area by using paint icon. \n- Then click on the `Calculate ROI mean spectrum`.\n- You can export as `.csv` file by using `Export spectrum data`.\n- If one m/z is needed, just zoom-in the spectrum plot window and export.\n- Before selecting the second ROI, remove the first selected area by using eraser or label 0.\n\n![ROI selection_v1](https://github.com/nmmtsaw/MSI-Explorer_User-Manual/assets/127961719/e79ca007-a0b5-4ba7-8cea-ae5e8ad6dd7d)\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"MSI-Explorer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MMV-Lab/MSI-Explorerissues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.imzML"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "MSI-Explorer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "cellcanvas",
    "name": "cellcanvas",
    "display_name": "cellcanvas",
    "version": "0.0.1",
    "created_at": "2024-03-10",
    "modified_at": "2024-03-10",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czii@kyleharrington.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/cellcanvas/",
    "home_github": "https://github.com/cellcanvas/cellcanvas",
    "home_other": null,
    "summary": "A tool for painting in cellular architecture",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy <2.0.0",
      "magicgui >=0.8.1",
      "mrcfile",
      "qtpy >=2.4.1",
      "scikit-image >=0.22.0",
      "toolz >=0.12.0",
      "scikit-learn >=1.3.2",
      "pyclesperanto-prototype",
      "pymeshfix",
      "psygnal >=0.9.5",
      "superqt >=0.6.1",
      "surforama",
      "starfile",
      "zarr >=2.16.1",
      "xgboost >=2",
      "matplotlib >=3.8.2",
      "tox ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "napari ; extra == 'dev'",
      "pyqt5 ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# cellcanvas\nA tool to support painting in cellular architecture\n\n![cellcanvas_screenshot](cover.png)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Example QWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "btrack",
    "name": "btrack",
    "display_name": "btrack",
    "version": "0.6.5",
    "created_at": "2023-04-17",
    "modified_at": "2024-03-05",
    "authors": [
      "Alan R. Lowe"
    ],
    "author_emails": [
      "\"Alan R. Lowe\" <a.lowe@ucl.ac.uk>"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/btrack/",
    "home_github": "https://github.com/quantumjot/btrack",
    "home_other": null,
    "summary": "A framework for Bayesian multi-object tracking",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "cvxopt >=1.3.1",
      "h5py >=2.10.0",
      "numpy >=1.17.3",
      "pandas >=2.0.3",
      "pooch >=1.0.0",
      "pydantic <2",
      "scikit-image >=0.16.2",
      "scipy >=1.3.1",
      "tqdm >=4.65.0",
      "black ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "ruff ; extra == 'dev'",
      "numpydoc ; extra == 'docs'",
      "pytz ; extra == 'docs'",
      "sphinx ; extra == 'docs'",
      "sphinx-automodapi ; extra == 'docs'",
      "sphinx-panels ; extra == 'docs'",
      "sphinx-rtd-theme ; extra == 'docs'",
      "magicgui >=0.5.0 ; extra == 'napari'",
      "napari-plugin-engine >=0.1.4 ; extra == 'napari'",
      "napari >=0.4.16 ; extra == 'napari'",
      "qtpy ; extra == 'napari'"
    ],
    "package_metadata_description": "[![PyPI](https://img.shields.io/pypi/v/btrack)](https://pypi.org/project/btrack)\n[![Downloads](https://static.pepy.tech/badge/btrack/month)](https://pepy.tech/project/btrack)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Tests](https://github.com/quantumjot/btrack/actions/workflows/test.yml/badge.svg)](https://github.com/quantumjot/btrack/actions/workflows/test.yml)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![Documentation](https://readthedocs.org/projects/btrack/badge/?version=latest)](https://btrack.readthedocs.io/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/quantumjot/btrack/branch/main/graph/badge.svg?token=QCFC9AWK0R)](https://codecov.io/gh/quantumjot/btrack)\n\n![logo](https://btrack.readthedocs.io/en/latest/_images/btrack_logo.png)\n\n# Bayesian Tracker (btrack) üî¨üíª\n\n`btrack` is a Python library for multi object tracking, used to reconstruct trajectories in crowded fields.\nHere, we use a probabilistic network of information to perform the trajectory linking.\nThis method uses spatial information as well as appearance information for track linking.\n\nThe tracking algorithm assembles reliable sections of track that do not contain splitting events (tracklets).\nEach new tracklet initiates a probabilistic model, and utilises this to predict future states (and error in states) of each of the objects in the field of view.\nWe assign new observations to the growing tracklets (linking) by evaluating the posterior probability of each potential linkage from a Bayesian belief matrix for all possible linkages.\n\nThe tracklets are then assembled into tracks by using multiple hypothesis testing and integer programming to identify a globally optimal solution.\nThe likelihood of each hypothesis is calculated for some or all of the tracklets based on heuristics.\nThe global solution identifies a sequence of high-likelihood hypotheses that accounts for all observations.\n\nWe developed `btrack` for cell tracking in time-lapse microscopy data.\n\n## Installation\n\n`btrack` has been tested with ![Python](https://img.shields.io/pypi/pyversions/btrack)\non `x86_64` `macos>=11`, `ubuntu>=20.04` and `windows>=10.0.17763`.\nNote that `btrack<=0.5.0` was built against earlier version of\n[Eigen](https://eigen.tuxfamily.org) which used `C++=11`, as of `btrack==0.5.1`\nit is now built against `C++=17`.\n\n### Installing the latest stable version\n\n```sh\npip install btrack\n```\n\n## Usage examples\n\nVisit [btrack documentation](https://btrack.readthedocs.io) to learn how to use it and see other examples.\n\n### Cell tracking in time-lapse imaging data\n\n We provide integration with Napari, including a plugin for graph visualization, [arboretum](https://btrack.readthedocs.io/en/latest/user_guide/napari.html).\n\n\n[![CellTracking](http://lowe.cs.ucl.ac.uk/images/youtube.png)](https://youtu.be/EjqluvrJGCg)  \n*Video of tracking, showing automatic lineage determination*\n\n\n<img src=\"https://user-images.githubusercontent.com/8217795/225356392-6eb4b68c-eda5-4b96-af50-76930fa45e9d.png\" width=\"700\" />\n\n\n---\n\n## Development\n\nThe tracker and hypothesis engine are mostly written in C++ with a Python wrapper.\nIf you would like to contribute to btrack, you will need to install the latest version from GitHub. Follow the [instructions on our developer guide](https://btrack.readthedocs.io/en/latest/dev_guide).\n\n\n---\n### Citation\n\nMore details of how this type of tracking approach can be applied to tracking cells in time-lapse microscopy data can be found in the following publications:\n\n**Automated deep lineage tree analysis using a Bayesian single cell tracking approach**  \nUlicna K, Vallardi G, Charras G and Lowe AR.  \n*Front in Comp Sci* (2021)  \n[![doi:10.3389/fcomp.2021.734559](https://img.shields.io/badge/doi-10.3389%2Ffcomp.2021.734559-blue)](https://doi.org/10.3389/fcomp.2021.734559)\n\n\n**Local cellular neighbourhood controls proliferation in cell competition**  \nBove A, Gradeci D, Fujita Y, Banerjee S, Charras G and Lowe AR.  \n*Mol. Biol. Cell* (2017)  \n[![doi:10.1091/mbc.E17-06-0368](https://img.shields.io/badge/doi-10.1091%2Fmbc.E17--06--0368-blue)](https://doi.org/10.1091/mbc.E17-06-0368)\n\n```\n@ARTICLE {10.3389/fcomp.2021.734559,\n   AUTHOR = {Ulicna, Kristina and Vallardi, Giulia and Charras, Guillaume and Lowe, Alan R.},\n   TITLE = {Automated Deep Lineage Tree Analysis Using a Bayesian Single Cell Tracking Approach},\n   JOURNAL = {Frontiers in Computer Science},\n   VOLUME = {3},\n   PAGES = {92},\n   YEAR = {2021},\n   URL = {https://www.frontiersin.org/article/10.3389/fcomp.2021.734559},\n   DOI = {10.3389/fcomp.2021.734559},\n   ISSN = {2624-9898}\n}\n```\n\n```\n@ARTICLE {Bove07112017,\n  author = {Bove, Anna and Gradeci, Daniel and Fujita, Yasuyuki and Banerjee,\n    Shiladitya and Charras, Guillaume and Lowe, Alan R.},\n  title = {Local cellular neighborhood controls proliferation in cell competition},\n  volume = {28},\n  number = {23},\n  pages = {3215-3228},\n  year = {2017},\n  doi = {10.1091/mbc.E17-06-0368},\n  URL = {http://www.molbiolcell.org/content/28/23/3215.abstract},\n  eprint = {http://www.molbiolcell.org/content/28/23/3215.full.pdf+html},\n  journal = {Molecular Biology of the Cell}\n}\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.h5",
      "*.hdf",
      "*.hdf5"
    ],
    "contributions_writers_filename_extensions": [
      ".hdf5",
      ".h5",
      ".hdf"
    ],
    "contributions_widgets": [
      "Track"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bil-data-viewer",
    "name": "napari-bil-data-viewer",
    "display_name": "napari-bil-data-viewer",
    "version": "0.6.0",
    "created_at": "2022-01-24",
    "modified_at": "2024-02-28",
    "authors": [
      "Alan M Watson"
    ],
    "author_emails": [
      "alan.watson@pitt.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-bil-data-viewer/",
    "home_github": "https://github.com/brain-image-library/napari-bil-data-viewer",
    "home_other": null,
    "summary": "Napari plugin for viewing Brain Image Library datasets",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari[all]",
      "napari-plugin-engine >=0.1.4",
      "scikit-image",
      "fsspec",
      "requests",
      "aiohttp",
      "imagecodecs",
      "beautifulsoup4",
      "dask",
      "neurom ==3.2.2",
      "napari-ome-zarr ==0.5.2"
    ],
    "package_metadata_description": "<p href=\"https://www.brainimagelibrary.org/\">\n    <align=\"center\" width=\"100%\">\n    <img width=\"100%\" src=\"https://i.imgur.com/ljZKq8h.png\">\n</p>\n\n\n# Description\n\nView datasets archived at the **[Brain Image Library](https://www.brainimagelibrary.org/)**.\n\n**NOTE: This plugin is under early development.  Currently, only a subset of single-channel, fMOST datasets which include projections are available to view.  An example can be found [here]( https://download.brainimagelibrary.org/2b/da/2bdaf9e66a246844/mouseID_405429-182725/).\n\n\n\n![Plugin Demo GIF](https://imgur.com/gkDCsMd.gif \"Plugin Demo GIF\")\n\n\n\n### Features\n\n* Multiscale Rendering\n  * In datasets that include multiple resolution representations of the data, each resolution can be combined to improve the speed of browsing and user experience.  An example of a dataset with multiple resolution projections can be found [here](https://download.brainimagelibrary.org/2b/da/2bdaf9e66a246844/mouseID_405429-182725/).\n  * All datasets included in the current release of napari-bil-data-viewer use multi-resolution datasets.\n* 3D rendering of whole datasets.  The lowest resolution is used for rendering.  Currently, this is a limitation imposed by napari.\n* The plugin does NOT require a BIL account as datasets are already accessible via https.\n\n### Known Issues / limitations\n* Currently the only datasets that are available are those which have been manually selected by the developers.  If you would like a specific dataset to be included please consider adding the dataset(s) to the [dataset_info.py](https://github.com/brain-image-library/napari-bil-data-viewer/blob/main/napari_bil_data_viewer/dataset_info.py) file and submitting a pull request.\n* To inquire about this plugin please contact Brain Image Library support:  bil-support@psc.edu\n* The plugin is still under development.  We appreciate all [reports of issues / errors](https://github.com/brain-image-library/napari-bil-data-viewer/issues) which occur during use.\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nOption #1: Install plugin via the napari plugin menu\n\n1. Menu: Plugins >> Install/Uninstall Plugins\n2. Search: napari-bil-data-viewer\n3. Select install\n\nOption #2:  Install a fresh python virtual environment\n\n```bash\n# Example of venv creation using conda\nconda create -y -n bil-viewer python=3.8\nconda activate bil-viewer\n\n# Install napari-bil-data-viewer\npip install napari-bil-data-viewer\n\n# Run Napari\nnapari\n```\n\n## Contributing\n\nPlease consider contributing to this project!  Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bil-data-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/brain-image-library/napari-bil-data-viewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n## Change Log:\n\n##### <u>v0.1.0:</u>\n\nInitial release.\n\n<u>**v0.1.1 & v0.1.2:**</u>\n\nChanges to documentation\n\n<u>**v0.1.3:**</u>\n\nAdded all available summary fMOST datasets\n\n<u>**v0.2.0:**</u>\n\nAdded support for SWC neuron tracings\n\n<u>**v0.3.0:**</u>\n\nAdded support for multiscale OME zarr data\n\n<u>**v0.4.0:**</u>\n\nAdd scale controls for layers\n\n<u>**v0.4.2:**</u>\n\nAdd URL input to visualize image stacks (tif, tiff, jp2)\n\n<u>**v0.5.0:**</u>\n\nSplit the plugin into 5 widgets:<br/>\n- Load Curated Datasets\n- Load Image Stack From URL\n- Load Multiscale Data From URL\n- Load Neuron Morphology From URL\n- Layer Scale Controls\n\n<u>**v0.5.1:**</u>\n\nAdd metadata link to curated datasets\n\n<u>**v0.6.0:**</u>\n\nAdd widget to visualize histology RGB tiffs\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "LoadCuratedDatasets",
      "LoadImageStackFromURL",
      "LoadMultiscaleDataFromURL",
      "LoadNeuronMorphologyFromURL",
      "LayerScaleControls",
      "LoadHistologyImageFromURL"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "acquifer-napari",
    "name": "acquifer-napari",
    "display_name": "acquifer-napari",
    "version": "0.0.2",
    "created_at": "2023-07-07",
    "modified_at": "2024-02-27",
    "authors": [
      "Laurent Thomas"
    ],
    "author_emails": [],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/acquifer-napari/",
    "home_github": "https://github.com/Luxendo/acquifer-napari",
    "home_other": null,
    "summary": "Loader plugin for napari, to load Acquifer Imaging Machine datasets in napari, using dask for efficient lazy data-loading.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "acquifer",
      "napari",
      "numpy",
      "sortedcontainers",
      "dask-image",
      "xarray"
    ],
    "package_metadata_description": "# acquifer-napari\n\nThe acquifer-napari plugin allows loading IM04 dataset directory, as multi-dimensional images in napari.  \nSliders for well, channel, time and Z are automatically rendered when there are more than 1 coordinates along the dimension.  \nThe plugin uses Dask-Image for efficient data-loading \"on request\" similar to the VirtualStack in ImageJ.  \n\n## Installation\nVia the napari plugin manager : acquifer-napari.\nOr with pip : `pip install acquifer-napari`.\n\nUse `pip install -e .` to install in developement mode, so any change in the source code is directly reflected.  \nUse `npe2 list` to check that the plugin is correctly installed and visible by napari.  \nFor instance here, the package defines 1 command, which is a reader.  \nOne could have more commands, which would be implement other types.   \nThis should output something like following \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Name                         ‚îÇ Version ‚îÇ Npe2 ‚îÇ Contributions                                             ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ acquifer-napari              ‚îÇ 0.0.1   ‚îÇ ‚úÖ   ‚îÇ commands (1), readers (1)\n\nThe plugin should be installed in an environment with napari installed.  \nNapari can be started with the `napari`command in a command prompt with a system wide python installation.  \nOnce installed, napari can be opened in a IPython interactive session with\n\n```python\n>> import napari\n>> napari.Viewer()\n```\n\n## Configurations\nThe file `napari.yaml` in `acquifer_napari_plugin` defines what functions of the python package are visible to napari.  \nThe top level `name` field must be the same than the python package name defined in `setup.cfg`.\nIt first define a set of commands, which have a custom `id`, and a `python_name`, which is the actual location of the function in the python package (or module).  \nThen the napari.yaml has optional subsections `readers`, `writers`, `widget`, to reference some of the commands previously defined, to notify napari that they implemente those standard functions.  \nFor instance I first define a command myReader pointing to myPackage.myReader, and I reference that command using the id it in the section readers  \nSee https://napari.org/stable/plugins/first_plugin.html#add-a-napari-yaml-manifest  \n\n\n## Issues\nIf you encounter any problems, please [file an issue](https://github.com/Luxendo/acquifer-napari/issues) along with a detailed description.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-h5",
    "name": "napari-h5",
    "display_name": "napari-h5",
    "version": "0.0.8",
    "created_at": "2023-08-07",
    "modified_at": "2024-02-19",
    "authors": [
      "Luis Perdigao"
    ],
    "author_emails": [
      "luis.perdigao@rfi.ac.uk"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-h5/",
    "home_github": "https://github.com/rosalindfranklininstitute/napari-h5",
    "home_other": null,
    "summary": "A hdf5 file reader plugin for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "h5py",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-h5\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-h5.svg?color=green)](https://github.com/rosalindfranklininstitute/napari-h5/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-h5.svg?color=green)](https://pypi.org/project/napari-h5)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-h5.svg?color=green)](https://python.org)\n[![tests](https://github.com/rosalindfranklininstitute/napari-h5/workflows/tests/badge.svg)](https://github.com/rosalindfranklininstitute/napari-h5/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-h5)](https://napari-hub.org/plugins/napari-h5)\n\nA file reader plugin for napari\n\n\nIt opens simple *.h5 files. Reads all Datasets inside the file and converts to\na napari Image object (np.array).\n\nIt can also save \"image\" or \"labels\" data. Note that these will be saved individually.\n\nIt does not support data organised internally in \"groups\".\nFor these more complicated h5 data structures please try other plugins.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-h5` via [pip]:\n\n    pip install napari-h5\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-h5\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.h5"
    ],
    "contributions_writers_filename_extensions": [
      ".h5"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-rembg",
    "name": "napari-rembg",
    "display_name": "Napari Select Foreground",
    "version": "0.0.7",
    "created_at": "2023-09-30",
    "modified_at": "2024-02-17",
    "authors": [
      "Mallory Wittwer"
    ],
    "author_emails": [
      "mallory.wittwer@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-rembg/",
    "home_github": "https://github.com/EPFL-Center-for-Imaging/napari-rembg.git",
    "home_other": null,
    "summary": "AI-based foreground extraction in scientific and natural images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "qtpy",
      "napari[all] >=0.4.16",
      "rembg ; extra == 'local'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "![EPFL Center for Imaging logo](https://imaging.epfl.ch/resources/logo-for-gitlab.svg)\n# napari-rembg\n\nSegment images using a collection of fast and lightweight generalist segmentation models in Napari. This plugin is based on the [rembg](https://github.com/danielgatis/rembg) project.\n\n![demo](./assets/demo.gif)\n\n**Key features**\n\n- Choose among **five generalist segmentation models**, including SAM (Segment Anything Model).\n- Quickly annotate individual objects by drawing **bounding boxes** around them.\n- Possibility to generate predictions via a remote **web API** and keep the installation lightweight on client machines.\n- Compatible with 2D, RGB, 2D+t, and 3D images (slice by slice).\n\n## Installation\n\nYou can install `napari-rembg` via [pip]. If you wish to use your local machine for the predictions (most users):\n\n    pip install \"napari-rembg[local]\"\n\nIf you wish to generate predictions from a [web api](#running-the-segmentation-via-a-web-api), go for a minimal install:\n\n    pip install napari-rembg\n\n## Models\n\n- [u2net](https://github.com/xuebinqin/U-2-Net): A pre-trained model for general use cases.\n- [u2netp](https://github.com/xuebinqin/U-2-Net): A lightweight version of u2net.\n- [silueta](https://github.com/xuebinqin/U-2-Net/issues/295): Same as u2net with a size reduced to 43 Mb.\n- [isnet](https://github.com/xuebinqin/DIS): A pre-trained model for general use cases.\n- [sam](https://github.com/facebookresearch/segment-anything): Segment Anything Model pre-trained for any use cases (`vit_b`)\n\n![models](./assets/comparison.png)\n\nThe models automatically get downloaded in the user's home folder in the `.u2net` directory the first time inference is run.\n\n## Usage\n\nStart `napari-rembg` from the `Plugins` menu of Napari:\n\n```\nPlugins > Napari Select Foreground > Select foreground\n```\n\n### Segment an image loaded into Napari\n\nSelect your image in the `Image` dropdown and press `Run`. The output segmentation appears in the `Labels` layer selected in the `Mask` field (if no layer is selected, a new one is created).\n\n### Segment individual objects using bounding boxes\n\n- Click on the `Add` button next to the `ROI` field. This adds a `Shapes layer` to the viewer.\n- Click and drag bounding boxes around objects in the image. Each time you draw a bounding box a segmentation is generated in the region selected.\n\n![screenshot](./assets/screenshot.gif)\n\nYou can choose to auto-increment the label index to distinguish individual objects. Deselect that option to annotate a single foreground class.\n\n## Running the segmentation via a web API\n\nYou can run the `rembg` segmentation via a web API running in a `docker` container.\n\n**Advantages**\n- The segmentation can be run on a remote machine with optimization (e.g. GPU).\n- The segmentation models will be downloaded inside the docker container instead of the user's file system.\n- You can minimally install the package with `pip install napari-rembg` on the client's machine. This will *not* install the `rembg` library, which can solve potential dependency conflicts or bugs.\n\n**Setup**\n\nSee [these instructions](./src/rembg-api/README.md) on how to set up the docker container and web API.\n\n**Usage**\n\nStart `napari-rembg` from the `Plugins` menu of Napari:\n\n```\nPlugins > Napari Select Foreground > Select foreground (Web API)\n```\n\n## Related projects\n\nIf you are looking for similar generalist segmentation plugins, check out these related projects:\n\n- [napari-sam](https://github.com/MIC-DKFZ/napari-sam)\n- [napari-segment-anything](https://github.com/royerlab/napari-segment-anything)\n\n## Contributing\n\nContributions are very welcome. \n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-rembg\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please file an issue along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Select foreground",
      "Select foreground (Web API)"
    ],
    "contributions_sample_data": [
      "Tabueran Kiribati"
    ]
  },
  {
    "normalized_name": "napari-tracing",
    "name": "napari-tracing",
    "display_name": "Napari Tracer Plugin",
    "version": "1.0.2",
    "created_at": "2023-04-23",
    "modified_at": "2024-02-13",
    "authors": [
      "Vasudha Jha"
    ],
    "author_emails": [
      "reachvasudha27@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-tracing/",
    "home_github": "https://github.com/mapmanager/napari-tracing",
    "home_other": null,
    "summary": "A plugin to trace the brightest path between two points in an image",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "brightest-path-lib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-tracing\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-tracing.svg?color=green)](https://github.com/mapmanager/napari-tracing/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tracing.svg?color=green)](https://pypi.org/project/napari-tracing)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tracing.svg?color=green)](https://python.org)\n<!-- [![tests](https://github.com/mapmanager/napari-tracing/workflows/tests/badge.svg)](https://github.com/mapmanager/napari-tracing/actions) -->\n<!-- [![codecov](https://codecov.io/gh/mapmanager/napari-tracing/branch/main/graph/badge.svg)](https://codecov.io/gh/mapmanager/napari-tracing) -->\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tracing)](https://napari-hub.org/plugins/napari-tracing)\n\n## Napari Tracer Plugin\n\nThe `Napari Tracer Plugin` provides an intuitive interface for users to load images, perform brightest path tracing, and visualize the results. This plugin, which is built on top of the Napari viewer, enables users to explore and annotate complex images, and take advantage of the viewer's built-in features such as zooming, panning, and adjusting contrast while viewing their tracings. The `Napari Tracer Plugin` uses the brightest path tracing algorithms from [brightest-path-lib](https://github.com/mapmanager/brightest-path-lib) to provide an interactive path building process for users to create traced segments in 2D and 3D images.\n\n## Examples\n\n<video loop muted autoplay controls >\n  <source src=\"sample-2d-tracing.mp4\" type=\"video/mp4\">\n</video>\n\nYou can download our [2D](data/sample-2d.tif) and [3D](sample-3d.tif) example tif files.\n\n## Features\n\n1. Load images and trace paths in 2D and 3D.\n1. Offloads computations to a background thread to ensure a responsive user interface.\n1. Two tracing modes: disjoint and continuous. Disjoint segments refer to paths that do not share any points, while continuous segments start from the endpoint of a previously traced path.\n1. Verify traced segments and cancel tracing if necessary.\n1. Save traced paths in SWC format commonly used in biology to represent neuronal morphology.\n1. Load previously saved tracings in SWC format.\n\n## Installation\n\nYou can install `napari-tracing` via pip:\n\n    pip install napari-tracing\n\nTo install latest development version :\n\n    pip install git+https://github.com/mapmanager/napari-tracing.git\n\n## Usage\n\nOnce installed, the Napari Tracer Plugin can be accessed from the Napari menu under \"Plugins\" > \"napari tracing: Tracer Widget\". This will open the plugin interface, where you can load your image and start tracing.\n\n## Tracing\n\n1. To trace a path, select the \"Trace\" mode and the image layer that you want to trace from their respective dropdowns.\n2. Once you select the image, a points layer called the terminal points layer will be created on the Napari viewer where you can add the start and end point.\n3. Click the \"Start Tracing\" button to perform brightest path tracing between the points.\n4. The traced path will appear in a new points layer called the tracing result result layer in the Napari viewer as a line overlay.\n5. Each new traced segment is verified, so you can either accept the tracing or reject it. If you choose to reject the tracing, you can try again with a different set of points if necessary.\n6. You can click on the \"Cancel Tracing\" button to cancel a tracing that is in progress.\n\n## Saving and loading tracings\n\n1. To save a tracing, click on the \"Save Trace\" button from the plugin menu. This will save the traced path in SWC format to a file of your choosing.\n1. To load a previously saved tracing, click on the \"Load Trace\" button and choose the SWC file you want to load. The traced path will appear in the Napari viewer.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-tracing\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[file an issue]: https://github.com/mapmanager/napari-tracing/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Tracer Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spofi",
    "name": "napari-spofi",
    "display_name": "Spot Finder",
    "version": "0.0.1",
    "created_at": "2024-02-07",
    "modified_at": "2024-02-09",
    "authors": [
      "Christian Schulze"
    ],
    "author_emails": [
      "drchrisch@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-spofi/",
    "home_github": "https://github.com/drchrisch/napari-spofi",
    "home_other": null,
    "summary": "napari plugin to interactively train and test a StarDist model",
    "categories": [
      "Annotation",
      "Segmentation",
      "Visualization"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "magicgui",
      "qtpy",
      "scikit-image",
      "pyclesperanto",
      "tensorflow",
      "stardist",
      "tox ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-spofi\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-spofi.svg?color=green)](https://github.com/githubuser/napari-spofi/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spofi.svg?color=green)](https://pypi.org/project/napari-spofi)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spofi.svg?color=green)](https://python.org)\n[![tests](https://github.com/githubuser/napari-spofi/workflows/tests/badge.svg)](https://github.com/githubuser/napari-spofi/actions)\n[![codecov](https://codecov.io/gh/githubuser/napari-spofi/branch/main/graph/badge.svg)](https://codecov.io/gh/githubuser/napari-spofi)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spofi)](https://napari-hub.org/plugins/napari-spofi)\n\nnapari plugin to interactively train and test a StarDist model\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Description\n\nThis plugin provides tools for annotating spots in a 3D two-channel image (hdf5 type input file),\nsubmitting tiles for StarDist model generation or model re-training, and refining initial annotations\nbased on predictions (kind of human-in-the-loop approach).\n\nThe objects of interest in the image are sphere-like spots with a diameter of just a\nfew pixels and are thus well suited for StarDist instance segmentation. The image \ndimensions are typically 1024x1024 pixels in xy and ‚â• 64 sections in z.\n\n\n## Installation\n\nWith python and pip installed (e.g., via miniconda or miniforge),\nit is recommended to create a new environment and install `napari-spofi` using pip.\n\n    pip install napari napari-spofi\n\n## Starting `napari-spofi`\n\nStart `napari` and select \"spot finder (napari-spofi)\" from the \"plugin\" menu.\n\n### Annotate image\nGo to the 'annotation' section of the widget and create a new directory for annotations. Add an image\nfolder containing at least one h5 file (foreground and background, e.g., 'ch1' & 'ch2'). Select an image file, foreground and background\nchannels. Load the image file.\n\nInspect the image for distinct regions. To help locate relevant tile positions, make\nthe 'checkerboard' layer visible. While the 'tiles' layer is active, double-click a tile\nto add it to the list of tiles. This list will be used to generate a set of \nimages and masks for training purposes.\n\nSwitch to napari's 2D view. Navigate to the centre section of each spot in the active tile\nand annotate by adding points (one point per spot) using the 'true' points layer. The\nbuilt-in heuristic will automatically annotate pixels that belong to individual spots.\nSome image enhancement step before loading images may be beneficial. \n\nAnnotate tiles in one or a multiple images.\nTo prepare training data, use the 'extract spots' button.\n\n### Train a StarDist model\nGo to the 'training' section of the widget. Adjust the \"number of epochs\". For a first\ncheck, 100 epochs is a good start. The plugin uses a simplified setup for StarDist\nconfigurations (please see [StarDist](https://github.com/stardist/stardist/) for a full discussion).\n\nStart training and watch the 'loss' and 'val_loss' values, which should decrease\nsteadily while their ratio should roughly remain at 1 as training progresses.\n\nThe retrain option allows the selection of an existing model for retraining.\n\n### Predict instances\nGo to the 'prediction' section of the widget to start spot prediction for the\ncurrently loaded image. Select the appropriate model from the given annotation\ndirectory. The 'threshold' value is calculated from the validation data and can be\nadjusted. Start a new prediction and load the predicted spots when the process has\nfinished. (It is possible to load an existing prediction).\n\n### Polish annotation\nPredicted spots will be loaded into two new layers: 'predicted' and 'edited'. The\n'predicted' layer is not editable and gives an overview of the spots found. Check\nyour annotation in the active tiles ('true' layer) and compare it carefully with\nthe spots in the 'edited' layer.\nAdjust the positions of the spots or remove any incorrect spots from the 'edited'\nlayer. Extract the spots and train a new model or retrain the model.\n\n\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-spofi\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "spot finder"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-locpix",
    "name": "napari-locpix",
    "display_name": "napari-locpix",
    "version": "0.0.6",
    "created_at": "2023-01-16",
    "modified_at": "2024-02-08",
    "authors": [
      "Oliver Umney"
    ],
    "author_emails": [
      "scou@leeds.ac.uk"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-locpix/",
    "home_github": "https://github.com/oubino/napari-locpix",
    "home_other": null,
    "summary": "Load in SMLM data and annotate within napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "polars",
      "pyarrow",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-locpix\n\n[![License MIT](https://img.shields.io/pypi/l/napari-locpix.svg?color=green)](https://github.com/oubino/napari-locpix/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-locpix.svg?color=green)](https://pypi.org/project/napari-locpix)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-locpix.svg?color=green)](https://python.org)\n[![tests](https://github.com/oubino/napari-locpix/workflows/tests/badge.svg)](https://github.com/oubino/napari-locpix/actions)\n[![codecov](https://codecov.io/gh/oubino/napari-locpix/branch/main/graph/badge.svg)](https://codecov.io/gh/oubino/napari-locpix)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-locpix)](https://napari-hub.org/plugins/napari-locpix)\n\nLoad in SMLM data and annotate within napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-locpix` via [pip]:\n\n    pip install napari-locpix\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/oubino/napari-locpix.git\n\n\n## Usage\n\nThis plugin allows a user to\n\n1. Read in SMLM data\n2. Visualise SMLM data in a histogram\n3. Add segmentations to the data\n4. Extract the underlying localisations from the segmentations\n\n### IO\n\nThe input data can be in the form of a .csv or .parquet.\n\nWe expect there to be 4 columns at least, which should he identified inthe file column selection:\n\n* X coordinate\n* Y coordinate\n* Frame\n* Channel\n\nIf the data has been annotated with this software we can also load this in.\nNote however we currently only support loading in annotated data saved as a .parquet folder.\nTherefore, we recommend always keeping a .parquet copy until loading in an annotated .csv\nis supported.\n\nThe data can be outputted to a .parquet or a .csv\n\nDrop localisations with zero label, gives you the option to only save the localisations which have been annotated i.e. labels 1 and above.\n\nChannels labels allows you to give a real name label to each of the channels e.g. Chan 0 label: 'Alexa 647'\n\n### Visualisation\n\nUsing the render button you can render the loaded in data according to the histogram settings\n\nX/Y bins defines the number of bins for the histogram\n\nVis interpolation defines how to interpolate the image before viewing\n\n### Annotations\n\nAnnotations can be added using Napari's viewer.\n\nSimply click the add Labels.\n\nNote that this software will expect the labels to be called \"Labels\"\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-locpix\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/oubino/napari-locpix/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotate"
    ],
    "contributions_sample_data": [
      "napari-locpix"
    ]
  },
  {
    "normalized_name": "cryocanvas",
    "name": "cryocanvas",
    "display_name": "cryocanvas",
    "version": "0.0.1",
    "created_at": "2024-02-02",
    "modified_at": "2024-02-02",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czii@kyleharrington.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/cryocanvas/",
    "home_github": "https://github.com/kephale/cryocanvas",
    "home_other": null,
    "summary": "A plugin for interactive segmentation of CryoET data using ML embeddings",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "toolz",
      "scikit-learn",
      "psygnal",
      "superqt",
      "zarr",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# cryocanvas\nA tool to support interactive machine learning for cryoET data\n\n![cryocanvas screenshot](cover.png)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Example QWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-potential-field-navigation",
    "name": "napari-potential-field-navigation",
    "display_name": "Differentiable Potential Field Navigation",
    "version": "0.1.1",
    "created_at": "2024-02-02",
    "modified_at": "2024-02-02",
    "authors": [
      "Robin CREMESE"
    ],
    "author_emails": [
      "robin.cremese@gmail.com"
    ],
    "license": "MPL-2.0",
    "home_pypi": "https://pypi.org/project/napari-potential-field-navigation/",
    "home_github": "https://github.com/rcremese/napari-potential-field-navigation",
    "home_other": null,
    "summary": "A simple plugin for trajectories visualisations in napari for lung navigation in CTs scans",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari-itk-io",
      "taichi",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-potential-field-navigation\n\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-potential-field-navigation.svg?color=green)](https://github.com/rcremese/napari-potential-field-navigation/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-potential-field-navigation.svg?color=green)](https://pypi.org/project/napari-potential-field-navigation)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-potential-field-navigation.svg?color=green)](https://python.org)\n[![tests](https://github.com/rcremese/napari-potential-field-navigation/workflows/tests/badge.svg)](https://github.com/rcremese/napari-potential-field-navigation/actions)\n[![codecov](https://codecov.io/gh/rcremese/napari-potential-field-navigation/branch/main/graph/badge.svg)](https://codecov.io/gh/rcremese/napari-potential-field-navigation)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-potential-field-navigation)](https://napari-hub.org/plugins/napari-potential-field-navigation)\n\nA simple plugin for trajectories visualisations in napari for lung navigation in CTs scans\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-potential-field-navigation` via [pip]:\n\n    pip install napari-potential-field-navigation\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/rcremese/napari-potential-field-navigation.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Mozilla Public License 2.0] license,\n\"napari-potential-field-navigation\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/rcremese/napari-potential-field-navigation/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Diff APF"
    ],
    "contributions_sample_data": [
      "Differentiable Potential Field Navigation",
      "Image / Label Samples"
    ]
  },
  {
    "normalized_name": "napari-openfibsem",
    "name": "napari-openfibsem",
    "display_name": "OpenFIBSEM Napari",
    "version": "0.1.5",
    "created_at": "2023-09-22",
    "modified_at": "2024-02-01",
    "authors": [
      "Patrick Cleeve"
    ],
    "author_emails": [
      "patrick.cleeve@monash.edu"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-openfibsem/",
    "home_github": "https://github.com/DeMarcoLab/napari-openfibsem",
    "home_other": null,
    "summary": "OpenFIBSEM Applications",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "fibsem >=0.3.2a1",
      "autolamella >=0.3.2a1",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-openfibsem\n\n[![License MIT](https://img.shields.io/pypi/l/napari-openfibsem.svg?color=green)](https://github.com/DeMarcoLab/napari-openfibsem/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-openfibsem.svg?color=green)](https://pypi.org/project/napari-openfibsem)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-openfibsem.svg?color=green)](https://python.org)\n[![tests](https://github.com/DeMarcoLab/napari-openfibsem/workflows/tests/badge.svg)](https://github.com/DeMarcoLab/napari-openfibsem/actions)\n[![codecov](https://codecov.io/gh/DeMarcoLab/napari-openfibsem/branch/main/graph/badge.svg)](https://codecov.io/gh/DeMarcoLab/napari-openfibsem)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-openfibsem)](https://napari-hub.org/plugins/napari-openfibsem)\n\nOpenFIBSEM Applications\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-openfibsem` via [pip]:\n\n    pip install napari-openfibsem\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/DeMarcoLab/napari-openfibsem.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-openfibsem\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/DeMarcoLab/napari-openfibsem/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "OpenFIBSEM UI",
      "OpenFIBSEM Image Viewer",
      "OpenFIBSEM Image Labelling UI",
      "OpenFIBSEM Keypoint Labelling UI",
      "AutoLamella UI",
      "AutoLiftout UI"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-points2regions",
    "name": "napari-points2regions",
    "display_name": "Points2Regions",
    "version": "0.0.2",
    "created_at": "2024-01-26",
    "modified_at": "2024-01-26",
    "authors": [
      "Jonas Windhager"
    ],
    "author_emails": [
      "jonas@windhager.io"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-points2regions/",
    "home_github": "https://github.com/wahlby-lab/napari-points2regions",
    "home_other": null,
    "summary": "A napari plugin for Points2Regions",
    "categories": [
      "Annotation",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "colorcet",
      "magicgui",
      "napari",
      "numpy",
      "pandas",
      "points2regions >=0.0.4",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-points2regions\n\n[![License MIT](https://img.shields.io/pypi/l/napari-points2regions.svg?color=green)](https://github.com/wahlby-lab/napari-points2regions/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-points2regions.svg?color=green)](https://pypi.org/project/napari-points2regions)\n[![tests](https://github.com/wahlby-lab/napari-points2regions/workflows/tests/badge.svg)](https://github.com/wahlby-lab/napari-points2regions/actions)\n[![codecov](https://codecov.io/gh/wahlby-lab/napari-points2regions/branch/main/graph/badge.svg)](https://codecov.io/gh/wahlby-lab/napari-points2regions)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-points2regions.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-points2regions)](https://napari-hub.org/plugins/napari-points2regions)\n\nA napari plugin for Points2Regions\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-points2regions` via [pip]:\n\n    pip install napari-points2regions\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/wahlby-lab/napari-points2regions.git\n\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-points2regions\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/wahlby-lab/napari-points2regions/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Load points",
      "Points2Regions",
      "Adjust point display",
      "Export point features"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-label-interpolator",
    "name": "napari-label-interpolator",
    "display_name": "napari label interpolator",
    "version": "0.1.1",
    "created_at": "2022-09-08",
    "modified_at": "2024-01-24",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "brisvag@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-label-interpolator/",
    "home_github": "https://github.com/brisvag/napari-label-interpolator",
    "home_other": null,
    "summary": "A napari plugin to interpolate any number of nd-labels across a single dimension.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "edt",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-label-interpolator\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-label-interpolator.svg?color=green)](https://github.com/brisvag/napari-label-interpolator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-label-interpolator.svg?color=green)](https://pypi.org/project/napari-label-interpolator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-label-interpolator.svg?color=green)](https://python.org)\n[![tests](https://github.com/brisvag/napari-label-interpolator/workflows/tests/badge.svg)](https://github.com/brisvag/napari-label-interpolator/actions)\n[![codecov](https://codecov.io/gh/brisvag/napari-label-interpolator/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-label-interpolator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-label-interpolator)](https://napari-hub.org/plugins/napari-label-interpolator)\n\nA napari plugin to interpolate any number of nd-labels across a single dimension.\n\nTo use, simply label a few slices along the desired dimension, then use the widget to interpolate along the desired axis.\n\n![](https://user-images.githubusercontent.com/23482191/189153632-40ef38b7-be89-40b3-b583-b17f3241c67b.png)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-label-interpolator` via [pip]:\n\n    pip install napari-label-interpolator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/brisvag/napari-label-interpolator.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-label-interpolator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/brisvag/napari-label-interpolator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Label interpolator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "mikro-napari",
    "name": "mikro-napari",
    "display_name": "Mikro Napari",
    "version": "0.1.63",
    "created_at": "2022-10-11",
    "modified_at": "2024-01-23",
    "authors": [
      "jhnnsrs"
    ],
    "author_emails": [
      "jhnnsrs@gmail.com"
    ],
    "license": "CC BY-NC 3.0",
    "home_pypi": "https://pypi.org/project/mikro-napari/",
    "home_github": "https://github.com/jhnnsrs/mikro-napari",
    "home_other": null,
    "summary": "A napari plugin to interact with and provide functionality for a connected arkitekt server",
    "categories": [],
    "package_metadata_requires_python": ">=3.8,<=3.12",
    "package_metadata_requires_dist": [
      "arkitekt[fluss,mikro,reaktion,rekuest,unlok] (>=0.5.58)"
    ],
    "package_metadata_description": "# mikro-napari\n\n[![codecov](https://codecov.io/gh/jhnnsrs/mikro-napari/branch/master/graph/badge.svg?token=UGXEA2THBV)](https://codecov.io/gh/jhnnsrs/mikro-napari)\n[![PyPI version](https://badge.fury.io/py/mikro-napari.svg)](https://pypi.org/project/mikro-napari/)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://pypi.org/project/mikro-napari/)\n![Maintainer](https://img.shields.io/badge/maintainer-jhnnsrs-blue)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/mikro-napari.svg)](https://pypi.python.org/pypi/mikro-napari/)\n[![PyPI status](https://img.shields.io/pypi/status/mikro-napari.svg)](https://pypi.python.org/pypi/mikro-napari/)\n\nmikro napari enables napari on the mikro/arkitekt platform\n\n# DEVELOPMENT\n\n## Idea\n\nThis is a napari plugin, that provides a simple user interface to use napari with mikro you can view and annotate\ndata on the mikro platform (synchronised between all of your napari instances) and use napari within arkitekt workflows\n(can be extended with other plugins)\n\n## Install\n\nSimple install this plugin via naparis plugin-manager and enable it. \nLogin with your local mikro/arkitekt platform and start using it in workflows\n\nYou can also install mikro-napari directly in your enviroment \n\n```bash\npip install mikro-napari napari[pyqt5]\n```\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Arkitekt Widget",
      "Arkitekt Sidebar"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-argos-archive-reader",
    "name": "napari-argos-archive-reader",
    "display_name": "Dioptic ARGOS Archive Reader",
    "version": "0.0.7",
    "created_at": "2024-01-03",
    "modified_at": "2024-01-23",
    "authors": [
      "Volker Hilsenstein"
    ],
    "author_emails": [
      "hilsenstein@dioptic.de"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-argos-archive-reader/",
    "home_github": "https://github.com/dioptic/napari-argos-archive-reader",
    "home_other": null,
    "summary": "A plugin to read Dioptic ARGOS archive files",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "scikit-image",
      "pydantic",
      "ruamel.yaml",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-argos-archive-reader\n\n[![License MIT](https://img.shields.io/pypi/l/napari-argos-archive-reader.svg?color=green)](https://github.com/dioptic/napari-argos-archive-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-argos-archive-reader.svg?color=green)](https://pypi.org/project/napari-argos-archive-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-argos-archive-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/dioptic/napari-argos-archive-reader/workflows/tests/badge.svg)](https://github.com/dioptic/napari-argos-archive-reader/actions)\n[![codecov](https://codecov.io/gh/dioptic/napari-argos-archive-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/dioptic/napari-argos-archive-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-argos-archive-reader)](https://napari-hub.org/plugins/napari-argos-archive-reader)\n\nA plugin to read Dioptic ARGOS archive files\n\n----------------------------------\n\nThis repo contains a reader plugin for [DIOPTIC ARGOS](https://www.dioptic.de/en/argos-en/) Archive files, which\nhave `.zip` file extension.\nIndividual ARGOS layers are grouped into napari layer with stacks according to\ntheir illumination, stage XY position and Z-stack information.\n\nThe plugin implements delayed reading using `dask.delayed` so that one can quickly\nsee the contents even for large archives with many layers. Note!: switching to\nvolume rendering or swapping axes can trigger the loading of all ARGOS layers, which\ncan take a long time for large archives.\n\n[ARGOS](https://www.dioptic.de/en/argos-en/) is an automated system\nfor surface inspection according to ISO 10110-7.\n\nThis plugin is still experimental and does not support all features of ARGOS archives.\n\nCurrently, the plugin\n\n* can read Argos matrix archives containing regular image layers including:\n  * ‚úÖ segmentation masks\n  * ‚úÖ Z-stack metadata\n  * ‚úÖ Illumination metadata\n  * ‚úÖ proper scaling and affine transformation of layers\n* can read ‚ùî‚úÖ Argos line scan (polar) archives with minimal support (no metadata parsing)\nThis has not been tested on many archives.\n\nNot supported are:\n\n* ‚ùå annotated archives\n* ‚ùå pyramid image structures\n* ‚ùå Line segmentation metadata\n* ‚ùå color metadata\n* ‚ùå ...\n\n## Usage\n\n### Opening files\n\nSimply drag and drop an ARGOS Archive `.zip` file onto the napari canvas or use `File->Open` to open it.\n\n### Synchronizing contrast limits\n\nBy default, after reading an archive, each napari layer will have their own contrast limits, so you can\nadjust these contrast limits individually.\n\nThe reader plugin registers a custom key binding after reading an ARGOS archive. Pressing the `s` key will allow\nyou to synchronize the contrast limits for a set of layers:\n\n* If you select _a single_ napari layer corresponding to an image/stack from an ARGOS archive, all napari image\nlayers that were loaded from this archive now have their contrast limits synchronized, i.e. changing the\ncontrast limits of _any_ of them will adjust the contrast limits of _all_ of the layers belonging to the same\narchive.\n* If you select _multiple_ napari layers and press `s` all of these and only these napari layers will have\ntheir contrast limits synchronized, regardless of whether they belong to the same ARGOS archive or not.\n\n## Installation\n\nIf you have napari installed you can install the plugin from the napari hub through the `Plugins -> Plugin Manger` menu\nentry. After waiting a short while for napari to retrieve the plugins available from the hub, simply enter \"argos\" in\nthe filter line entry field at the top to narrow down the plugin list and click install.\n\nYou can install `napari-argos-archive-reader` via [pip]:\n\n    pip install napari-argos-archive-reader\n\nTo install latest development version :\n\n    pip install git+https://github.com/dioptic/napari-argos-archive-reader.git\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-argos-archive-reader\" is free and open source software\n\n[MIT]: http://opensource.org/licenses/MIT\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.zip"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-morphodynamics",
    "name": "napari-morphodynamics",
    "display_name": "napari-morphodynamics",
    "version": "0.1.2",
    "created_at": "2023-11-08",
    "modified_at": "2024-01-22",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-morphodynamics/",
    "home_github": "https://github.com/guiwitz/napari-morphodynamics",
    "home_other": null,
    "summary": "Interface to run the morphodynamics package.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "morphodynamics",
      "napari-convpaint",
      "napari-guitils",
      "napari-matplotlib",
      "cellpose ; extra == 'cellpose'"
    ],
    "package_metadata_description": "# napari-morphodynamics\n\n[![License](https://img.shields.io/pypi/l/napari-morphodynamics.svg?color=green)](https://github.com/guiwitz/napari-morphodynamics/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-morphodynamics.svg?color=green)](https://pypi.org/project/napari-morphodynamics)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-morphodynamics.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-morphodynamics/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-morphodynamics/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-morphodynamics/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-morphodynamics)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-morphodynamics)](https://napari-hub.org/plugins/napari-morphodynamics)\n\nThis plugin offers an interface for the [Morphodynamics](https://github.com/guiwitz/MorphoDynamics) package which allows to study the shape and intra-cellular dynamics of cells imaged as time-lapses by fluorescence microscopy. The plugin offers a single place to perfrom segmentation, windowing (partition cells into small regions of interests that are tracked over time) and results visualization. The software depends on [napari-convpaint](https://github.com/guiwitz/napari-convpaint) a pixel-classifier and/or on [cellpose](https://cellpose.readthedocs.io/en/latest/index.html) for segmentation. \n\n## Installation\n\nYou can install the plugin via [pip] with:\n\n    pip install napari-morphodynamics\n\nTo install latest development version :\n\n    pip install git+https://github.com/guiwitz/napari-morphodynamics.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-morphodynamics\" is free and open source software\n\n## Authors\n\nThis plugin has been developed by Guillaume Witz and Ana Stojiljkovic at the Data Science Lab, University of Bern, in collaboration with Lucien Hinderling and Olivier Pertz from the Pertz Lab, University of Bern. Development has been partially funded by a [Chan Zuckerberg Initiative grant](https://chanzuckerberg.com/science/programs-resources/imaging/napari/napari-morphodynamics-a-plugin-to-quantify-cellular-dynamics/).\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-morphodynamics/issues\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MorphoWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-biomag-annotator",
    "name": "napari-biomag-annotator",
    "display_name": "BIOMAG Annotator",
    "version": "0.0.3",
    "created_at": "2024-01-11",
    "modified_at": "2024-01-11",
    "authors": [
      "Reka Hollandi",
      "David Bauer"
    ],
    "author_emails": [
      "hunreka93@hotmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-biomag-annotator/",
    "home_github": "https://github.com/biomag-lab/napari-biomag-annotator",
    "home_other": null,
    "summary": "An annotator tool collection by the BIOMAG group.",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine >=0.1.4",
      "napari-annotatorj",
      "napari-nD-annotator",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-biomag-annotator\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-biomag-annotator.svg?color=green)](https://github.com/biomag-lab/napari-biomag-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-biomag-annotator.svg?color=green)](https://pypi.org/project/napari-biomag-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-biomag-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/biomag-lab/napari-biomag-annotator/workflows/tests/badge.svg)](https://github.com/biomag-lab/napari-biomag-annotator/actions)\n[![codecov](https://codecov.io/gh/biomag-lab/napari-biomag-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/biomag-lab/napari-biomag-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-biomag-annotator)](https://napari-hub.org/plugins/napari-biomag-annotator)\n\nAn annotator tool collection by the BIOMAG group.\n\nThis plugin allows object annotation on 2/3D images using 4 assisted annotation methods arising from two napari plugins:\n\n- [napari-annotatorj](https://github.com/spreka/napari-annotatorj)\n- [napari-nD-annotator](https://github.com/bauerdavid/napari-nD-annotator)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-biomag-annotator` via [pip]:\n\n    pip install napari[all]\n    pip install napari-biomag-annotator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/biomag-lab/napari-biomag-annotator.git\n\n\nOn Linux distributions, the following error may arise upon napari startup after the installation of the plugin: `Could not load the Qt platform plugin ‚Äúxcb‚Äù in ‚Äú‚Äù even though it was found`. In this case, the manual install of `libxcb-xinerama0` for Qt is required:\n\n    sudo apt install libxcb-xinerama0\n\n### Bundled napari app\nThe bundled application version of [napari](https://github.com/napari/napari/releases) allows the pip install of plugins in the .zip distribution. After installation of this release, napari-annotatorj can be installed from the `Plugins --> Install/Uninstall plugins...` menu by searching for its name and clicking on the `Install` button next to it.\n\n### Script\nSingle-file install is supported on [**Windows**](#windows) and [Linux](#linux) (currently). It will create a virtual environment named `napariAnnotatorEnv` in the parent folder of the cloned repository, install the package via pip and start napari. It requires a valid Python install.\n\n#### Windows\nTo start it, run in the Command prompt\n\n    git clone https://github.com/biomag-lab/napari-biomag-annotator.git\n    cd napari-biomag-annotator\n    install.bat\n\nOr download [install.bat](https://github.com/biomag-lab/napari-biomag-annotator/blob/main/install.bat) and run it from the Command prompt.\n\nAfter install, you can use [startup_napari.bat](https://github.com/biomag-lab/napari-biomag-annotator/blob/main/startup_napari.bat) to activate your installed virtual environment and run napari. Run it from the Command prompt with:\n\n    startup_napari.bat\n\n\n#### Linux\nTo start it, run in the Terminal\n\n    git clone https://github.com/biomag-lab/napari-biomag-annotator.git\n    cd napari-annotatorj\n    install.sh\n\nOr download [install.sh](https://github.com/biomag-lab/napari-biomag-annotator/blob/main/install.sh) and run it from the Terminal.\n\nAfter install, you can use [startup_napari.sh](https://github.com/biomag-lab/napari-biomag-annotator/blob/main/startup_napari.sh) to activate your installed virtual environment and run napari. Run it from the Terminal with:\n\n    startup_napari.sh\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-biomag-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/biomag-lab/napari-biomag-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "napari-biomag-annotator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bee-annotator",
    "name": "napari-bee-annotator",
    "display_name": "Bee annotator",
    "version": "0.0.1",
    "created_at": "2024-01-09",
    "modified_at": "2024-01-09",
    "authors": [
      "Florian Aymanns"
    ],
    "author_emails": [
      "florian.aymanns@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-bee-annotator/",
    "home_github": "https://github.com/EPFL-Center-for-Imaging/napari-bee-annotator",
    "home_other": null,
    "summary": "Napari plugin for the annotation of bee entering and leaving the hive.",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "<img style=\"float: right;\" src=\"https://imaging.epfl.ch/resources/logo-for-gitlab.svg\">\n\n\n# napari-bee-annotator\nDeveloped by the [EPFL Center for Imaging](https://imaging.epfl.ch/) for the [Mobile Robotic Systems Group](https://www.epfl.ch/labs/mobots/) in Dec 2023.\nThis napari plugin provides an easy way for the researches to annotate honey bees leaving/entering the hive. The annotations will serve as ground truth for the validation of various automated animal tracking approaches.\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bee-annotator.svg?color=green)](https://github.com/EPFL-Center-for-Imaging/napari-bee-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bee-annotator.svg?color=green)](https://pypi.org/project/napari-bee-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bee-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/EPFL-Center-for-Imaging/napari-bee-annotator/workflows/tests/badge.svg)](https://github.com/EPFL-Center-for-Imaging/napari-bee-annotator/actions)\n[![codecov](https://codecov.io/gh/EPFL-Center-for-Imaging/napari-bee-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/EPFL-Center-for-Imaging/napari-bee-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bee-annotator)](https://napari-hub.org/plugins/napari-bee-annotator)\n\n## Installation\n\nYou can install `napari-bee-annotator` via [pip]:\n\n    pip install napari-bee-annotator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/EPFL-Center-for-Imaging/napari-bee-annotator.git\n\n## Getting started\n\n1. Open napari with the plugin and your video using the following command `napari -w napari-bee-annotator --plugin video path/to/video.mp4`. Note that you need to have [napari_video](https://www.napari-hub.org/plugins/napari_video) installed to read `mp4` files.\n\n2. Select the orientation of your video: horizontal/vertical refers to the direction of the bee's leaving/entering the hive.\n\n3. Start annotating: A simple left click indicates a bee moving up or to the left depending on the orientation selected. You can hold down the shift key to annotate a bee moving down or to the right. Annotations can be deleted with a right click on the annotation you want to delete. To move to the next frame, you can either hold down `ctrl` and scroll with the mouse wheel or click on the play button. Playback parameters, such as the playback speed, can be changed by right clicking on the play button.\n\n4. Saving and loading tracks: To save a tracks layer selected from the list of layers and click on `File > Save selected layers`. Choose a name and the csv extension. If you want to continue to work on the annotations for a specific video, you first have to load the corresponding csv file by clicking on `Open with Plugin > Open file(s)...`. Select the file you want to load and click on open. A dialog should pop up that asks you to select the reader to use for loading the csv file. Select `Bee annotator`. Lastly, you have to tell the plugin to interact with the layer you just loaded by selecting it in the `Tracks layer` drop down menu.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bee-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/EPFL-Center-for-Imaging/napari-bee-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [
      ".csv"
    ],
    "contributions_widgets": [
      "Bee annotator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-shape-odyssey",
    "name": "napari-shape-odyssey",
    "display_name": "shape odyssey",
    "version": "0.1.1",
    "created_at": "2023-08-31",
    "modified_at": "2024-01-02",
    "authors": [
      "Johannes Soltwedel"
    ],
    "author_emails": [
      "johannes_richard.soltwedel@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-shape-odyssey/",
    "home_github": "https://github.com/jo-mueller/napari-shape-odyssey",
    "home_other": null,
    "summary": "Analyze shapes of meshes",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "vedo",
      "pandas",
      "napari",
      "napari-stress",
      "napari-process-points-and-surfaces",
      "pyfmaps",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "PyQt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-shape-odyssey\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-shape-odyssey.svg?color=green)](https://github.com/jo-mueller/napari-shape-odyssey/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-shape-odyssey.svg?color=green)](https://pypi.org/project/napari-shape-odyssey)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-shape-odyssey.svg?color=green)](https://python.org)\n[![tests](https://github.com/jo-mueller/napari-shape-odyssey/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-shape-odyssey/actions)\n[![codecov](https://codecov.io/gh/jo-mueller/napari-shape-odyssey/branch/main/graph/badge.svg)](https://codecov.io/gh/jo-mueller/napari-shape-odyssey)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-shape-odyssey)](https://napari-hub.org/plugins/napari-shape-odyssey)\n\nAnalyze shapes of meshes: This plugin provides advanced measures of shape for meshes. It is based largely on the following libraries and tools:\n\n* [PyFM](https:/github.com/robinmagnet/pyfm)\n* [boundary-first-flattening](https://github.com/GeometryCollective/boundary-first-flattening)\n\n## Shape analysis\n\nThis plugin provides Laplace spectra ([Reuter, Wolter, Peinecke (2005)](https://dl.acm.org/doi/abs/10.1145/1060244.1060256)), heat kernel signatures ([Bronstein & Kokkinos (2010)](https://ieeexplore.ieee.org/abstract/document/5539838/)) & wave kernel signatures ([Audrey, Schlickewei, Cremers et al.](https://ieeexplore.ieee.org/abstract/document/6130444)).\n\n**Laplace spectra** can be imagined to be the equivalent of resonance modes on the surface of a mesh. The resonance and the resonance modes can, for typical objects, look like this:\n\n![](https://github.com/jo-mueller/napari-shape-odyssey/raw/main/docs/imgs/Eigenvalues.gif)\n\n**Heat kernel signatures**: Heat dissipation on a mesh depends on local geometry. You can use the heat kernel signature to easily generate a large number of local features of shape\n\n![](https://github.com/jo-mueller/napari-shape-odyssey/raw/main/docs/imgs/heat_kernel_signature.gif)\n\n## Unwrapping\n\nThis plugin provides a number of methods to unwrap a mesh into basic shapes such as spheres or disks. The method relies on [boundary-first flattening](https://github.com/GeometryCollective/boundary-first-flattening) - currently it's only available on Windows.\n\n![](https://github.com/jo-mueller/napari-shape-odyssey/raw/main/docs/imgs/unwrap_to_sphere.png)\n\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-shape-odyssey` via [pip]:\n\n¬¥¬¥¬¥bash\n    pip install napari-shape-odyssey\n    napari-skimage-regionprops @ git+https://github.com/jo-mueller/napari-skimage-regionprops.git\n    pyFM @ git+https://github.com/RobinMagnet/pyFM.git\n¬¥¬¥¬¥\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-shape-odyssey\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.ply",
      "*.vtp",
      "*.obj",
      "*.stl",
      "*.vtk",
      "*.off"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Spectral shape analysis",
      "Approximate intensity in LBO basis",
      "Heat kernel signature",
      "Wave kernel signature",
      "Show shape correspondencens",
      "Unwrap surface into sphere",
      "Unwrap surface into disk",
      "Mercator projection",
      "Generate sample data from shape odyssey"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-curviewer",
    "name": "napari-curviewer",
    "display_name": "Curviewer",
    "version": "0.1.0",
    "created_at": "2023-12-25",
    "modified_at": "2023-12-25",
    "authors": [
      "Romain Fernandez"
    ],
    "author_emails": [
      "romain.fernandez@cirad.fr"
    ],
    "license": "LGPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-curviewer/",
    "home_github": "https://github.com/Rocsg/napari-curviewer",
    "home_other": null,
    "summary": "A plugin to unroll organs along their curved central line",
    "categories": [
      "Annotation",
      "Segmentation",
      "Acquisition"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "vedo",
      "vtk",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-curviewer\n\n[![License GNU LGPL v3.0](https://img.shields.io/pypi/l/napari-curviewer.svg?color=green)](https://github.com/Rocsg/napari-curviewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-curviewer.svg?color=green)](https://pypi.org/project/napari-curviewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-curviewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/Rocsg/napari-curviewer/workflows/tests/badge.svg)](https://github.com/Rocsg/napari-curviewer/actions)\n[![codecov](https://codecov.io/gh/Rocsg/napari-curviewer/branch/main/graph/badge.svg)](https://codecov.io/gh/Rocsg/napari-curviewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-curviewer)](https://napari-hub.org/plugins/napari-curviewer)\n\nA plugin to unroll organs along their curved central line\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-curviewer` via [pip]:\n\n    pip install napari-curviewer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Rocsg/napari-curviewer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU LGPL v3.0] license,\n\"napari-curviewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Rocsg/napari-curviewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Container Threshold",
      "Magic Threshold",
      "Autogenerate Threshold",
      "Example QWidget"
    ],
    "contributions_sample_data": [
      "Curviewer"
    ]
  },
  {
    "normalized_name": "napari-yolo5-mitosis-detector",
    "name": "napari-yolo5-mitosis-detector",
    "display_name": "Yolo5 Mitosis Detector",
    "version": "0.0.1",
    "created_at": "2023-12-15",
    "modified_at": "2023-12-15",
    "authors": [
      "Titouan Poquillon"
    ],
    "author_emails": [
      "titouan.poquillon@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-yolo5-mitosis-detector/",
    "home_github": "https://github.com/TPoquillon/napari-yolo5-mitosis-detector",
    "home_other": null,
    "summary": "A simple plugin to use yolo5 for mitosis detection with napari",
    "categories": [
      "Annotation"
    ],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy ==1.26.2",
      "magicgui ==0.8.0",
      "qtpy ==2.4.1",
      "scikit-image ==0.20.0",
      "opencv-python ==4.8.1.78",
      "torch ==2.1.1",
      "ultralytics ==8.0.222",
      "shapely ==2.0.2",
      "importlib-resources ==6.1.1",
      "pandas ==2.1.3",
      "napari-aicsimageio ==0.7.2",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-yolo5-mitosis-detector\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-yolo5-mitosis-detector.svg?color=green)](https://github.com/TPoquillon/napari-yolo5-mitosis-detector/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-yolo5-mitosis-detector.svg?color=green)](https://pypi.org/project/napari-yolo5-mitosis-detector)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-yolo5-mitosis-detector.svg?color=green)](https://python.org)\n[![tests](https://github.com/TPoquillon/napari-yolo5-mitosis-detector/workflows/tests/badge.svg)](https://github.com/TPoquillon/napari-yolo5-mitosis-detector/actions)\n[![codecov](https://codecov.io/gh/TPoquillon/napari-yolo5-mitosis-detector/branch/main/graph/badge.svg)](https://codecov.io/gh/TPoquillon/napari-yolo5-mitosis-detector)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-yolo5-mitosis-detector)](https://napari-hub.org/plugins/napari-yolo5-mitosis-detector)\n\nA simple plugin to use yolo5 for mitosis detection with napari\n\ntpoquillon@gmail.com\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-yolo5-mitosis-detector` via [pip]:\n\n    pip install napari-yolo5-mitosis-detector\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/TPoquillon/napari-yolo5-mitosis-detector.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-yolo5-mitosis-detector\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/TPoquillon/napari-yolo5-mitosis-detector/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "NY5MD"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-annotatorj",
    "name": "napari-annotatorj",
    "display_name": "napari-annotatorj",
    "version": "0.0.8",
    "created_at": "2022-05-26",
    "modified_at": "2023-12-14",
    "authors": [
      "Reka Hollandi"
    ],
    "author_emails": [
      "reka.hollandi@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-annotatorj/",
    "home_github": "https://github.com/spreka/napari-annotatorj",
    "home_other": null,
    "summary": "The napari adaptation of the ImageJ/Fiji plugin AnnotatorJ for easy image annotation.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "roifile",
      "scikit-image",
      "opencv-python >=4.5.5",
      "keras",
      "tensorflow >=2.5.0",
      "tifffile",
      "imagecodecs",
      "tqdm",
      "pyqtgraph"
    ],
    "package_metadata_description": "# napari-annotatorj\n\n[![License](https://img.shields.io/pypi/l/napari-annotatorj.svg?color=green)](https://github.com/spreka/napari-annotatorj/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-annotatorj.svg?color=green)](https://pypi.org/project/napari-annotatorj)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-annotatorj.svg?color=green)](https://python.org)\n[![tests](https://github.com/spreka/napari-annotatorj/workflows/tests/badge.svg)](https://github.com/spreka/napari-annotatorj/actions)\n[![codecov](https://codecov.io/gh/spreka/napari-annotatorj/branch/main/graph/badge.svg)](https://codecov.io/gh/spreka/napari-annotatorj)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-annotatorj)](https://napari-hub.org/plugins/napari-annotatorj)\n\nThe napari adaptation of the ImageJ/Fiji plugin [AnnotatorJ](https://github.com/spreka/annotatorj) for easy image annotation.\n\n![image](https://drive.google.com/uc?export=view&id=1fVfvanffTdrXvLE0m1Yo6FV5TAjh6sb2)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nInstallation is possible with [pip](#pip), [napari](#bundled-napari-app) or [scripts](#script).\n### Pip\nYou can install `napari-annotatorj` via [pip]:\n\n    pip install napari[all]\n\tpip install napari-annotatorj\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/spreka/napari-annotatorj.git\n\n\nOn Linux distributions, the following error may arise upon napari startup after the installation of the plugin: `Could not load the Qt platform plugin ‚Äúxcb‚Äù in ‚Äú‚Äù even though it was found`. In this case, the manual install of `libxcb-xinerama0` for Qt is required:\n\n\tsudo apt install libxcb-xinerama0\n\n### Bundled napari app\nThe bundled application version of [napari](https://github.com/napari/napari/releases) allows the pip install of plugins in the .zip distribution. After installation of this release, napari-annotatorj can be installed from the `Plugins --> Install/Uninstall plugins...` menu by searching for its name and clicking on the `Install` button next to it.\n\n### Script\nSingle-file install is supported on [**Windows**](#windows) and [Linux](#linux) (currently). It will create a virtual environment named `napariAnnotatorjEnv` in the parent folder of the cloned repository, install the package via pip and start napari. It requires a valid Python install.\n\n#### Windows\nTo start it, run in the Command prompt\n\n\tgit clone https://github.com/spreka/napari-annotatorj.git\n\tcd napari-annotatorj\n\tinstall.bat\n\nOr download [install.bat](https://github.com/spreka/napari-annotatorj/blob/main/install.bat) and run it from the Command prompt.\n\nAfter install, you can use [startup_napari.bat](https://github.com/spreka/napari-annotatorj/blob/main/startup_napari.bat) to activate your installed virtual environment and run napari. Run it from the Command prompt with:\n\n\tstartup_napari.bat\n\n\n#### Linux\nTo start it, run in the Terminal\n\n\tgit clone https://github.com/spreka/napari-annotatorj.git\n\tcd napari-annotatorj\n\tinstall.sh\n\nOr download [install.sh](https://github.com/spreka/napari-annotatorj/blob/main/install.sh) and run it from the Terminal.\n\nAfter install, you can use [startup_napari.sh](https://github.com/spreka/napari-annotatorj/blob/main/startup_napari.sh) to activate your installed virtual environment and run napari. Run it from the Terminal with:\n\n\tstartup_napari.sh\n\n***\n## Intro\n\nnapari-annotatorj has several convenient functions to speed up the annotation process, make it easier and more fun. These *modes* can be activated by their corresponding checkboxes on the left side of the main AnnotatorJ widget.\n\n- [Contour assist mode](#contour-assist-mode)\n- [Edit mode](#edit-mode)\n- [Class mode](#class-mode)\n- [Overlay](#overlay)\n\nFreehand drawing is enabled in the plugin. The \"Add polygon\" tool is selected by default upon startup. To draw a freehand object (shape) simply hold the mouse and drag it around the object. The contour is visualized when the mouse button is released.\n\nSee the [guide](#how-to-annotate) below for a quick start or a [demo](#demo). See [shortcuts](#shortcuts) for easy operation.\n\n***\n## How to annotate\n\n1. Open --> opens an image\n2. (Optionally) \n\t- ... --> Select annotation type --> Ok --> a default tool is selected from the toolbar that fits the selected annotation type\n\t- The default annotation type is instance\n\t- Selected annotation type is saved to a config file\n3. Start annotating objects\n\t- [instance](#instance-annotation): draw contours around objects\n\t- [semantic](#semantic-annotation): paint the objects' area\n\t- [bounding box](#bounding-box-annotation): draw rectangles around the objects\n4. Save --> Select class --> saves the annotation to a file in a sub-folder of the original image folder with the name of the selected class\n\n5. (Optionally)\n\t- Load --> continue a previous annotation\n\t- Overlay --> display a different annotation as overlay (semi-transparent) on the currently opened image\n\t- Colours --> select annotation and overlay colours\n\t- ... (coming soon) --> set options for semantic segmentation and *Contour assist* mode\n\t- checkboxes --> Various options\n\t\t- (default) Add automatically --> adds the most recent annotation to the ROI list automatically when releasing the left mouse button\n\t\t- Smooth (coming soon) --> smooths the contour (in instance annotation type only)\n\t\t- Show contours --> displays all the contours in the ROI list\n\t\t- Contours assist --> suggests a contour in the region of an initial, lazily drawn contour using the deep learning method U-Net\n\t\t- Show overlay --> displays the overlayed annotation if loaded with the Overlay button\n\t\t- Edit mode --> edits a selected, already saved contour in the ROI list by clicking on it on the image\n\t\t- Class mode --> assigns the selected class to the selected contour in the ROI list by clicking on it on the image and displays its contour in the class's colour (can be set in the Class window); clicking on the object a second time unclassifies it\n\t- [^] --> quick export in 16-bit multi-labelled .tiff format; if classified, also exports by classes\n\n***\n## Instance annotation\nAllows freehand drawing of object contours (shapes) with the mouse as in ImageJ.\n\nShape contour points are tracked automatically when the left mouse button is held and dragged to draw a shape. The shape is closed when the mouse button is released, automatically, and added to the default shapes layer (named \"ROI\"). In direct selection mode (from the layer controls panel), you can see the saved contour points. The slower you drag the mouse, the more contour points saved, i.e. the more refined your contour will be.\n\nClick to watch demo video below.\n\n[![instance-annot-demo](https://drive.google.com/uc?export=view&id=1sBg19d_hqGH-UI8irkrwame7ZjrldwHr)](https://drive.google.com/uc?export=view&id=1wELreE9MdCZq4Kf4oCWdxIw4e5o05XzK \"Click to watch instance annotation demo\")\n\n***\n## Semantic annotation\nAllows painting with the brush tool (labels).\n\nUseful for semantic (e.g. scene) annotation. Currently saves all labels to binary mask only (foreground-background).\n\n***\n## Bounding box annotation\nAllows drawing bounding boxes (shapes, rectangles) around objects with the mouse.\n\nUseful for object detection annotation.\n\n***\n## Contour assist mode\nAssisted annotation via a pre-trained deep learning model's suggested contour.\n\n1. initialize a contour with mouse drag around an object\n2. the suggested contour is displayed automatically\n3. modify the contour:\n    - edit with mouse drag or \n    - erase holding \"Alt\" or\n\t- invert with pressing \"u\"\n4. finalize it\n    - accept with pressing \"q\" or\n    - reject with pressing \"Ctrl\" + \"Del\"\n\n- if the suggested contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \"q\" to accept it\n- this mode requires a Keras model to be present in the [model folder](#configure-model-folder)\n\nClick to watch demo video below\n\n[![contour-assist-demo](https://drive.google.com/uc?export=view&id=1Mw2fCPdm5WHBVRgNnp8fGNmqxI84F_9I)](https://drive.google.com/uc?export=view&id=1VTd6RScjNfAwi3vMk-bU87U4ucPmOO_M \"Click to watch contour assist demo\")\n\n***\n## Edit mode\nAllows to modify created objects with a brush tool.\n\n1. select an object (shape) to modify by clicking on it\n2. an editing layer (labels layer) is created for painting automatically\n3. modify the contour:\n    - edit with mouse drag or \n    - erase holding \"Alt\"\n4. finalize it\n    - accept with pressing \"q\" or\n    - delete with pressing \"Ctrl\" + \"Del\" or\n    - revert changes with pressing \"Esc\" (to the state before editing)\n\n- if the edited contour is a merge of multiple objects, you can erase the dividing line around the object you wish to keep, and keep erasing (or splitting with the eraser) until the object you wish to keep is the largest, then press \"q\" to accept it\n\nClick to watch demo video below\n\n[![edit-mode-demo](https://drive.google.com/uc?export=view&id=1M-XdEWPXMsIOtO0ncyUtvGACS0SRX-3K)](https://drive.google.com/uc?export=view&id=10MQm53hblLKQlfBNrfUsi1vxvIdTbzCZ \"Click to watch edit mode demo\")\n\n***\n## Class mode\nAllows to assign class labels to objects by clicking on shapes.\n\n1. select a class from the class list to assign\n2. click on an object (shape) to assign the selected class label to it\n3. the contour colour of the clicked object will be updated to the selected class colour, plus the class label is updated in the text properties of object (turn on \"display text\" on the layer control panel to see the text properties as `objectID:(classLabel)` e.g. 1:(0) for the first object)\n\n- optionally, you can set a default class for all currently unlabelled objects on the ROI (shapes) layer by selecting a class from the drop-down menu on the right to the text label \"Default class\"\n- class colours can be changed with the drop-down menu right to the class list; upon selection, all objects whose class label is the currently selected class will have their contour colour updated to the selected colour\n- clicking on an object that has already been assigned a class label will unclassify it: assign the label *0* to it\n\nClick to watch demo video below\n\n[![class-mode-demo](https://drive.google.com/uc?export=view&id=1EV1cn_mySO11S_ZDFv6Dl1laAk30jGJk)](https://drive.google.com/uc?export=view&id=1uOmznUvfHEFvviWTtOnUHty8rkKyWR7Q \"Click to watch class mode demo\")\n\n***\n## Export\nSee also: [Quick export](#quick-export)\n\nThe exporter plugin AnnotatorJExport can be invoked from the Plugins menu under the plugin name `napari-annotatorj`. It is used for batch export of annotations to various formats directly suitable to train different types of deep learning models. See a [demonstrative figure](https://raw.githubusercontent.com/spreka/annotatorj/master/demos/annotation_and_export_types.png) in the [AnnotatorJ repository](https://github.com/spreka/annotatorj) and further description in its [README](https://github.com/spreka/annotatorj#export) or [documentation](https://github.com/spreka/annotatorj/blob/master/AnnotatorJ_documentation.pdf).\n\n1. browse original image folder with either the\n    - \"Browse original...\" button or\n    - text input field next to it\n2. browse annotation folder with either the\n    - \"Browse annot...\" button or\n    - text input field next to it\n3. select the export options you wish to export the annotations to (see tooltips on hover for help)\n    - at least one export option must be selected to start export\n    - (optional) right click on the checkbox \"Coordinates\" to switch between the default COCO format and YOLO format; see [explanation](#coordinate-formats)\n4. click on \"Export masks\" to start the export\n    - this will open a progress bar in the napari window and close it upon finish\n\nThe folder structure required by the exporter is as follows:\n\n```\nimage_folder\n\t|--- image1.png\n\t|--- another_image.png\n\t|--- something.png\n\t|--- ...\n\nannotation_folder\n\t|--- image1_ROIs.zip\n\t|--- another_image_ROIs.zip\n\t|--- something_ROIs.zip\n\t|--- ...\n```\n\nMultiple export options can be selected at once, any selected will create a subfolder in the folder where the annotations are saved.\n\n\nClick to watch demo video below\n\n[![exporter-demo](https://drive.google.com/uc?export=view&id=1QoaJrI9pKziUzYwiZNdWlfRD7PcvJB9U)](https://drive.google.com/uc?export=view&id=1uJz-x_ypEOjc7SYPUTqrEt0ieyNLFy6u \"Click to watch exporter demo\")\n\n***\n## Quick export\nClick on the \"[^]\" button to quickly save annotations and export to mask image. It saves the current annotations (shapes) to an ImageJ-compatible roi.zip file and a generated a 16-bit multi-labelled mask image to the subfolder \"masks\" under the current original image's folder.\n\n\n***\n## Coordinate formats\nIn the AnnotatorJExport plugin 2 coordinates formats can be selecting by right clicking on the Coordinates checkbox: COCO or YOLO. The default is COCO.\n\n*COCO format*:\n- `[x, y, width, height]` based on the top-left corner of the bounding box around the object\n- coordinates are not normalized\n- annotations are saved with header to \n    - .csv file\n    - tab delimeted\n\n*YOLO format*:\n- `[class, x, y, width, height]` based on the center point of the bounding box around the object\n- coordinates are normalized to the image size as floating point values between 0 and 1\n- annotations are saved with header to\n    - .txt file\n    - whitespace delimeted\n    - class is saved as the 1st column\n\n***\n## Overlay\nA separate annotation file can be loaded as overlay for convenience, e.g. to compare annotations.\n\n1. load another annotation file with the \"Overlay\" button\n\n- (optional) switch its visibility with the \"Show overlay\" checkbox\n- (optional) change the contour colour of the overlay shapes with the [\"Colours\" button](#change-colours)\n\n***\n## Change colours\nClicking on the \"Colours\" button opens the Colours widget where you can set the annotation and overlay colours.\n\n1. select a colour from the drop-down list either next to the text label \"overlay\" or \"annotation\"\n2. click the \"Ok\" button to apply changes\n\n- contour colour of shapes on the annotation shapes layer (named \"ROI\") that already have a class label assigned to them will **not** be updated to the new annotation colour, only those not having a class label (the class label can be displayed with the \"display text\" checkbox on the layer controls panel as `objectID:(classLabel)` e.g. 1:(0) for the first object)\n- contour colour of shapes on the overlay shapes layer (named \"overlay\") will all have the overlay colour set, regardless of any existing class information saved to the annotation file loaded as overlay\n\n***\n## Configure model folder\nThe Contour assist mode imports a pre-trained Keras model from a folder named *models* under exactly the path *napari_annotatorj*. This is automatically created on the first startup in your user folder:\n- `C:\\Users\\Username\\.napari_annotatorj` on Windows\n- `\\home\\username\\.napari_annotatorj` on Linux\n\nA pre-trained model for nucleus segmentation is automatically downloaded from the GitHub repository of the [ImageJ version of AnnotatorJ](https://github.com/spreka/annotatorj/releases/tag/v0.0.2-model). The model will be saved to `[your user folder]\\.napari_annotatorj\\models\\model_real.h5`. This location is printed to the console (command prompt or powershell on Windows, terminal on Linux).\n\n(deprecated) When bulding from source the model folder is located at *path\\to\\napari-annotatorj\\src\\napari_annotatorj\\models* whereas installing from pypi it is located at *path\\to\\virtualenv\\Lib\\site-packages\\napari_annotatorj\\models*.\n\nThe model must be in either of these file formats:\n- config .json file + weights file: *model_real.json* and *model_real_weights.h5*\n- combined weights file: *model_real.hdf5*\n\nYou can also train a new model on your own data in e.g. Python and save it with this code block:\n\n```python\n\t# save model as json\n\tmodel_json=model.to_json()\n\twith open(‚Äòmodel_real.json‚Äô, ‚Äòw‚Äô) as f:\n\t\tf.write(model_json)\n\t\n\t# save weights too\n\tmodel.save_weights(‚Äòmodel_real_weights.h5‚Äô)\n\n```\nYou can also train in the [train widget](#Training).\n\nThis configuration will change in the next release to allow model browse and custom model name in an [options widget](#options).\n\n***\n## Training\nTo start training a new model or refine an existing one click the **Train** button on the right of the napari-annotatorj widget. This will open the training widget where you can set input paths and training options. During training a progress bar will show the epochs passed and plot the loss on a graph. See a [guide](#how-to-train) below.\n\nThe trained model will be saved to the `model` folder under the located training data folder which is named `training` by default when preparing data. Each new training will be saved to a new training folder with increased numbering e.g. `training_1`, `training_2` etc.\n\nWhen an existing training data folder is browsed with the \"Browse train ...\" button, the `model` folder will be created under it without an additiona `training` folder.\n\nAfter training is finished, a message is shown to indicate the newly trained model can be tested by drawing bounding boxes (rectangles) to initiate [contour assist](#contour-assist-mode) prediction. The presented region on the editing layer (Label layer) can be modified with the paint brush tool (automatically selected) as in [contour assist](#contour-assist-mode).\n\nThe trained model can be further refined by selecting the \"Retrain latest\" checkbox from the [training parameters](#training-parameters) (‚öô button on the right).\n\nTo use this new model for annotation in [contour assist mode](#contour-assist-mode), you mush set the model path in the [Options widget](#options) or in the configuration file (see how to [here](#configure-model-folder)), then restart napari.\n\n### How to train\n1. On current annotation\n\t1. \"Use current annotation\" checkbox --> use this image and its current annotation for training\n\t2. Prep data --> prepare data to [suitable format](#training-data-format)\n\t3. (optional) ‚öô --> [set parameters](#training-parameters)\n\t4. Start --> start training\n2. Additional data\n\t1. Select images and annotations to use for training\n\t\t- Browse original ... --> locate folder of original images\n\t\t- Browse annot ... --> locate folder of annotations\n\t\t- Prep data --> prepare data to [suitable format](#training-data-format)\n\t2. Browse train ... --> select already prepared training data\n\t3. (optional) ‚öô --> [set parameters](#training-parameters)\n\t4. Start --> start training\n\n### Training data format\nThe data format expected by the training widget is as follows.\n\n```\nimages\n\t|--- image1.png\n\t|--- another_image.png\n\t|--- something.png\n\t|--- ...\n\nunet_masks\n\t|--- image1.tiff\n\t|--- another_image.tiff\n\t|--- something.tiff\n\t|--- ...\n```\n\nMasks are 8-bit binary (black and white) .tiff images that can be exported from the [Exporter widget](#export) selecting the Semantic (binary) export option. When the \"Prep data\" button is clicked in the Training widget, these folders are automatically created from the located annotation files and original images.\n\n### Training parameters\nThe following configurable parameters can be set after clicking on the ‚öô icon:\n| Parameter | Description | Default value |\n| --------- | ----------- | ------------- |\n| Epochs | number of epochs to train | 5 |\n| Steps | number of steps in each epoch | 1 |\n| Batch size | number of samples in an iteration| 1 |\n| Image size | size of training images | 256 |\n| Start from scratch | train a new model from scratch| `False` |\n| Retrain latest | re-fine latest training | `False` |\n| Write pred | write test image prediction to file| `False` |\n| Test image | path to test image | `None`|\n\nNote: by default CPU will be used for training. This can be changed to GPU in the [Options widget](#options) if your computer has a capable CUDA-device.\n\n***\n## Options\nSettings found in the configuration file can be set in the Options widget opened with the \"...\" button on the right of the main plugin. For changes to take effect you must save your changes with the \"Ok\" button at the bottom of the Options widget.\n\nThe following options can be configured:\n|Group|Option|Description|Default|Valid values|\n|-----|------|-----------|-------|------------|\n|General|Annotation type | see [instance](#instance-annotation), [bbox](#bounding-box-annotation), [semantic](#semantic-annotation) | instance |instance, bbox, semantic |\n| |Remember annotation type|use the same annotation type on next startup|`True`|`True`, `False`|\n| |Colours|select annotation and overlay colours; see [here](#change-colours)|white|white, red, green, blue, cyan, magenta, yellow, orange, black|\n| |Classes|names of folders to save annotations when not classified*|normal|(any string)**|\n|Semantic segmentation|Brush size|size of the brush|50|`int`|\n|*Advanced settings*|\n|Contour assist|Max distance|number of pixels to extend the initial contour with|17|`int`|\n||Threshold|intensity threshold after prediction|||\n||- gray||0.1|`float` in [0,1]|\n||- R (red)||0.2|`float` in [0,1]|\n||- G (green)||0.4|`float` in [0,1]|\n||- B (blue)||0.2|`float` in [0,1]|\n||Brush size|correction brush size|5|`int`|\n||Method|contour assist prediction method|U-Net|U-Net, Classic***|\n||Model|U-Net model to use for Contour assist prediction|||\n||folder|path to the model folder|`user/.napari_annotatorj/models`|existing `models` folder path|\n||.json file|name of the model .json file **without** extension|model_real|any string**|\n||weights file|name of the model weights file|model_real_weights.h5|any string**|\n||full file|name of the combined config+weights file|model_real.hdf5|any string**|\n||Device|computation device to perform prediction|cpu|cpu,`int`****|\n|Mask/text import|\n||Auto mask load|load annotation files automatically when a new image is opened|`False`|`True`, `False`|\n||Enable mask load|load instance annotation mask image|`False`|`True`, `False`|\n||Enable text load|load object detection bounding box coordinate text file|`False`|`True`, `False`|\n||Method|load as editable or overlay|load|load, overlay|\n|Others|\n||Save outlines|save image with annotations outlined|`False`|`True`, `False`|\n||Show help on startup|show the help window upon every startup|`False`|`True`, `False`|\n||Save annot times|save annotation times to text file*****|`False`|`True`, `False`|\n\n*: right click the last element (other...) to add a new item to the list. When annotations are assigned class labels in [class mode](#class-mode), they will be saved to the folder `masks` by default.\n\n**: do not use whitespace (' ') if possible\n\n***: region growing classical algorithm\n\n****: valid id of a GPU device e.g. `0` or `3`; if your computer has only one GPU the id is `0`\n\n*****: currently disabled, used for development\n\n***\n## Demo\nRun a demo of napari-annotatorj with sample data: a small 3-channel RGB image as original image and an ImageJ roi.zip file as annotations loaded.\n\n```shell\n    # from the napari-annotatorj folder\n\tpython src/napari_annotatorj/load_imagej_roi.py\n```\nAlternatively, you can startup the napari-annotatorj plugin by running\n\n```shell\n    # from the napari-annotatorj folder\n\tpython src/napari_annotatorj/startup_annotatorj.py\n```\n\n***\n## Shortcuts\n\n| Function | Shortcut |\n| -------- | -------- |\n| Contour assist | `a` |\n| Class mode | `c` |\n| Edit mode | `Shift` + `e` |\n| Show contours | `Shift` + `v` |\n| Accept Contour assist | `q` |\n| Reject Contour assist | `Ctrl` + `del` |\n| Invert Contour assist | `u` |\n| Erase in Edit/Contour assist mode | `Alt` (hold) |\n| Revert changes in Edit mode | `Esc` |\n\n\n***\n## Setting device for deep learning model prediction\nThe [Contour assist](#contour-assist-mode) mode uses a pre-trained U-Net model for suggesting contours based on a lazily initialized contour drawn by the user. The default configuration loads and runs the model on the CPU so all users can run it. It is possible to switch to GPU if you have:\n- a CUDA-capable GPU in your computer\n- nVidia's CUDA toolkit + cuDNN installed\n\nSee installation guide on [nVidia's website](https://developer.nvidia.com/cuda-downloads) according to your system.\n\nTo switch to GPU utilization, edit [_dock_widget.py](https://github.com/spreka/napari-annotatorj/blob/main/src/napari_annotatorj/_dock_widget.py#L112) and set to the device you would like to use. Valid values are `'cpu','0','1','2',...`. The default value is `cpu`. The default GPU device is `0` if your system has any CUDA-capable GPU. If the device you set cannot be found or utilized by the code, it will fall back to `cpu`. An informative message is printed to the console upon plugin startup.\n\n***\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-annotatorj\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/spreka/napari-annotatorj/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "<EDIT_ME>"
    ],
    "contributions_writers_filename_extensions": [
      ".tiff"
    ],
    "contributions_widgets": [
      "AnnotatorJ",
      "AnnotatorJExport"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-roi-registration",
    "name": "napari-roi-registration",
    "display_name": "Roi Registration",
    "version": "0.1.4",
    "created_at": "2022-05-04",
    "modified_at": "2023-12-13",
    "authors": [
      "Andrea Bassi and Giorgia Tortora"
    ],
    "author_emails": [
      "giorgia.tortora@polimi.it"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-roi-registration/",
    "home_github": "https://github.com/GiorgiaTortora/napari-roi-registration",
    "home_other": null,
    "summary": "A plugin to perform registration of regions-of-interests in time-lapse data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "opencv-python",
      "matplotlib",
      "openpyxl",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "scikit-image ; extra == 'testing'",
      "opencv-python-headless ; extra == 'testing'",
      "matplotlib ; extra == 'testing'",
      "openpyxl ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-roi-registration\n\n[![License](https://img.shields.io/pypi/l/napari-roi-registration.svg?color=green)](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-roi-registration.svg?color=green)](https://pypi.org/project/napari-roi-registration)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-roi-registration.svg?color=green)](https://python.org)\n[![tests](https://github.com/GiorgiaTortora/napari-roi-registration/workflows/tests/badge.svg)](https://github.com/GiorgiaTortora/napari-roi-registration/actions)\n[![codecov](https://codecov.io/gh/GiorgiaTortora/napari-roi-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/GiorgiaTortora/napari-roi-registration)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi-registration)](https://napari-hub.org/plugins/napari-roi-registration)\n\nA Napari plugin for the registration of regions of interests (ROI) in a time lapse acquistion and processing of the intensity of the registered data.\n\nThe ROI are defined using a Labels layer. Registration of multiple ROIs is supported.  \n\nThe `Registration` widget uses the user-defined labels, constructs a rectangular ROI around each of them and registers the ROIs in each time frame.\n\nThe `Processing` widget measures the ROI displacements and extracts the average intensity of the ROI, calculated on the area of the labels.\n\nThe `Subtract background` widget subtracts a background on each frame, calculated as the mean intensity on a Labels layer.\nTipically useful when ambient light affects the measurement.  \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/roi_registration.gif)\n\n## Installation\n\nYou can install `napari-roi-registration` via [pip]:\n\n    pip install napari-roi-registration\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/GiorgiaTortora/napari-roi-registration.git\n\n## Usage\n\nA detailed guide which shows how to use the widgets of the napari-roi-registration plugin and how to properly choose the parameters can be found [here]. A demo video is available at this [link](https://www.youtube.com/watch?v=oXyAqZdFrSE). [Sample datasets](https://polimi365-my.sharepoint.com/:f:/g/personal/10853110_polimi_it/ErHvu3QXhktGq-NLqFdZXMYBWXaRNIZWlQhWg5EdOgbmWg?e=HeExQl) are available.\n\n### Registration Widget\n\n1. Create a new Labels layer and draw one or more labels where you want to select a ROI (Region Of Interest). Each color in the same Labels layer represents a different label which will correspond to a different ROI.\n\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture1.png)\n\n2. Push the `Register ROIs` button: registration of the entire stack will be performed. When the registration is finished two new layers will appear in the viewer. One layer contains the centroids of the drawn labels while the other contains the bounding boxes enclosing the ROIs.\nThe registration starts from the currently selected frame. If `register entire stack` is selected, the registration will create a new layer for each label, with the registered ROI stacks.\n\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture2.png)\n\n### Processing Widget\n\nPushing the `Process registered ROIs` button, the registered ROIs will be analyzed. The intensity of the registered ROIs (measured on the area of the selected label) and the displacement of the ROIs will be calculated.\nIf `plot results` is selected the plot of displacement vs time index and mean intensity vs time index will appear in the Console.\nChoosing the `save results` option, an excel file containing ROIs positions, displacements and intensities, will be saved. \n\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture3.png)\n\n### Background Widget\n\n1. Create a new Labels layer and draw a label on the area where you want to calculate the background. \n\n![raw](https://github.com/GiorgiaTortora/napari-roi-registration/raw/main/images/Picture4.png)\n\n2. Push the `Subtract background` button. A new image layer will appear in the viewer. This layer contains the image to which the background was subtracted.\n\n## Contributing \n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-roi-registration\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[here]: https://github.com/GiorgiaTortora/napari-roi-registration/blob/main/docs/index.md\n\n[file an issue]: https://github.com/GiorgiaTortora/napari-roi-registration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Background Widget",
      "Registration Widget",
      "Processing Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sketchpose",
    "name": "napari-sketchpose",
    "display_name": "Sketchpose",
    "version": "0.1.8",
    "created_at": "2023-11-07",
    "modified_at": "2023-12-08",
    "authors": [
      "Cl√©ment Cazorla"
    ],
    "author_emails": [
      "clement.cazorla31@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-sketchpose/",
    "home_github": null,
    "home_other": null,
    "summary": "A segmentation plugin to adapt Omnipose implementation to partial labelling.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "cellpose-omni ==0.9.1",
      "omnipose ==0.4.4",
      "pyqtgraph ==0.13.3",
      "matplotlib",
      "light-the-torch",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-sketchpose\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-sketchpose.svg?color=green)](https://github.com/koopa31/napari-sketchpose/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sketchpose.svg?color=green)](https://pypi.org/project/napari-sketchpose)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sketchpose.svg?color=green)](https://python.org)\n[![tests](https://github.com/koopa31/napari-sketchpose/workflows/tests/badge.svg)](https://github.com/koopa31/napari-sketchpose/actions)\n[![codecov](https://codecov.io/gh/koopa31/napari-sketchpose/branch/main/graph/badge.svg)](https://codecov.io/gh/koopa31/napari-sketchpose)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sketchpose)](https://napari-hub.org/plugins/napari-sketchpose)\n\nA plugin to adapt the Omnipose implementation to frugal labeling. It aims to facilitate the training from scratch or the \nuse of transfer learning with little data, by not needing to draw entire cells, but a few squiggles instead (see GIF below).\n\n\nIf you use this plugin please cite the [paper](https://hal.science/hal-04330824): \n\nCl√©ment Cazorla, Nathana√´l Munier, Renaud Morin, Pierre Weiss. Sketchpose: Learning to Segment\nCells with Partial Annotations. 2023. ffhal-04330824f\n\n```bibtex\n@unpublished{cazorla:hal-04330824,\n      TITLE = {{Sketchpose: Learning to Segment Cells with Partial Annotations}},\n      AUTHOR = {Cazorla, Cl{\\'e}ment and Munier, Nathana{\\\"e}l and Morin, Renaud and Weiss, Pierre},\n      URL = {https://hal.science/hal-04330824},\n      NOTE = {working paper or preprint},\n      YEAR = {2023},\n      MONTH = Dec,\n      KEYWORDS = {Cellpose -Segmentation -Frugal learning -Napari -Deep learning -Distance map},\n      PDF = {https://hal.science/hal-04330824/file/sketchpose_hal.pdf},\n      HAL_ID = {hal-04330824},\n      HAL_VERSION = {v1},\n    }\n\n```\n\n\n![](https://bitbucket.org/koopa31/napari-sketchpose/raw/b691817e9e20a3c1c2bc69277579f6fb9b26354e/images/frugalpose.gif)\nImage Credit: Eduard Muzhevskyi\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\n\n\nFirst, we advise you to create a conda environment in Python 3.10, in which you will run Napari:\n\n    conda create -n sketchpose_env python=3.10\n    conda activate sketchpose_env\n    conda install pip\n    python -m pip install \"napari[all]\" --upgrade\n\nYou can install `napari_sketchpose` via [pip]:\n\n    pip install napari_sketchpose\n\nWARNING:\n\nFor Windows users, CUDA version of PyTorch may not be installed properly. When the plugin starts for the first time, it checks whether\nCUDA version is installed. If not, it tries to install it using light-the-torch library. If this does not work, you should re-install \nCUDA torch and torchvision versions manually, otherwise the plugin will not work properly.\n\n## Tutorial\n\nWe strongly recommend reading the [documentation] to get the most out of the plugin.\nA step-by-step tutorial illustrated with GIFs will guide you through the various stages.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-sketchpose\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[documentation]: https://sketchpose-doc.readthedocs.io/en/latest/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Sketchpose"
    ],
    "contributions_sample_data": [
      "Sketchpose"
    ]
  },
  {
    "normalized_name": "tracking-challenge-demo",
    "name": "tracking-challenge-demo",
    "display_name": "Tracking Challenge Solver",
    "version": "0.0.8",
    "created_at": "2022-05-03",
    "modified_at": "2023-11-29",
    "authors": [
      "Draga Doncila"
    ],
    "author_emails": [
      "ddoncila@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/tracking-challenge-demo/",
    "home_github": "https://github.com/DragaDoncila/tracking-challenge-demo",
    "home_other": null,
    "summary": "A demo plugin to load, segment and save tracking challenge data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "dask[array]",
      "imagecodecs",
      "napari",
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "scikit-image",
      "tifffile",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# tracking-challenge-demo\n\n[![License](https://img.shields.io/pypi/l/tracking-challenge-demo.svg?color=green)](https://github.com/DragaDoncila/tracking-challenge-demo/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/tracking-challenge-demo.svg?color=green)](https://pypi.org/project/tracking-challenge-demo)\n[![Python Version](https://img.shields.io/pypi/pyversions/tracking-challenge-demo.svg?color=green)](https://python.org)\n[![tests](https://github.com/DragaDoncila/tracking-challenge-demo/workflows/tests/badge.svg)](https://github.com/DragaDoncila/tracking-challenge-demo/actions)\n[![codecov](https://codecov.io/gh/DragaDoncila/tracking-challenge-demo/branch/main/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/tracking-challenge-demo)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/tracking-challenge-demo)](https://napari-hub.org/plugins/tracking-challenge-demo)\n\nA demo plugin to load, segment and save tracking challenge data.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `tracking-challenge-demo` via [pip]:\n\n    pip install tracking-challenge-demo\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/DragaDoncila/tracking-challenge-demo.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"tracking-challenge-demo\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/DragaDoncila/tracking-challenge-demo/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".zip"
    ],
    "contributions_widgets": [
      "Example QWidget",
      "Segment by Threshold"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "darth-d",
    "name": "darth-d",
    "display_name": "darth-d",
    "version": "0.4.0",
    "created_at": "2023-10-26",
    "modified_at": "2023-11-21",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@uni-leipzig.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/darth-d/",
    "home_github": "https://github.com/haesleinhuepf/darth-d",
    "home_other": null,
    "summary": "A simple to use image generator based on OpenAIs DALL-E",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "openai >=1.2.0",
      "Pillow",
      "numpy",
      "stackview >=0.7.1"
    ],
    "package_metadata_description": "# Darth-D\n[![License](https://img.shields.io/pypi/l/darth-d.svg?color=green)](https://github.com/haesleinhuepf/darth-d/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/darth-d.svg?color=green)](https://pypi.org/project/darth-d)\n[![Python Version](https://img.shields.io/pypi/pyversions/darth-d.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/darth-d/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/darth-d/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/darth-d/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/darth-d)\n[![Development Status](https://img.shields.io/pypi/status/darth-d.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/darth-d)](https://napari-hub.org/plugins/darth-d)\n\nA simple to use image generator based on [OpenAIs DALL-E 2/3](https://openai.com/dall-e-2).\nIt comes as [napari](https://napari.org/) plugin and has a Python interface. \nYou need an [OpenAI API KEY](https://openai.com/blog/openai-api/) to use it.\n\nUsing some of the functions on scientific images could be seen as scientific misconduct. Handle these functions with care.\n\n![](https://github.com/haesleinhuepf/darth-d/raw/main/docs/images/replace_screencast.gif)\n\n## Usage\n\n### From Python\n\nYou can generate images from text prompts in Python like this ([see this notebool](https://github.com/haesleinhuepf/darth-d/blob/main/demo/demo_darth-d.ipynb)).\n\n```\nfrom darth_d import create\n```\n\n```\nimage = create(\"an image of a cat\")\n\nimage\n```\n\n![](https://github.com/haesleinhuepf/darth-d/raw/main/docs/images/jupyter_screenshot.png)\n\nYou can also vary images ([see this notebook](https://github.com/haesleinhuepf/darth-d/blob/main/demo/demo_vary.ipynb)):\n```\nfrom darth_d import vary\n\noutput_image = vary(input_image)\n```\n\n![](https://github.com/haesleinhuepf/darth-d/raw/main/docs/images/vary_screenshot.png)\n\nReplacing regions in images is also possible. Note: Using this function on scientific images could be seen as scientific misconduct. Handle this function with care.\n\n### In Napari\n\nTo generate images in Napari, click the `Tools > Generate > Image` menu. You can for example enter the prompt\n\"a professional comic with white background showing a cat having an idea. the idea is visualized using a light bulb.\n\n![](https://github.com/haesleinhuepf/darth-d/raw/main/docs/images/napari_screenshot.png)\n\n\n## Installation\n\n```\npip install darth-d\n```\n\nIf you want to use it from napari, please also install napari and the [tools menu](https://github.com/haesleinhuepf/napari-tools-menu):\n\n```\nmamba install napari pyqt napari-tools-menu -c conda-forge\n```\n\n## Similar tools and plugins\n\n* https://github.com/kephale/napari-stable-diffusion\n* https://github.com/seankmartin/napari-stable-diffusion\n\n## Feedback welcome!\n\nThe `darth-d` is developed in the open because we believe in the open source community. Feel free to drop feedback as [github issue](https://github.com/haesleinhuepf/darth-d) or via [image.sc](https://image.sc)\n\n## Contributing\n\nContributions are very welcome. \n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"darth-d\" is free and open source software\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "create_gui",
      "vary_gui",
      "replace_gui"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-3d-ortho-viewer",
    "name": "napari-3d-ortho-viewer",
    "display_name": "Ortho Viewer Widget",
    "version": "0.1.5",
    "created_at": "2023-11-21",
    "modified_at": "2023-11-21",
    "authors": [
      "Niklas Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-3d-ortho-viewer/",
    "home_github": "https://github.com/gatoniel/napari-3d-ortho-viewer",
    "home_other": null,
    "summary": "Napari 3D Ortho Viewer - an ortho viewer for napari for 3D images",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-3d-ortho-viewer\n\n[![License](https://img.shields.io/pypi/l/napari-3d-ortho-viewer.svg?color=green)](https://github.com/gatoniel/napari-3d-ortho-viewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-3d-ortho-viewer.svg?color=green)](https://pypi.org/project/napari-3d-ortho-viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-3d-ortho-viewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-3d-ortho-viewer/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-3d-ortho-viewer/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-3d-ortho-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-3d-ortho-viewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-3d-ortho-viewer)](https://napari-hub.org/plugins/napari-3d-ortho-viewer)\n\nNapari 3D Ortho Viewer - an ortho viewer for napari for 3D images\n\n----------------------------------\n\nhttps://github.com/gatoniel/napari-3d-ortho-viewer/assets/40384506/4296dc11-ea37-40a0-8b17-eeb77480672f\n\nThis plugin is heavily inspired by [ortho-view-napari].\n\nCheck out this post on image.sc (https://forum.image.sc/t/napari-visualization-in-3-planes/57768) for more infos about multiview support in [napari].\n\nThis viewer has some additional features:\n- double click to jump to specific position in all slices\n- additional 3d view of 3d stack with lines or planes indicating current position\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-3d-ortho-viewer` via [pip]:\n\n    pip install napari-3d-ortho-viewer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-3d-ortho-viewer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-3d-ortho-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-3d-ortho-viewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[ortho-view-napari]: https://github.com/JoOkuma/ortho-view-napari\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Ortho Viewer",
      "Crop with labels",
      "Crop with label list"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-time-slicer",
    "name": "napari-time-slicer",
    "display_name": "napari-time-slicer",
    "version": "0.5.0",
    "created_at": "2021-11-12",
    "modified_at": "2023-11-12",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-time-slicer/",
    "home_github": "https://github.com/haesleinhuepf/napari-time-slicer",
    "home_other": null,
    "summary": "A meta plugin for processing timelapse data in napari timepoint by timepoint",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "toolz",
      "napari-tools-menu",
      "napari-workflows"
    ],
    "package_metadata_description": "# napari-time-slicer\n\n[![License](https://img.shields.io/pypi/l/napari-time-slicer.svg?color=green)](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-time-slicer.svg?color=green)](https://pypi.org/project/napari-time-slicer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-time-slicer.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-time-slicer/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-time-slicer/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-time-slicer/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-time-slicer)\n[![Development Status](https://img.shields.io/pypi/status/napari-time-slicer.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-time-slicer)](https://napari-hub.org/plugins/napari-time-slicer)\n\nA meta plugin for processing timelapse data timepoint by timepoint. It \nenables a list of napari plugins to process 2D+t or 3D+t data step by step when the user goes \nthrough the timelapse. Currently, these plugins are using `napari-time-slicer`:\n* [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes)\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\n* [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\n* [napari-stress](https://www.napari-hub.org/plugins/napari-stress)\n* [napari-process-points-and-surfaces](https://www.napari-hub.org/plugins/napari-process-points-and-surfaces)\n\n`napari-time-slicer` enables inter-plugin communication, e.g. allowing to combine the plugins listed above in \none image processing workflow for segmenting a timelapse dataset:\n\n![](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/screencast1.gif)\n\nThe workflow can then also be exported as a script. The 'Generate Code' button can be found in the [Workflow Inspector](https://www.napari-hub.org/plugins/napari-workflow-inspector)\n\n\nIf you want to convert a 3D dataset into a 2D + time dataset, use the \nmenu `Tools > Utilities > Convert 3D stack to 2D timelapse (time-slicer)`. It will turn the 3D dataset to a 4D datset\nwhere the Z-dimension (index 1) has only 1 element, which will in napari be displayed with a time-slider. Note: It is \nrecommended to remove the original 3D dataset after this conversion.\n\n## Working with large on-the-fly processed datasets\n\nUsing the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) complex image processing workflows on timelapse datasets can be setup. \nIn combination with the time-slicer it is possible to process time-lapse data that is larger than available computer memory.\nIn case the workflow only consists of images and label-images and out-of-memory issues arise, consider storing intermediate results on disk following this procedure: \nAfter setting up the workflow and testing it on a couple of selected frames, store the entire processed timelapse dataset to disk \nusing the menu `Tools > Utilities > Convert to file-backed timelapse data (time-slicer)`. It will open this dialog, where you can select \n![img.png](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/convert_to_file_backed_timelapse.png)\n\nIt is recommended to enter a folder location in the text field. \nIf not provided, a temporary folder will be created, typically in the User's temp folder in the home directory. \nThe user is responsible for emptying this folder from time to time.\nThe data stored in this folder can also be loaded into napari using its `File > Open Folder...` menu.\n\nExecuting this operation can take time as every timepoint of the timelapse is computed. \nAfterwards, there will be another layer available in napari, which is typically faster to navigate through. \nConsider removing the layer(s) that were only needed to determine the new file-backed layer.\n\n![img.png](https://github.com/haesleinhuepf/napari-time-slicer/raw/main/images/new_file_backed_layer.png)\n\n## Usage for plugin developers\n\nPlugins which implement the `napari_experimental_provide_function` hook can make use of the `@time_slicer`. At the moment,\nonly functions which take `napari.types.ImageData`, `napari.types.LabelsData` and basic python types such as `int` \nand `float` are supported. If you annotate such a function with `@time_slicer` it will internally convert any 4D dataset\nto a 3D dataset according to the timepoint currently selected in napari. Furthermore, when the napari user changes the\ncurrent timepoint or the input data of the function changes, a re-computation is invoked. Thus, it is recommended to \nonly use the `time_slicer` for functions which can provide [almost] real-time performance. Another constraint is that \nthese annotated functions have to have a `viewer` parameter. This is necessary to read the current timepoint from the \nviewer when invoking the re-computions.\n\nExample\n```python\nimport napari\nfrom napari_time_slicer import time_slicer\n\n@time_slicer\ndef threshold_otsu(image:napari.types.ImageData, viewer: napari.Viewer = None) -> napari.types.LabelsData:\n    # ...\n```\n\nYou can see a full implementations of this concept in the napari plugins listed above.\n\nIf you want to combine slicing in time and processing z-stack images slice-by-slice, you can use the `@slice_by_slice` annotation.\nMake sure, to insert it after `@time_slicer` as shown below and implemented in [napari-pillow-image-processing](https://github.com/haesleinhuepf/napari-pillow-image-processing/blob/4d846b226739843124953f16059241d917cde8e1/src/napari_pillow_image_processing/__init__.py#L151)\n\n```python\nfrom napari_time_slicer import slice_by_slice\n\n@time_slicer\n@slice_by_slice\ndef blur_2d(image:napari.types.ImageData, sigma:float = 1, viewer: napari.Viewer = None) -> napari.types.LabelsData:\n    # ...\n```\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-time-slicer` via [pip]:\n\n    pip install napari-time-slicer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/haesleinhuepf/napari-time-slicer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-time-slicer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-time-slicer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "convert_to_2d_timelapse"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-trackpy",
    "name": "napari-trackpy",
    "display_name": "Particle tracking",
    "version": "0.3.0",
    "created_at": "2023-11-07",
    "modified_at": "2023-11-09",
    "authors": [
      "Roy Hoitink"
    ],
    "author_emails": [
      "L.D.Hoitink@uu.nl"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-trackpy/",
    "home_github": "https://github.com/rhoitink/napari-trackpy",
    "home_other": null,
    "summary": "Plugin to do trackpy particle tracking on microscopy data within napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "napari-aicsimageio",
      "readlif",
      "trackpy",
      "matplotlib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-trackpy\n\n[![License MIT](https://img.shields.io/pypi/l/napari-trackpy.svg?color=green)](https://github.com/rhoitink/napari-trackpy/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-trackpy.svg?color=green)](https://pypi.org/project/napari-trackpy)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-trackpy.svg?color=green)](https://python.org)\n[![tests](https://github.com/rhoitink/napari-trackpy/workflows/tests/badge.svg)](https://github.com/rhoitink/napari-trackpy/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-trackpy)](https://napari-hub.org/plugins/napari-trackpy)\n\nPlugin to do [trackpy] particle tracking on 3D microscopy data within [napari]. Currently only tracking of XYZ data is implemented.\n\n## Installation\n\nYou can install `napari-trackpy` via [pip]:\n\n    pip install napari-trackpy\n\nTo install latest development version :\n\n    pip install git+https://github.com/rhoitink/napari-trackpy.git\n\n## How to use this plugin?\n* Load your XYZ data (using [napari-aicsimageio])\n* Make sure to split channels into different layers, such that the layer only contains 3D (XYZ) data\n* Open the widget for the tracking plugin via `Plugins` > `XYZ particle tracking`\n* Optimize the tracking settings for your dataset, for an extensive description of the settings, visit [this tutorial](http://soft-matter.github.io/trackpy/dev/tutorial/tracking-3d.html)\n* Save your tracking data into the `.xyz` file format using `Ctrl`+`S` (on the points layer) or via the menu `File` > `Save Selected Layer(s)...`\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-trackpy\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[trackpy]: https://github.com/soft-matter/trackpy\n[napari-aicsimageio]: https://github.com/AllenCellModeling/napari-aicsimageio\n[MIT]: http://opensource.org/licenses/MIT\n\n[file an issue]: https://github.com/rhoitink/napari-trackpy/issues\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".xyz"
    ],
    "contributions_widgets": [
      "XYZ particle tracking"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-apr-viewer",
    "name": "napari-apr-viewer",
    "display_name": "napari-apr-viewer",
    "version": "1.0.1",
    "created_at": "2021-11-30",
    "modified_at": "2023-11-08",
    "authors": [
      "Joel Jonsson"
    ],
    "author_emails": [
      "jonsson@mpi-cbg.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-apr-viewer/",
    "home_github": null,
    "home_other": "None",
    "summary": "A simple plugin to view APR images in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pyapr >=1.0.0rc1",
      "napari",
      "napari-plugin-engine >=0.1.4",
      "qtpy",
      "magicgui"
    ],
    "package_metadata_description": "# napari-apr-viewer\n\n[![License](https://img.shields.io/pypi/l/napari-apr-viewer.svg?color=green)](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-apr-viewer.svg?color=green)](https://pypi.org/project/napari-apr-viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-apr-viewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/AdaptiveParticles/napari-apr-viewer/workflows/tests/badge.svg)](https://github.com/AdaptiveParticles/napari-apr-viewer/actions)\n[![codecov](https://codecov.io/gh/AdaptiveParticles/napari-apr-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/AdaptiveParticles/napari-apr-viewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-apr-viewer)](https://napari-hub.org/plugins/napari-apr-viewer)\n\nA simple plugin to create and view APR images in napari\n\n## Usage\n\nTo get started, open an image of your choice (2D or 3D grayscale) in napari and open the `convert_image_to_apr` panel. Select the image layer to convert, an appropriate data type, and hit `Run`. \n\n**Note:** choosing a data type smaller than the input type may lead to overflow and thus erroneous results.\n\nConversion parameters can often be left to their default values, thanks to the automatic parameter tuning. For very noisy images, it is sometimes useful to increase the `smoothing` parameter. In order to get a more (or less) aggressive adaptation, change the `relative error` parameter.\n\n![conversion.png](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/docs/conversion.png)\n\nTo save the result to file, simply save the newly created layer using the `File` menu. We use the extension `.apr`, although the file is actually written in `hdf5` format (and can be opened/explored as such). In this example, the APR is roughly 80 times smaller than the original image on disk. APR files can be opened directly in napari, e.g. by drag and drop.\n\n![apr_file.png](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/docs/apr_file.png)\n\nTo better understand the workings of the APR on your data, you can use the `APR Viewer` panel to change the `View mode` for a selected APR layer to `level`. This shows you a visualization of the adaptive resolution. Particles in the brightest regions correspond exactly to pixels (lossless), while each shade darker corresponds to downsampling by a factor of 2 in each dimension.\n\n![view_level.png](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/docs/view_level.png)\n\nThe `Downsample` slider can be used to reduce the resolution of the displayed data for the selected layer. This can be used to explore large volumes in 3D, where rendering the full data requires too much memory. \n\n**Note:** We do not offer APR-native rendering at this time, so this step will reconstruct the entire pixel volume (at the selected resolution). Thus, for large volumes, be sure to increase the downsampling before toggling the 3D viewer. \n\n![view_3D.png](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/docs/view_3D.png)\n\n![view_3D_ds.png](https://github.com/AdaptiveParticles/napari-apr-viewer/raw/main/docs/view_3D_downsampled.png)\n\n_The data shown in these examples was taken from the Platynereis-ISH-Nuclei-CBG dataset available [here](https://github.com/juglab/EmbedSeg/releases)._\n\n&nbsp;\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-apr-viewer` via [pip]:\n\n    pip install napari-apr-viewer\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-apr-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[file an issue]: https://github.com/AdaptiveParticles/napari-apr-viewer/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "APRViewer",
      "convert_image_to_apr",
      "convert_apr_to_image",
      "threshold"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-correct-drift",
    "name": "napari-correct-drift",
    "display_name": "Napari Correct Drift",
    "version": "0.4.0",
    "created_at": "2023-04-26",
    "modified_at": "2023-11-06",
    "authors": [
      "Christoph Sommer"
    ],
    "author_emails": [
      "christoph.sommer@ist.ac.at"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-correct-drift/",
    "home_github": "https://github.com/sommerc/napari-correct-drift",
    "home_other": null,
    "summary": "Drift correction 2D/3D for Napari similar to Fijis Correct 3D drift macro",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "qtpy",
      "pandas",
      "scikit-image",
      "scipy",
      "dask",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-correct-drift\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-correct-drift.svg?color=green)](https://github.com/sommerc/napari-correct-drift/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-correct-drift.svg?color=green)](https://pypi.org/project/napari-correct-drift)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-correct-drift.svg?color=green)](https://python.org)\n[![tests](https://github.com/sommerc/napari-correct-drift/workflows/tests/badge.svg)](https://github.com/sommerc/napari-correct-drift/actions)\n[![codecov](https://codecov.io/gh/sommerc/napari-correct-drift/branch/main/graph/badge.svg)](https://codecov.io/gh/sommerc/napari-correct-drift)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-correct-drift)](https://napari-hub.org/plugins/napari-correct-drift)\n[![Documentation Status](https://readthedocs.org/projects/napari-correct-drift/badge/?version=latest)](https://napari-correct-drift.readthedocs.io/en/latest/?badge=latest)\n\nNapari-correct-drift brings the functionality of Fiji‚Äôs popular Correct-3D-drift macro to Napari for flexible and efficient correction of stage and sample drift common in time-lapse microscopy.\n\nNapari-correct-drift supports drift correction for 2D/3D multi-channel data.\n\n----------------------------------\n## Example\n\nhttps://user-images.githubusercontent.com/895863/235100349-83379350-06a5-4fe7-9323-f3e5771cca2e.mp4\n\n\n## Test data\nNapari-correct-drift contains synthetic sample data. To test it on real data download an example Arabidopsis growing [root tip](https://seafile.ist.ac.at/f/b05362d4f358430c8c59/?dl=1) file.\n\n## Installation\n\nYou can install `napari-correct-drift` via [pip]:\n\n    pip install napari-correct-drift\n\nTo install latest development version :\n\n    pip install git+https://github.com/sommerc/napari-correct-drift.git\n\n## Roadmap\n\n- [x] Basic CorrectDrift interface\n- [x] Synthetic test data\n- [x] Unit tests\n- [x] 2D/3D multi-channel support\n- [x] ROI support (rectangles)\n- [x] Saving and loading of drift tables\n- [x] Outlier handling\n- [x] Sphinx documentation\n- [x] How-tos\n- [x] Tutorials and Guides\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-correct-drift\" is free and open source software\n\n### Acknowledgment\nThis project has been made possible in part by grant number NP2-0000000051 from the napari Plugin Foundations Grants (Cycle 2).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Napari Correct Drift"
    ],
    "contributions_sample_data": [
      "Synthetic Drift 2D",
      "Synthetic Drift 3D",
      "Synthetic Drift 2D + Channel"
    ]
  },
  {
    "normalized_name": "napari-accelerated-pixel-and-object-classification",
    "name": "napari-accelerated-pixel-and-object-classification",
    "display_name": "napari-accelerated-pixel-and-object-classification",
    "version": "0.14.1",
    "created_at": "2021-10-02",
    "modified_at": "2023-11-04",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-accelerated-pixel-and-object-classification/",
    "home_github": "https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification",
    "home_other": null,
    "summary": "Pixel and label classification using OpenCL-based Random Forest Classifiers",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "apoc >=0.12.0",
      "napari-tools-menu >=0.1.17",
      "napari-time-slicer",
      "superqt",
      "imageio !=2.22.1",
      "napari >=0.4.11",
      "napari-assistant >=0.4.7"
    ],
    "package_metadata_description": "# napari-accelerated-pixel-and-object-classification (APOC)\n\n[![License](https://img.shields.io/pypi/l/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://pypi.org/project/napari-accelerated-pixel-and-object-classification)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-accelerated-pixel-and-object-classification.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-accelerated-pixel-and-object-classification/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-accelerated-pixel-and-object-classification)\n[![Development Status](https://img.shields.io/pypi/status/napari-accelerated-pixel-and-object-classification.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-accelerated-pixel-and-object-classification)](https://napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\n[![DOI](https://zenodo.org/badge/412525441.svg)](https://zenodo.org/badge/latestdoi/412525441)\n \n[clesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) meets [scikit-learn](https://scikit-learn.org/stable/) to classify pixels and objects in images, on a [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) using [OpenCL](https://www.khronos.org/opencl/) in [napari].\n\n![](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/screencast.gif)\nThe processed example image was kindly acquired by Daniela Vorkel, Myers lab, MPI-CBG / CSBD ([Download full video](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_lund.mp4))\n\nFor using the accelerated pixel and object classifiers in python, check out [apoc](https://github.com/haesleinhuepf/apoc).\nTraining classifiers from pairs of image and label-mask folders is explained in \n[this notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/train_on_folders.ipynb).\nFor executing APOC's pixel and object classifiers in [Fiji](https://fiji.sc) using [clij2](https://clij.github.io) please read the documentation of the [corresponding Fiji plugin](https://github.com/clij/clijx-accelerated-pixel-and-object-classification). Table classifiers and object mergers are not compatible with Fiji yet.\n\n![](https://github.com/clij/clijx-accelerated-pixel-and-object-classification/raw/main/docs/screenshot.png)\n\n\n\n## Usage\n\n### Object and Semantic Segmentation\n\nStarting point is napari with at least one image layer and one labels layer (your annotation).\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_segmentation_starting_point.png)\n\nYou find Object and Semantic Segmentation in the `Tools > Segmentation / labeling`. When starting those, the following graphical user interface will show up.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_and_semantic_segmentation.png)\n\n1. Choose one or multiple images to train on. These images will be considered as multiple channels. Thus, they need to be spatially correlated. \n   Training from multiple images showing different scenes is not (yet) supported from the graphical user interface. Check out [this notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/demp_pixel_classifier_continue_training.ipynb) if you want to train from multiple image-annotation pairs.\n2. Select a file where the classifier should be saved. If the file exists already, it will be overwritten.\n3. Select the ground-truth annotation labels layer. \n4. Select which label corresponds to foreground (not available in Semantic Segmentation)\n5. Select the feature images that should be considered for segmentation. If segmentation appears pixelated, try increasing the selected sigma values and untick `Consider original image`.\n6. Tree depth and number of trees allow you to fine-tune how to deal with manifold regions of different characteristics. The higher these numbers, the longer segmentation will take. In case you use many images and many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n7. The estimation of memory consumption allows you to tune the configuration to your GPU-hardware. Also consider the GPU-hardware of others who want to use your classifier.\n8. Click on Run when you're done with configuring. If the segmentation doesn't fit after the first execution, consider fine-tuning the ground-truth annotation and try again.\n\nA successful segmentation can for example look like this:\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_segmentation_result.png)\n\nAfter your classifier has been trained successfully, click on the \"Application / Prediction\" tab. If you apply the classifier again, python code will be generated. \nYou can use this code for example to apply the same classifier to a folder of images. If you're new to this, check out [this notebook](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/image_processing/12_process_folders.ipynb).\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/code_generation.png)\n\nA pre-trained classifier can be [applied from scripts as shown in the example notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/demo_object_segmenter.ipynb) or from the `Tools > Segmentation / labeling > Object segmentation (apply pretrained, APOC)`.\n\n### Integration with the napari-assistant\n\nPre-trained models can also be assembled to workflows using the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant). You find APOC-operations in the categories `Filter`, `Label` and `Label Filters`:\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/assistant.png)\n\n### Semantic segmentation\n\nUsers can also generate semantic segmentation label images where the label identifier corresponds to a class the pixel has been allocated to. \nThe tool can be found in the menu `Tools > Segmentation / labeling > Semantic segmentation (APOC)`.\nIt works analogously like the Object Segmenter, just without the need to specify the class identifier that objects correspond to.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/semantic_segmentation.png)\n\n### Probability maps\n\nThe tool for generating probability maps (`Tools > Filtering > Probability Mapper (APOC)` menu) works analogously to the Object Segmenter as well. \nThe only difference is that the result image is not a label image but an intensity image where the intensity represents the probability (between 0 and 1)\nthat a pixel belongs to a given class. In this example: The raw image (grey) has been annotated with three classes: background (black, label 1), foreground (white, label 2) and edges (grey, label 3).\nThe probability mapper was configured to create probability image (shown in green) for edges (label 3):\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/probability_mapper.png)\n\n### Classifier statistics\n\nWhile training, you can also activate the `Show classifier statistics` checkbox. \nWhen doing so, it is recommended to increase the number of trees so that the measurements are more reliable, especially when selecting many features.\nThis will open a small table after training where you can see how large the share of decision trees are for each analysed feature image.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/classifier_statistics.png)\n\nIt is recommended to turn on/off the features that hold a very large share (green) or a very small share (magenta) of trees in the random forest. \nRetrain the classifier to see how the features influence the decision making.\n\nNote: Multiple of these parameters may be correlated. \nIf you select 11 feature images, which all allow to make the pixel classification similarly, but 10 of those are correlated, these 10 may appear with a share of about 0.05 while the 11th parameter has a share of 0.5. \nThus, study these values with care.\n\n### Merging objects\n\nAfter segmentation, you can merge labeled objects using the `Tools > Segmentation post-processing > Merge objects (APOC)` menu. \nAnnotate label edges that should be merged with intensity 1 and those which should be kept with intensity 2 in a blank label image.\nSelect which features should be considered for merging:\n* `touch_portion`: The relative amount an object touches another. E.g. in a symmetric, honey-comb like tissue, neighboring cells have a touch-portion of `1/6` to each other.\n* `touch_count`: The number of pixels where object touch. When using this parameter, make sure that images used for training and prediction have the same voxel size.\n* `mean_touch_intensity`: The mean average intensity between touching objects. When using this parameter, make sure images used for training and prediction are normalized the same way.\n* `centroid_distance`: The distance (in pixels or voxels) between centroids of labeled objects. \n* `mean_intensity_difference`: The absolute difference between the mean intensity of the two objects. This measurement allows differentiating bright and dark object and [not] merging them.\n* `standard_deviation_intensity_difference`: The absolute difference between the standard deviation of the two objects. This measurement allows to differentiate [in]homogeneous objects and [not] merge them.\n* `area_difference`: The difference in area/volume/pixel-count allows differentiating small and large objects and [not] merging them.\n* `mean_max_distance_to_centroid_ratio_difference`: This parameter is a shape descriptor, similar to elongation, allowing to differentiate roundish and elongate object and [not] merging them.\n\nNote: most features are recommended to be used in isotropic images only.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/merge_objects.gif)\n\nFor training, use an image with equivalized intensity (1), an over-segmented label image (2) and annotations (3). When drawing annotations in a new labels layer, make sure to misguide the algorithm draw on edges of touching objects a 1 if those should be merged and a 2 if they should be kept. Make sure there are no 1/2 annotation circles on both: labels which should be merged and kept.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/merge_objects2.png)\n\n### Object classification\n\nClick the menu `Tools > Segmentation post-processing > Object classification (APOC)`. \n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/menu.png)\n\nThis user interface will be shown:\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_classifier_gui.png)\n\n1. The image layer will be used for intensity based feature extraction (see below).\n2. The labels layer should be contain the segmentation of objects that should be classified. \n   You can use the Object Segmenter explained above to create this layer.\n3. The annotation layer should contain manual annotations of object classes. \n   You can draw lines crossing single and multiple objects of the same kind. \n   For example draw a line through some elongated objects with label \"1\" and another line through some rather roundish objects with label \"2\".\n   If these lines touch the background, that will be ignored.\n4. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n5. Select the right features for training. For example, for differentiating objects according to their shape as suggested above, select \"shape\".\n   The features are extracted using clEsperanto and are shown by example in [this notebook](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/tissues/parametric_maps.ipynb).\n6. Click on the `Run` button. If classification doesn't perform well in the first attempt, try changing selected features.  \n\nIf classification worked well, it may for example look like this. Note the two thick lines which were drawn to annotate elongated and roundish objects with brown and cyan:\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/object_classification_result.png)\n\nA pre-trained model can later be applied [from scripts as shown in the example notebook](https://github.com/haesleinhuepf/apoc/blob/main/demo/cell_classification.ipynb) or using the menu `Tools > Segmentation post-processing > Object classification (apply pretrained, APOC)`.\n\n### Object selection\n\nAnalogously to object classification, the object selector removes all objects from a label image that do not belong to a specified class.\nIt can be found in the menu `Tools > Segmentation post-processing > Object selection (APOC)`. \n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/select_objects.gif)\n\n\n### Feature correlation matrix\n\nWhen training object classifiers it is crucial to investigate to which degree features are correlated and select the right, ideally uncorrelated features to classify objects robustly.\nAfter measuring features with any compatible napari plugin listed below, you can visualize the feature correlation matrix using the menu `Tools > Measurement tables > Show feature correlation matrix (pandas, APOC)` and by selecting the labels layer which has been analyzed.\nBefore computing the correlation matrix, all rows containing [NaN](https://en.wikipedia.org/wiki/NaN) values are removed.\nFor further details, please refer to the [documentation of the underlying function in pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html).\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/feature_correlation_matrix.png)\n\n### Surface Vertex Classification (SVeC)\n\nWhen using napari-APOC in combination with [napari-process-points-and-surfaces>=0.3.3](https://github.com/haesleinhuepf/napari-process-points-and-surfaces), \none can also classify vertices. Therefore, use for example the menu `Measurement > Surface quality table (vedo, nppas)` to determine quantitative measurements\nand the menu `Surfaces > Annotate surface manually (nppas)` for manual annotations. It is recommended to annotate the entire surface with value 1 as background, and specific regions of interest with integer numbers > 1.\nAfter measurements have been extracted and annotations were made, start SVeC from the `Surfaces > Surface vertex classification (custom properties, APOC)` menu. It can be used like the Object Classifier explained above.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_vertex_classification.gif)\n\n[Download full video](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/demo_vertex_classification.mp4)\n\n### Classifier statistics\nAfter classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes `Show classifier statistics` and `Show feature correlation matrix`.\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/correlation_matrix2.png)\n\nThis can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\n\n### Object classification from custom measurements\n\nYou can also classify labeled objects according to custom measurements. For deriving those measurements, you can use these napari plugins:\n\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\n* [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\n* [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\n* [napari-skimage-regionprops](https://www.napari-hub.org/plugins/napari-skimage-regionprops)\n\nFurthermore, if you use napari from Python, you can also create a dictionary or pandas DataFrame with measurements and store it in the `labels_layer.features` to make them available in the object classifier.\n\nAfter labels have been measured, you can start the `Object Classifier (custom properties, APOC)` from the `Tools > Segmentation post-processing` menu:\n\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/table_row_classifier_gui.png)\n\n1. Select the labels layers that has been measured.\n2. The annotation layer should contain manual annotations of object classes. \n   You can draw lines crossing single and multiple objects of the same kind. \n   For example draw a line through some elongated objects with label \"1\" and another line through some rather roundish objects with label \"2\".\n   If these lines touch the background, that will be ignored.\n3. Select the measurements / features that should be used for object classification.\n4. Use the `Update Measurements` button in case you did new measurements after Object classifier dialog was opened.\n5. Enter the filename of the classifier to be trained here. This file will be overwritten in case it existed already.\n6. Tree depth and number of trees allow you to fine-tune how to deal with manifold objects of different characteristics. The higher these numbers, the longer classification will take. In case you use many features, high depth and number of trees might be necessary. (See also `max_depth` and `n_estimators` in the [scikit-learn documentation of the Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n7. The classification result will be stored under this name in the labels-layer's properties.\n8. Choose if the results table should be shown. Choose if classifier statistics should be shown. [Read more about classifier statistics](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/forest_statistics.html).\n9. Click on `Run` to start training and prediction.\n\nYou can also train those classifiers from Python and reuse them: [Read more about using the TableRowClassifier from python](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/apoc_simpleitk_object_classification.html)\n\n### Classifier statistics and correlation matrix\nAfter classifier training, you can study the share of the individual features/measurements and how they are correlated by activating the checkboxes `Show classifier statistics` and `Show correlation matrix`.\n![img.png](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/correlation_matrix.png)\n\nThis can help understanding how the classifier works. Furthermore, you can accelerate the classifier by reducing the number of correlated features.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nIt is recommended to install the plugin in a conda environment. Therefore install conda first, e.g. [mini-conda](https://docs.conda.io/en/latest/miniconda.html).\nIf you never worked with conda before, reading this [short introduction](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/conda_basics/01_conda_environments.md) might be helpful.\n\nOptional: Setup a fresh conda environment, activate it and install napari:\n\n```\nconda create --name napari_apoc python=3.9\nconda activate napari_apoc\nconda install napari\n```\n\nIf your conda environment is set up, you can install `napari-accelerated-pixel-and-object-classification` using [pip]. Note: you need [pyopencl](https://documen.tician.de/pyopencl/) first.\n\n```\nconda install -c conda-forge pyopencl\npip install napari-accelerated-pixel-and-object-classification\n```\n\nMac-users please also install this:\n\n    conda install -c conda-forge ocl_icd_wrapper_apple\n    \nLinux users please also install this:\n    \n    conda install -c conda-forge ocl-icd-system\n\n\n## Contributing\n \nContributions, feedback and suggestions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Similar software\nThere are other napari plugins and other software with similar functionality for interactive classification of pixels and objects.\n\n* [napari-feature-classifier](https://github.com/fractal-napari-plugins-collection/napari-feature-classifier)\n* [napari-buds](https://www.napari-hub.org/plugins/napari-buds)\n* [ilastik](https://www.ilastik.org/)\n* [Fiji's Trainable Weka Segmentation](https://imagej.net/plugins/tws/)\n* [scikit-learn](https://scikit-learn.org/stable/)\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-accelerated-pixel-and-object-classification\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ObjectSegmentation",
      "SemanticSegmentation",
      "CustomObjectClassifierWidget",
      "Train_object_merger",
      "ObjectClassification"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sim-processor",
    "name": "napari-sim-processor",
    "display_name": "napari SIM processor",
    "version": "0.1.1",
    "created_at": "2022-05-04",
    "modified_at": "2023-11-01",
    "authors": [
      "Andrea Bassi and Mark Neil"
    ],
    "author_emails": [
      "andrea1.bassi@polimi.it"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-sim-processor/",
    "home_github": "https://github.com/andreabassi78/napari-sim-processor",
    "home_other": null,
    "summary": "A plugin to process Structured Illumination Microscopy data with gpu acceleration",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "superqt >=0.3.2",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "matplotlib ; extra == 'testing'",
      "numpy ; extra == 'testing'",
      "scipy ; extra == 'testing'",
      "superqt ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-sim-processor\n\n[![License](https://img.shields.io/pypi/l/napari-sim-processor.svg?color=green)](https://github.com/andreabassi78/napari-sim-processor/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sim-processor.svg?color=green)](https://pypi.org/project/napari-sim-processor)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sim-processor.svg?color=green)](https://python.org)\n[![tests](https://github.com/andreabassi78/napari-sim-processor/workflows/tests/badge.svg)](https://github.com/andreabassi78/napari-sim-processor/actions)\n[![codecov](https://codecov.io/gh/andreabassi78/napari-sim-processor/branch/main/graph/badge.svg)](https://codecov.io/gh/andreabassi78/napari-sim-processor)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sim-processor)](https://napari-hub.org/plugins/napari-sim-processor)\n\nA Napari plugin for the reconstruction of Structured Illumination Microscopy (SIM) with GPU acceleration (pytorch/cupy if installed).\nCurrently supports:    \n   - conventional SIM data with a generic number of angles and phases (typically, 3 angles and 3 phases are used for resolution improvement in 2D, but any combination can be processed by the widget)\n   - hexagonal SIM data with 7 phases, as used in [this] publication.\n   - 3D SIM, for resolution enhancement in three dimensions. This is available in the [3dSIM] branch  \n\nThe SIM processing widget accepts image stacks organized in 5D (`angle`,`phase`,`z`,`y`,`x`).\n\nThe reshape widget can be used to easily reshape the data if they are not organized as 5D (angle,phase,z,y,x).\n\nFor 3D stacks (raw images) with multiple z-frames, a batch reconstruction method is available, as described [here].\n\nSyntetic raw-image stacks of Structured Illumination Microscopy can be easily simulated using the napari [SIMulator] pluging.\n\t \n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-sim-processor` via [pip]:\n\n    pip install napari-sim-processor\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/andreabassi78/napari-sim-processor.git\n\n\n## Usage\n\n1) Open napari. \n\n2) Launch the reshape and sim-processor widgets.\n\n3) Open your raw image stack (using the napari built-in or your own file opener).\n\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture1.png)\n\n4) If your image is ordered as a 5D stack (angle, phase, z-frame, y, x) go to point 6. \n\n5) In the reshape widget, select the actual number of acquired angles, phases, and frames (red arrow) and press `Reshape Stack`.\n Note that the label axis of the viewer will be updated (green arrow).\n\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture1b.png)\n\n6) In the sim-reconstruction widget press the Select image layer button. Note that the number of phases and angles will be updated (blue arrow). \n\n7) Choose the correct parameters of the SIM acquisition (`NA`, `pixelsize`, `M`, etc.) and processing parameters (`alpha`, `beta`, w, `eta`, `group`):\n   - `w`: parameter of the Weiner filter.\n   - `eta`: constant used for calibration. It should be slightly smaller than the carrier frequency (in pupil radius units).\n   - `group`: for stacks with multiple z-frames, it is the number of frames that are used together for the calibration process.\n\t\nFor details on the other parameters see [here].\n\n8) Calibrate the SIM processor, pressing the `Calibrate` button. This will find the carrier frequencies (red circles if the `Show Carrier` checkbox is selected), the modulation amplitude and the phase, using cross correlation analysis.\n\n9) Click on the checkboxes to show the power spectrum of the raw image (`Show power spectrum`) or the cross-correlation (`Show Xcorr`), to see if the found carrier frequency is correct.\n\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture2b.png)\n**Napari viewer showing the power spectrum of the raw stack. The pupil circle is in blue. A circle corresponding to `eta` is shown in green.**\n\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture2.png)\n**Napari viewer showing the cross-correlation of the raw stack. The red circles indicate the found carrier frequencies**\n\n10) Run the reconstruction of a single plane (`SIM reconstruction`) or of a stack (`Stack reconstruction`). After execution, a new image_layer will be added to the napari viewer. Click on the `Batch reconstruction` checkbox in order to process an entire stack in one shot. Click on the pytorch checkbox for gpu acceleration.\n\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture3b.png)\n**Napari viewer with widgets showing a pseudo-widefield reconstruction**\n\n![raw](https://github.com/andreabassi78/napari-sim-processor/raw/main/images/Picture3.png)\n**Napari viewer with widgets showing a SIM reconstruction**\n\n## GPU processing\n\nThe underlying processing classes will use numpy (and FFTW if available) for \nits calculations. For GPU accelerated processing you need to have either the \nPyTorch (tested with torch v1.11.0+cu113) or the CuPy (tested with cupy-cuda113 \nv10.4.0) package installed.  Make sure to match the package cuda version to the CUDA library \ninstalled on your system otherwise PyTorch will default to CPU and CuPy will not work at all.  \n\nBoth packages give significant speedup on even relatively modest CUDA GPUs compared \nto Numpy, and PyTorch running on the CPU only can show improvements relative to numpy \nand FFTW. Selection of which processing package to use is via a ComboBox in the \nnapari_sim_processor widget.  Only available packages are shown. \n\nOther than requiring a CUDA GPU it is advisable to have significant GPU memory \navailable, particularly when processing large datasets.  Batch processing is the \nmost memory hungry of the methods, but can process 280x512x512 datasets on a 4GB GPU.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-sim-processor\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/andreabassi78/napari-sim-processor/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[here]: https://doi.org/10.1098/rsta.2020.0162\n[this]: https://doi.org/10.1364/OE.466225\n[3dSIM]: https://github.com/andreabassi78/napari-sim-processor/tree/3dSIM\n[SIMulator]: https://www.napari-hub.org/plugins/napari-generic-SIMulator\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SIM processor",
      "Reshape stack to 5D (angles,phases,z,y,x)"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-psf-simulator",
    "name": "napari-psf-simulator",
    "display_name": "PSF simulator",
    "version": "0.3.1",
    "created_at": "2022-04-14",
    "modified_at": "2023-10-31",
    "authors": [
      "Andrea Bassi"
    ],
    "author_emails": [
      "andrea1.bassi@polimi.it"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-psf-simulator/",
    "home_github": "https://github.com/andreabassi78/napari-psf-simulator",
    "home_other": null,
    "summary": "A plugin for simulations of the Point Spread Function, with aberrations",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "PyCustomFocus >=3.3.6",
      "matplotlib",
      "scikit-image",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "qtpy ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "PyCustomFocus >=3.3.6 ; extra == 'testing'",
      "matplotlib ; extra == 'testing'",
      "scikit-image ; extra == 'testing'",
      "scipy ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-psf-simulator\n\n[![License](https://img.shields.io/pypi/l/napari-psf-simulator.svg?color=green)](https://github.com/andreabassi78/napari-psf-simulator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-psf-simulator.svg?color=green)](https://pypi.org/project/napari-psf-simulator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-psf-simulator.svg?color=green)](https://python.org)\n[![tests](https://github.com/andreabassi78/napari-psf-simulator/workflows/tests/badge.svg)](https://github.com/andreabassi78/napari-psf-simulator/actions)\n[![codecov](https://codecov.io/gh/andreabassi78/napari-psf-simulator/branch/main/graph/badge.svg)](https://codecov.io/gh/andreabassi78/napari-psf-simulator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-psf-simulator)](https://napari-hub.org/plugins/napari-psf-simulator)\n\nA plugin for the simulation of the 3D Point Spread Function of an optical systen, particularly a microscope objective.\n \nCalculates the PSF using scalar and vectorial models.  \nThe following aberrations are included:\n- phase aberration described by a Zernike polynomials with n-m coefficients.\n- aberration induced by a slab, with a refractive index different from the one at the object.  \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-psf-simulator` via [pip]:\n\n    pip install napari-psf-simulator\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/andreabassi78/napari-psf-simulator.git\n\n\n## Usage\n\n1) Lauch the plugin and select the parameters of the microscope: `NA` (numerical aperture), `wavelenght`, `n` (refractive index at the object),\n   `FOV xy` (field of view in the transverse direction), `FOV z` (field of view in the axial direction), `dxy` (pixel size, transverse sampling), `dz` (voxel depth, axial sampling), `lens radius` (physical aperture of the lens, used in vectorial model)\n\n2) Select a propagation model between `scalar` and `vectorial`.  \n\n3) Select an aberration type (if needed).\n\n4) Press `Calculate PSF` to run the simulator. This will create a new image layer with the 3D PSF.\n \n   The option `Show Airy disk` creates a circle with radius given by the diffraction limit (Rayleigh criterion).\n\n![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/figure.png)\n**Napari viewer with the psf-simulator widget showing the in-focus plane of an aberrated PSF**\n\n![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/animation.gif)\n**Slicing through a PSF aberrated with Zernike polynomials of order N=3, M=1 (coma)**\n\n3) Click on the `Plot PSF Profile in Console` checkbox to see the x and z profiles of the PSF.\n   They will show up in  the viewer console when `Calculate PSF` is executed.\n\n![raw](https://github.com/andreabassi78/napari-psf-simulator/raw/main/images/Plot.png)\n**Plot profile of the PSF, shown in the Console**\n\n## Detailed documentation\n\nAn exhaustive documentation of the use of the plugin on scalar and vectoral propagation models can be found in [this] presentation.\n\nA detailed explanation of the uses and advantages that simulating a PSF brings can be found [here].\n\nThe vectorial propagation model implements a secondary library: [pyfocus](https://github.com/fcaprile/PyFocus). The full documentation of this library can be found at [read the docs](https://pyfocus.readthedocs.io/en/latest/) and in the paper: \"PyFocus: A Python package for vectorial calculations of focused optical fields under realistic conditions. Application to toroidal foci.\" https://doi.org/10.1016/j.cpc.2022.108315\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request. \nThe plugin has been concived to be modular allowing the insertion of new aberations and pupils. Please contact the developers on github for adding new propagations and aberrations types. \nAny suggestions or contributions are welcome.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-psf-simulator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/andreabassi78/napari-psf-simulator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[this]: https://github.com/andreabassi78/napari-psf-simulator/raw/main/docs/napari_psf_simullator_presentation.pdf\n\n[here]: https://github.com/andreabassi78/napari-psf-simulator/raw/main/docs/pyfocus_seminar.pptx\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PSF simulator widget",
      "PSF combiner widget",
      "Contrast setter widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sif-reader",
    "name": "napari-sif-reader",
    "display_name": "napari sif file reader",
    "version": "0.0.2",
    "created_at": "2022-11-03",
    "modified_at": "2023-10-31",
    "authors": [
      "Ruben Lopez"
    ],
    "author_emails": [
      "rjlopez2@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-sif-reader/",
    "home_github": "https://github.com/rjlopez2/napari-sif-reader",
    "home_other": null,
    "summary": "This is a simple wraper to read .sif format files from Andor Technology.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "sif-parser",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pillow ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-sif-reader\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-sif-reader.svg?color=green)](https://github.com/rjlopez2/napari-sif-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sif-reader.svg?color=green)](https://pypi.org/project/napari-sif-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sif-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/rjlopez2/napari-sif-reader/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-sif-reader/actions)\n[![codecov](https://codecov.io/gh/rjlopez2/napari-sif-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-sif-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sif-reader)](https://napari-hub.org/plugins/napari-sif-reader)\n\nThis is a simple wraper to read .sif format files from Andor Technology.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-sif-reader` via [pip]:\n\n    pip install napari-sif-reader\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/rjlopez2/napari-sif-reader.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-sif-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/rjlopez2/napari-sif-reader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.sif"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "napari sif file reader"
    ]
  },
  {
    "normalized_name": "vollseg-napari-mtrack",
    "name": "vollseg-napari-mtrack",
    "display_name": "VollSeg Napari MTrack Plugin",
    "version": "1.4.7",
    "created_at": "2022-12-20",
    "modified_at": "2023-10-28",
    "authors": [
      "Varun Kapoor"
    ],
    "author_emails": [
      "randomaccessiblekapoor@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/vollseg-napari-mtrack/",
    "home_github": "https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack",
    "home_other": null,
    "summary": "Segment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "caped-ai",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# vollseg-napari-mtrack\n\n[![License BSD-3](https://img.shields.io/pypi/l/vollseg-napari-mtrack.svg?color=green)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/vollseg-napari-mtrack.svg?color=green)](https://pypi.org/project/vollseg-napari-mtrack)\n[![Python Version](https://img.shields.io/pypi/pyversions/vollseg-napari-mtrack.svg?color=green)](https://python.org)\n[![tests](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/workflows/tests/badge.svg)](https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/actions)\n[![codecov](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-mtrack/branch/main/graph/badge.svg)](https://codecov.io/gh/Kapoorlabs-CAPED/vollseg-napari-mtrack)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vollseg-napari-mtrack)](https://napari-hub.org/plugins/vollseg-napari-mtrack)\n\nSegment kymographs of microtubules, actin filaments and perform Ransac based fits to compute dynamic instability parameters for individual kymographs and also in batch\n\n----------------------------------\n\nElaborate documentation for users of this repository at this [documentation]\n\nThis [napari] plugin was generated with [Cookiecutter] using [@caped]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `vollseg-napari-mtrack` via [pip]:\n\n    pip install vollseg-napari-mtrack\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"vollseg-napari-mtrack\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[@caped]: https://github.com/Kapoorlabs-CAPED/\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/Kapoorlabs-CAPED/cookiecutter-kapoorlabs-napari-plugin\n[documentation]: https://kapoorlabs-caped.github.io/vollseg-napari-mtrack\n[file an issue]: https://github.com/Kapoorlabs-CAPED/vollseg-napari-mtrack/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MTrack"
    ],
    "contributions_sample_data": [
      "Test Microtubule Kymographs"
    ]
  },
  {
    "normalized_name": "napari-cursor-tracker",
    "name": "napari-cursor-tracker",
    "display_name": "Cursor tracker",
    "version": "0.1.3",
    "created_at": "2023-10-05",
    "modified_at": "2023-10-25",
    "authors": [
      "Florian Aymanns"
    ],
    "author_emails": [
      "florian.aymanns@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-cursor-tracker/",
    "home_github": "https://github.com/faymanns/napari-cursor-tracker",
    "home_other": null,
    "summary": "Plugin for easy manual annotation/tracking of 3D or 2D + t dataset by following the cursor.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-cursor-tracker\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-cursor-tracker.svg?color=green)](https://github.com/faymanns/napari-cursor-tracker/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cursor-tracker.svg?color=green)](https://pypi.org/project/napari-cursor-tracker)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cursor-tracker.svg?color=green)](https://python.org)\n[![tests](https://github.com/faymanns/napari-cursor-tracker/workflows/tests/badge.svg)](https://github.com/faymanns/napari-cursor-tracker/actions)\n[![codecov](https://codecov.io/gh/faymanns/napari-cursor-tracker/branch/main/graph/badge.svg)](https://codecov.io/gh/faymanns/napari-cursor-tracker)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cursor-tracker)](https://napari-hub.org/plugins/napari-cursor-tracker)\n\nPlugin for easy manual annotation/tracking of 3D or 2D + t dataset by following the cursor.\n\n----------------------------------\n\n<!--\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-cursor-tracker` via [pip]:\n\n    pip install napari-cursor-tracker\n\n## Getting Started with napari-cursor-tracker\n\nWelcome to `napari-cursor-tracker`, a tool that simplifies the annotation of points in stacks of images by tracking your cursor's position. This documentation will guide you through the process of setting up and using this plugin effectively.\n\n### Points Layer Setup\n\nBefore you can start tracking, you need to create a points layer, which will store the positions of your cursor for each image in the stack. Here's how to set it up:\n\n1. **Choose a Reference Image:** Start by selecting a \"Reference image\" from your image stack. The number of frames or slices in the reference image determines how many points your new layer will have (one per frame/slice).\n\n2. **Specify Point Name:** Assign a name to the tracked point. This name will also serve as the name for the new layer. This step is particularly useful when tracking multiple points.\n\n3. **Create the Layer:** Click on \"Add new layer\" to create the points layer. Initially, all points will be located at the origin (0, 0, 0), but their positions will be updated as you start tracking.\n\n### Tracking Your Cursor\n\nNow that you have set up the points layer, you can start tracking your cursor's position. Follow these steps:\n\n1. **Select the Active Layer:** Choose the points layer where you want to save the tracking results as the \"Active layer.\"\n\n2. **Initiate Tracking:** Begin tracking your cursor's position by pressing the 't' key on your keyboard. To stop tracking, press 't' again. If the \"Auto play when tracking is started\" option is enabled, playback will start automatically when you press 't'. Alternatively, you can manually scroll through the images, and your cursor's position will be saved whenever the slice/frame index changes.\n\n3. **Customize Playback:** To facilitate or expedite tracking, you can adjust playback parameters as needed.\n\n### Tracking Multiple Points\n\nIf you need to track multiple points of interest, you can follow these steps:\n\n1. **Create Individual Layers:** Create a separate points layer for each point you want to track.\n\n2. **Select Active Layer:** Use the \"Active layer\" dropdown menu to select the specific points layer you want to work with.\n\n3. **Start Tracking:** Begin tracking the selected point following the previously mentioned tracking process.\n\n### Saving Your Results\n\nThe results from your tracking sessions can be saved as CSV files. Any points that were not tracked will be marked at the origin point (0, 0, 0) in the saved file.\n\nWith these guidelines, you should be well-prepared to efficiently annotate points in your image stacks using `napari-cursor-tracker`. Happy tracking!\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-cursor-tracker\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Cursor tracker"
    ],
    "contributions_sample_data": [
      "Cursor tracker"
    ]
  },
  {
    "normalized_name": "napari-microtubule-analyzer",
    "name": "napari-microtubule-analyzer",
    "display_name": "Microtubule Analyzer",
    "version": "0.0.1a7",
    "created_at": "2023-10-03",
    "modified_at": "2023-10-24",
    "authors": [
      "Daniel Krentzel"
    ],
    "author_emails": [
      "dkrentzel@pm.me"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-microtubule-analyzer/",
    "home_github": "https://github.com/krentzd/napari-microtubule-analyzer",
    "home_other": null,
    "summary": "A plugin to analyze microtubule organization",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "setuptools",
      "packaging",
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python",
      "matplotlib",
      "scikit-image",
      "tqdm",
      "tifffile",
      "scipy",
      "pyefd",
      "pyqtgraph",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-microtubule-analyzer\n\n[![License MIT](https://img.shields.io/pypi/l/napari-microtubule-analyzer.svg?color=green)](https://github.com/krentzd/napari-microtubule-analyzer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-microtubule-analyzer.svg?color=green)](https://pypi.org/project/napari-microtubule-analyzer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-microtubule-analyzer.svg?color=green)](https://python.org)\n[![tests](https://github.com/krentzd/napari-microtubule-analyzer/workflows/tests/badge.svg)](https://github.com/krentzd/napari-microtubule-analyzer/actions)\n[![codecov](https://codecov.io/gh/krentzd/napari-microtubule-analyzer/branch/main/graph/badge.svg)](https://codecov.io/gh/krentzd/napari-microtubule-analyzer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-microtubule-analyzer)](https://napari-hub.org/plugins/napari-microtubule-analyzer)\n\nA plugin to analyze microtubule organization \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-microtubule-analyzer` via [pip]:\n\n    pip install napari-microtubule-analyzer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/krentzd/napari-microtubule-analyzer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-microtubule-analyzer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/krentzd/napari-microtubule-analyzer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Microtubule Analyzer"
    ],
    "contributions_sample_data": [
      "Sample siRNA data"
    ]
  },
  {
    "normalized_name": "napari-workshop-browser",
    "name": "napari-workshop-browser",
    "display_name": "Napari Workshop Browser",
    "version": "0.0.3",
    "created_at": "2023-07-05",
    "modified_at": "2023-10-17",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "napari@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-workshop-browser/",
    "home_github": "https://github.com/kephale/napari-workshop-browser",
    "home_other": null,
    "summary": "A plugin to browse and follow napari workshops",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "superqt",
      "qtpy",
      "notebook <7.0.0",
      "jupytext",
      "napari",
      "appdirs",
      "requests",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-workshop-browser\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-workshop-browser.svg?color=green)](https://github.com/kephale/napari-workshop-browser/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-workshop-browser.svg?color=green)](https://pypi.org/project/napari-workshop-browser)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-workshop-browser.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-workshop-browser/workflows/tests/badge.svg)](https://github.com/kephale/napari-workshop-browser/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-workshop-browser/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-workshop-browser)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workshop-browser)](https://napari-hub.org/plugins/napari-workshop-browser)\n\nA plugin to browse and follow napari workshops\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## How to use this\n\n1. Download napari as an app (e.g. the latest napari releases are\n   available here: https://github.com/napari/napari/releases)\n2. Open napari\n3. Select `Plugins \\ Plugin Manager`\n4. Install this plugin, and restart napari.\n6. Run this plugin and enter the URL of your workshop's zip file\n7. Click `Launch workshop`\n\n## Installation\n\nYou can install `napari-workshop-browser` via [pip]:\n\n    pip install napari-workshop-browser\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-workshop-browser.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-workshop-browser\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-workshop-browser/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Open a napari workshop"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ufish",
    "name": "napari-ufish",
    "display_name": "U-FISH",
    "version": "0.0.1",
    "created_at": "2023-09-30",
    "modified_at": "2023-09-30",
    "authors": [
      "Weize Xu"
    ],
    "author_emails": [
      "vet.xwz@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-ufish/",
    "home_github": null,
    "home_other": "None",
    "summary": "Deep learning-based FISH spot calling method.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "ufish",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-ufish\n\n[![License MIT](https://img.shields.io/pypi/l/napari-ufish.svg?color=green)](https://github.com/UFISH-Team/napari-ufish/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ufish.svg?color=green)](https://pypi.org/project/napari-ufish)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ufish.svg?color=green)](https://python.org)\n[![tests](https://github.com/UFISH-Team/napari-ufish/workflows/tests/badge.svg)](https://github.com/UFISH-Team/napari-ufish/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ufish)](https://napari-hub.org/plugins/napari-ufish)\n\nDeep learning-based FISH spot calling method.\nThe napari plugin for [U-FISH](https://github.com/UFISH-Team/U-FISH).\n\n## Links\n\n+ [U-FISH](https://github.com/UFISH-Team/U-FISH)\n+ [U-FISH models](https://huggingface.co/GangCaoLab/U-FISH)\n+ [FISH_spots dataset](https://huggingface.co/datasets/GangCaoLab/FISH_spots)\n\n## TODO List\n\n- [x] Sample image\n- [x] Inference interface\n    - [x] Inference parameters\n    - [x] Load model from path\n    - [x] Help information dialog\n- [x] Training interface\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-ufish` via [pip]:\n\n    pip install napari-ufish\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-ufish\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "U-FISH Inference",
      "U-FISH Train"
    ],
    "contributions_sample_data": [
      "U-FISH"
    ]
  },
  {
    "normalized_name": "napari-conference",
    "name": "napari-conference",
    "display_name": "Napari Conference",
    "version": "0.1.0",
    "created_at": "2023-09-28",
    "modified_at": "2023-09-28",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czi@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-conference/",
    "home_github": "https://github.com/kephale/napari-conference",
    "home_other": null,
    "summary": "A simple plugin that allows you to use napari + your webcam in video calls",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python",
      "pyvirtualcam",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-conference\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-conference.svg?color=green)](https://github.com/kephale/napari-conference/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-conference.svg?color=green)](https://pypi.org/project/napari-conference)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-conference.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-conference/workflows/tests/badge.svg)](https://github.com/kephale/napari-conference/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-conference/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-conference)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-conference)](https://napari-hub.org/plugins/napari-conference)\n\nA simple plugin that allows you to use napari + your webcam in video\ncalls\n\n![Example screenshot of a person using napari conference with the\nnapari viewer and napari conference widget shown](napari_conference_example.png)\n\n## Usage\n\n1. `Plugins>start conference`\n2. Check `running` checkbox\n3. Press `Update` button to update any setting (and start/stop)\n\nIf things work in zoom but you don't show up, then make sure `blur\nbackground` is disabled.\n\n## Installation\n\n### Prerequisites\n\nYou will need to:\n\n- follow `pyvirtualcam`'s installation instructions:\nhttps://github.com/letmaik/pyvirtualcam#installation\n- install `napari` from source to get the new async slicing updates \n\nNote: I needed to install `pyvirtualcam` from source on my MacOS M1\nwith python=3.10.\n\n\n\n[Not available on pypi yet] You can install `napari-conference` via [pip]:\n\n    pip install napari-conference\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-conference.git\n\n\n## Known Issues\n\n- resizing the napari window while streaming causes a crash\n- cannot be restarted after stopping the widget\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-conference\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-conference/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "start conference"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mzarr",
    "name": "napari-mzarr",
    "display_name": "Napari Mzarr",
    "version": "0.0.3",
    "created_at": "2023-06-02",
    "modified_at": "2023-09-26",
    "authors": [
      "Karol Gotkowski"
    ],
    "author_emails": [
      "karol.gotkowski@dkfz.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-mzarr/",
    "home_github": null,
    "home_other": "None",
    "summary": "A reader and writer plugin for the Mzarr image format.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "zarr",
      "numcodecs",
      "imagecodecs ==2023.1.23",
      "dask",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-mzarr\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-mzarr.svg?color=green)](https://github.com/Karol-G/napari-mzarr/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mzarr.svg?color=green)](https://pypi.org/project/napari-mzarr)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mzarr.svg?color=green)](https://python.org)\n[![tests](https://github.com/Karol-G/napari-mzarr/workflows/tests/badge.svg)](https://github.com/Karol-G/napari-mzarr/actions)\n[![codecov](https://codecov.io/gh/Karol-G/napari-mzarr/branch/main/graph/badge.svg)](https://codecov.io/gh/Karol-G/napari-mzarr)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mzarr)](https://napari-hub.org/plugins/napari-mzarr)\n\nA reader and writer plugin for the Mzarr image format.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-mzarr` via [pip]:\n\n    pip install napari-mzarr\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-mzarr\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mzz",
      "*.mzarr"
    ],
    "contributions_writers_filename_extensions": [
      ".mzarr"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bfio",
    "name": "napari-bfio",
    "display_name": "Bfio",
    "version": "0.0.4",
    "created_at": "2023-08-18",
    "modified_at": "2023-09-25",
    "authors": [
      "Sameeul B Samee"
    ],
    "author_emails": [
      "sameeul.samee@nih.gov"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-bfio/",
    "home_github": "https://github.com/PolusAI/napari-bfio",
    "home_other": null,
    "summary": "A plugin to read and write images using bfio within napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "bfio >=2.3.1",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-bfio\n\n[![License MIT](https://img.shields.io/pypi/l/napari-bfio.svg?color=green)](https://github.com/PolusAI/napari-bfio/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bfio.svg?color=green)](https://pypi.org/project/napari-bfio)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bfio.svg?color=green)](https://python.org)\n[![tests](https://github.com/PolusAI/napari-bfio/workflows/tests/badge.svg)](https://github.com/PolusAI/napari-bfio/actions)\n[![codecov](https://codecov.io/gh/PolusAI/napari-bfio/branch/main/graph/badge.svg)](https://codecov.io/gh/PolusAI/napari-bfio)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bfio)](https://napari-hub.org/plugins/napari-bfio)\n\nA plugin to read and write images using bfio within napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-bfio` via [pip]:\n\n    pip install napari-bfio\n\n`napari-bfio` depends on `bfio` package to read/write the data. By default, `bfio` package and the core dependencies (numpy, tifffile, imagecodecs, scyjava) are installed during the installation process of `napari-bfio`.\n\nTo install latest development version :\n\n    pip install git+https://github.com/PolusAI/napari-bfio.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-bfio\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/PolusAI/napari-bfio/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.lms",
      "*.oif",
      "*.pcoraw",
      "*.seq",
      "*.c01",
      "*.pr3",
      "*.rec",
      "*.fff",
      "*.spc",
      "*.labels",
      "*.ids",
      "*.mod",
      "*.am",
      "*.ics",
      "*.tfr",
      "*.img",
      "*.omp2info",
      "*.cr2",
      "*.pds",
      "*.sdt",
      "*.pcx",
      "*.pict",
      "*.nd2",
      "*.nii.gz",
      "*.r3d",
      "*.vsi",
      "*.jp2",
      "*.obsep",
      "*.msr",
      "*.afi",
      "*.his",
      "*.xdce",
      "*.lim",
      "*.zvi",
      "*.pnl",
      "*.dm4",
      "*.sld",
      "*.nrrd",
      "*.al3d",
      "*.mov",
      "*.res",
      "*.lei",
      "*.klb",
      "*.sxm",
      "*.lsm",
      "*.wpi",
      "*.avi",
      "*.ffr",
      "*.xys",
      "*.2fl",
      "*.bmp",
      "*.mdb",
      "*.aim",
      "*.ipl",
      "*.acff",
      "*.hdr",
      "*.mvd2",
      "*.txt",
      "*.pgm",
      "*.frm",
      "*.grey",
      "*.oib",
      "*.ch5",
      "*.dm3",
      "*.ome.tif",
      "*.xv",
      "*.ppm",
      "*.ims",
      "*.jpk",
      "*.mrcs",
      "*.pic",
      "*.im3",
      "*.bif",
      "*.gel",
      "*.nd",
      "*.psd",
      "*.zfp",
      "*.gif",
      "*.nii",
      "*.ipm",
      "*.htd",
      "*.par",
      "*.fdf",
      "*.rcpnl",
      "*.apl",
      "*.bip",
      "*.ome.tiff",
      "*.naf",
      "*.png",
      "*.exp",
      "*.wat",
      "*.zfr",
      "*.tnb",
      "*.dv",
      "*.xlef",
      "*.j2k",
      "*.cxd",
      "*.ps",
      "*.qptiff",
      "*.tf2",
      "*.ndpi",
      "*.jpeg",
      "*.nef",
      "*.raw",
      "*.vff",
      "*.liff",
      "*.jpx",
      "*.mrc",
      "*.sif",
      "*.tiff",
      "*.stp",
      "*.bin",
      "*.dicom",
      "*.1sc",
      "*.tif",
      "*.lof",
      "*.amiramesh",
      "*.nhdr",
      "*.i2i",
      "*.vms",
      "*.xqd",
      "*.tf8",
      "*.map",
      "*.scn",
      "*.tga",
      "*.cfg",
      "*.lif",
      "*.dti",
      "*.dib",
      "*.l2d",
      "*.svs",
      "*.st",
      "*.flex",
      "*.hdf",
      "*.ome.zarr",
      "*.mea",
      "*.arf",
      "*.mtb",
      "*.sm2",
      "*.spi",
      "*.inr",
      "*.db",
      "*.fits",
      "*.afm",
      "*.cif",
      "*.mng",
      "*.dat",
      "*.xqf",
      "*.epsi",
      "*.fli",
      "*.dcm",
      "*.crw",
      "*.jpf",
      "*.oir",
      "*.sldy",
      "*.ipw",
      "*.vws",
      "*.dm2",
      "*.sm3",
      "*.stk",
      "*.ali",
      "*.h5",
      "*.hed",
      "*.eps",
      "*.hx",
      "*.obf",
      "*.v",
      "*.pbm",
      "*.top",
      "*.ndpis",
      "*.jpg",
      "*.spe",
      "*.mnc",
      "*.btf",
      "*.mrw"
    ],
    "contributions_writers_filename_extensions": [
      ".ome.tif",
      ".ome.tiff",
      ".ome.zarr"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-parallel",
    "name": "napari-parallel",
    "display_name": "Napari Parallel",
    "version": "0.0.2",
    "created_at": "2023-09-20",
    "modified_at": "2023-09-24",
    "authors": [
      "Artem Tomilo",
      "Nafisa Anjum",
      "Himanshu Kaloni"
    ],
    "author_emails": [
      "artem.tomilo@mailbox.tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-parallel/",
    "home_github": null,
    "home_other": "None",
    "summary": "Plugin to process images in parallel using several computers",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-parallel\n\nThis plugin is used for parallel computing of image processing using the code\ngeneration capabilities of the `napari-assistant` plugin and the distributed\ncomputing library `dask`.\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-parallel.svg?color=green)](https://github.com/bridgeArchitect/napari-parallel/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-parallel.svg?color=green)](https://pypi.org/project/napari-parallel)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-parallel.svg?color=green)](https://python.org)\n[![tests](https://github.com/bridgeArchitect/napari-parallel/workflows/tests/badge.svg)](https://github.com/bridgeArchitect/napari-parallel/actions)\n[![codecov](https://codecov.io/gh/bridgeArchitect/napari-parallel/branch/main/graph/badge.svg)](https://codecov.io/gh/bridgeArchitect/napari-parallel)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-parallel)](https://napari-hub.org/plugins/napari-parallel)\n\nPlugin to process images in parallel using several computers\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-parallel` via [pip]:\n\n    pip install napari-parallel\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-parallel\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Parallel QWidget"
    ],
    "contributions_sample_data": [
      "Napari Parallel"
    ]
  },
  {
    "normalized_name": "napari-czann-segment",
    "name": "napari-czann-segment",
    "display_name": "CZANN Segmentation",
    "version": "0.0.18",
    "created_at": "2022-07-11",
    "modified_at": "2023-09-22",
    "authors": [
      "Sebastian Rhode"
    ],
    "author_emails": [
      "sebrhode@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-czann-segment/",
    "home_github": "https://github.com/sebi06/napari-czann-segment",
    "home_other": null,
    "summary": "Semantic Segmentation using Deep Learning ONNX models packaged as *.czann files",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "cztile",
      "czmodel[pytorch] >=5",
      "onnxruntime-gpu",
      "aicsimageio",
      "pytest",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-czann-segment\n\n[![License](https://img.shields.io/pypi/l/napari-czann-segment.svg?color=green)](https://github.com/sebi06/napari-czann-segment/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-czann-segment.svg?color=green)](https://pypi.org/project/napari-czann-segment)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-czann-segment.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-czann-segment)](https://napari-hub.org/plugins/napari-czann-segment)\n\nSemantic Segmentation of multidimensional images using Deep Learning ONNX models packaged as *.czann files.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n![Train on APEER and use model in Napari](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/Train_APEER_run_Napari_CZANN_no_highlights_small.gif)\n\n## Installation\n\nBefore installing, please setup a conda environment. If you have never worked with conda environments, go through [this tutorial](https://biapol.github.io/blog/johannes_mueller/anaconda_getting_started/) first.\n\nYou can then install `napari-czann-segment` via [pip]:\n\n    pip install napari-czann-segment\n\n## What does the plugin do\n\nThe plugin allows you to:\n\n- Use a *.czann file containing the Deep Neural Network (ONNX) for semantic segmentation and metadata\n- Segmentation will be applied per 2D plane for all dimensions\n- Processing larger multidimensional images it uses the [cztile] package to chunk the individual 2d arrays using a specific overlap.\n- multidimensional images will be processed plane-by-plane\n\n## What does the plugin NOT do\n\n**Before one can actually use a model it needs to be trained, which is NOT done by this plugin**.\n\nThere are two main ways hwo such a model can be created:\n\n- Train the segmentation model fully automated on [APEER] and download the *.czann file\n- Train your model in a Jupyter notebook etc. and package it using the [czmodel] python package as an *.czann\n\n## Using this plugin\n\n### Sample Data\n\nA test image and a *.czann model file can be downloaded [here](https://github.com/sebi06/napari-czann-segment/tree/main/src/napari_czann_segment/_data).\n\n- `PGC_20X.ome.tiff` --> use `PGC_20X_nucleus_detector.czann` to segment\n\nIn order to use this plugin the user has to do the following things:\n\n- Open the image using \"File - Open Files(s)\" (requires [napari-aicsimageio] plugin).\n- Click **napari-czann-segment: Segment with CZANN model** in the \"Plugins\" menu.\n- **Select a czann file** to use the model for segmentation.\n- metadata of the model will be shown (see example below)\n\n| Parameter    | Value                                        | Explanation                                             |\n| :----------- | :------------------------------------------- | ------------------------------------------------------- |\n| model_type   | ModelType.SINGLE_CLASS_SEMANTIC_SEGMENTATION | see: [czmodel] for details                              |\n| input_shape  | [1024, 1024, 1]                              | tile dimensions of model input                          |\n| output_shape | [1024, 1024, 3]                              | tile dimensions of model output                         |\n| model_id     | ba32bc6d-6bc9-4774-8b47-20646c7cb838         | unique GUID for that model                              |\n| min_overlap  | [128, 128]                                   | tile overlap used during training (for this model)      |\n| classes      | ['background', 'grains', 'inclusions']       | available classes                                       |\n| model_name   | APEER-trained model                          | name of the model                                       |\n\n![Napari - Image loaded and czann selected](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/napari_czann1.png)\n\n- Adjust the **minimum overlap** for the tiling (optional, see [cztile] for details).\n- Select the **layer** to be segmented.\n- Toggle **Use GPU for inference** checkbox to enable / disable using a GPU (Nvidia) for the segmentation (experimental feature).\n- Press **Segment Selected Image Layer** to run the segmentation.\n\n![Napari - Image successfully segmented](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/napari_czann3.png)\n\nA successful is obviously only the starting point for further image analysis steps to extract the desired numbers from the segmented image.\nAnother example is shown below demonstrating a simple \"Grain Size Analysis\" using a deep-learning model trained on [APEER] used in [napari]\n\n![Napari - Simple Grain Size Analysis](https://github.com/sebi06/napari-czann-segment/raw/main/readme_images/grainsize_czann_napari.png)\n\n### Remarks\n\n> **IMPORTANT**: Currently the plugin only supports using models trained on a **single channel** image. Therefore, make sure that during the training on [APEER] or somewhere else the correct inputs images are used.\n> It is quite simple to train a single RGB image, which actually has three channels, load this image in [napari] and notice only then that the model will not work, because the image will 3 channels inside [napari].\n\n- Only the CPU will be used for the inference using the ONNX runtime for the [ONNX-CPU] runtime\n- GPUs are supported but require the [ONNX-GPU] runtime and the respective CUDA libraries.\n- Please check the [YAML](env_napari_czann_segment.yml) for an example environment with GPU support.\n- See also [pytorch] for instruction on how to install pytorch\n\n## For developers\n\n- **Please clone this repository first using your favorite tool.**\n\n- **Ideally one creates a new [conda] environment or use an existing environment that already contains [Napari].**\n\nFeel free to create a new environment using the [YAML](env_napari_czann_segment.yml) file at your own risk:\n\n    cd the-github-repo-with-YAML-file\n    conda env create --file conda_env_napari_czann_segment.yml\n    conda activate napari_czmodel\n\n- **Install the plugin locally**\n\nPlease run the following command:\n\n    pip install -e .\n\nTo install latest development version:\n\n    pip install git+https://github.com/sebi06/napari_czann_segment.git\n\n## Contributing\n\nContributions and Feedback are very welcome.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-czann-segment\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/sebi06/napari-czann-segment/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[czmodel]: https://pypi.org/project/czmodel/\n[cztile]: https://pypi.org/project/cztile/\n[APEER]: https://www.apeer.com\n[napari-aicsimageio]: https://github.com/AllenCellModeling/napari-aicsimageio\n[ONNX-GPU]: https://pypi.org/project/onnxruntime-gpu/\n[ONNX-CPU]: https://pypi.org/project/onnxruntime/\n[conda]: https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html\n[pytorch]: https://pytorch.org/get-started/locally\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segment with CZANN Model"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-demo",
    "name": "napari-demo",
    "display_name": "Demo plugin ported from npe2 example",
    "version": "0.2.5",
    "created_at": "2021-01-27",
    "modified_at": "2023-09-21",
    "authors": [
      "napari hub team"
    ],
    "author_emails": [
      "team@napari-hub.org"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-demo/",
    "home_github": "https://github.com/chanzuckerberg/napari-demo",
    "home_other": null,
    "summary": "example plugin for napari plugin developers",
    "categories": [
      "Acquisition",
      "Dataset",
      "Segmentation"
    ],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "pydantic",
      "npe2",
      "numpy"
    ],
    "package_metadata_description": "# napari-demo\n\n[![License](https://img.shields.io/pypi/l/napari-demo.svg?color=green)](https://github.com/chanzuckerberg/napari-demo/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-demo.svg?color=green)](https://pypi.org/project/napari-demo)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-demo.svg?color=green)](https://python.org)\n[![tests](https://github.com/chanzuckerberg/napari-demo/workflows/tests/badge.svg)](https://github.com/chanzuckerberg/napari-demo/actions)\n[![codecov](https://codecov.io/gh/chanzuckerberg/napari-demo/branch/master/graph/badge.svg)](https://codecov.io/gh/chanzuckerberg/napari-demo)\n\nThis is a demo plugin implementation of https://github.com/napari/napari/blob/master/examples/magic_image_arithmetic.py\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-demo` via [pip]:\n\n    pip install napari-demo\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-demo\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\nTo report security issues, see [security](SECURITY.md)\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/chanzuckerberg/napari-demo/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n## Code of Conduct\n\nThis project adheres to the Contributor Covenant [code of conduct](https://github.com/chanzuckerberg/.github/blob/master/CODE_OF_CONDUCT.md). By participating, you are expected to uphold this code. Please report unacceptable behavior to [opensource@chanzuckerberg.com](mailto:opensource@chanzuckerberg.com).\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.fzzy",
      "http://*",
      "https://*",
      "*.fzy"
    ],
    "contributions_writers_filename_extensions": [
      ".xyz",
      ".tif",
      ".e57",
      ".tiff",
      ".pcd"
    ],
    "contributions_widgets": [
      "calculate a new layer arithmetically from two existing layers"
    ],
    "contributions_sample_data": [
      "Some Random Data (10 x 10 x 10)"
    ]
  },
  {
    "normalized_name": "napari-workshop-plugin",
    "name": "napari-workshop-plugin",
    "display_name": "Workshop 2023 demo plugin",
    "version": "1.0.4",
    "created_at": "2023-09-06",
    "modified_at": "2023-09-18",
    "authors": [
      "MetaCell"
    ],
    "author_emails": [
      "sean.martin@metacell.us"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-workshop-plugin/",
    "home_github": "https://github.com/seankmartin/napari-software-development-workshop",
    "home_other": null,
    "summary": "A plugin to demonstrate some concepts from the 2023 workshop on software development related to napari",
    "categories": [
      "Dataset",
      "Image Processing"
    ],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy <2,>=1.23",
      "magicgui",
      "qtpy",
      "mkdocs-material ; extra == 'docs'",
      "mkdocstrings-python ; extra == 'docs'",
      "mkdocstrings ; extra == 'docs'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-workshop-plugin\n\n[![License MIT](https://img.shields.io/pypi/l/napari-workshop-plugin.svg?color=green)](https://github.com/MetaCell/napari-workshop-plugin/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-workshop-plugin.svg?color=green)](https://pypi.org/project/napari-workshop-plugin)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-workshop-plugin.svg?color=green)](https://python.org)\n[![tests](https://github.com/seankmartin/napari-software-development-workshop/actions/workflows/test.yml/badge.svg)](https://github.com/seankmartin/napari-software-development-workshop/actions)\n[![codecov](https://codecov.io/gh/seankmartin/napari-software-development-workshop/branch/main/graph/badge.svg)](https://codecov.io/gh/seankmartin/napari-software-development-workshop)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workshop-plugin)](https://napari-hub.org/plugins/napari-workshop-plugin)\n\nThe purpose of this repository is to provide a template for a napari plugin that can be used as a starting point for the napari software development workshop 2023.\nIt should help you to see the basics of building documentation, testing, and continuous integration for a napari plugin.\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\nThe cookiecutter plugin is also a great jumping off point for your own plugin development.\n\n## Check out a template you can use for your own README\n\n[template.md](template.md)\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-workshop-plugin` via [pip]:\n\n    pip install napari-workshop-plugin\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-workshop-plugin\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please file an issue along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[pip]: https://pypi.org/project/pip/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segmentation demo widget"
    ],
    "contributions_sample_data": [
      "Calcium imaging data"
    ]
  },
  {
    "normalized_name": "generate-dense-patches",
    "name": "generate-dense-patches",
    "display_name": "Generate Dense Patches",
    "version": "0.0.2",
    "created_at": "2023-09-12",
    "modified_at": "2023-09-12",
    "authors": [
      "Aayush Bhatawadekar"
    ],
    "author_emails": [
      "asbhatawadekar@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/generate-dense-patches/",
    "home_github": "https://github.com/volume-em/generate-dense-patches",
    "home_other": null,
    "summary": "A simple plugin to create a lot of training data from a 3D volume and mask",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari >=0.4.18",
      "napari-plugin-engine >=0.2.0",
      "numpy ==1.22",
      "scikit-image >=0.19",
      "magicgui",
      "imagecodecs",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# generate-dense-patches\n\n[![License BSD-3](https://img.shields.io/pypi/l/generate-dense-patches.svg?color=green)](https://github.com/volume-em/generate-dense-patches/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/generate-dense-patches.svg?color=green)](https://pypi.org/project/generate-dense-patches)\n[![Python Version](https://img.shields.io/pypi/pyversions/generate-dense-patches.svg?color=green)](https://python.org)\n[![tests](https://github.com/volume-em/generate-dense-patches/workflows/tests/badge.svg)](https://github.com/volume-em/generate-dense-patches/actions)\n[![codecov](https://codecov.io/gh/volume-em/generate-dense-patches/branch/main/graph/badge.svg)](https://codecov.io/gh/volume-em/generate-dense-patches)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/generate-dense-patches)](https://napari-hub.org/plugins/generate-dense-patches)\n\nA simple plugin to create a lot of training data from a 3D volume and mask. For help with this plugin please open an issue, for issues with napari specifically raise an issue here instead.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nIt's recommended to have installed napari and pyqt through conda. \n\n    conda install napari pyqt\n\nThen to install this plugin via [pip]:\n\n    pip install generate-dense-patches\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/volume-em/generate-dense-patches.git\n\n\n## Usage\nTo use this plugin with napari:\n1. Drag and drop an image and/or segmentation mask (tif) into the viewer.\n2. Open \"Plugins\" Toolbar and select \"Generate dense patches\" and click \"Generate 2D Patches\"\n\nThis plugin works to create a lot of 2D training data by generating an $n^3$ cube, rotating every $\\theta$ slices and saving every (step size) slice of the generated volume.\n\n3. Make sure the \"save directory box\", \"step size\", \"rotation theta\", and \"patch size\" is filled in\n\nIf no point is placed, then the center of the image will be used as the center of the cube. If a point is placed, then the center of the cube will be the point.\n\n4. Press run and wait for the patches to be generated.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"generate-dense-patches\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/volume-em/generate-dense-patches/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Generate 2D Patches"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-hough-circle-detector",
    "name": "napari-hough-circle-detector",
    "display_name": "Hough circle detector",
    "version": "0.0.5",
    "created_at": "2023-04-03",
    "modified_at": "2023-09-06",
    "authors": [
      "Florian Aymanns"
    ],
    "author_emails": [
      "florian.aymanns@epfl.ch"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-hough-circle-detector/",
    "home_github": null,
    "home_other": "None",
    "summary": "An interactive Hough transform for napari.",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "napari[all]",
      "opencv-contrib-python-headless",
      "numpy",
      "pyqt5",
      "scikit-image"
    ],
    "package_metadata_description": "# napari-hough-circle-detector\n\nA plugin for napari that detects circles using the Hough transform.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Circle detector"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nucleaizer",
    "name": "napari-nucleaizer",
    "display_name": "Napari nucleAIzer plugin",
    "version": "0.2.5",
    "created_at": "2021-09-08",
    "modified_at": "2023-09-01",
    "authors": [
      "Ervin Tasnadi"
    ],
    "author_emails": [
      "tasnadi.ervin@brc.hu"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-nucleaizer/",
    "home_github": "https://github.com/etasnadi/napari_nucleaizer",
    "home_other": null,
    "summary": "A GUI interface for training and prediction using the nucleAIzer nuclei detection method.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "qtpy",
      "jsonpickle",
      "numpy",
      "scikit-image",
      "imageio",
      "nucleaizer-backend"
    ],
    "package_metadata_description": "# napari_nucleaizer\n\n[![License](https://img.shields.io/pypi/l/napari-nucleaizer.svg?color=green)](https://github.com/etasnadi/napari-nucleaizer/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nucleaizer.svg?color=green)](https://pypi.org/project/napari-nucleaizer)\n[![Python package](https://github.com/etasnadi/napari_nucleaizer/actions/workflows/test_and_deploy.yml/badge.svg)](https://github.com/etasnadi/napari_nucleaizer/actions/workflows/test_and_deploy.yml)\n[![codecov](https://codecov.io/gh/etasnadi/napari_nucleaizer/branch/master/graph/badge.svg?token=5XC36PA6OQ)](https://codecov.io/gh/etasnadi/napari_nucleaizer)\n[![Documentation Status](https://readthedocs.org/projects/napari-nucleaizer-docs/badge/?version=latest)](https://napari-nucleaizer-docs.readthedocs.io/en/latest/?badge=latest)\n\n<!--\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nucleaizer.svg?color=green)](https://python.org)\n[![tests](https://github.com/etasnadi/napari_nucleaizer/workflows/tests/badge.svg)](https://github.com/etasnadi/napari-nucleaizer/actions)\n[![codecov](https://codecov.io/gh/etasnadi/napari-nucleaizer/branch/master/graph/badge.svg)](https://codecov.io/gh/etasnadi/napari-nucleaizer)\n-->\n\nGUI for the nucleaAIzer method in Napari.\n\n![Plugin interface in napari.](https://github.com/etasnadi/napari_nucleaizer/blob/main/napari_screenshot.png?raw=true)\n\n## Overview\n\nThis is a napari plugin to execute the nucleaizer nuclei segmentation algorithm.\n\n### Main functionalities\n\nUsing this plugin will be able to\n\n1. Load your image into Napar, then outline the nuclei.\n2. Specify an image folder containing lots of images and an output folder, and automatically segment all of the images in the input folder.\n3. If you are not satisfied with the results, you can train your own model:\n    1. You can use our pretrained models and fine tune them on your data.\n    2. You can skip the nucleaizer pipeline and train only on your data.\n\n\n### Supported image types\n\nWe have several pretrained models for the following image modelities:\n* fluorescent microscopy images\n* IHC stained images\n* brightfield microscopy images,\n\namong others. For the detailed descriptions of our models, see: https://zenodo.org/record/6800341.\n\n### How it works?\n\nFor the description of the algorithm, see our paper: \"Hollandi et al.: nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer, Cell Systems, 2020. https://doi.org/10.1016/j.cels.2020.04.003\"\n\nThe original code (https://github.com/spreka/biomagdsb) is partially transformed into a python package (nucleaizer_backend) to actually perform the operations. See the project page of the backend at: https://github.com/etasnadi/nucleaizer_backend.\n\nIf you wish to use the web interface, check: http://nucleaizer.org.\n\n![All functionalities.](https://github.com/etasnadi/napari_nucleaizer/blob/main/nucleaizer_screenshot.png?raw=true)\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Install\n\n1. Create an environment (recommended).\n\n2. Install napari: `pip install \"napari[pyqt5]\"`. Other methods: https://napari.org/tutorials/fundamentals/installation.html\n\n3. Install the plugin into napari:\n\n    * User mode from [PyPI](https://pypi.org/project/napari-nucleaizer/): start Napari (command line: `napari`) and select he **Install/Uninstall Plugins...** under the **Plugins** menu. In the popup, filter for `napari-nucleaizer`.\n\n    * Developer mode: clone this project and use `pythhon3 -m pip install -e <path>` to install the project locally **into the same evnrionment as napari**. It has the advantage that you will have the latest version.\n## Run\n\n1. Start Napari by calling `napari` from the command line.\n2. Then, activate the plugin in the `Plugins` menu. If you successfully installed the plugin, you have to see something like this:\n\n![Plugin interface in napari.](https://github.com/etasnadi/napari_nucleaizer/blob/main/napari_plugin_launch.png?raw=true)\n\n## Further help\n\nSee the [documentation](https://napari-nucleaizer-docs.readthedocs.io/en/latest/index.html) (work in progress).\n\n## Issues\n\nUse the github issue tracker if you experinece unexpected behaviour.\n\n## Contact\n\nYou can contact me in [e-mail](mailto:tasnadi.ervin@MY-INSTITUTE) where MY-INSTITUTE is `brc.hu`.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Nuclei segmentation and training"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-segment",
    "name": "napari-segment",
    "display_name": "Segment organoid",
    "version": "0.3.12",
    "created_at": "2022-10-05",
    "modified_at": "2023-09-01",
    "authors": [
      "Andrey Aristov"
    ],
    "author_emails": [
      "aaristov@pasteur.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-segment/",
    "home_github": "https://github.com/aaristov/napari-segment",
    "home_other": null,
    "summary": "Segment organoids and measure intensities",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "dask",
      "imageio-ffmpeg",
      "matplotlib",
      "napari",
      "nd2",
      "numpy",
      "pytest-qt",
      "scikit-image",
      "zarr"
    ],
    "package_metadata_description": "# napari-segment\n\n[![License](https://img.shields.io/pypi/l/napari-segment.svg?color=green)](https://github.com/aaristov/napari-segment/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-segment.svg?color=green)](https://pypi.org/project/napari-segment)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment.svg?color=green)](https://python.org)\n[![tests](https://github.com/aaristov/napari-segment/workflows/tests/badge.svg)](https://github.com/aaristov/napari-segment/actions)\n[![codecov](https://codecov.io/gh/aaristov/napari-segment/branch/main/graph/badge.svg)](https://codecov.io/gh/aaristov/napari-segment)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment)](https://napari-hub.org/plugins/napari-segment)\n\nInteractively segment organoids/spheroids/aggregates in brightfield/fluorescence from nd2 multipositional stack.\n----------------------------------\n\n![image](https://user-images.githubusercontent.com/11408456/201948817-255717a6-5f5c-45a2-ae01-2e0cbb1e29e8.png)\n\n\n## Installation\n\n```pip install napari-segment```\n\nor\n\nFrom napari plugin\n\n![image](https://user-images.githubusercontent.com/11408456/201949692-33f94eaf-ac43-44dd-8c21-e9f9a460c5b2.png)\n\n\n## Usage for segmentation\n\n1. Drag your nd2 file into napari (otherwise try the Sample data from File / Open Sample / napari-segment)\n2. Lauch Plugins -> napari-segment: Segment multipos\n3. Select the brightfield channel\n4. The data is lazily loaded from nd2 dataset and dynamically segmented in the viewer.\n5. Binning 1-8 allows to skip small features and focus on bigger objects, also makes processing faster.\n![image](https://user-images.githubusercontent.com/11408456/201701163-70c4af51-8a3a-42a0-adb9-32f0114eb49d.png)\n6. Various preprocessing modes allow segmentation of different objects:\n![image](https://user-images.githubusercontent.com/11408456/201701809-f16a23ea-d14a-4b38-8b8c-08a113416509.png)\n\n  - Invert: will use the dark shadow around aggregate - best for very old aggregates , out of focus (File / Open Sample / napari-segment / Old aggregate)\n  \n  ![image](https://user-images.githubusercontent.com/11408456/201701950-efd86fae-d85b-471c-bb44-a0e328e26adc.png)\n\n  - Gradient: best for very sharp edges, early aggregates, single cells (File / Open Sample / napari-segment / Early aggregate) \n  \n  ![image](https://user-images.githubusercontent.com/11408456/201705697-5d0d0643-44b6-4cb9-9208-4a29dd899d8c.png)\n  \n  \n  - Gauss diff: Fluorescence images\n  The result of preprocessing will be shown in the \"Preprocessing\" layer.\n7. Smooth, Theshold and Erode parameters allow you to adjust the preliminary segmentation -> they all will appear in the \"Detections\" layer as outlines \n\n  ![image](https://user-images.githubusercontent.com/11408456/201703675-cff6bac1-bb2a-4d45-963f-6e6d00309c77.png)\n\n8. Min/max diameter and eccentricity allow you to filter out unwanted regions -> the good regions will appear in the \"selected labels\" layer as filled areas.\n\n![image](https://user-images.githubusercontent.com/11408456/201703754-2c83a8d6-70c2-444a-8e30-54a39c901cd0.png)\n![image](https://user-images.githubusercontent.com/11408456/201707025-9121f0dc-3939-48f0-ae75-884891be8d66.png)\n\n\n9. Once satisfied, click \"Save the params!\" - it will automatically create file.nd2.params.yml file, so you can recall how the segmentation was done. Next time you open the same dataset, the parameters will be loaded automatically from this file. \n\n10. Next section is for quantifying the sizes. Pixel size will be retrieved automatically from metadata. If not: update it manually and click Update plots to see the correct sizes. Click on any suspected value to see the corresponding frame and try to adjust the above parameters. \n\n![image](https://user-images.githubusercontent.com/11408456/201704881-b2303b9a-50c6-49c7-80ff-a6099cc2a151.png)\n\n11. If impossible to get good results with automatic pipeline, click Clone for manual correction: this will create an editable \"Manual\" layer which you can edin with built-in tools in napari. Click \"Update plots\" to see the updated values. \n\n12. \"Save csv!\" will generate a csv file with regionprops. \n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-segment\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/aaristov/napari-segment/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.nd2",
      "*.npy",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Segment multipos"
    ],
    "contributions_sample_data": [
      "Old aggregate",
      "Early aggregate"
    ]
  },
  {
    "normalized_name": "napari-ism",
    "name": "napari-ISM",
    "display_name": "Napari-ISM",
    "version": "1.0.7",
    "created_at": "2022-05-26",
    "modified_at": "2023-08-30",
    "authors": [
      "Alessandro Zunino"
    ],
    "author_emails": [
      "Alessandro Zunino <alessandro.zunino@iit.it>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-ism/",
    "home_github": "https://github.com/VicidominiLab/napari-ism",
    "home_other": null,
    "summary": "A Napari plugin for analysing and simulating ISM images",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "h5py",
      "PyQt5",
      "brighteyes-ism >=1.2.2",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-ISM\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ISM)](https://napari-hub.org/plugins/napari-ISM)\n[![License](https://img.shields.io/pypi/l/napari-ISM.svg?color=green)](https://github.com/VicidominiLab/napari-ISM/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ISM.svg?color=green)](https://pypi.org/project/napari-ISM)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ISM.svg?color=green)](https://python.org)\n\n\nThis plugin is built upon the python package [BrightEyes-ISM]. Napari-ISM enables the simulation, loading, and analysis of ISM datasets.\nMore in detail, it performs:\n\n* Loading and compression of .h5 files generated by the [MCS software].\n* Simulation of a realistic dataset of tubulin filaments.\n* Simulation of realistic ISM Point Spread Functions.\n* Summing over the detector array dimension\n* Adaptive Pixel Reassignment\n* Multi-image deconvolution\n* Focus-ISM\n\n----------------------------------\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-ISM` via [PyPI]:\n\n    pip install napari-ISM\n    \nor by using [napari hub].\n\nIt requires the following Python packages\n\n    numpy\n    scipy\n    h5py\n    PyQt5\n    brighteyes-ism>=1.2.0\n\n## Documentation\n\nTo generate a simulated dataset, go to `File > Open Sample > ISM dataset`. \n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/sample.png)\n\nTo acces the plugin list, go to `Plugins > Napari-ISM`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/plugin_list.png)\n\nTo open a .h5 file, go to `File > Open `.\nYou can then sum over the dimensions that are not needed, using the command `integrateDims`.\nThe default axes are 0 (repetition), 1 (axial position), and 4 (time).\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/file.png)\n\nNote that all the analysis commands expect an input with size `X x Y X Ch`.\n\nTo see the result of summing over the SPAD dimensions `Ch`, use the plugin command `Sum`. Then, press `Run`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/sum.png)\n\nTo see the result of Adaptive Pixel Reassignment, use the plugin command `APR_stack`.\nSelect as reference image (`ref`) the central one. Select an upsampling factor (`usf`), \nwhich corresponds to the sub-pixel precision of the shift-vector estimation. Then, press `Run`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/apr.png)\n\nTo generate the PSFs, use the plugin command `PSFs`. Select an image layer (`img layer`), \nit will be used to determine the number of pixels and the pixel size.\nThen, select the detector pixel size (`pxsize`) and pixel pitch (`pxpitch`) in microns.\nSelect the magnification of the system (`M`). Select the excitation (`exWl`) and emission wavelength (`emWl`) in nanometers.\nThen, press `Run`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/PSF.png)\n\nTo see the result of multi-image deconvolution, use the plugin command `Deconvolution`.\nSelect an image layer (`img layer`) containing the ISM dataset to deconvolve and another image layer (`psf layer`) containing the PSFs, either simulated or experimental.\nThen, press `Run`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/deconv.png)\n\nTo use Focus-ISM, first select a region on the input dataset using a `shapes` layer.\nSelect a rectangle containing mainly in-focus emitters. It will be used as a calibration.\nThen, use the plugin command `Focus-ISM`. Select an image layer (`img layer`) containing the ISM dataset and a shape layer (`shape layer`) defining the calibration region.\nSelect a lower bound for the standard deviation of the out-of-focus curve (`sigma B bound`) in units of standard deviations of the in-focus term. We suggest to never select a value below 2.\nSelect a threshold (`threshold`) in units of photon counts. Scan coordinates with less photons than the threshold will be skipped in the analysis and classified as background. Then, press `Run`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/shapes.png)\n\nTo use FRC, prepare the dataset to be in the shape `xyt`.\nSelect the theshodling method (`method`) and smoothing method (`smoothing`) among those available.\nThen, press `Calculate`.\n\n![](https://github.com/VicidominiLab/napari-ISM/raw/main/docs/frc.png)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU LGPL v3.0] license,\n\"napari-ISM\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/VicidominiLab/napari-ISM/issues\n\n[napari hub]: https://www.napari-hub.org/plugins/napari-ISM\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/project/napari-ISM/\n\n[BrightEyes-ISM]: https://github.com/VicidominiLab/BrightEyes-ISM\n[MCS software]: https://github.com/VicidominiLab/BrightEyes-MCS\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.h5",
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".h5",
      ".npy"
    ],
    "contributions_widgets": [
      "APR_stack",
      "Fingerprint",
      "Sum",
      "Deconvolution",
      "PSFs",
      "integrateDims",
      "Focus_ISM",
      "FRC"
    ],
    "contributions_sample_data": [
      "ISM dataset"
    ]
  },
  {
    "normalized_name": "spots-in-yeasts",
    "name": "spots-in-yeasts",
    "display_name": "spots in yeasts",
    "version": "1.2.0",
    "created_at": "2023-08-29",
    "modified_at": "2023-08-30",
    "authors": [
      "Cl√©ment H. Benedetti"
    ],
    "author_emails": [
      "clement.benedetti@mri.cnrs.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/spots-in-yeasts/",
    "home_github": "https://github.com/MontpellierRessourcesImagerie/spots-in-yeasts",
    "home_other": null,
    "summary": "A Napari plugin segmenting yeast cells and fluo spots to extract statistics.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "magic-class",
      "qtpy",
      "opencv-python",
      "matplotlib",
      "termcolor",
      "scikit-image",
      "tifffile",
      "cellpose",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# spots-in-yeasts\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/MontpellierRessourcesImagerie/spots-in-yeasts/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/spots-in-yeasts.svg?color=green)](https://pypi.org/project/spots-in-yeasts)\n[![Python Version](https://img.shields.io/pypi/pyversions/spots-in-yeasts.svg?color=green)](https://python.org)\n[![tests](https://github.com/MontpellierRessourcesImagerie/spots-in-yeasts/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/spots-in-yeasts/actions)\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/spots-in-yeasts/branch/master/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/spots-in-yeasts)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/spots-in-yeasts)](https://napari-hub.org/plugins/spots-in-yeasts)\n\nA Napari plugin segmenting yeast cells and fluo spots to extract statistics.\n\n----------------------------------\n\nThe skeleton on this [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Introduction\n\n> This Napari plugin's purpose is to extract statistics about fluo spots in yeast cells. We produce a segmentation of cells (based on the brightfield) and a segmentation of spots (based on the fluo channel). Then, we associate the measures to each cells.\n\nUnless you use the `NapariJ` plugin to open images, or the `cast_extension.ijm` script to cast files, you can only launch this plugin on `.tif` images.\n\nFor now, the code produces JSON files compiling the metrics such as:\n- The number of spots per cell.\n- The average intensity of a spot.\n- The area of each spot.\n- The location of each spot.\n\nWe provide `cast_extension.ijm` which is another script meant to be used in Fiji/ImageJ. It is able to convert `.nd` and `.czi` images into basic `.tif` images so you can open them in Napari.\n\nYou can process your images either in __one-shot__ _(image per image)_ or in __batch mode__ _(by providing the path to a folder)_. In case you used batch mode, a control image is created so you can quickly check whether your segmentation was correctly performed.\n\nRequired packages in your environment:\n- `napari`\n- `cellpose`\n- `numpy`\n- `skimage`\n- `termcolor`\n- `matplotlib`\n- `cv2`\n\n\n## Installation\n\nYou can install `spots-in-yeasts` via [pip]:\n\n    pip install spots-in-yeasts\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MontpellierRessourcesImagerie/spots-in-yeasts.git\n\n\n## Example\n\n- Your images must have exactly two channels. The number of slices in each channel is totally up to you.\n- __First channel__: fluo spots, __second channel__: brightfield.\n- If you want to use the batch mode, you must use `.tif` images.\n\nThe two following images are the __brightfield__ and __fluo spots__ channels of the same image:\n\n![Brightfield](https://dev.mri.cnrs.fr/attachments/download/3017/bf.png)\n![Spots](https://dev.mri.cnrs.fr/attachments/download/3018/fluo.png)\n\nThe following images are the __cells labels__ and the __spots positions__:\n\n![Labeled cells](https://dev.mri.cnrs.fr/attachments/download/3016/cells.png)\n![Detected spots](https://dev.mri.cnrs.fr/attachments/download/3019/spots.png)\n\n## Usage\n\n### One-shot\n\n- Open Napari. Keep the terminal opened, it provides lots of information.\n- Before starting, make sure that no layer is currently open. You can clear your viewer with the `Clear layers` button.\n- Drag'n'drop your image into the Napari viewer. It should show up in the left column.\n- Click the `Split channels` button to separate the brightfield and the fluo on two different layers. Now, you should have two layers named \"brightfield\" and \"fluo-spots\".\n- To segment yeast cells, click the `Segment cells` button. The interface will certainly freeze during a few seconds (~10/30s). A new layer should appear, containing a value of intensity for each individual cell.\n- Click on the `Segment spots` button. This is a pretty fast operation. A new layer containing spots just appeared. Spots are represented as small white dots. You can change that in the layer's settings you struggle controling the result.\n- Finally, you can use the `Extract stats` button to create a JSON file. This file will automatically be opened in your default text editor, but it is a __temporary file__, which means that it is not saved anywhere and will get lost if you don't save it yourself.\n- Once you are done, you can press the `Clear layers` button again and pass to your next image, repeating the previous steps.\n\n### Batch mode\n\n- Before starting, you need a folder containing correctly formated `.tif` files.\n- Open Napari, and keep the terminal opened to see provided information.\n- Set the `input folder` field to your folder containing `.tif` images.\n- Set the `output folder` field to the path of a folder (preferably empty) that will receive the control images and the JSON files generated by the script.\n- You can click the `Run batch` button to launch the process.\n\n__Note:__ In batch mode, your viewer won't show anything. You must rely on the terminal's content and the progress bar to know what is going on. To open the progress bar in Napari, click on `activity` in the lower-right corner.\n\n## Messages:\n\n- `Export directory set to: /some/path/to/output`: Folder provided by the user to receive produced files (JSON, controls)\n- `===== Working on: d1-230421-11S_2 (1/32)====`: Name of the image currently processed and its rank. For example here, \"d1-230421-11S_2\" is being processed and it is the first image processed from a folder containing 32 images.\n- `Selected slices: (4, 8). (11 slices available)`: The script doesn't use all the slices in the image. Instead, it detects the most is-focus slice and takes N slices before and after it. In this example, 11 slices were available in the image. We are using the slices 4, 5, 6, 7, 8 for processing, so the most in-focus one is the 6th.\n- `Segmenting cells...`: Notification that the script started segmenting yeasts cells.\n- `Cells segmentation done. 219 cells found.`: End of cells segmentation. This message also provides you with the number of indiviual detected. This number is displayed before labels touching the borders are removed.\n- `Segmented cells from d1-230421-11S_2 in 10.0s.`: Operations are timed. This is just the time report of cells segmentation.\n- `Starting spots segmentation...`: Notification that the script started segmenting spots in the fluo channel.\n- `102 spots found .`: Number of spots detected during the segmentation.\n- `Segmented spots from d1-230421-11S_2 in 1.0s.`: Duration elapsed during spots segmentation.\n- `Spots exported to: /some/path/to/output/d1-230421-11S_2.json`: Path to the exported metrics.\n- `Focused slice too far from center!`: We don't use all the slices available. We detect the most in-focus one and take N slices before and after. This message means that there isn't N slices available after (or before) the most in-focus one. The process won't get interupted, but you want to be more careful about the segmentation of this image.\n- `The image d1-230421 BG- failed to be processed.`: A basic sanity check is applied to the results before they get exported to reduce the amount of manual checking to perform. This message simply means that either the cells segmentation, or the spots segmentation is so bad that this image will be skipped.\n- `========= DONE. (288.0s) =========`: Indicates that all the images contained in your folder were processed, the batch is over. The total amount of time if also displayed.\n\n----------------------------------\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"spots-in-yeasts\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MontpellierRessourcesImagerie/spots-in-yeasts/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.ysc"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Spots In Yeasts"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-n2v",
    "name": "napari-n2v",
    "display_name": "napari n2v",
    "version": "0.1.1",
    "created_at": "2022-10-24",
    "modified_at": "2023-08-29",
    "authors": [
      "Tom Burke",
      "Joran Deschamps"
    ],
    "author_emails": [
      "joran.deschamps@fht.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-n2v/",
    "home_github": "https://github.com/juglab/napari-n2v",
    "home_other": null,
    "summary": "A self-supervised denoising algorithm now usable by all in napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "scikit-image",
      "bioimageio.core",
      "n2v >=0.3.2",
      "napari-time-slicer >=0.4.9",
      "napari",
      "qtpy",
      "pyqtgraph",
      "tensorflow >=2.10.0 ; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "tensorflow-metal ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "numpy <1.24.0 ; python_version < \"3.9\"",
      "numpy ; python_version >= \"3.9\"",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-n2v\n\n[![License](https://img.shields.io/pypi/l/napari-n2v.svg?color=green)](https://github.com/juglab/napari-n2v/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-n2v.svg?color=green)](https://pypi.org/project/napari-n2v)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-n2v.svg?color=green)](https://python.org)\n[![tests](https://github.com/juglab/napari-n2v/workflows/build/badge.svg)](https://github.com/juglab/napari-n2v/actions)\n[![codecov](https://codecov.io/gh/juglab/napari-n2v/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/napari-n2v)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-n2v)](https://napari-hub.org/plugins/napari-n2v)\n\nA self-supervised denoising algorithm now usable by all in napari.\n\n<img src=\"https://raw.githubusercontent.com/juglab/napari-n2v/master/docs/images/noisy_denoised.png\" width=\"800\" />\n----------------------------------\n\n## Installation\n\nCheck out the [documentation](https://juglab.github.io/napari-n2v/installation.html) for more detailed installation \ninstructions. \n\nYou can then start the napari plugin by clicking on \"Plugins > napari_n2v > Training\",\nor run the plugin directly from a [script](scripts/start_plugin.py).\n\n\n\n## Quick demo\n\nYou can try out a demo by loading the `N2V Demo prediction` plugin and directly clicking on `Predict`. This model was trained using the [N2V2 example](https://juglab.github.io/napari-n2v/examples.html).\n\n\n<img src=\"https://raw.githubusercontent.com/juglab/napari-n2v/master/docs/images/demo.gif\" width=\"800\" />\n\n\n## Documentation\n\nDocumentation is available on the [project website](https://juglab.github.io/napari-n2v/).\n\n\n## Contributing and feedback\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request. You can also \nhelp us improve by [filing an issue] along with a detailed description or contact us\nthrough the [image.sc](https://forum.image.sc/) forum (tag @jdeschamps).\n\n\n## Citations\n\n### N2V\n\nAlexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"[Noise2void-learning denoising from single noisy images.](https://ieeexplore.ieee.org/document/8954066)\" \n*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2019.\n\n### structN2V\n\nColeman Broaddus, et al. \"[Removing structured noise with self-supervised blind-spot networks.](https://ieeexplore.ieee.org/document/9098336)\" *2020 IEEE 17th \nInternational Symposium on Biomedical Imaging (ISBI)*. IEEE, 2020.\n\n### N2V2\n\nEva Hoeck, Tim-Oliver Buchholz, et al. \"[N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture](https://arxiv.org/abs/2211.08512)\", arXiv (2022). \n\n## Acknowledgements\n\nThis plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the \nChan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant _2021-240383_.\n\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-n2v\" is a free and open source software.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[filing an issue]: https://github.com/juglab/napari-n2v/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "N2V Train",
      "N2V Predict",
      "N2V Demo prediction"
    ],
    "contributions_sample_data": [
      "Download data (2D)",
      "Download data (3D)",
      "Download data (RGB)",
      "Download data (SEM)"
    ]
  },
  {
    "normalized_name": "napari-clipboard",
    "name": "napari-clipboard",
    "display_name": "napari clipboard",
    "version": "0.0.1",
    "created_at": "2023-08-28",
    "modified_at": "2023-08-28",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czi@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-clipboard/",
    "home_github": "https://github.com/kephale/napari-clipboard",
    "home_other": null,
    "summary": "A plugin for creating napari layers from the System clipboard",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-clipboard\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-clipboard.svg?color=green)](https://github.com/kephale/napari-clipboard/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-clipboard.svg?color=green)](https://pypi.org/project/napari-clipboard)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-clipboard.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-clipboard/workflows/tests/badge.svg)](https://github.com/kephale/napari-clipboard/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-clipboard/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-clipboard)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-clipboard)](https://napari-hub.org/plugins/napari-clipboard)\n\nA plugin for creating napari layers from the System clipboard\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-clipboard` via [pip]:\n\n    pip install napari-clipboard\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-clipboard.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-clipboard\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-clipboard/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "New Image from clipboard"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "large-image-viewer",
    "name": "Large-Image-Viewer",
    "display_name": "Large Image Viewer",
    "version": "1.1.0",
    "created_at": "2023-08-08",
    "modified_at": "2023-08-21",
    "authors": [
      "Nima Mojtahedi"
    ],
    "author_emails": [
      "nima.mojtahedi@wysscenter.ch"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/large-image-viewer/",
    "home_github": "https://github.com/WyssCenter/Large-Image-Viewer",
    "home_other": null,
    "summary": "A simple plugin to view large images",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "dask[array]",
      "dask-image",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# Large-Image-Viewer\n\n[![License MIT](https://img.shields.io/pypi/l/Large-Image-Viewer.svg?color=green)](https://github.com/WyssCenter/Large-Image-Viewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Large-Image-Viewer.svg?color=green)](https://pypi.org/project/Large-Image-Viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/Large-Image-Viewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/WyssCenter/Large-Image-Viewer/workflows/tests/badge.svg)](https://github.com/WyssCenter/Large-Image-Viewer/actions)\n[![codecov](https://codecov.io/gh/WyssCenter/Large-Image-Viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/WyssCenter/Large-Image-Viewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/Large-Image-Viewer)](https://napari-hub.org/plugins/Large-Image-Viewer)\n\nA simple plugin to view large images\n\n----------------------------------\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `Large-Image-Viewer` via [pip]:\n\n    pip install Large-Image-Viewer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/WyssCenter/Large-Image-Viewer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"Large-Image-Viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/WyssCenter/Large-Image-Viewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n\n\n# Napari Large Image Viewer Plugin\n\nThe Napari Large Image Viewer Plugin is a powerful extension for the [napari](https://napari.org/) image visualization software. This plugin is designed to enable the visualization of large TIFF | TIF  files directly from disk, without the need to load the entire image into RAM. This is particularly useful when working with large datasets that exceed the available memory of your system.\n\n\n## Features\n\n- **Efficient Large Image Visualization**: The plugin allows you to open and visualize large files that are too big to fit into memory. It utilizes efficient memory-mapping techniques to display image data without fully loading it into RAM.\n\n- **Interactive Exploration**: With the Napari Large Image Viewer Plugin, you can interactively explore large datasets using familiar zooming, panning, and slicing actions.\n\n- **Quick Installation**: Installing the plugin is simple and straightforward, and it seamlessly integrates with the napari environment.\n\n- **User-Friendly Interface**: The plugin provides an intuitive user interface that integrates seamlessly into the napari interface, making it easy to use for both beginners and experienced users.\n\n## Installation\n\n1. **Prerequisites**: Make sure you have [napari](https://napari.org/) installed on your system. If not, you can install it using:\n\n   ```bash\n   pip install napari\n   ```\n\n2. **Install the Plugin**: You can install the plugin directly from GitHub using pip:\n\n   ```bash\n   pip install git+https://github.com/WyssCenter/Large-Image-Viewer.git\n   ```\n\n3. **Launch napari**: Launch napari from your terminal:\n\n   ```bash\n   napari\n   ```\n\n4. **Activate the Plugin**: Once napari is launched, go to the `Plugins` menu and select `Large Image Viewer` to activate the plugin.\n\n5. **Open Large TIFF | TIF  File**: With the plugin activated, you can now open a large file by dragging and dropping it to the napari viewer.\n\n## Usage\n\n1. Open a Large TIFF | TIF  File: Follow the installation instructions above to open a large TIFF | TIF  file using the plugin.\n\n2. Explore the Image: Once the image is loaded, you can use the mouse to zoom in/out, pan, and interactively explore the data. You can also adjust the colormap, contrast, and other visualization settings from the napari interface.\n\n3. Slicing and Navigation: Use the slicing and navigation tools in napari to navigate through different sections of the large file.\n\n4. Save Visualizations: You can save snapshots or screenshots of the current visualization using the napari interface.\n\n## Contributions\n\nContributions to the Napari Large Image Viewer Plugin are welcome! If you encounter issues or have suggestions for improvements, please open an issue on the [GitHub repository](https://github.com/WyssCenter/Large-Image-Viewer.git).\n\n## License\n\nThis plugin is licensed under the [MIT License](LICENSE).\n\n## Contact\n\nFor any inquiries or questions, you can reach out to the author at nima.mojtahedi@wysscenter.ch\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cupy-image-processing",
    "name": "napari-cupy-image-processing",
    "display_name": "napari-cupy-image-processing",
    "version": "0.4.1",
    "created_at": "2021-10-22",
    "modified_at": "2023-08-17",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-cupy-image-processing/",
    "home_github": "https://github.com/haesleinhuepf/napari-cupy-image-processing",
    "home_other": null,
    "summary": "GPU-accelerated image processing using CUDA",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine >=0.1.4",
      "numpy",
      "toolz",
      "cupy",
      "napari-tools-menu",
      "scikit-image",
      "napari-time-slicer >=0.4.8",
      "napari-skimage-regionprops",
      "napari-assistant",
      "stackview >=0.3.2"
    ],
    "package_metadata_description": "# napari-cupy-image-processing\n\n[![License](https://img.shields.io/pypi/l/napari-cupy-image-processing.svg?color=green)](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cupy-image-processing.svg?color=green)](https://pypi.org/project/napari-cupy-image-processing)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cupy-image-processing.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-cupy-image-processing/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-cupy-image-processing/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-cupy-image-processing/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-cupy-image-processing)\n[![Development Status](https://img.shields.io/pypi/status/napari-cupy-image-processing.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cupy-image-processing)](https://napari-hub.org/plugins/napari-cupy-image-processing)\n\n\nGPU-accelerated image processing using [cupy](https://cupy.dev) and [CUDA](https://en.wikipedia.org/wiki/CUDA)\n\n## Usage\n\nThis napari plugin adds some menu entries to the Tools menu. You can recognize them with their suffix `(n-cupy)` in brackets.\nFurthermore, it can be used from the [napari-assistant](https://www.napari-hub.org/plugins/napari-assistant) graphical user interface. \nTherefore, just click the menu `Tools > Utilities > Assistant (na)` or run `naparia` from the command line.\n\n![img.png](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/main/docs/screenshot-with-tools-menu.png)\n\nYou can also call operations from python, e.g. as shown in this [demo notebook](https://github.com/haesleinhuepf/napari-cupy-image-processing/raw/main/docs/demo.ipynb).\n\n## Installation\n\nYou can install `napari-cupy-image-processing` using conda:\n\n    mamba install -c conda-forge cupy cudatoolkit=11.2 napari-cupy-image-processing\n\n## Troubleshooting installation\n\nIn case of issues, follow the [instructions for installing cupy](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-conda-forge). \n\nA more detailed example for installation (change 11.2 to your desired CUDA version):\n```\nmamba create --name cupy_p39 python=3.9 \nconda activate cupy_p39\n```\nAnd then:\n```\nmamba install -c conda-forge cupy cudatoolkit=11.2 napari-cupy-image-processing\n```\n\n## Contributing\n\nContributions are very welcome. Adding [cupy ndimage](https://docs.cupy.dev/en/stable/reference/ndimage.html) functions is quite easy as you can see in the \n[implementation of the current operations](https://github.com/haesleinhuepf/napari-cupy-image-processing/blob/main/napari_cupy_image_processing/_cupy_image_processing.py#L48). \nIf you need another function in napari, just send a PR. Please make sure the tests pass locally before submitting a PR.\n\n```\npip install pytest-cov pytest-qt\npytest --cov=napari_cupy_image_processing\n```\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-cupy-image-processing\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-cupy-image-processing/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "gaussian_filter",
      "gaussian_laplace",
      "median_filter",
      "percentile_filter",
      "white_tophat",
      "morphological_gradient",
      "morphological_laplace",
      "wiener",
      "threshold_otsu",
      "binary_fill_holes",
      "label",
      "black_tophat",
      "minimum_filter",
      "maximum_filter",
      "binary_closing",
      "binary_erosion",
      "binary_opening",
      "binary_dilation",
      "measurements"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "okapi-em",
    "name": "okapi-em",
    "display_name": "napari okapi-em",
    "version": "0.0.10",
    "created_at": "2022-10-19",
    "modified_at": "2023-08-14",
    "authors": [
      "Luis Perdigao"
    ],
    "author_emails": [
      "luis.perdigao@rfi.ac.uk"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/okapi-em/",
    "home_github": null,
    "home_other": "None",
    "summary": "napari plugin to deal with charging artifacts in tomography electron microscopy data",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "chafer",
      "napari[all]",
      "opencv-python",
      "quoll >=0.0.4",
      "imageio-ffmpeg ; extra == 'all'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "package_metadata_description": "# okapi-em\n\nhttps://github.com/rosalindfranklininstitute/okapi-em\n\n<!--\n[![License](https://img.shields.io/pypi/l/okapi-em.svg?color=green)](https://github.com/rosalindfranklininstitute/okapi-em/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/okapi-em.svg?color=green)](https://pypi.org/project/okapi-em)\n[![Python Version](https://img.shields.io/pypi/pyversions/okapi-em.svg?color=green)](https://python.org)\n[![tests](https://github.com/perdigao1/okapi-em/workflows/tests/badge.svg)](https://github.com/rosalindfranklininstitute/okapi-em/actions)\n[![codecov](https://codecov.io/gh/perdigao1/okapi-em/branch/main/graph/badge.svg)](https://codecov.io/gh/rosalindfranklininstitute/okapi-em)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/okapi-em)](https://napari-hub.org/plugins/okapi-em)\n-->\n\nA napari plugin for processing serial-FIB-SEM data.\n\nPowered by [chafer] and [quoll].\n\n\nA full description of this software is presented in biorXiv preprint paper:\n\nhttps://doi.org/10.1101/2022.12.15.520541\n\nThis [napari] plugin contains the following tools:\n\n- slice alignment using constrained SIFT\n- two charge artifact suppression filters\n    - directional fourier bandapass filter\n    - line-by-line filter function optimiser and subtraction (requires charge artifact labels) - uses [chafer]\n- fourier ring correlation (FRC) resolution estimation - uses [quoll]\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `okapi-em` via [pip]:\n\n`>pip install okapi-em`\n\nor using napari's plugin installation engine `Plugins->Install/Uninstall Plugins...` and filter for **Okapi-EM**.\n\nFor installing in development mode , clone this package then navigate to the cloned `okapi-em` folder and run:\n\n`>pip install -e .`\n\nOkapi-EM is a napari plugin. Launching napari is therefore required.\n\n`>napari`\n\nand then navigate `Menu->Plugins->Okapi-EM`\n\nNote that to launch napari in older versions of python (<=3.7) you will need to use the command:\n\n`>python -m napari`\n\n## Computing requirements\nOkapi-EM does not require powerful computers to run. None of the tools use GPU accelaration.\n\nThe minimum recommended RAM depends on the size of the data being used in napari. For a full image stack of 1Gb, it is recommended that user ensure that 3Gb of RAM is available or can be used. Modern OS's can extend physical RAM using `swap` memory (Linux) or cache (in Windows and also known as virtual memory), but processing can be significantly slower.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"okapi-em\" is free and open source software\n\n## Citing\n\nPlease cite usage using the following reference.\n\nPerdig√£o, L. M. A. et al. Okapi-EM ‚Äì a napari plugin for processing and analysing cryogenic serial FIB/SEM images. 2022.12.15.520541 Preprint at https://doi.org/10.1101/2022.12.15.520541 (2022).\n\n\n## Issues\n\nThere is currently a known issue with napari running in Linux machines, that it does not find the OpenGL driver correctly.\nThis will hopefully be resolved in the near future. If you bump into this issue we recommend trying to downgrade the python version. This is not an Okapi-EM problem.\n\nIf you encounter any problems, please file an issue along with a detailed description.\n\n[quoll]: https://github.com/rosalindfranklininstitute/quoll\n[chafer]: https://github.com/rosalindfranklininstitute/chafer\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Okapi-EM"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-boardgame-maker",
    "name": "napari-boardgame-maker",
    "display_name": "FooBar Segmentation",
    "version": "0.0.2",
    "created_at": "2023-08-13",
    "modified_at": "2023-08-13",
    "authors": [
      "Johannes Soltwedel"
    ],
    "author_emails": [
      "johannes_richard.soltwedel@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-boardgame-maker/",
    "home_github": null,
    "home_other": "None",
    "summary": "Make boardgame tiles",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari-stl-exporter",
      "vedo",
      "napari-tools-menu",
      "imagecodecs",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-boardgame-maker\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-boardgame-maker.svg?color=green)](https://github.com/jo-mueller/napari-boardgame-maker/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-boardgame-maker.svg?color=green)](https://pypi.org/project/napari-boardgame-maker)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-boardgame-maker.svg?color=green)](https://python.org)\n[![tests](https://github.com/jo-mueller/napari-boardgame-maker/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-boardgame-maker/actions)\n[![codecov](https://codecov.io/gh/jo-mueller/napari-boardgame-maker/branch/main/graph/badge.svg)](https://codecov.io/gh/jo-mueller/napari-boardgame-maker)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boardgame-maker)](https://napari-hub.org/plugins/napari-boardgame-maker)\n\nThis plugin turns 2D grayscale images into 3D-printable landscape tiles for a certain all-time tabletop boardgame which revolves around building settlements, obtaining ressources expanding and collecting more points than your opponents.\n\nIn short, images (for instance, [digital elevation models](https://en.wikipedia.org/wiki/Digital_elevation_model)) can be turned into surfaces like this:\n\n| Image | Created tile|\n| --- | --- |\n| <img src=\"https://github.com/jo-mueller/napari-boardgame-maker/raw/main/docs/imgs/sample.png\"> | <img src=\"https://github.com/jo-mueller/napari-boardgame-maker/raw/main/docs/imgs/sample_as_tile.png\"> |\n\n## Data\n\nIn principle, all 2D grayscale image data can be used to create a tile. However, using digital elevation models is particularly cool. Such data is publicly available at [OpenTopography.org](https://portal.opentopography.org/raster?opentopoID=OTSDEM.032021.4326.2). Acknowledgement:\n\n```text\n NASA JPL. NASADEM Merged DEM Global 1 arc second V001. 2020, distributed by NASA EOSDIS Land Processes DAAC, https://doi.org/10.5067/MEaSUREs/NASADEM/NASADEM_HGT.001.\n```\n## Usage\n\nTo use the boardgame tile maker, open it from the plugins menu (`Plugins > napari-boardgame-maker: Boardgame Tile Maker`) or from the tools menu (`Tools > Boardgame tile maker (npbgm)`). There are a few steps and parameters to set before the tile can be created.\n\n[](https://github.com/jo-mueller/napari-boardgame-maker/raw/main/docs/imgs/GUI_screenshot.jpg)\n\nClicking on `Make hexagon` and `Make number field` will create a hexagonal shape in the viewer (which will be the outline of the tile) and a circular field (which can later be used to put some markers, figures, chips, etc. On the center of the board).\n\n![](https://github.com/jo-mueller/napari-boardgame-maker/raw/main/docs/imgs/sample_with_shapes.png)\n\nThe next step is to set the parameters for the tile. The following parameters can be set:\n\n### Radii and sizes\n\nThe following sketch shows the different radii and sizes that can be set:\n\n![](https://github.com/jo-mueller/napari-boardgame-maker/raw/main/docs/imgs/stride_and_town.png)\n\n- `hexagon radius`: The radius of the hexagon (in pixels). Upon export, this will be rescaled to a desired physical size in mm.\n- `number field radius`: The radius of the number field (in pixels). Can also be set in mm units. The pixels are changed accordingly if the size of the whole hexagon is changed.\n- `stride`: The region next to the edge of the tile that should remain flat.\n- `town radius`: A circular region around the edges of the hexagonal tiles that should remain flat.\n\n### Topography\n\nThe following parameters can be set to create the topography of the tile:\n\n![](https://github.com/jo-mueller/napari-boardgame-maker/raw/main/docs/imgs/slope_and_heights.png)\n\n- `slope`: Adds a smooth transition of a given width between the edge of the cropped topography and the level of the base platte. Setting this to zero will result in a sharp edge.\n- `z-multiplier`: The height of the topography is multiplied by this factor. This can be used to scale the topography to the desired height.\n- `Plate thickness`: The thickness of the base plate (in mm).\n\n### Export\n\n- CLicking on `produce tile` will run the workflow to create the tile\n- Clicking `Export` will open a dialog to save the tile as an `.stl` file. *Note*: The tile will be exported in the size of the hexagon radius. If the hexagon radius is set to 100 mm, the tile will be exported as a 100 mm hexagon.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-boardgame-maker` via [pip]:\n\n    pip install napari-boardgame-maker\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-boardgame-maker\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Boardgame Tile Maker"
    ],
    "contributions_sample_data": [
      "Rhone glacier (DEM)"
    ]
  },
  {
    "normalized_name": "napari-zelda",
    "name": "napari-zelda",
    "display_name": "napari-zelda",
    "version": "0.1.12",
    "created_at": "2021-10-17",
    "modified_at": "2023-08-10",
    "authors": [
      "Rocco D'Antuono",
      "Giuseppina Pisignano"
    ],
    "author_emails": [
      "rocco.dantuono@hotmail.it"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-zelda/",
    "home_github": "https://github.com/RoccoDAnt/napari-zelda",
    "home_other": null,
    "summary": "ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "datatable",
      "json5",
      "magicgui",
      "matplotlib>=3.4.3",
      "napari!=0.4.11",
      "napari-plugin-engine>=0.1.4",
      "numpy",
      "pandas",
      "scikit-image",
      "scipy"
    ],
    "package_metadata_description": "# napari-zelda\n\n[![License](https://img.shields.io/pypi/l/napari-zelda.svg?color=green)](https://github.com/RoccoDAnt/napari-zelda/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-zelda.svg?color=green)](https://pypi.org/project/napari-zelda)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-zelda.svg?color=green)](https://python.org)\n[![tests](https://github.com/RoccoDAnt/napari-zelda/workflows/tests/badge.svg)](https://github.com/RoccoDAnt/napari-zelda/actions)\n[![codecov](https://codecov.io/gh/RoccoDAnt/napari-zelda/branch/master/graph/badge.svg)](https://codecov.io/gh/RoccoDAnt/napari-zelda)\n\n## ZELDA: a 3D Image Segmentation and Parent-Child relation plugin for microscopy image analysis in napari\n#### Authors: Rocco D'Antuono, Giuseppina Pisignano\n\n###### Article: Front. Comput. Sci., 04 January 2022 | https://doi.org/10.3389/fcomp.2021.796117\n\n###### Examples of 2D and 3D data sets: [https://doi.org/10.5281/zenodo.5651284](https://zenodo.org/record/5651284#.YYgn_WDP2Ch)\n----------------------------------\n\n## What you can do with ZELDA plugin for napari\nThe plugin can be used to analyze 2D/3D image data sets.  \nMultidimensional images (each channel corresponding to a napari layer) can be used to:\n\n1. Segment objects such as cells and organelles in 2D/3D.\n\n2. Segment two populations in 2D/3D (e.g. cells and organelles, nuclei and nuclear spots, tissue structures and cells) establishing the \"Parent-Child\" relation: count how many mitochondria are contained in each cell, how many spots localize in every nucleus, how many cells are within a tissue compartment.\n\n  Example: cell cytoplasms (parent objects) and mitochondria (child objects)\n  ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488.png) <br> **Actin** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT.png) <br> **Mitochondria**| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488_MT.png) <br> **Merge**\n  ------ | ------| -----\n  ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-AF488_parents.png) <br> **Parent cell cytoplasms** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT_children.png) <br> **Children mitochondria**| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/2D-MT_childrenbyParent.png) <br> **Children labelled by Parents**\n\nThe images shown above are available in the [**docs**](https://github.com/RoccoDAnt/napari-zelda/tree/main/docs) folder of this repository and were segmented using ZELDA with the following parameters:\n\n\n   | **Parent objects** | **GB: sigma=2.0-> Th_parents=60.0-> DistMap-> Maxima: min_dist=10** |\n   | -----|  ----|\n   | **Children objects** | **GB: sigma=0.3-> Th_children=450.0 -> DistMap-> Maxima: min_dist=2**|\n\nFor small monitors it may be convenient to float the protocol panel\n\n  |![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin-set_panel_to_float.png) <br> **Float a panel in napari** |\n  ------ |\n\n3. Plot results within napari interface.\n\n    ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Plot_hist_Area.png) <br> **Histogram** | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Plot_scatter_Area-EqDiam.png) <br> **Scatterplot**|\n    ------ | ------|\n\n4. Customize an image analysis workflow in graphical mode (no scripting knowledge required).\n\n    | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/CustomProtocol.png) <br> **Custom image analysis workflow** |\n    ------ |\n\n5. Import and Export Protocols (image analysis workflows) in graphical mode (share with the community!).\n\n    | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_Import_and_Export_Protocols.png) <br> **Import and Export of ZELDA Protocols** |\n    ------ |\n\n## Installation\n\n**Option A.** The easiest option is to use the napari interface to install ZELDA (make sure napari!=0.4.11):\n1. Plugins / Install/Uninstall Package(s)\n\n  ![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_install_in_napari.png)\n\n2. Choose ZELDA\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_install_ZELDA_in_napari_Arrow.png)\n\n3. ZELDA is installed\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Plugin_installed_ZELDA_in_napari_Arrow.png)\n\n4. Launch ZELDA\n![](https://raw.githubusercontent.com/RoccoDAnt/napari-zelda/main/docs/Clipboard_ZELDA_Launch_ZELDA.png)\n\n\n**Option B.** You can install `napari-zelda` also via [pip]. For the best experience, create a conda environment and use napari!=0.4.11, using the following instructions:\n\n    conda create -y -n napari-env python=3.8  \n    conda activate napari-env\n    conda install napari pyqt  \n    pip install napari-zelda  \n\n\n**Option C.** Alternatively, clone the repository and install locally via [pip]:\n\n    pip install -e .\n\n**Option D.** Get the latest code with [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) and [pip]:\n\n    conda create -y -n napari-env python=3.8 git\n    conda activate napari-env\n    conda install napari pyqt\n    pip install git+https://github.com/RoccoDAnt/napari-zelda.git\n\n\n## Specifications\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\nThe GUI has been developed using [magicgui](https://github.com/napari/magicgui) widgets, while the image analysis and processing include functions from [scikit-image](https://scikit-image.org/), [SciPy](https://scipy.org/), and [NumPy](https://numpy.org/). Results are handled with [pandas](https://pandas.pydata.org/) and [datatable](https://datatable.readthedocs.io/en/latest/). Plots are obtained with [matplotlib](https://matplotlib.org/).  \n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n\n## Contributing\n\nContributions are welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\nUsers can add new protocol steps to their local installation using [magicgui](https://github.com/napari/magicgui) widgets.\nCode can be added at the end of napari_zelda.py file:\n\n>###Add here new functionalities for ZELDA ###\n>\n>###@magicgui(layout=\"vertical\")\n>\n>###def new_functionality_widget(viewer: 'napari.Viewer'):\n>\n>###...\n>\n>###\n>\n>###End###\n\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-zelda\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/RoccoDAnt/napari-zelda/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "launch_ZELDA"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-emd",
    "name": "napari-EMD",
    "display_name": "EMD File Viewer",
    "version": "0.1.1",
    "created_at": "2023-08-01",
    "modified_at": "2023-08-08",
    "authors": [
      "Nicolette Shaw"
    ],
    "author_emails": [
      "shaw.nicki@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-emd/",
    "home_github": "https://github.com/NickiShaw/napari-EMD.git",
    "home_other": null,
    "summary": "A simple plugin to view .emd files in napari (Velox files)",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "h5py",
      "magicgui",
      "ujson",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-EMD\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-EMD.svg?color=green)](https://github.com/NickiShaw/napari-EMD/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-EMD.svg?color=green)](https://pypi.org/project/napari-EMD)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-EMD.svg?color=green)](https://python.org)\n[![tests](https://github.com/NickiShaw/napari-EMD/workflows/tests/badge.svg)](https://github.com/NickiShaw/napari-EMD/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-EMD)](https://napari-hub.org/plugins/napari-EMD)\n\nA simple plugin to view .emd files in napari (i.e. Velox files). Allows users to track metadata as it changes over the course of a video/stack, developed for analysis of in-situ microscopy data, where users may be changing magnification, focus, etc. during aquisition.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-EMD` via [pip]:\n\n    `pip install napari-EMD`\n\nYou can install napari and access the plugin through the GUI. [Reccomended install command for napari](https://napari.org/stable/tutorials/fundamentals/installation.html):\n\n    `python -m pip install \"napari[all]\"`\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-EMD\" is free and open source software\n\n## Issues and Requests\n\n> **Warning: The metadata viewer does not work in the current [Napari bundle](https://napari.org/stable/tutorials/fundamentals/installation.html#install-as-a-bundled-app) version (August 2023). Use the [python package version](https://napari.org/stable/tutorials/fundamentals/installation.html#install-as-python-package-recommended) of Napari for this feature.**\n\nIf you encounter any problems or would like any functionality added, please [file an issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/creating-an-issue) along with a detailed description.\n\nCurrent maintainer(s): [Nicki Shaw](https://docs.github.com/en/issues/tracking-your-work-with-issues/creating-an-issue)\n\n## Preview\n\nImages A and B show different frames in the same image stack, the metadata plugin on the right shows the changing focus value.\n![NapariEMD screenshots](Images/napariEMD_screenshots.jpg)\n\n## To Do\n\n- Attatch last-opened information, so the widget does not reset when frames are changed and open toggle options are open remain.\n- Add a search bar for navigating metadata.\n- Output metadata as file option.\n- Add note to change order of open files to replacee active metadata view.\n- Make Singleframe note update automatically on change of file order.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.emd"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "EMD Reader Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-merge-stardist-masks",
    "name": "napari-merge-stardist-masks",
    "display_name": "StarDist OPP",
    "version": "0.1.1",
    "created_at": "2022-08-31",
    "modified_at": "2023-08-08",
    "authors": [
      "Niklas Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-merge-stardist-masks/",
    "home_github": "https://github.com/gatoniel/napari-merge-stardist-masks",
    "home_other": null,
    "summary": "Segment non-star-convex objects with StarDist by merging masks.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "importlib-resources",
      "stardist-napari >=2022.7.5",
      "merge-stardist-masks >=0.1.0",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# StarDist OPP napari plugin\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-merge-stardist-masks.svg?color=green)](https://github.com/gatoniel/napari-merge-stardist-masks/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-merge-stardist-masks.svg?color=green)](https://pypi.org/project/napari-merge-stardist-masks)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-merge-stardist-masks.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-merge-stardist-masks/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-merge-stardist-masks/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-merge-stardist-masks/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-merge-stardist-masks)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-merge-stardist-masks)](https://napari-hub.org/plugins/napari-merge-stardist-masks)\n\nThis is the [napari] plugin for [StarDist OPP]. Checkout our [paper] for more information.\n\n----------------------------------\n\n## Usage\n\nRead the [tutorial] and download pre-trained models from our [Zenodo repository].\n\nIn PowerShell, when you do not have sufficient GPU support, run napari without CUDA support, i.e.,:\n```\n$env:CUDA_VISIBLE_DEVICES=-1; napari\n```\n\n\n## Installation\n\nYou can install `napari-merge-stardist-masks` via [pip]:\n\n    pip install napari-merge-stardist-masks\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-merge-stardist-masks.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-merge-stardist-masks\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## How to cite\n```bibtex\n@article{https://doi.org/10.1111/mmi.15064,\nauthor = {Jelli, Eric and Ohmura, Takuya and Netter, Niklas and Abt, Martin and Jim√©nez-Siebert, Eva and Neuhaus, Konstantin and Rode, Daniel K. H. and Nadell, Carey D. and Drescher, Knut},\ntitle = {Single-cell segmentation in bacterial biofilms with an optimized deep learning method enables tracking of cell lineages and measurements of growth rates},\njournal = {Molecular Microbiology},\nvolume = {n/a},\nnumber = {n/a},\npages = {},\nkeywords = {3D segmentation, biofilm, deep learning, image analysis, image cytometry, Vibrio cholerae},\ndoi = {https://doi.org/10.1111/mmi.15064},\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mmi.15064},\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/mmi.15064},\nabstract = {Abstract Bacteria often grow into matrix-encased three-dimensional (3D) biofilm communities, which can be imaged at cellular resolution using confocal microscopy. From these 3D images, measurements of single-cell properties with high spatiotemporal resolution are required to investigate cellular heterogeneity and dynamical processes inside biofilms. However, the required measurements rely on the automated segmentation of bacterial cells in 3D images, which is a technical challenge. To improve the accuracy of single-cell segmentation in 3D biofilms, we first evaluated recent classical and deep learning segmentation algorithms. We then extended StarDist, a state-of-the-art deep learning algorithm, by optimizing the post-processing for bacteria, which resulted in the most accurate segmentation results for biofilms among all investigated algorithms. To generate the large 3D training dataset required for deep learning, we developed an iterative process of automated segmentation followed by semi-manual correction, resulting in >18,000 annotated Vibrio cholerae cells in 3D images. We demonstrate that this large training dataset and the neural network with optimized post-processing yield accurate segmentation results for biofilms of different species and on biofilm images from different microscopes. Finally, we used the accurate single-cell segmentation results to track cell lineages in biofilms and to perform spatiotemporal measurements of single-cell growth rates during biofilm development.}\n}\n```\n\n## Credits\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[paper]: https://doi.org/10.1111/mmi.15064\n[StarDist OPP]: https://github.com/gatoniel/merge-stardist-masks\n[tutorial]: https://merge-stardist-masks.readthedocs.io/en/latest/napari-plugin.html\n[Zenodo repository]: https://doi.org/10.5281/zenodo.7704410\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-merge-stardist-masks/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "StarDist OPP"
    ],
    "contributions_sample_data": [
      "StarDist OPP sample data"
    ]
  },
  {
    "normalized_name": "napari-superres",
    "name": "napari-superres",
    "display_name": "superres",
    "version": "0.1.1",
    "created_at": "2023-06-29",
    "modified_at": "2023-08-08",
    "authors": [
      "\"Rocco D'Antuono",
      "Ad√°n Guerrero",
      "Ra√∫l Pinto C√°mara",
      "Pa√∫l Hern√°ndez Herrera",
      "Esley Torres Garcia",
      "Haydee Hern√°ndez",
      "Juli√°n Mej√≠a\""
    ],
    "author_emails": [
      "rocco.dantuono@hotmail.it"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-superres/",
    "home_github": "https://github.com/RoccoDAnt/napari-superres",
    "home_other": null,
    "summary": "Fluorescence Fluctuation-based Super Resolution (FF-SRM) Methods",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "matplotlib",
      "magicgui",
      "qtpy",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-superres\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-superres.svg?color=green)](https://github.com/RoccoDAnt/napari-superres/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-superres.svg?color=green)](https://pypi.org/project/napari-superres)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-superres.svg?color=green)](https://python.org)\n[![tests](https://github.com/RoccoDAnt/napari-superres/workflows/tests/badge.svg)](https://github.com/RoccoDAnt/napari-superres/actions)\n[![codecov](https://codecov.io/gh/RoccoDAnt/napari-superres/branch/main/graph/badge.svg)](https://codecov.io/gh/RoccoDAnt/napari-superres)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/RoccoDAnt/napari-superres)](https://napari-hub.org/plugins/napari-superres)\n\n\nA collection of super-resolution microscopy FF-SRM methods.\n\nOpen-source implementation of methods for Fluorescence Fluctuation based Super Resolution Microscopy (FF-SRM):\n\nReview: [Alva et al., 2022. ‚ÄúFluorescence Fluctuation-Based Super-Resolution Microscopy: Basic Concepts for an Easy Start.‚Äù Journal of Microscopy, August.](https://onlinelibrary.wiley.com/doi/10.1111/jmi.13135)\n\nMSSR article: [Torres-Garc√≠a, E., Pinto-C√°mara, R., Linares, A. et al. Extending resolution within a single imaging frame. Nat Commun 13, 7452 (2022).](https://doi.org/10.1038/s41467-022-34693-9)\n\nESI article: [Idir Yahiatene, Simon Hennig, Marcel M√ºller, Thomas Huser (2015/2016). \"Entropy-based Super-resolution Imaging (ESI): From Disorder to Fine Detail\" ACS Photonics 8, 2 (2015)](https://doi.org/10.1021/acsphotonics.5b00307)\n\nSOFI article: [T. Dertinger, R. Colyer, G. Iyer, and J. Enderlein. Fast, background-free, 3D super-resolution optical fluctuation imaging (SOFI). PNAS 52, 106 (2009) ](https://doi.org/10.1073/pnas.0907866106)\n\nSRRF article: [Gustafsson, N., Culley, S., Ashdown, G., D. M. Owen, P. Matos Pereira, and R. Henriques. Fast live-cell conventional fluorophore nanoscopy with ImageJ through super-resolution radial fluctuations. Nat Commun 7, 12471 (2016)](https://www.nature.com/articles/ncomms12471)\n\nMUSICAL article: [K. Agarwal and R. Machan, Multiple Signal Classification Algorithm for super-resolution fluorescence microscopy, Nature Communications, vol. 7, article id. 13752, (2016)](https://www.nature.com/articles/ncomms13752)\n\n\n\nMethods implemented:\n- MSSR\n- ESI\n- SOFI\n- SRRF\n- MUSICAL\n- Split channels\n\n\n| **Super Resolution Radial Fluctuations (SRRF)**  | **Mean-Shift Super Resolution (MSSR)** | **Entropy-based Super-resolution Imaging (ESI)** |\n| --- | --- | --- |\n| ![](https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/Fig_7_SRRF_Alva_2022.png) | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/Fig_2a_MSSR_Garcia_2021.png) | ![](https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/Fig_6_ESI_Alva_2022.png) |\nfrom Fig. 7 of [Alva et al., 2022](https://onlinelibrary.wiley.com/doi/10.1111/jmi.13135) | from Fig. 2 of [Garc√≠a et al., 2021](https://www.biorxiv.org/content/10.1101/2021.10.17.464398v2.full)|  from Fig. 6 of [Alva et al., 2022](https://onlinelibrary.wiley.com/doi/10.1111/jmi.13135)|\n\n\nRepositories available:\n- [ESI](https://github.com/biophotonics-bielefeld/ESI) GitHub repository\n- [PySOFI](https://github.com/xiyuyi-at-LLNL/pysofi) GitHub repository\n- [MUSICAL](https://sites.google.com/site/uthkrishth/musical) Google site\n\n----------------------------------\n\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n\n## Installation\nFirst install napari viewer (if you haven't):\n\n    conda create -y -n napari-env -c conda-forge python=3.9\n    conda activate napari-env\n    pip install \"napari[all]\"\n\nFor details check: https://napari.org/stable/\n\n\n\n\nYou can install the plugin [graphically](https://github.com/LIBREhub/napari-LatAm-Workshop-2023/blob/napari-superres/docs/day3/napari-superres/napari-superres_installation_guide.pdf).\n\nor install latest development version :\n\n    pip install git+https://github.com/RoccoDAnt/napari-superres.git\n\nYou might need to install [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) first.\n\n----------------------------------\nExamples of use:\n\n| **Original**  | **tMSSR** |\n| --- | --- |\n| <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/single-frame-good-exposure.png\" width=100% height=100%> </p>| <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/tmssr-mean-mag2.png\" width=48% height=48%> </p>|\n| Parameters: | Amplification: 2, Order: 0, PSF FWHM: 6, <br> Interpolation: Bicubic, Statistical integration: CV*sigma |\n\n| **Original**  | **ESI** |\n| --- | --- |\n| <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/synt.png\" width=40% height=40%> </p> | <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/ESI.png\" width=50% height=50%> </p> |\n| Parameters: | image in output: 2, bins: 2, Order: 2 |\n\n| **Original**  | **SOFI** |\n| --- | --- |\n|<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/noSOFI.png\" width=100% height=100%> </p> | <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/SOFI.png\" width=100% height=100%> </p> |\n| Parameters: | Amplification factor: 2, Moment Order: 4, lambda parameter: 1.5, No. Iterations: 20, Window size: 100|\n\n| **Original**  | **SRRF** |\n| --- | --- |\n|<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/synt.png\" width=50% height=50%> </p> | <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/SRRF.png\" width=50% height=50%> </p>|\n| Parameters: | Amplification: 2, Spatial radius: 5, Symmetry Axis: 6, Start frame: 0, End frame: 48|\n\n| **Original**  | **MUSICAL** |\n| --- | --- |\n| <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/musical_mean.png\" width=70% height=100%> </p> | <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/RoccoDAnt/napari-superres/main/docs/MUSICAL-CardioMyoblast_Mitochondria.png\" width=70% height=100%> </p>|\n| Parameters: | Emission [nm]: 510 NA: 1.4, Mag: 100, Pizel size: 8000, Threshold: -0.5, Alpha: 4, Subpixels per pixel: 20|\n----------------------------------\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-superres\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/RoccoDAnt/napari-superres/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "MSSR",
      "ESI",
      "SOFI",
      "SRRF",
      "MUSICAL",
      "Split Channels"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-gemspa",
    "name": "napari-gemspa",
    "display_name": "GEMspa",
    "version": "0.0.4",
    "created_at": "2023-05-29",
    "modified_at": "2023-08-04",
    "authors": [
      "Sarah Keegan"
    ],
    "author_emails": [
      "sarah.keegan@nyulangone.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-gemspa/",
    "home_github": "https://github.com/liamholtlab/napari-gemspa",
    "home_other": null,
    "summary": "A plugin for analysis of single particle tracking experiments",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "napari",
      "scikit-image",
      "gemspa-spt",
      "matplotlib",
      "trackpy",
      "nd2",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-gemspa\n\nThis plugin provides for analysis tools for data from single particle tracking experiments.  It provides an interface for particle localization and tracking using [trackpy](http://soft-matter.github.io/trackpy/dev/index.html).  It also allows for import of tracking data from Mosaic and Trackmate.  These files must be tab/comma delimited text files.  It provides an option to exclude particles/tracks masked with a labels layer.\n\nThere are 5 tabs available in the plugin, following the workflow of data analysis:\n\n1) **New/Open**: open nd2/tiff time-lapse movie files and/or import a tracks layer (from Mosaic, Trackmate or napari-gemspa saved tracks layer)\n2) **Locate**: locate particles with trackpy\n3) **Link**: link particles with trackpy\n4) **Filter Links**: filter links with trackpy\n5) **Analyze**: Perform analysis on tracks from a tracks layer (can be from imported file from step 1 or layer created in step 3)\n\n**Detailed description of features:**\n\n1) **New/Open**\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/1_1.png)\n\n**Add layer** button will create a blank 2D (no time dimension) layer that is the same height/width as the currently selected image layer.  Alternatively, a labeled mask can be opened from a file.  The labels layer can be used to provide a mask for excluding areas of the image from analysis.\n\nTrack files from other software or previously saved by GEMspa can also be imported in this pane.  Only tab/comma (.csv/.txt/.tsv) delimited text files are allowed.\n\n**GEMspa** expects these columns in the header: ['track_id', 'frame', 'z', 'y', 'x']\n\n**Mosaic** expects these columns in the header: ['Trajectory', 'Frame', 'z', 'y', 'x']\n\n**Trackmate** expects these columns in the header: ['TRACK_ID', 'FRAME', 'POSITION_Z', 'POSITION_Y', 'POSITION_X'],\n* 3 rows will be skipped for Trackmate files (assumes data begins at the 4th row after the header)\n\n**Trackpy** expects these columns in the header: ['particle', 'frame', 'z', 'y', 'x']\n\n_(All columns are case and order insensitive)_\n\n2) **Locate**\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/2_1.png)\n\nIn this tab, adjust the parameters and perform particle localization with [trackpy.locate](http://soft-matter.github.io/trackpy/dev/generated/trackpy.locate.html#trackpy.locate).  To first test out parameters on a single frame, check the \"Process only current frame\" checkbox.  Please refer to the trackpy documentation for more details on parameters.\n\nAfter localization is performed, a new points layer will be created and particles will be shown circled in red.  In the example, we have used a labels layer to exclude particles outside an ROI (this is optional):\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/2_2.png)\n\nIn addition, the mass histogram and subpixel bias histograms will be shown for help with adjusting the mass and diameter parameters:\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/2_3.png)\n\n3) **Link**\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/3_1.png)\n\nIn this tab, adjust parameters and perform linking with [trackpy.link](http://soft-matter.github.io/trackpy/dev/generated/trackpy.link.html).  Once linking is performed a new tracks layer will be added.  Please refer to the trackpy documentation for more details on parameters.\n\nIn addition, scatter plots of mass vs. size and mass vs. eccentricity, as well as the track lengths histogram are shown for help with filtering tracks. (next step)\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/3_2.png)\n\n4) **Filter**\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/4_1.png)\n\nIn this tab, adjust parameters and filter links from trackpy output.  After filtering, a new layer will be added to napari with the filtered tracks and the same plots as shown in step 4 will be displayed.\n\n5) **Analyze**\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_1.png)\n\nIn this tab, adjust parameters and perform analysis.  You may process all tracks or enter a track id and deselect the \"Process all tracks\" check box.  Enter the appropriate parameters for converting pixels to microns and the time lag (in seconds) between frames of the movie.  \n\nGEMspa will calculate the effective diffusion coefficient (D) for each track based on the mean squared displacement values (MSD) for each time-lag using this equation:\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_2.png)\n\nThis is the diffusion coefficient with the assumption of Brownian motion.  GEMspa will also calculate the generalized diffusion coefficient and anomalous exponent using this equation:\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_3.png)\n\nThe fitting is performed on a log-log scale where the slope corresponds to the anomalous exponent (alpha):\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_4.png)\n\n**Definition of terms:**\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_5.png)\n\nThis review: [Manzo et al](https://pubmed.ncbi.nlm.nih.gov/26511974/)  is useful for learning more about these and other analysis methods.\n\n**Min track len for fit**: all tracks less than this length will be excluded from calculations of effective diffusion coefficient and anomalous exponent.\n\n**Max time lag for fit**: GEMspa will fit the MSD up to the max time-lag entered here.  (in frames)\n\n**Fit with error term**: check this box to allow a y-intercept when fitting for effective diffusion coefficient:\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_6.png)\n\nCheck these papers by [Martin, et al](https://www.sciencedirect.com/science/article/pii/S0006349502739714) and [Xavier Michalet](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3055791/) for some information on fitting MSD with localization error.\n\n**Rainbow tracks**\n\nGEMspa can output a plot where the tracks are colored by any of the listed quantities.  Check each box that you would like to see.  \n\nFor the effective diffusion coefficient and anomalous exponent, set the Min/Max cutoffs for the track color map.  Any tracks at or below the minimum will be colored with the minimum color (blue).  Any tracks at or above the maximum will be colored with the maximum color (red).\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_10.png)\n\n**Plots**\n\nGEMspa will also output summary plots including:\n* Ensemble average MSD shown on linear and log-scale with results from fitting the MSD vs time-lag data from the ensemble average MSD.\n* Track length histogram, Radius of gyration (for full track lengths) histogram, Scatter plot of track length vs. radius of gyration\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_7.png)\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_8.png)\n\n**Tracks data table**\n\nGEMspa will output a table of data in a new pop-up window with data for each track.  This table will only show one line for each track (not one line for every particle position) and it will output the following data:\n* track_id\n* frame_start\n* frame_end\n* radius_gyration: radius of gyration for the full track length (See [Elliot et al](https://doi.org/10.1039/c0cp01805h))\n* track_length\n* D: effective diffusion coefficient\n* E: y-intercept for the fit of MSD for D\n* r_sq (lin): R-squared value for goodness-of-fit for the fit of MSD vs time-lag\n* K: generalized diffusion coefficient\n* a: anomalous exponent (alpha)\n* r_sq (log): R-squared value for goodness-of-fit for the fit of log-log MSD vs. time-lag\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_9.png)\n\n**New tracks layer**\n\nGEMspa will add a new tracks layer and save all data for each track in the properties of the tracks layer.  Save the tracks layer to obtain a tab/comma-delimited text file with all analysis results.  The data included for each track is:\n* track_id\n* frame\n* y: y position in pixels\n* x: x position in pixels\n* frame_start\n* frame_end\n* y (microns): y position in microns\n* x (microns): x position in microns\n* tau: time-lag in seconds\n* MSD: mean squared displacement \n* t: time in seconds from start of track\n* step_size\n* radius_gyration: radius of gyration at each time point of track (See [Elliot et al](https://doi.org/10.1039/c0cp01805h))\n* track_length\n* D: effective diffusion coefficient\n* E: y-intercept for the fit of MSD for D\n* r_sq (lin): R-squared value for goodness-of-fit for the fit of MSD vs time-lag\n* K: generalized diffusion coefficient\n* a: anomalous exponent (alpha)\n* r_sq (log): R-squared value for goodness-of-fit for the fit of log-log MSD vs. time-lag\n\nTo extract this data, save the layer as a tab or comma delimited text file (txt/tsv/csv).  GEMspa will save all track information.\n\n**Analysis for a single track**\n\nGEMspa also provides the option to select a single track and output analysis results.  Detailed information is shown for the selected track, including a plot of the radius of gyration at each time point and a plot of the track itself.  \n\nHere is an example:\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_11.png)\n\n![My Image](https://raw.githubusercontent.com/liamholtlab/napari-gemspa/main/screen_shots/5_12.png)\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tsv",
      "*.txt",
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [
      ".tsv",
      ".txt",
      ".csv"
    ],
    "contributions_widgets": [
      "GEMspa"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sam",
    "name": "napari-sam",
    "display_name": "Segment Anything",
    "version": "0.4.13",
    "created_at": "2023-04-06",
    "modified_at": "2023-08-02",
    "authors": [
      "Karol Gotkowski"
    ],
    "author_emails": [
      "karol.gotkowski@dkfz.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-sam/",
    "home_github": "https://github.com/MIC-DKFZ/napari-sam",
    "home_other": null,
    "summary": "Segment anything with Meta AI's new SAM model!",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "vispy",
      "tqdm",
      "napari-nifti",
      "superqt",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# Segment Anything Model (SAM) in Napari\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-sam.svg?color=green)](https://github.com/MIC-DKFZ/napari-sam/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sam.svg?color=green)](https://pypi.org/project/napari-sam)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sam.svg?color=green)](https://python.org)\n[![tests](https://github.com/MIC-DKFZ/napari-sam/workflows/tests/badge.svg)](https://github.com/MIC-DKFZ/napari-sam/actions)\n[![codecov](https://codecov.io/gh/MIC-DKFZ/napari-sam/branch/main/graph/badge.svg)](https://codecov.io/gh/MIC-DKFZ/napari-sam)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sam)](https://napari-hub.org/plugins/napari-sam)\n\nSegment anything with our **Napari** integration of Meta AI's new **Segment Anything Model (SAM)**!\n\nSAM is the new segmentation system from Meta AI capable of **one-click segmentation of any object**, and now, our plugin neatly integrates this into Napari.\n\nWe have already **extended** SAM's click-based foreground separation to full **click-based semantic segmentation and instance segmentation**!\n\nAt last, our SAM integration supports both **2D and 3D images**!\n\n----------------------------------\n\nEverything mode             |  Click-based semantic segmentation mode |  Click-based instance segmentation mode\n:-------------------------:|:-------------------------:|:-------------------------:\n![](https://github.com/MIC-DKFZ/napari-sam/raw/main/cats_everything.png)  |  ![](https://github.com/MIC-DKFZ/napari-sam/raw/main/cats_semantic.png)  |  ![](https://github.com/MIC-DKFZ/napari-sam/raw/main/cats_instance.png)\n\n\n----------------------------------\n<h2 align=\"center\">SAM in Napari demo</h2>\n<div align=\"center\">\n\nhttps://user-images.githubusercontent.com/3471895/236152620-0de983db-954b-4480-97b9-901ee82f8edd.mp4\n\n</div>\n\n----------------------------------\n\n## Installation\n\nThe plugin requires `python>=3.8`, as well as `pytorch>=1.7` and `torchvision>=0.8`. Please follow the instructions here to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.\n\nInstall Napari via [pip]:\n    \n    pip install napari[all]\n\nYou can install `napari-sam` via [pip]:\n\n    pip install git+https://github.com/facebookresearch/segment-anything.git\n    pip install napari-sam\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MIC-DKFZ/napari-sam.git\n\n## Usage\n\nStart Napari from the console with:\n\n    napari\n\nThen navigate to `Plugins -> Segment Anything (napari-sam)` and drag & drop an image into Napari. At last create, a labels layer that will be used for the SAM predictions, by clicking in the layer list on the third button.\n\nYou can then auto-download one of the available SAM models (this can take 1-2 minutes),  activate one of the annotations & segmentation modes, and you are ready to go!\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-sam\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MIC-DKFZ/napari-sam/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n# Acknowledgements\n<img src=\"https://github.com/MIC-DKFZ/napari-sam/raw/main/HI_Logo.png\" height=\"100px\" />\n\n<img src=\"https://github.com/MIC-DKFZ/napari-sam/raw/main/dkfz_logo.png\" height=\"100px\" />\n\nnapari-sam is developed and maintained by the Applied Computer Vision Lab (ACVL) of [Helmholtz Imaging](http://helmholtz-imaging.de) \nand the [Division of Medical Image Computing](https://www.dkfz.de/en/mic/index.php) at the \n[German Cancer Research Center (DKFZ)](https://www.dkfz.de/en/index.html).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segment Anything"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-caphid",
    "name": "napari-caphid",
    "display_name": "CAphid",
    "version": "0.0.1",
    "created_at": "2023-07-30",
    "modified_at": "2023-07-30",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-caphid/",
    "home_github": "https://github.com/hereariim/napari-caphid",
    "home_other": null,
    "summary": "Annotation of aphid and update table",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python",
      "tqdm",
      "pandas",
      "Pillow",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-caphid\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-caphid.svg?color=green)](https://github.com/hereariim/napari-caphid/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-caphid.svg?color=green)](https://pypi.org/project/napari-caphid)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-caphid.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/napari-caphid/workflows/tests/badge.svg)](https://github.com/hereariim/napari-caphid/actions)\n[![codecov](https://codecov.io/gh/hereariim/napari-caphid/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-caphid)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-caphid)](https://napari-hub.org/plugins/napari-caphid)\n\nAnnotation of aphid and update table\n\n----------------------------------\n\nNapari-caphid was developed for updating table of quantitative data from images. Napari-caphid was developed by Imhorphen Team (french team of University of Angers and INRAe Angers) for ECLECTIC Team (french team of University of Paris-Saclay and CNRS).\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-caphid` via [pip]:\n\n    pip install napari-caphid\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/napari-caphid.git\n\n## Getting started\n\n### Foreword\n\nBefore using the plugin, the directory must be structured as follows:\n\n```\n‚îî‚îÄ‚îÄ Directory\n    ‚îú‚îÄ‚îÄ France\n    ‚îÇ   ‚îú‚îÄ‚îÄ image\n    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_1.tif\n    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_2.tif\n    ‚îÇ   ‚îÇ   ...\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ img_n.tif\n    ‚îÇ   ‚îú‚îÄ‚îÄ mask\n    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ msk_1.tif\n    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ msk_2.tif\n    ‚îÇ   ‚îÇ   ...\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ msk_n.tif\n    ‚îÇ   ‚îú‚îÄ‚îÄ img_1.tif\n    ‚îÇ   ‚îú‚îÄ‚îÄ msk_1.tif\n    ‚îÇ   ‚îú‚îÄ‚îÄ img_2.tif\n    ‚îÇ   ‚îú‚îÄ‚îÄ msk_2.tif\n    ‚îÇ   ...\n    ‚îÇ   ‚îú‚îÄ‚îÄ img_n.tif\n    ‚îÇ   ‚îî‚îÄ‚îÄ msk_n.tif\n    ‚îÇ \n    ‚îú‚îÄ‚îÄ Belgium\n    ‚îÇ   ‚îú‚îÄ‚îÄ image\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îÇ   ‚îú‚îÄ‚îÄ mask\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îú‚îÄ‚îÄ Spain\n    ‚îÇ   ‚îú‚îÄ‚îÄ image\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îÇ   ‚îú‚îÄ‚îÄ mask\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îî‚îÄ‚îÄ Aphid.csv\n```\n\nSome explanation about structure. The directory contained three folders (France, Spain, Belgium) and one file (Aphid.csv).\n- Each folders (France, Spain, Belgium) contains a set of images and masks and two folders (image, mask). The folder image contains images from the set of images. The folder mask contains masks from the set of masks.\n- The file Aphid.csv is a table with quantitative data of aphids from inital process of aphid image processing.\n\nImportant:\n- The structure of directory is very important because it will be useful to get image name.\n\n### Getting started\n\nThe widget get three input:\n- Mask : Mask stack\n- Pick a table : Path/to/Directory/Aphid.csv\n- Country : The country where images were taken\n\nThe widget gives one output:\n- A new table .csv which is the Aphid.csv updated.\n\n### What's it for ?\n\nThis widget gives quantitative data from Mask stack. These quantitative data will be contained into dataframe. Quantitative data linked to current masks contained in the Aphid.csv file will be deleted. Then, the new quantitative data contained in the dataframe will be integrated into the Aphid.csv file. In this way, the Aphid.csv file is updated.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-caphid\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/napari-caphid/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Update table"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "snouty-viewer",
    "name": "snouty-viewer",
    "display_name": "Snouty Viewer",
    "version": "0.2.5",
    "created_at": "2022-08-31",
    "modified_at": "2023-07-27",
    "authors": [
      "Austin E. Y. T. Lefebvre"
    ],
    "author_emails": [
      "austin.e.lefebvre@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/snouty-viewer/",
    "home_github": "https://github.com/aelefebv/snouty-viewer",
    "home_other": null,
    "summary": "A plugin to visualize, deskew, and combine Snouty data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "napari",
      "numpy",
      "ome-types",
      "tifffile",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# snouty-viewer\n\n[![License MIT](https://img.shields.io/pypi/l/snouty-viewer.svg?color=green)](https://github.com/aelefebv/snouty-viewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/snouty-viewer.svg?color=green)](https://pypi.org/project/snouty-viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/snouty-viewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/aelefebv/snouty-viewer/workflows/tests/badge.svg)](https://github.com/aelefebv/snouty-viewer/actions/workflows/test_and_deploy.yml)\n[![codecov](https://codecov.io/gh/aelefebv/snouty-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/aelefebv/snouty-viewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/snouty-viewer)](https://napari-hub.org/plugins/snouty-viewer)\n\n## Description\nEasy to use plugin for opening raw Snouty files and converting them to native view.\n\nAllows for saving to ome.tif files with corresponding OME-XML based metadata.\n\nAlso allows for bulk deskewing and saving directories.\n\n![Example](https://i.imgur.com/VirE5DM.gif)\n\n## Intended Audience & Supported Data\nThis plugin is intended for those using a SOLS (Snouty) microscope collected via\n[Alfred Millett-Sikking's code](https://github.com/amsikking/SOLS_microscope).\n\nThis plugin accepts a folder with at least subdirectories of data and metadata as an input.\n\n## Quickstart\n\n### A. Getting the plugin working (choose either a or b, you don't have to do both)\n#### a. Through pip-install:\n1. pip install snouty-viewer (within a virtual environment of Python 3.8, 3.9, or 3.10 recommended)\n2. Open up napari\n#### b. Through Napari:\n1. Open up napari\n2. Plugins > Install/Uninstall plugins\n3. Search for \"snouty-viewer\"\n4. Install\n5. (Maybe need to) reopen napari\n\n### B. Viewing raw Snouty data\n- Drag and drop a root folder of your Snouty data. This is the folder that includes the data and metadata subfolders.\n- Select \"Snouty Viewer\" for opening.\n\n### C. Converting raw Snouty data to its native view\n1. Click plugins, snouty-viewer -> Native View\n2. Select the file you want to convert\n3. Press Deskew\n\n### D. Saving your native view file\n1. Select the channel (or multi-channel) layer you want to save\n2. File > Save Selected Layer(s)...\n3. Select where you want to save your file\n4. Title your file, \".ome.tif\" will automatically be appended.\n5. Save with \"Snouty Writer\"\n6. Wait (this could take a few minutes depending on your file's size and your hardware)\n\n### E. Batch saving\n1. Click plugins, snouty-viewer -> Batch Deskew & Save\n2. Input a directory (without quotes) that contains 1 or more Snouty-acquired directories.\n3. If you want to view your deskewed outputs, check the box.\n4. If you want to automatically save the deskewed outputs, check the box.\n5. Press Deskew and save\n6. Wait (this could take a few minutes depending on your files' sizes and your hardware)\n## Getting Help\n- Open up an issue on [GitHub](https://github.com/aelefebv/snouty-viewer/issues).\n- Start a thread on [image.sc](https://forum.image.sc/)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `snouty-viewer` via [pip]:\n\n    pip install snouty-viewer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/aelefebv/snouty-viewer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"snouty-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/aelefebv/snouty-viewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".ome.tif"
    ],
    "contributions_widgets": [
      "Native View",
      "Batch Deskew & Save",
      "Position Extraction"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-time-series-plotter",
    "name": "napari-time-series-plotter",
    "display_name": "TSP",
    "version": "0.0.6",
    "created_at": "2021-12-01",
    "modified_at": "2023-07-25",
    "authors": [
      "Christopher Nauroth-Kress"
    ],
    "author_emails": [
      "nauroth_C@ukw.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-time-series-plotter/",
    "home_github": "https://github.com/ch-n/napari-time_series_plotter",
    "home_other": null,
    "summary": "A Plugin for napari to visualize pixel values over the first dimension (time -> t+3D, t+2D) as graphs.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-matplotlib (<1.0)",
      "numpy",
      "pandas",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-time_series_plotter\n\n[![License](https://img.shields.io/pypi/l/napari-time_series_plotter.svg?color=green)](https://github.com/ch-n/napari-time_series_plotter/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-time_series_plotter.svg?color=green)](https://python.org)\n[![PyPI](https://img.shields.io/pypi/v/napari-time_series_plotter.svg?color=blue)](https://pypi.org/project/napari-time_series_plotter)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/napari-time-series-plotter/badges/version.svg)](https://anaconda.org/conda-forge/napari-time-series-plotter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-time-series-plotter)](https://napari-hub.org/plugins/napari-time-series-plotter)\n[![tests](https://github.com/ch-n/napari-time_series_plotter/workflows/tests/badge.svg)](https://github.com/ch-n/napari-time_series_plotter/actions)\n[![codecov](https://codecov.io/gh/ch-n/napari-time_series_plotter/branch/main/graph/badge.svg)](https://codecov.io/gh/ch-n/napari-time_series_plotter)\n\n\n## Description\nNapari-time_series_plotter (TSP) is a plugin for the `napari` ndimensional image viewer. \n\n**TSP** adds live plotting of time-resolved images to napari. You can select and visualize pixel/voxel or ROI mean values from one or multiple image layers as intensity-over-time line plots (The first image dimension is handled as time) and save the figures or the underlying time series data as CSV file. TSP supports 3D to nD images (3D: t+2D, nD: t+nD).\n\n**Plotting** is handeled by the `Explorer` widget, it offers three different plotting modes: Voxel, Shapes, Points\n<br>--> Voxel mode offers live plotting while moving the cursor over an image layer\n<br>--> Shapes mode offers shape-based ROI plotting the ROI combination method can be one of [Mean, Median, STD, Sum, Min, Max]; multiple ROIs can be plotted simultaneously\n<br>--> Points mode offers simultaneous, point-based plotting of multiple voxels\n<br>You can modify and save the plots through the canvas toolbar.\n<br>Plotting powered by `napari-matplotlib`.\n\n**Viewing** the time series as a table is handled by the `Inspector` widget. You can load the data you've plotted and inspect the single time point values of each selection. The columns are named like the plots in the `Explorer`. You can copy the whole tabe or a selection to the clipboard or directly expot it to a CSV file to save the time series.\n\n----------------------------------\n\n## Installation\nYou can either install the latest version via pip or conda.\n\n**pip:**\n\n    pip install napari-time-series-plotter\n\nor download the packaged `tar.gz` file from the release assets and install it with \n    \n    pip install /path/to/file.tar.gz\n\n**conda:**\n\n    conda install -c conda-forge napari-time-series-plotter\n\n\nAlternatively, you can install the plugin directly in the `napari` viewer plugin manager, the napari hub, or the release assets.\n\n<br>\n\nTo install the latest development version install directly from the relevant GitHub branch.\n\n## Usage\n### Basics and Live plotting\n\n[![basic_demo](./demo_videos/TSP_basic_and_voxel_plotting_demo.jpg)](./demo_videos/TSP_basic_and_voxel_plotting_demo.webm)\n    \n1. Select the `TSPExplorer` widget in the `Plugins` tab of the napari viewer\n2. Use the `LayerSelector` to choose the image layers you want to source for plotting\n3. Move the corsor over the layer while holding \"Shift\"\n\nThe `Options` tab offers multiple options to customize your plot. \n- Set custom title or axe labels\n- Switch between autoscaling and manually defined max and min values of the axes\n- Switch to label truncation in the options tab if your layer names are too long for the figure legend (set max length manually)\n- Set a scaling factor for the X-axis\n\nThe plot can be modified and saved through its toolbar above.\n\n### Plotting ROIs\n\n[![roi_demo](./demo_videos/TSP_ROI_plotting_demo.jpg)](./demo_videos//TSP_ROI_plotting_demo.webm)\n\n1. Select the Shapes plotting mode via the `Options` tab (Voxel mode is the default).\n2. Use the `LayerSelector` to choose the image layers you want to source for plotting.\n2. Add one ore more shapes to the \"ROI Selection\" layer.\n   <br>The \"ROI Selection\" shapes are 2D only, effecting the currently displayed slice.\n   <br>(newly added shapes might have to be moved before they are correctly plottet)\n3. Reposition or remove shapes if needed.\n4. Change the ROI mode in the `Options` tab (Default: mean).\n\n### Plotting multiple Points\n\n[![points_demo](./demo_videos/TSP_Points_plotting_demo.jpg)](./demo_videos/TSP_Points_plotting_demo.webm)\n\n1. Select the Shapes plotting mode via the `Options` tab (Voxel mode is the default).\n2. Use the `LayerSelector` to choose the image layers you want to source for plotting.\n3. Add one or more points to the \"Point selection\" layer.\n   <br>The points can be on different slices (3D and 4D support only) or images (grid mode)\n4. Reposition or remove points if needed.\n\n### View time series as table\n\n[![points_demo](./demo_videos/TSP_Inspector_demo.jpg)](./demo_videos/TSP_Inspector_demo.webm)\n\n1. Select the `TSPInspector` widget in the `Plugins` tab of the napari viewer\n2. Press the load from plot button to load the currently displayed plots into the `Inspector`\n\nYou can copy the whole table or a selection to your clipboard or export it to CSV file through the buttons above.\n\n## ToDo (help welcome)\n- [ ] Add Sphinx documentation\n\n## Version 0.1.0 Milestones\n- [X] Update to napari-plugin-engine2 [#5](https://github.com/ch-n/napari-time_series_plotter/issues/5)\n- [X] Update widget GUI [#6](https://github.com/ch-n/napari-time_series_plotter/issues/6)\n- [x] Add widget to save pixel/voxel time series to file [#7](https://github.com/ch-n/napari-time_series_plotter/issues/7)\n- [X] Add ROI and multi-voxel plotting [#14](https://github.com/ch-n/napari-time_series_plotter/issues/14)\n- [ ] Evaluate and close remaining issues ([#22](https://github.com/ch-n/napari-time_series_plotter/issues/22), [#25](https://github.com/ch-n/napari-time_series_plotter/issues/25),)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-time_series_plotter\" is free and open-source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n--------------\n\n## References\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\nImages used in the demo gif were taken from [The Cancer Imaging Archive] <br>\n\n    DOI: https://doi.org/10.7937/K9/TCIA.2015.VOSN3HN1\n    Images: 1.3.6.1.4.1.9328.50.16.281868838636204210586871132130856898223\n            1.3.6.1.4.1.9328.50.16.254461916058189583774506642993503110733\n\n[The Cancer Imaging Archive]: https://www.cancerimagingarchive.net/\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/ch-n/napari-time_series_plotter/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Explorer",
      "Inspector"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mclabel",
    "name": "napari-mclabel",
    "display_name": "napari-mclabel",
    "version": "1.0.1.dev0",
    "created_at": "2023-07-09",
    "modified_at": "2023-07-09",
    "authors": [
      "Jonas Utz"
    ],
    "author_emails": [
      "jonas.utz@fau.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-mclabel/",
    "home_github": null,
    "home_other": null,
    "summary": "Napari plugin for semi-automatic labeling of macrophages",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari[all]",
      "napari-plugin-engine (>=0.1.4)",
      "imaris-ims-file-reader (>=0.1.5)",
      "numpy",
      "h5py",
      "dask",
      "napari-imaris-loader",
      "scikit-image",
      "scipy"
    ],
    "package_metadata_description": "# Napari McLabel\n\n## What is the purpose of this tool?\n\nMcLabel is a semi-automatic local thresholding tool that can help to label cellular objects such as macrophages in fluorescence microscopy images. In cases where a global threshold does not yield satisfactory results, a local threshold based on a ROI drawn by the user may give better results. See the video for an example:\n![Mclabel](./img/Mclabel.gif)\n\n\n\n## Installation\n\nThe plugin can be installed using pip:\n```bash\npip install napari-mclabel\n```\n\nAfter succesfull installation the plugin will appear in the plugins menu of napari.\n\n## Usage\n\n![gui](./img/gui.png)\n\nThe GUI of McLabel lives in the right pane of napari. If multiple layers are loaded, select the layer that you want to segment. The theshold finding algorithm is by default is triangle, however there are plenty of alternatives and depending on the data another algorithm might be better suited. \n\n1. Press \"Draw Label\"\n2. Draw a rough outline around the object of interest. \n3. Press \"Compute Label\"\n4. If not satisfied with result, adjust threshold using the slider\n5. Continue with next object\n\n![gui](./img/gui.gif) \n\n## Reference\n\nIf you use McLabel in your work, consider citing our background paper:\nhttps://doi.org/10.1007/978-3-658-41657-7_20\n\n\n\n```tex\n@InProceedings{10.1007/978-3-658-41657-7_20,\nauthor=\"Utz, Jonas\nand Schlereth, Maja\nand Qiu, Jingna\nand Thies, Mareike\nand Wagner, Fabian\nand Brahim, Oumaima B.\nand Gu, Mingxuan\nand Uderhardt, Stefan\nand Breininger, Katharina\",\neditor=\"Deserno, Thomas M.\nand Handels, Heinz\nand Maier, Andreas\nand Maier-Hein, Klaus\nand Palm, Christoph\nand Tolxdorff, Thomas\",\ntitle=\"McLabel\",\nbooktitle=\"Bildverarbeitung f{\\\"u}r die Medizin 2023\",\nyear=\"2023\",\npublisher=\"Springer Fachmedien Wiesbaden\",\naddress=\"Wiesbaden\",\npages=\"82--87\",\nabstract=\"In this work, we present a semi-automatic labelling tool for the annotation of complex cellular structures such as macrophages in fluorescence microscopy images. We present McLabel, a napari plugin that allows users to label structures of interest by simply scribbling outlines around the area of interest, using the triangle thresholding method with post-processing to identify the desired structure. Additionally, manual adaption of the threshold allows for quick and fine-grained local correction of the segmentation. The tool is evaluated in a user study with five experts, who annotated images both with and without the tool. The results show that variability in annotations between experts is reduced when the labelling tool is used and annotation time is reduced by a factor of five on average.\",\nisbn=\"978-3-658-41657-7\"\n}\n```\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "McLabel"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-script-editor",
    "name": "napari-script-editor",
    "display_name": "napari-script-editor",
    "version": "0.2.10",
    "created_at": "2021-11-06",
    "modified_at": "2023-07-08",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-script-editor/",
    "home_github": "https://github.com/haesleinhuepf/napari-script-editor",
    "home_other": null,
    "summary": "A python script editor for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "haesleinhuepf-pyqode.core (>=2.15.5)",
      "haesleinhuepf-pyqode.python (>=2.15.2)",
      "napari-tools-menu",
      "jedi (>=0.18.0)",
      "pyflakes (<=2.4.0)",
      "imageio (!=2.22.1)"
    ],
    "package_metadata_description": "# napari-script-editor\n\n[![License](https://img.shields.io/pypi/l/napari-script-editor.svg?color=green)](https://github.com/haesleinhuepf/napari-script-editor/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-script-editor.svg?color=green)](https://pypi.org/project/napari-script-editor)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-script-editor.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-script-editor/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-script-editor/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-script-editor/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-script-editor)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-script-editor)](https://napari-hub.org/plugins/napari-script-editor)\n\nA python script editor for napari based on [haesleinhuepf's fork of PyQode](https://github.com/haesleinhuepf/pyqode.core).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/screenshot2.png)\n\n## Usage\n\nStart the script editor from the menu `Tools > Scripts > Script Editor`. Use the auto-completion while typing, \ncheck out the [napari tutorials](https://napari.org/tutorials/) and the\n[example scripts](https://github.com/haesleinhuepf/napari-script-editor/tree/main/example_scripts). \nUse the `Run` button to execute a script.\n\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/type_and_run_screencast.gif)\n\nIf you save the script to the folder \".napari-scripts\" in your home directory, you will find the script in the \n`Tools > Scripts` menu in napari. You can then also start it from there.\n\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/run_from_menu_screencast.gif)\n\nNote: If you have scripts, that might be useful to others, please send them as \n[pull-request](https://github.com/haesleinhuepf/napari-script-editor/pulls) to the examples in \nrepository or share them in any other way that suits you.\n\n### chatGPT support\n\nIn case [openAI API](https://openai.com/blog/openai-api) is installed, you find another button in the script editor to `Ask chatGPT`. \nEnter a prompt in the script editor and click the button. The script editor will send the prompt to\nchatGPT and replace it with the answer. For example try entering:\n```python\nWrite Python code for segmenting an image using these steps:\n    * Apply a Gaussian blur\n    * Threshold the image using Otsu's method\n    * Apply connected component labeling\n```\nand it will replace it with code accordingly. If it doesn't work in the first attempt, try again. ChatGPT's answers are not always the same.\n\n![](https://github.com/haesleinhuepf/napari-script-editor/raw/main/docs/ask_chatgpt.gif)\n\n\n## Installation\n* Get a python environment, e.g. via [mini-conda](https://docs.conda.io/en/latest/miniconda.html). \n  If you never used python/conda environments before, please follow the instructions \n  [here](https://mpicbg-scicomp.github.io/ipf_howtoguides/guides/Python_Conda_Environments) first.\n* Install [napari](https://github.com/napari/napari) using conda. \n\n```\nconda install -c conda-forge napari\n```\n\nAfterwards, install `napari-script-editor` using pip:\n\n```\npip install napari-script-editor\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-script-editor\" is free and open source software\n\n## Known issues\n\n* Sometimes, the script editor thinks, the file has been changed on disk and asks to reload it.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-script-editor/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ScriptEditor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-stl-exporter",
    "name": "napari-stl-exporter",
    "display_name": "napari-stl-exporter",
    "version": "0.1.5",
    "created_at": "2021-10-06",
    "modified_at": "2023-07-04",
    "authors": [
      "Johannes Soltwedel"
    ],
    "author_emails": [
      "johannes_richard.soltwedel@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-stl-exporter/",
    "home_github": "https://github.com/jo-mueller/napari-stl-exporter",
    "home_other": null,
    "summary": "Exports label images to 3D-printable stl files.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "scikit-image",
      "vedo (>=2023.4.6)",
      "npe2",
      "numpy"
    ],
    "package_metadata_description": "# napari-stl-exporter\n\n[![License](https://img.shields.io/pypi/l/napari-stl-exporter.svg?color=green)](https://github.com/jo-mueller/napari-stl-exporter/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-stl-exporter.svg?color=green)](https://pypi.org/project/napari-stl-exporter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stl-exporter.svg?color=green)](https://python.org)\n[![tests](https://github.com/jo-mueller/napari-stl-exporter/workflows/tests/badge.svg)](https://github.com/jo-mueller/napari-stl-exporter/actions)\n[![codecov](https://codecov.io/gh/jo-mueller/napari-stl-exporter/branch/main/graph/badge.svg?token=9zctLzazD9)](https://codecov.io/gh/jo-mueller/napari-stl-exporter)\n\nThis plugin allows to import and export surface data in Napari to common file formats. The generated file formats can be read by other common applications, and - in particular - allow *3D-printing*.\n\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/model_and_printed_model.png)\n\n\n### Supported file formats:\nCurrently supported file formats for export include and rely on the [vedo io API](https://vedo.embl.es/autodocs/content/vedo/io.html#vedo.io).\n* *.stl*: [Standard triangle language](https://en.wikipedia.org/wiki/STL_%28file_format%29)\n* *.ply*: [Polygon file format](https://en.wikipedia.org/wiki/PLY_(file_format))\n* *.obj*: [Wavefront object](https://en.wikipedia.org/wiki/Wavefront_.obj_file)\n\n### Supported Napari layers:\n\nCurrently supported Napari layer types are:\n* [Surface layers](https://napari.org/howtos/layers/surface.html)\n* [Label layers](https://napari.org/howtos/layers/labels.html): The label data is converted to surface data under the hood using the [marching cubes algorithm](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.marching_cubes) implemented in [scikit-image](https://scikit-image.org/) and is then exported using [Vedo](https://vedo.embl.es/). Warning: This can be slow for large image data!\n\n### Import/export\n\n**Interactively:** To export the data, simply save the selected layer with `File > Save Selected Layer(s)` and specify the file ending to be `some_file.[file_ending]`, for supported file types, see above. Similarly, supported file types can be imported into Napari with `File > `\n\n**From code**: A [Napari Label layer](https://napari.org/api/napari.layers.Labels.html) can be added to the viewer as described in the [napari reference](https://napari.org/gallery/add_labels.html?highlight=add_labels) with this code snippet:\n\n```python\nimport napari\nimport numpy as np\n\n# Load and binarize image\nlabel_image = np.zeros((100, 100, 100), dtype=int)\nlabel_image[25:75, 25:75, 25:75] = 1\n\n# Add data to viewer\nviewer = napari.Viewer()\nlabel_layer = viewer.add_labels(data, name='3D object')\n\n# save the layer as 3D printable file to disc\nnapari.save_layers(r'/some/path/test.stl', [label_layer])\n```\n\n### Sample data\nYou can create sample label/surface data for export using the built-in functions as shown here:\n\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/1_sample_data.png)\n\n...or from code with\n\n```Python\nimport napari_stl_exporter\n\npyramid = napari_stl_exporter.make_pyramid_surface()\n\n```\n\n### 3D-printing\nTo actually send your object to a 3D-printer, it has to be further converted to the *.gcode* format with a Slicer program. The latter convert the 3D object to machine-relevant parameters (printing detail, motor trajectories, etc). Popular slicers are:\n\n* [Slic3r](https://slic3r.org/): Documentation [here](https://manual.slic3r.org/intro/overview)\n* [Prusa Slicer](https://www.prusa3d.com/prusaslicer/): Tutorial [here](https://help.prusa3d.com/en/article/first-print-with-prusaslicer_1753)\n\n*Note*: You can also upload the STL file to [github.com](https://github.com) and interact with it in the browser:\n\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/pyramid_browser_screenshot.png)\n\n#### Digital elevation models\n\nDIgital elevation models (DEMs) can be printed with the napari-stl-exporter following these steps:\n\n1. Go to the [open topography repository](https://portal.opentopography.org/raster?opentopoID=OTSDEM.032021.4326.2) and select a region of your choice, then download it as a GeoTiff file (`.tif`, intensity encodes elevation)\n2. Open the downloaded tif image use the image conversion plugin (¬¥Plugins > napari-stl-exporter > 2D image to surface¬¥) to convert the downloaded image to a surface. CHeck the `solidify` option to make it readily 3D-printable.\n\n![](https://github.com/jo-mueller/napari-stl-exporter/raw/main/doc/landscape_to_surface.png)\n\n3. Export the created surface layer as `.stl` or `.ply` file. Open it in your Slicer of choice (you may have to scale it according to the size limitations of your printer) and off you go!\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-stl-exporter` via [pip]:\n\n    pip install napari-stl-exporter\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-stl-exporter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/jo-mueller/napari-stl-exporter/issues) along with a detailed description or post to image.sc and tag ```El_Pollo_Diablo```\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.stl",
      "*.ply",
      "*.obj"
    ],
    "contributions_writers_filename_extensions": [
      ".obj",
      ".ply",
      ".stl"
    ],
    "contributions_widgets": [
      "2D Image to surface",
      "Extrude mesh"
    ],
    "contributions_sample_data": [
      "Pyramid label image",
      "Pyramid surface",
      "Landscape elevation model"
    ]
  },
  {
    "normalized_name": "napari-stable-diffusion",
    "name": "napari-stable-diffusion",
    "display_name": "Stable Diffusion",
    "version": "0.1.1",
    "created_at": "2022-10-27",
    "modified_at": "2023-07-03",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "napari@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-stable-diffusion/",
    "home_github": "https://github.com/kephale/napari-stable-diffusion",
    "home_other": null,
    "summary": "A demo of stable diffusion in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "magicgui",
      "qtpy",
      "diffusers",
      "transformers",
      "torch",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-stable-diffusion\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-stable-diffusion.svg?color=green)](https://github.com/kephale/napari-stable-diffusion/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-stable-diffusion.svg?color=green)](https://pypi.org/project/napari-stable-diffusion)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-stable-diffusion.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-stable-diffusion/workflows/tests/badge.svg)](https://github.com/kephale/napari-stable-diffusion/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-stable-diffusion/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-stable-diffusion)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-stable-diffusion)](https://napari-hub.org/plugins/napari-stable-diffusion)\n\nA demo of stable diffusion in napari.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n![demo image of napari-stable-diffusion of the prompt \"a unicorn and a dinosaur eating cookies and drinking tea\"](https://github.com/kephale/napari-stable-diffusion/raw/main/napari_stable_diffusion_demo.png)\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-stable-diffusion` via [pip]:\n\n    pip install napari-stable-diffusion\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-stable-diffusion.git\n\nYou will also need to sign up with HuggingFace and [generate an access\ntoken](https://huggingface.co/docs/hub/security-tokens) to get access to the\nStable Diffusion model we use.\n\nWhen you have generated your access token you can either permanently\nset the `HF_TOKEN_SD` environment variable in your `.bashrc` or whichever file\nyour OS uses, or you can include it on the command line\n\n```\nHF_TOKEN_SD=\"hf_aaaAaaaasdadsadsaoaoaoasoidijo\" napari\n```\n\nFor more information on the Stable Diffusion model itself, please see https://huggingface.co/CompVis/stable-diffusion-v1-4.\n\n### Apple M1 specific instructions\n\nTo utilize the M1 GPU, the nightly version of PyTorch needs to be\ninstalled. Consider using `conda` or `mamba` like this:\n\n```\nmamba create -c pytorch-nightly -n napari-stable-diffusion python=3.9 pip pyqt pytorch torchvision\npip install git+https://github.com/kephale/napari-stable-diffusion.git\n```\n\n## Next steps\n\n- Image 2 Image support\n- Inpainting support\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-stable-diffusion\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-stable-diffusion/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Text to Image",
      "Image to Image",
      "Inpainting"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "tootapari",
    "name": "tootapari",
    "display_name": "Tootapari",
    "version": "0.0.1",
    "created_at": "2023-07-03",
    "modified_at": "2023-07-03",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czi@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/tootapari/",
    "home_github": "https://github.com/kephale/tootapari",
    "home_other": null,
    "summary": "A plugin to send Mastodon toots from napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "python-dotenv",
      "Mastodon.py",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# tootapari\n\n[![License BSD-3](https://img.shields.io/pypi/l/tootapari.svg?color=green)](https://github.com/kephale/tootapari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/tootapari.svg?color=green)](https://pypi.org/project/tootapari)\n[![Python Version](https://img.shields.io/pypi/pyversions/tootapari.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/tootapari/workflows/tests/badge.svg)](https://github.com/kephale/tootapari/actions)\n[![codecov](https://codecov.io/gh/kephale/tootapari/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/tootapari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/tootapari)](https://napari-hub.org/plugins/tootapari)\n\nA plugin to send Mastodon toots from napari\n\n----------------------------------\n\n## Installation\n\nYou can install `tootapari` via [pip]:\n\n    pip install tootapari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/tootapari.git\n\n\n## Setup\n\n1. Setup your `.env` file in your XDG config directory. On MacOS this is `/Users/<username>/Library/Application\\ Support/tootapari/`\n\nIt should include:\n\n```\nMASTODON_INSTANCE_URL=\"https://mastodon.social\"\nMASTODON_ACCESS_TOKEN=\"<your access token>\"\n```\n\n2. You can generate your access token by following these instructions: https://learn.adafruit.com/intro-to-mastodon-api-circuitpython/generate-your-mastodon-token\n\nTODO: someone should port these instructions to this readme.\n\n3. Start tooting!\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"tootapari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/tootapari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Tootapari!"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-aphid",
    "name": "napari-aphid",
    "display_name": "Aphid",
    "version": "1.1.7",
    "created_at": "2023-01-13",
    "modified_at": "2023-06-30",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-aphid/",
    "home_github": "https://github.com/hereariim/napari-aphid",
    "home_other": null,
    "summary": "A plugin to classify aphids by stage of development.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python-headless",
      "scikit-learn",
      "scikit-image",
      "h5py",
      "matplotlib",
      "pandas",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-aphid\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-aphid.svg?color=green)](https://github.com/hereariim/napari-aphid/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-aphid.svg?color=green)](https://pypi.org/project/napari-aphid)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-aphid.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/napari-aphid/workflows/tests/badge.svg)](https://github.com/hereariim/napari-aphid/actions)\n[![codecov](https://codecov.io/gh/hereariim/napari-aphid/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-aphid)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-aphid)](https://napari-hub.org/plugins/napari-aphid)\n\nA plugin to classify aphids by stage of development.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-aphid` via [pip]:\n\n    pip install napari-aphid\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/napari-aphid.git\n\n## Description\n\nThis plugin is a tool to count the number of aphids from two models developed on ilastik. Implemented in napari, this tool allows the correction of pixels and labels that are not well \npredicted. \n\nIn this plugin we find our two main parts of the aphid counting model presented in two widgets. A third widget allows to save the updates applied on the segmentation mask.\n\nThis plugin is an use cas, dedicated to private use of french laboratory.\n\n## Plugin input\n\n### Segmentation\n\nThe user must give two objects as input:\n\n- Compressed file in .zip format\n- Ilastik pixel classification model in .ilp format\n\nIn particular, compressed file must be organized as follows:\n\n```\n.\n‚îî‚îÄ‚îÄ Country.zip\n    ‚îî‚îÄ‚îÄ Country\n        ‚îú‚îÄ‚îÄ Area1\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_1.tif\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_1.h5\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_2.tif \n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_2.h5  \n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_3.tif\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_3.h5\n        ‚îÇ   ...\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area1.im_n.tif\n        ‚îÇ   ‚îî‚îÄ‚îÄ Area1.im_n.h5\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Area2\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_1.tif\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_1.h5\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_2.tif\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_2.h5\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_3.tif\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_3.h5\n        ‚îÇ   ...\n        ‚îÇ   ‚îú‚îÄ‚îÄ Area2.im_n.tif\n        ‚îÇ   ‚îî‚îÄ‚îÄ Area2.im_n.h5\n        ‚îÇ\n        ...\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ Arean\n            ‚îú‚îÄ‚îÄ Arean.im_1.tif\n            ‚îú‚îÄ‚îÄ Arean.im_1.h5\n            ‚îú‚îÄ‚îÄ Arean.im_2.tif\n            ‚îú‚îÄ‚îÄ Arean.im_2.h5\n            ‚îú‚îÄ‚îÄ Arean.im_3.tif\n            ‚îú‚îÄ‚îÄ Arean.im_3.h5\n            ...\n            ‚îú‚îÄ‚îÄ Arean.im_n.tif\n            ‚îî‚îÄ‚îÄ Arean.im_n.h5\n```\n\nIn each folder Area1, Area2, ..., Arean, we notice that **each tif image is accompanied by its h5 version**. The images in h5 format were generated by the Export h5 widget of the Ilastik plugin in the ImageJ software.\n\n### Classification\n\nThe user must give the Ilastik object classification model in .ilp format.\n\n## Widget: Image segmentation\n\nThis widget is a tool to segment a set of images. It takes as input a compressed file of images and an ilastik segmentation model. A Run button is used to start the image segmentation process. In the background, the console presents the progress status. This widget returns a menu which is a list of processed images. This list allows an RGB image and its segmentation mask to be displayed in the napari window.\n\n![segmentation_cpe](https://user-images.githubusercontent.com/93375163/212323051-bc84d597-a9ff-46ca-b897-cb18a0e77b4c.png)\n\n**User conduct :** In this widget, the user corrects the image with the annotation tools (brush and eraser only). With the brush, he/she has to add the same colour presented in the image. To obtain this colour, the user can take the color with the color picker tool. With the eraser, he/she erase colour not well predicted. Tous les annotations appliqu√©es dans l'image doit √™tre sauvegarder avec le bouton *Save* du widget **Save modification**\n\n## Widget: Save modification\n\nThis is the backup of the segmentation mask. It saves updates applied to the mask.\n\n## Widget: Object classification\n\nThis widget is a tool to classify segmented images. It takes as input an ilastik object classification model. A Run button is used to start the classification process. In the background, the console shows the progress of the image processing. This widget returns a menu that lists the processed images. This list provides two elements. The first is the display of the selected image in the window. The second is the display of a table that shows the predicted classes for each object.\n\n![classification_cpe](https://user-images.githubusercontent.com/93375163/212323369-32423622-4f41-4dcb-800b-39ff66be67f9.png)\n\n**User conduct :** In this widget, the user corrects labels not well predicted in the table at the bottom right. He must not forget to save his correction with the Save button.\nWhen the user has finished with all his images, he uses the Export button to import a quantitative table. This table contains for each image, the name of the aphid type and its size in pixels.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-aphid\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/napari-aphid/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segmentation",
      "Save annotation",
      "Classification"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "hesperos",
    "name": "hesperos",
    "display_name": "Hesperos application",
    "version": "0.2.1",
    "created_at": "2022-05-30",
    "modified_at": "2023-06-27",
    "authors": [
      "Charlotte Godard"
    ],
    "author_emails": [
      "charlotte.godard@pasteur.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/hesperos/",
    "home_github": "https://github.com/chgodard/hesperos",
    "home_other": null,
    "summary": "A plugin to manually or semi-automatically segment medical data and correct previous segmentation data.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "tifffile",
      "scikit-image",
      "scikit-learn",
      "SimpleITK",
      "pandas",
      "napari (<0.4.15)",
      "napari-plugin-engine",
      "imageio-ffmpeg",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "<div align=\"justify\">\n    \n# HESPEROS PLUGIN FOR NAPARI\n\n[![License](https://img.shields.io/pypi/l/hesperos.svg?color=green)](https://github.com/DBC/hesperos/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/hesperos.svg?color=green)](https://pypi.org/project/hesperos)\n[![Python Version](https://img.shields.io/pypi/pyversions/hesperos.svg?color=green)](https://python.org)\n[![tests](https://github.com/DBC/hesperos/workflows/tests/badge.svg)](https://github.com/DBC/hesperos/actions)\n[![codecov](https://codecov.io/gh/DBC/hesperos/branch/main/graph/badge.svg)](https://codecov.io/gh/DBC/hesperos)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/hesperos)](https://napari-hub.org/plugins/hesperos)\n\nA Napari plugin for pre-defined manual segmentation or semi-automatic segmentation with a one-shot learning procedure. The objective was to simplify the interface as much as possible so that the user can concentrate on annotation tasks using a pen on a tablet, or a mouse on a computer. \n    \nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n    \n# Table of Contents\n- [Installation and Usage](#installation-and-usage)\n    * [Automatic installation](#automatic-installation)\n    * [Manual installation](#manual-installation)\n    * [Upgrade Hesperos version](#upgrade-hesperos-version)\n- [Hesperos: *Manual Segmentation and Correction* mode](#hesperos-manual-segmentation-and-correction-mode)\n    * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)\n    * [Layer controls](#layer-controls)\n    * [Annotate your image](#annotate-your-image-use-panel-2)\n    * [Select slices of interest](#select-slices-of-interest-use-panel-3----only-displayed-for-the-shoulder-bones-category)\n    * [Export annotations](#export-annotations-use-panel-3----or-4-if-the-shoulder-bones-category-is-selected)\n- [Hesperos: *OneShot Segmentation* mode](#hesperos-oneshot-segmentation-mode)\n    * [Import and adjust your image](#import-and-adjust-your-image-use-panel-1)\n    * [Annotate your image](#annotate-your-image-use-panel-2)\n    * [Run automatic segmentation](#run-automatic-segmentation-use-panel-3)\n    * [Export annotations](#export-annotations-use-panel-4)\n\n        \n# Installation and Usage\nThe Hesperos plugin is designed to run on Windows (11 or less) and MacOS with Python 3.8 / 3.9 / 3.10.\n     \n    \n## Automatic installation\n1. Install [Anaconda] and unselect *Add to PATH*. Keep in mind the path where you choose to install anaconda.\n2. Only download the *script_files* folder for [Windows](/script_files/for_Windows/) or [Macos](/script_files/for_Windows/). \n3. Add your Anaconda path in these script files:\n    1. <ins>For Windows</ins>: \n    Right click on the .bat files (for [installation](/script_files/for_Windows/install_hesperos_env.bat) and [running](/script_files/for_Windows/run_hesperos.bat)) and select *Modify*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.\n        > for exemple:\n        ```\n        anaconda_dir=C:\\Users\\chgodard\\anaconda3\n        ```\n    2. <ins>For Macos</ins>:\n        1. Right click on the .command files (for [installation](/script_files/for_Macos/install_hesperos_env.command) and [running](/script_files/for_Macos/run_hesperos.command)) and select *Open with TextEdit*. Change *PATH_TO_ADD* with your Anaconda path. Then save the changes.\n            > for exemple:\n            ```\n            source ~/opt/anaconda3/etc/profile.d/conda.sh\n            ```\n        2. In your terminal, change the permissions to allow the following .command files to be run (change *PATH* with the path of your .command files): \n            ``` \n            chmod u+x PATH/install_hesperos_env.command \n            chmod u+x PATH/run_hesperos.command \n            ```\n4. Double click on the **install_hesperos_env file** to create a virtual environment in Anaconda with python 3.9 and Napari 0.4.14. \n    > /!\\ The Hesperos plugin is not yet compatible with Napari versions superior to 0.4.14.\n5. Double click on the **run_hesperos file** to run Napari from your virtual environment.\n6. In Napari: \n    1. Go to *Plugins/Install Plugins...*\n    2. Search for \"hesperos\" (it can take a while to load).\n    3. Install the **hesperos** plugin.\n    4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.\n7. Double click on the **run_hesperos file** to run Napari.\n8. In Napari, use the Hesperos plugin with *Plugins/hesperos*.\n\n    \n## Manual installation\n1. Install [Anaconda] and unselect *Add to PATH*.\n2. Open your Anaconda prompt command.\n3. Create a virtual environment with Python 3.8 / 3.9 / 3.10:\n    ```\n    conda create -n hesperos_env python=3.9\n    ```\n4. Install the required Python packages in your virtual environment:\n    ```\n    conda activate hesperos_env\n    conda install -c conda-forge napari=0.4.14 \n    conda install -c anaconda pyqt\n    pip install hesperos\n    ```\n    > /!\\ Hesperos plugin is not yet compatible with napari version superior to 0.4.14.\n5. Launch Napari:\n    ```\n    napari\n    ```\n    \n## Upgrade Hesperos version\n1. Double click on the **run_hesperos file** to run Napari. \n2. In Napari: \n    1. Go to *Plugins/Install Plugins...*\n    2. Search for \"hesperos\" (it can take a while to load).\n    3. Click on *Update* if a new version of Hesperos has been found. You can check the latest version of Hesperos in the [Napari Hub](https://www.napari-hub.org/plugins/hesperos).\n    4. When the installation is done, close Napari. A restart of Napari is required to finish the plugin installation.\n   \n    \n# Hesperos: *Manual Segmentation and Correction* mode\n    \n The ***Manual Segmentation and Correction*** mode of the Hesperos plugin is a simplified and optimized interface to do basic 2D manual segmentation of several structures in a 3D image using a mouse or a stylet with a tablet.\n\n    \n <img src=\"https://user-images.githubusercontent.com/49953723/193262711-710673f2-5b53-4eb6-a7c7-6dada9d28d92.PNG\" width=\"1000px\"/>\n    \n## Import and adjust your image *(use Panel 1)*\nThe Hesperos plugin can be used with Digital Imaging and COmmunications in Medicine (DICOM), Neuroimaging Informatics Technology Initiative (NIfTI) or Tagged Image File Format (TIFF) images. To improve performances, use images that are located on your own disk.\n\n1. To import data:\n    - use the <img src=\"https://user-images.githubusercontent.com/49953723/193262334-3c28e733-36ab-4504-9a6d-acd298c15994.PNG\" width=\"100px\"/> button for *(.tiff, .tif, .nii or .nii.gz)* image files.\n    - use the <img src=\"https://user-images.githubusercontent.com/49953723/193262624-149a4461-fbac-4498-a2b8-33bdd88e3a9f.PNG\" width=\"100px\"/> button for a DICOM serie. /!\\ Folder with multiple DICOM series is not supported.  \n2. After the image has loaded, a slider appears that allows to zoom in/out: <img src=\"https://user-images.githubusercontent.com/49953723/193262738-7e6e68a9-0890-4e18-92a9-dbf2168a6bb5.PNG\" width=\"100px\"/>. Zooming is also possible with the <img src=\"https://user-images.githubusercontent.com/49953723/193262725-7d4f7b09-d119-45cf-a9d4-c42c5f848c1a.PNG\" width=\"25px\"/> button in the layer controls panel. \n3. If your data is a DICOM serie, you have the possibility to directly change the contrast of the image (according to the Hounsfield Unit):\n    - by choosing one of the two predefined contrasts: *CT bone* or *CT Soft* in <img src=\"https://user-images.githubusercontent.com/49953723/193262708-17e1d301-0a9a-497f-9feb-613e69893c06.PNG\" width=\"150px\"/>.\n    - by creating a custom default contrast with the <img src=\"https://user-images.githubusercontent.com/49953723/193262707-466917b4-b885-429b-9924-6481fa6410bb.PNG\" width=\"30px\"/> button and selecting *Custom Contrast*. Settings can be exported as a .json file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262709-e1ad5321-1f60-4b60-a715-7c494670e1cd.PNG\" width=\"30px\"/> button.\n    - by loading a saved default contrast with the <img src=\"https://user-images.githubusercontent.com/49953723/193262710-c9f66354-f896-4e59-8718-70e5509875af.PNG\" width=\"30px\"/> button and selecting *Custom Contrast*.\n4. In the bottom left corner of the application you also have the possibility to: \n    - <img src=\"https://user-images.githubusercontent.com/49953723/193262716-d9947eb9-d87f-4251-af76-2d906cd36018.PNG\" width=\"25px\"/>: change the order of the visible axis (for example go to sagittal, axial or coronal planes).\n    - <img src=\"https://user-images.githubusercontent.com/49953723/193262717-12afbfb1-49ae-4a77-a83e-5bc99850734a.PNG\" width=\"25px\"/>: transpose the 3D image on the current axis being displayed.\n\n\n## Layer controls\n\nWhen data is loading, two layers are created: the *`image`* layer and the *`annotations`* layer. Order in the layer list correspond to the overlayed order. By clicking on these layers you will have acces to different layer controls (at the top left corner of the application). All actions can be undone/redone with the Ctrl-Z/Shift-Ctrl-Z keyboard shortcuts. You can also hide a layer by clicking on its eye icon on the layer list.\n    \n    \n<ins>For the *image* layer:</ins>\n- *`opacity`*: a slider to control the global opacity of the layer.\n- *`contrast limits`*: a double slider to manually control the contrast of the image (same as the <img src=\"https://user-images.githubusercontent.com/49953723/193262708-17e1d301-0a9a-497f-9feb-613e69893c06.PNG\" width=\"150px\"/> option for DICOM data).\n    \n\n<ins>For the *annotations* layer:</ins>\n- <img src=\"https://user-images.githubusercontent.com/49953723/193262718-30882770-59eb-4d2b-9cfe-8b88537560c4.PNG\" width=\"25px\"/>: erase brush to erase all labels at once (if *`preserve labels`* is not selected) or only erase the selected label (if *`preserve labels`* is selected).\n- <img src=\"https://user-images.githubusercontent.com/49953723/193262722-6bb6e6a4-ae7a-4ad1-b7f8-898e54ad62c3.PNG\" width=\"25px\"/>: paint brush with the same color than the *`label`* rectangle.\n- <img src=\"https://user-images.githubusercontent.com/49953723/193262719-f816b21e-78fd-4ba7-b415-30a461cbd652.PNG\" width=\"25px\"/>: fill bucket with the same color than the *`label`* rectangle.\n- <img src=\"https://user-images.githubusercontent.com/49953723/193262725-7d4f7b09-d119-45cf-a9d4-c42c5f848c1a.PNG\" width=\"25px\"/>: select to zoom in and out with the mouse wheel (same as the zoom slider at the top right corner in Panel 1).\n- *`label`*: a colored rectangle to represent the selected label.  \n- *`opacity`*: a slider to control the global opacity of the layer.  \n- *`brush size limits`*: a slider to control size of the paint/erase brush.    \n- *`preserve labels`*: if selected, all actions are applied only on the selected label (see the *`label`* rectangle); if not selected, actions are applied on all labels.\n- *`show selected`*: if selected, only the selected label will be display on the layer; if not selected, all labels are displayed.\n   \n    \n>*Remark*: a second option for filling has been added\n>1. Drawn the egde of a closed shape with the paint brush mode.  \n>2. Double click to activate the fill bucket.  \n>3. Click inside the closed area to fill it.  \n>4. Double click on the filled area to deactivate the fill bucket and reactivate the paint brush mode.\n    \n\n## Annotate your image *(use Panel 2)*\n    \nManual annotation and correction on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. /!\\ You have to choose a structure to start annotating *(see 2.)*.\n1. To modify an existing segmentation, you can directy open the segmented file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262702-df3b4fb8-63d0-4a1b-b1c9-8391cf8c3f22.PNG\" width=\"130px\"/> button. The file needs to have the same dimensions as the original image. \n    > /!\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files.  \n    \n2. Choose a structure to annotate in the drop-down menu\n    - *`Fetus`*: to annotate pregnancy image.\n    - *`Shoulder`*: to annotate bones and muscles for shoulder surgery.\n    - *`Shoulder Bones`*: to annotate only few bones for shoulder surgery.\n    - *`Feta Challenge`*: to annotate fetal brain MRI with the same label than the FeTA Challenge (see ADD LIEN WEB).\n    \n> When selecting a structure, a new panel appears with a list of elements to annotate. Each element has its own label and color. Select one element in the list to automatically activate the paint brush mode with the corresponding color (color is updated in the *`label`* rectangle in the layer controls panel).\n    \n3. All actions can be undone with the <img src=\"https://user-images.githubusercontent.com/49953723/193265848-8c458035-609a-433e-aa82-5d9588971425.PNG\" width=\"30px\"/> button or Ctrl-Z.\n    \n4. If you need to work on a specific slice of your 3D image, but also have to explore the volume to understand some complex structures, you can use the locking option to facilitate the annotation task.\n    - <ins>To activate the functionality</ins>: \n        1. Go to the slice of interest.\n        2. Click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262706-40f3dbca-5589-406d-81e8-e150ae8bfab6.PNG\" width=\"30px\"/> button => will change the button to <img src=\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\" width=\"30px\"/> and save the layer index.\n        3. Scroll in the z-axis to explore the data (with the mouse wheel or the slider under the image).\n        4. To go back to your slice of interest, click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\" width=\"30px\"/> button.\n    - <ins>To deactivate the functionality</ins> (or change the locked slice index): \n        1. Go to the locked slice.\n        2. Click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262703-2b2ea2dc-24fa-438b-a75c-3aa42b210f53.PNG\" width=\"30px\"/> button  => change the button to <img src=\"https://user-images.githubusercontent.com/49953723/193262706-40f3dbca-5589-406d-81e8-e150ae8bfab6.PNG\" width=\"30px\"/> and \"unlock\" the slice.\n\n\n## Select slices of interest *(use Panel 3 -- only displayed for the Shoulder Bones category)*\n\nThis panel will only be displayed if the *`Shoulder Bones`* category is selected. A maxiumum of 10 slices can be selected in a 3D image and the corresponding z-indexes will be integrated in the metadata during the exportation of the segmentation file.\n   \n   > /!\\ Metadata integration is available only for exported .tiff and .tif files and with the *`Unique`* save option. \n\n- <img src=\"https://user-images.githubusercontent.com/49953723/201736039-4ed10553-4a4b-4d5e-9d61-826dc139e437.png\" width=\"25px\"/> : to add the currently displayed z-index in the drop-down menu.\n- <img src=\"https://user-images.githubusercontent.com/49953723/201736105-a9c45264-412a-453b-8475-5a9ab856b07d.png\" width=\"25px\"/> : to remove the currently displayed z-index from the drop-down menu.\n- <img src=\"https://user-images.githubusercontent.com/49953723/201736152-319d8559-dbfc-4e52-aeb3-e8e34445f67a.png\" width=\"25px\"/> : to go to the z-index selected in the drop-down menu. The icon will be checked when the currently displayed z-index matches the selected z-index in the drop-down menu.\n- <img src=\"https://user-images.githubusercontent.com/49953723/201733835-7bee453a-bc07-416f-8b95-aaf803683cac.png\" width=\"100px\"/> : a drop-down menu containing the list of selected z-indexes. Select a z-index from the list to work with it more easily.\n\n\n## Export annotations *(use Panel 3 -- or 4 if the Shoulder Bones category is selected)*\n    \n1. Annotations can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\"https://user-images.githubusercontent.com/49953723/201735102-113f64b7-4da4-40ee-b058-9900268d270d.png\" width=\"95px\"/> button in one of the two following saving mode:\n    - *`Unique`*: segmented data is exported as a unique 3D image with corresponding label ids (1-2-3-...). This file can be re-opened in the application.\n    - *`Several`*: segmented data is exported as several binary 3D images (0 or 255), one for each label id.\n2. <img src=\"https://user-images.githubusercontent.com/49953723/193262699-95758bdb-ac40-439b-8959-d924781a2368.PNG\" width=\"100px\"/>: delete annotation data.\n3. *`Automatic segmentation backup`*: if selected, the segmentation data will be automatically exported as a unique 3D image when the image slice is changed.\n    > /!\\ This process can slow down the display if the image is large.\n\n# Hesperos: *OneShot Segmentation* mode\n    \n The ***OneShot Segmentation*** mode of the Hesperos plugin is a 2D version of the VoxelLearning method implemented in DIVA (see [our Github](https://github.com/DecBayComp/VoxelLearning) and the latest article [Gu√©rinot, C., Marcon, V., Godard, C., et al. (2022). New Approach to Accelerated Image Annotation by Leveraging Virtual Reality and Cloud Computing. _Frontiers in Bioinformatics_. doi:10.3389/fbinf.2021.777101](https://www.frontiersin.org/articles/10.3389/fbinf.2021.777101/full)).\n    \n\nThe principle is to accelerate the segmentation without prior information. The procedure consists of:\n1. A **rapid tagging** of few pixels in the image with two labels: one for the structure of interest (named positive tags), and one for the other structures (named negative tags).\n2. A **training** of a simple random forest classifier with these tagged pixels and their features (mean, gaussian, ...).\n3. An **inference** of all the pixels of the image to automatically segment the structure of interest. The output is a probability image (0-255) of belonging to a specific class.\n4. Iterative corrections if needed.\n    \n<img src=\"https://user-images.githubusercontent.com/49953723/193262714-8699cd59-3825-4d71-b27a-bbcad1e36d55.PNG\" width=\"1000px\"/>\n\n    \n## Import and adjust your image *(use Panel 1)*\n    \nSame panel as the *Manual Segmentation and Correction* mode *(see [panel 1 description](#import-and-adjust-your-image-use-panel-1))*.\n   \n    \n## Annotate your image *(use Panel 2)*\n    \nAnnotations and corrections on the segmented file is done using the layer controls of the *`annotations`* layer. Click on the layer to display them. Only two labels are available: *`Structure of interest`* and *`Other`*. \n\nThe rapid manual tagging step of the one-shot learning method aims to learn and attribute different features to each label.\n<img align=\"right\" src=\"https://user-images.githubusercontent.com/49953723/193262735-5dce56fb-8a2c-4aeb-9ee7-9727122d8089.PNG\" width=\"220px\"/> \nTo achieve that, the user has to:\n- with the label *`Structure of interest`*, tag few pixels of the structure of interest.\n- with the label *`Other`*, tag the greatest diversity of uninteresting structures in the 3D image (avoid tagging too much pixels).\n\n> see the exemple image with *`Structure of interest`* label in red and *`Other`* label in cyan.\n    \n1. To modify an existing segmentation, you can directy open the segmented file with the <img src=\"https://user-images.githubusercontent.com/49953723/193266118-dfd241f6-8f0b-4cb9-94e7-5e74a3ce8b6e.PNG\" width=\"130px\"/> button. The file needs to have the same dimensions as the original image. \n    > /!\\ Only .tiff, .tif, .nii and .nii.gz files are supported as segmented files. \n2. All actions can be undone with the <img src=\"https://user-images.githubusercontent.com/49953723/193265848-8c458035-609a-433e-aa82-5d9588971425.PNG\" width=\"30px\"/> button or Ctrl-Z.\n\n    \n## Run automatic segmentation *(use Panel 3)*\n\nFrom the previously tagged pixels, features are extracted and used to train a basic classifier : the Random Forest Classifier (RFC). When the training of the pixel classifier is done, it is applied to each pixel of the complete volume and outputs a probability to belong to the structure of interest.\n\nTo run training and inference, click on the <img src=\"https://user-images.githubusercontent.com/49953723/193262731-719c226a-f7c5-4252-b2bb-fade4ab7f5b3.PNG\" width=\"115px\"/> button:\n1. You will be asked to save a .pckl file which corresponds to the model.\n2. A new status will appears under the *Panel 4* : *`Computing...`*. You must wait for the message to change to: *`Ready`* before doing anything in the application (otherwise the application may freeze or crash).\n3. When the processing is done, two new layers will appear:\n    - the *`probabilities`* layer which corresponds to the direct probability (between 0 and 1) of a pixel to belong to the structure of interest. This layer is disabled by default, to enable it click on its eye icon in the layer list.\n    - the *`segmented probabilities`* layer which corresponds to a binary image obtained from the probability image normed and thresholded according to a value manually defined with the *`Probability threshold`* slider: <img src=\"https://user-images.githubusercontent.com/49953723/193262730-6998c8a5-92f1-4ff1-bbf5-6972a373afd2.PNG\" width=\"80px\"/>.\n\n>Remark: If the output is not perfect, you have two possibilities to improve the result:\n>1. Add some tags with the paint brush to take in consideration unintersting structures or add information in critical areas of your structure of interest (such as in thin sections). Then, run the training and inference process again. /!\\ This will overwrite all previous segmentation data.\n>2. Export your segmentation data and re-open it with the *Manual Annotation and Correction* mode of Hesperos to manually erase or add annotations.\n    \n    \n## Export annotations *(use Panel 4)*\n    \n1. Segmented probabilites can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262734-57159a97-2f46-4aba-b3bf-b55a35dfacbd.PNG\" width=\"105px\"/> button. The image is exported as a unique 3D binary image (value 0 and 255). This file can be re-opened in the application for correction.\n2. Probabilities can be exported as .tif, .tiff, .nii or .nii.gz file with the <img src=\"https://user-images.githubusercontent.com/49953723/193262733-26e37392-55b2-4c36-9287-b2f5d8d30e03.PNG\" width=\"105px\"/> button as a unique 3D image. The probabilities image is normed between 0 and 255.\n3. <img src=\"https://user-images.githubusercontent.com/49953723/193266056-9514b648-b3e0-43f5-901a-a45fa1390f00.PNG\" width=\"100px\"/>: delete annotation data.\n\n\n# License\n\nDistributed under the terms of the [BSD-3] license, **Hesperos** is a free and open source software.\n\n    \n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[Anaconda]: https://www.anaconda.com/products/distribution#Downloads\n[VoxelLearning]: https://github.com/DecBayComp/VoxelLearning\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Manual Segmentation or Correction",
      "OneShot Segmentation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-allencell-segmenter",
    "name": "napari-allencell-segmenter",
    "display_name": "napari-allencell-segmenter",
    "version": "2.1.12",
    "created_at": "2021-06-24",
    "modified_at": "2023-06-21",
    "authors": [
      "Allen Institute for Cell Science"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-allencell-segmenter/",
    "home_github": "https://github.com/AllenCell/napari-allencell-segmenter",
    "home_other": null,
    "summary": "A plugin that enables 3D image segmentation provided by Allen Institute for Cell Science",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari (>=0.4.9)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "aicssegmentation (>=0.5.3)",
      "magicgui (>=0.2.9)",
      "aicsimageio (~=4.0.5)",
      "opencv-python-headless (>=4.5.1)",
      "importlib-metadata (==4.11.4)",
      "npe2",
      "napari (>=0.4.9) ; extra == 'all'",
      "napari-plugin-engine (>=0.1.4) ; extra == 'all'",
      "numpy ; extra == 'all'",
      "aicssegmentation (>=0.5.3) ; extra == 'all'",
      "magicgui (>=0.2.9) ; extra == 'all'",
      "aicsimageio (~=4.0.5) ; extra == 'all'",
      "opencv-python-headless (>=4.5.1) ; extra == 'all'",
      "importlib-metadata (==4.11.4) ; extra == 'all'",
      "npe2 ; extra == 'all'",
      "black (>=19.10b0) ; extra == 'all'",
      "codecov (>=2.0.22) ; extra == 'all'",
      "docutils (<0.16,>=0.10) ; extra == 'all'",
      "flake8 (>=3.7.7) ; extra == 'all'",
      "psutil (>=5.7.0) ; extra == 'all'",
      "pytest (>=4.3.0) ; extra == 'all'",
      "pytest-cov (==2.6.1) ; extra == 'all'",
      "pytest-raises (>=0.10) ; extra == 'all'",
      "pytest-qt (>=3.3.0) ; extra == 'all'",
      "quilt3 (>=3.1.12) ; extra == 'all'",
      "pytest-runner ; extra == 'all'",
      "bumpversion (>=0.5.3) ; extra == 'all'",
      "coverage (>=5.0a4) ; extra == 'all'",
      "gitchangelog (>=3.0.4) ; extra == 'all'",
      "ipython (>=7.5.0) ; extra == 'all'",
      "m2r (>=0.2.1) ; extra == 'all'",
      "pytest-runner (>=4.4) ; extra == 'all'",
      "Sphinx (<3,>=2.0.0b1) ; extra == 'all'",
      "sphinx-rtd-theme (>=0.1.2) ; extra == 'all'",
      "tox (==3.25.0) ; extra == 'all'",
      "twine (>=1.13.0) ; extra == 'all'",
      "wheel (>=0.33.1) ; extra == 'all'",
      "black (>=19.10b0) ; extra == 'dev'",
      "bumpversion (>=0.5.3) ; extra == 'dev'",
      "coverage (>=5.0a4) ; extra == 'dev'",
      "docutils (<0.16,>=0.10) ; extra == 'dev'",
      "flake8 (>=3.7.7) ; extra == 'dev'",
      "gitchangelog (>=3.0.4) ; extra == 'dev'",
      "ipython (>=7.5.0) ; extra == 'dev'",
      "m2r (>=0.2.1) ; extra == 'dev'",
      "pytest (>=4.3.0) ; extra == 'dev'",
      "pytest-cov (==2.6.1) ; extra == 'dev'",
      "pytest-raises (>=0.10) ; extra == 'dev'",
      "pytest-runner (>=4.4) ; extra == 'dev'",
      "pytest-qt (>=3.3.0) ; extra == 'dev'",
      "quilt3 (>=3.1.12) ; extra == 'dev'",
      "Sphinx (<3,>=2.0.0b1) ; extra == 'dev'",
      "sphinx-rtd-theme (>=0.1.2) ; extra == 'dev'",
      "tox (==3.25.0) ; extra == 'dev'",
      "twine (>=1.13.0) ; extra == 'dev'",
      "wheel (>=0.33.1) ; extra == 'dev'",
      "pytest-runner ; extra == 'setup'",
      "black (>=19.10b0) ; extra == 'test'",
      "codecov (>=2.0.22) ; extra == 'test'",
      "docutils (<0.16,>=0.10) ; extra == 'test'",
      "flake8 (>=3.7.7) ; extra == 'test'",
      "psutil (>=5.7.0) ; extra == 'test'",
      "pytest (>=4.3.0) ; extra == 'test'",
      "pytest-cov (==2.6.1) ; extra == 'test'",
      "pytest-raises (>=0.10) ; extra == 'test'",
      "pytest-qt (>=3.3.0) ; extra == 'test'",
      "quilt3 (>=3.1.12) ; extra == 'test'"
    ],
    "package_metadata_description": "# napari-allencell-segmenter\n\n[![License](https://img.shields.io/pypi/l/napari-allencell-segmenter.svg?color=green)](https://github.com/AllenCell/napari-allencell-segmenter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-allencell-segmenter.svg?color=green)](https://pypi.org/project/napari-allencell-segmenter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-segmenter.svg?color=green)](https://python.org)\n[![Anaconda](https://anaconda.org/conda-forge/napari-allencell-segmenter/badges/version.svg)](https://anaconda.org/conda-forge/napari-allencell-segmenter)\n[![tests](https://github.com/AllenCell/napari-allencell-segmenter/workflows/tests/badge.svg)](https://github.com/AllenCell/napari-allencell-segmenter/actions)\n[![codecov](https://codecov.io/gh/AllenCell/napari-allencell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AllenCell/napari-allencell-segmenter)\n\n\nA plugin that enables 3D image segmentation provided by Allen Institute for Cell Science\n\nThe Allen Cell & Structure Segmenter plugin for napari provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). ‚Äã[The Allen Cell & Structure Segmenter](https://allencell.org/segmenter) is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes.\n\nMore details about Segmenter can be found at https://allencell.org/segmenter\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\n### Option 1 (recommended):\n\nAfter you installed the lastest version of napari, you can go to \"Plugins\" --> \"Install/Uninstall Package(s)\". Then, you will be able to see all available napari plugins and you can find us by name `napari-allencell-segmenter`. Just click the \"install\" button to install the Segmenter plugin.\n\n### Option 2:\n\nYou can also install `napari-allencell-segmenter` via [pip]:\n\n    pip install napari-allencell-segmenter\n\n## Quick Start\n\nIn the current version, there are two parts in the plugin: **workflow editor** and **batch processing**. The **workflow editor** allows users adjusting parameters in all the existing workflows in the lookup table, so that the workflow can be optimized on users' data. The adjusted workflow can be saved and then applied to a large batch of files using the **batch processing** part of the plugin. \n\n1. Open a file in napari (the plugin is able to support multi-dimensional data in .tiff, .tif. ome.tif, .ome.tiff, .czi)\n2. Start the plugin (open napari, go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"workflow editor\")\n3. Select the image and channel to work on\n4. Select a workflow based on the example image and target segmentation based on user's data. Ideally, it is recommend to start with the example with very similar morphology as user's data.\n5. Click \"Run All\" to execute the whole workflow on the sample data.\n6. Adjust the parameters of steps, based on the intermediate results. For instruction on the details on each function and the effect of each parameter, click the tooltip button. A complete list of all functions can be found [here](https://github.com/AllenCell/aics-segmentation/blob/main/aicssegmentation/structure_wrapper_config/function_params.md)\n7. Click \"Run All\" again after adjusting the parameters and repeat step 6 and 7 until the result is satisfactory.\n8. Save the workflow\n9. Close the plugin and open the **batch processing** part by (go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"batch processing\")\n10. Load the customized workflow (or an off-the-shelf workflow) json file\n11. Load the folder with all the images to process\n12. Click \"Run\"\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-allencell-segmenter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/AllenCell/napari-allencell-segmenter/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "WorkflowEditorWidget",
      "BatchProcessingWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-labelprop",
    "name": "napari-labelprop",
    "display_name": "napari Label Propagation",
    "version": "1.0.0",
    "created_at": "2023-06-19",
    "modified_at": "2023-06-21",
    "authors": [
      "nathandecaux"
    ],
    "author_emails": [
      "nathan.decaux@imt-atlantique.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-labelprop/",
    "home_github": null,
    "home_other": "None",
    "summary": "Label propagation through deep registration",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "deep-labelprop",
      "napari-nifti",
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-labelprop\n\n[![License](https://img.shields.io/pypi/l/napari-labelprop.svg?color=green)](https://github.com/nathandecaux/napari-labelprop/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-labelprop.svg?color=green)](https://pypi.org/project/napari-labelprop)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labelprop.svg?color=green)](https://python.org)\n[![tests](https://github.com/nathandecaux/napari-labelprop/workflows/tests/badge.svg)](https://github.com/nathandecaux/napari-labelprop/actions)\n[![codecov](https://codecov.io/gh/nathandecaux/napari-labelprop/branch/main/graph/badge.svg)](https://codecov.io/gh/nathandecaux/napari-labelprop)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelprop)](https://napari-hub.org/plugins/napari-labelprop)\n\n\n\n3D semi-automatic segmentation using deep registration-based 2D label propagation\n---------------------------------------------------------------------------------\n---\n\nThis [napari][napari] plugin was generated with [Cookiecutter][Cookiecutter] using [@napari][@napari]'s [cookiecutter-napari-plugin][cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## About\n\nSee \"Semi-automatic muscle segmentation in MR images using deep registration-based label propagation\" paper : \n\n[[Paper]![Paper](https://www.integrad.nl/assets/uploads/2016/02/cta-elsevier_logo-no_bg.png)](https://www.sciencedirect.com/science/article/pii/S0031320323002297?casa_token=r5FPBVXYXX4AAAAA:mStyUXb0i4lGqBmfF1j5fV1T9FuCMrpYfwh3lwQve2XAnzUBPZviAiFgMtH7lv6hdcWsA7yM) [[PDF]![PDF](https://www.ouvrirlascience.fr/wp-content/uploads/2018/12/HAL-3.png)](https://hal.science/hal-03945559/document)\n<p>\n  <img src=\"https://github.com/nathandecaux/labelprop.github.io/raw/main/demo_cut.gif\" width=\"600\">\n</p>\n\n## Installation\n\nTo install this project :\n\n    pip install napari['all']\n    pip install git+https://github.com/nathandecaux/napari-labelprop.git\n\n## Usage\n\nDownload [pretrained weights](https://raw.githubusercontent.com/nathandecaux/napari-labelprop/main/pretrained.ckpt).\n\nOpen napari from terminal and start using functions from 'napari-labelprop' plugin (Under Plugins scrolling menu).\n\nAvailable functions are :\n\n- Inference : Propagate labels from trained weights (Pytorch checkpoint required)\n- Training : Start training from scratch or from the pretrained weights.\n\nPS : \"Unsupervised pretraining\" is not yet implemented. See CLI option at [LabelProp](https://github.com/nathandecaux/labelprop) repository.\n\nEvery operation is done in the main thread. So, napari is not responsive during training or inference, but you can still follow the progress in the terminal.\n\n##### Training\n\nTo train a model, reach the plugin in the menu bar :\n\n    Plugins > napari-labelprop > Training\n\nFill the fields with the following information :\n\n- `Image` : Select a loaded napari.layers.Image layer to segment\n- `Labels` : Select a loaded napari.layers.Labels layer with the initial labels\n- `hints` : Select a loaded napari.layers.Labels layer with scribbled pseudo labels\n- `Pretrained checkpoint` : Select a pretrained checkpoint from the server-side checkpoint directory\n- `Slices shape` : Slices are resample to this shape for training and inference, then resampled to original shape. So far, slices must be squares.  \n- `Propagation axis` : Set the axis to use for the propagation dimension\n- `Max epochs` : Set the maximum number of epochs to train the model\n- `Checkpoint output directory`\n- `Checkpoint name`\n- `Weighting criteria` : Defines the criteria used to weight each direction of propagation `ncc = normalized cross correlation (slow but smooth), distance = distance to the nearest label (fast but less accurate)`\n- `Reduction` : When using ncc, defines the reduction to apply to the ncc map `mean / local_mean / none`. Default is `none`\n- `Use GPU` : Set if whether to use the GPU or not. Default is `True` (GPU). GPU:0 is used by default. To use another GPU, set the `CUDA_VISIBLE_DEVICES` environment variable before launching napari.\n\n##### Inference\n\nTo run inference on a model, reach the plugin in the menu bar :\n\n    Plugins > napari-labelprop-remote > Inference\n\nFill the fields like in the training section. Then, click on the `Run` button.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox][tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3][BSD-3] license,\n\"napari-labelprop\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Inference",
      "Training"
    ],
    "contributions_sample_data": [
      "napari Label Propagation"
    ]
  },
  {
    "normalized_name": "elastix-napari",
    "name": "elastix-napari",
    "display_name": "elastix-napari",
    "version": "0.2.1",
    "created_at": "2021-03-24",
    "modified_at": "2023-06-20",
    "authors": [
      "Viktor van der Valk"
    ],
    "author_emails": [
      "v.o.van_der_valk@lumc.nl"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/elastix-napari/",
    "home_github": "https://github.com/SuperElastix/elastix-napari",
    "home_other": null,
    "summary": "A toolbox for rigid and nonrigid registration of images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "itk-elastix (>=0.11.1)",
      "numpy (>=1.19.0)",
      "napari[all] (>=0.4.6)",
      "napari-plugin-engine (>=0.1.4)",
      "magicgui (>=0.4.0)",
      "itk-napari-conversion (>=0.3.1)",
      "napari-itk-io (>=0.1.0)"
    ],
    "package_metadata_description": "# elastix-napari\n\n[![License](https://img.shields.io/pypi/l/elastix-napari.svg?color=green)](https://github.com/SuperElastix/elastix-napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/elastix-napari.svg?color=green)](https://pypi.org/project/elastix-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/elastix-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/SuperElastix/elastix-napari/workflows/tests/badge.svg)](https://github.com/SuperElastix/elastix-napari/actions)\n[![codecov](https://codecov.io/gh/SuperElastix/elastix-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/SuperElastix/elastix-napari)\n[![Youtube](https://img.shields.io/badge/YouTube-Demo-red)](https://www.youtube.com/watch?v=GzbP-qUR034)\n\nThe [napari] plugin for [elastix], a toolbox for rigid and nonrigid registration of images, based on [itk-elastix].\n\nFor a demo video see [youtube] channel.\nFor tutorials on how to use elastix, see our [Jupyter notebooks].\n\nTo find parameters that work well with specific datasets, see the [elastix Model Zoo].\n\n<img width=\"1438\" alt=\"Screenshot 2021-05-12 at 15 07 24\" src=\"https://user-images.githubusercontent.com/33719474/117980045-d6009b00-b333-11eb-9976-f64d34f4f7cc.png\">\n\n## Installation\n\nYou can install `elastix-napari` via [pip]:\n\n    pip install elastix-napari\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"elastix-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/SuperElastix/elastix-napari/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[elastix]: https://elastix.lumc.nl/\n[itk-elastix]: https://github.com/InsightSoftwareConsortium/ITKElastix\n[elastix Model Zoo]: https://elastix.lumc.nl/modelzoo/\n[Jupyter notebooks]: https://mybinder.org/v2/gh/InsightSoftwareConsortium/ITKElastix/master?urlpath=lab/tree/examples%2FITK_Example01_SimpleRegistration.ipynb\n[youtube]: https://www.youtube.com/watch?v=GzbP-qUR034\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "elastix_registration",
      "transformix"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mrcfile-reader",
    "name": "napari-mrcfile-reader",
    "display_name": "napari-mrcfile-reader",
    "version": "0.2.0",
    "created_at": "2021-09-16",
    "modified_at": "2023-06-08",
    "authors": [
      "Alister Burt"
    ],
    "author_emails": [
      "alisterburt@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-mrcfile-reader/",
    "home_github": "https://github.com/alisterburt/napari-mrcfile-reader",
    "home_other": null,
    "summary": "Read MRC2014 files in napari using mrcfile.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "mrcfile",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-mrcfile-reader\n\n[![License](https://img.shields.io/pypi/l/napari-mrcfile-reader.svg?color=green)](https://github.com/alisterburt/napari-mrcfile-reader/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mrcfile-reader.svg?color=green)](https://pypi.org/project/napari-mrcfile-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mrcfile-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/alisterburt/napari-mrcfile-reader/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-mrcfile-reader/actions)\n[![codecov](https://codecov.io/gh/alisterburt/napari-mrcfile-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-mrcfile-reader)\n\nRead MRC format image files into napari using the [mrcfile] package from [CCP-EM]\n\n----------------------------------\n![example usage](example.gif)\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-mrcfile-reader` via [pip]:\n\n    pip install napari-mrcfile-reader\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mrcfile-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n[CCP-EM]: https://www.ccpem.ac.uk/\n[mrcfile]: https://github.com/ccpem/mrcfile\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/alisterburt/napari-mrcfile-reader/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.map",
      "*.ali",
      "*.rec",
      "*.mrcs",
      "*.preali",
      "*.mrc",
      "*.st"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-arboretum",
    "name": "napari-arboretum",
    "display_name": "napari-arboretum",
    "version": "0.1.2",
    "created_at": "2021-05-11",
    "modified_at": "2023-06-07",
    "authors": [
      "Alan R. Lowe"
    ],
    "author_emails": [
      "\"Alan R. Lowe\" <a.lowe@ucl.ac.uk>"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-arboretum/",
    "home_github": "https://github.com/lowe-lab-ucl/arboretum",
    "home_other": null,
    "summary": "Track graph and lineage tree visualization with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "matplotlib",
      "napari-matplotlib (>=0.2.1)",
      "napari (>=0.4.0)",
      "numpy (>=1.17.3)",
      "pandas",
      "pooch (>=1)",
      "qtpy",
      "scikit-image",
      "vispy"
    ],
    "package_metadata_description": " <!--[![Downloads](https://pepy.tech/badge/napari-arboretum)](https://pepy.tech/project/napari-arboretum)-->\n\n[![License](https://img.shields.io/pypi/l/napari-arboretum.svg?color=green)](https://github.com/lowe-lab-ucl/arboretum/blob/main/LICENSE.md)\n[![PyPI](https://img.shields.io/pypi/v/napari-arboretum.svg?color=green)](https://pypi.org/project/napari-arboretum)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-arboretum.svg?color=green)](https://python.org)\n[![tests](https://github.com/lowe-lab-ucl/arboretum/workflows/tests/badge.svg)](https://github.com/lowe-lab-ucl/arboretum/actions)\n[![codecov](https://codecov.io/gh/lowe-lab-ucl/arboretum/branch/main/graph/badge.svg?token=2M2HhM60op)](https://app.codecov.io/gh/lowe-lab-ucl/arboretum/tree/main)\n\n# Arboretum\n\n\n\nhttps://github.com/lowe-lab-ucl/arboretum/assets/8217795/d98c22c4-73bb-493a-9f8f-c224d615209d\n\n\n_Automated cell tracking and lineage tree reconstruction_.\n\n### Overview\n\nA dockable widget for [Napari](https://github.com/napari/napari) for visualizing cell lineage trees.\n\nFeatures:\n\n- Lineage tree plot widget\n- Integration with [btrack](https://github.com/quantumjot/btrack)\n\n---\n\n### Usage\n\nOnce installed, Arboretum will be visible in the `Plugins > Add Dock Widget > napari-arboretum` menu in napari. To visualize a lineage tree, (double) click on one of the tracks in a napari `Tracks` layer.\n\n### Examples\n\nYou can use the example script to display some sample tracking data in napari and load the arboretum tree viewer:\n\n```sh\npython ./examples/show_sample_data.py\n```\n\nAlternatively, you can use _btrack_ to generate tracks from your image data. See the example notebook here:\nhttps://github.com/quantumjot/btrack/blob/main/examples\n\n---\n\n### History\n\nThis project has changed considerably. The `Tracks` layer, originally developed for this plugin, is now an official layer type in napari. Read the napari documentation here:\nhttps://napari.org/stable/api/napari.layers.Tracks.html\n\nTo view the legacy version of this plugin, visit the legacy branch:\nhttps://github.com/quantumjot/arboretum/tree/v1-legacy\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Arboretum"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-data-preview",
    "name": "napari-data-preview",
    "display_name": "napari-data-preview",
    "version": "0.0.3a0",
    "created_at": "2023-02-17",
    "modified_at": "2023-06-07",
    "authors": [
      "Vivien Gaillet",
      "Jules Scholler"
    ],
    "author_emails": [
      "jules.scholler@wysscenter.ch"
    ],
    "license": "MPL-2.0",
    "home_pypi": "https://pypi.org/project/napari-data-preview/",
    "home_github": "https://github.com/WyssCenter/napari-data-preview",
    "home_other": null,
    "summary": "Preview lightsheet microscopes acquisition, and allow the creation of an XML for importing the data into TeraStitcher.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "napari[all]",
      "lxml",
      "napari-tools-menu",
      "magic-class",
      "napari-plugin-engine (>=0.1.4)",
      "dask",
      "tifffile",
      "dask-image",
      "elementpath",
      "tk (>=0.1.0)",
      "zarr",
      "scikit-image",
      "PySimpleGUI",
      "pytest-shutil"
    ],
    "package_metadata_description": "# napari-data-preview\n\nPreview lightsheet microscopes acquisition, and allow the creation of an XML for importing the data into TeraStitcher.\n\nThis package is very experimental, and you should expect bug/errors.\n\n## Installation\n\nYou can install `napari-data-preview` via [pip]:\n\n    pip install napari-data-preview\n\nTo install latest development version :\n\n    pip install git+https://github.com/WyssCenter/napari-data-preview.git\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Lightsheet data preview"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-annotate",
    "name": "napari-annotate",
    "display_name": "napari-annotate",
    "version": "0.0.2",
    "created_at": "2022-11-03",
    "modified_at": "2023-06-05",
    "authors": [
      "Jules Scholler"
    ],
    "author_emails": [
      "jules.scholler@wysscenter.ch"
    ],
    "license": "MPL-2.0",
    "home_pypi": "https://pypi.org/project/napari-annotate/",
    "home_github": null,
    "home_other": "https://github.com/WyssCenter",
    "summary": "Annotate large 2D slides",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari[all]",
      "napari-tools-menu",
      "magic-class",
      "napari-plugin-engine (>=0.1.4)"
    ],
    "package_metadata_description": "Plugin for annotating TissueScope data.\n\nThe plugin will be automatically deployed on Pypi and Napari hub upon release on GitHub (assign a new tag for it to be pushed correctly.)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Annotate next slide Huron",
      "Review ROI Huron",
      "Review segmentations Huron",
      "Add annotations Huron",
      "Annotate next slide Olympus",
      "Review ROI Olympus"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-figure",
    "name": "napari-figure",
    "display_name": "Figure",
    "version": "0.1.1",
    "created_at": "2023-06-02",
    "modified_at": "2023-06-02",
    "authors": [
      "romainGuiet"
    ],
    "author_emails": [
      "romain.guiet@epfl.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-figure/",
    "home_github": "https://github.com/BIOP/napari-figure",
    "home_other": null,
    "summary": "Making Figure with napari more easily",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "microfilm",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-figure\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-figure.svg?color=green)](https://github.com/romainGuiet/napari-figure/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-figure.svg?color=green)](https://pypi.org/project/napari-figure)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-figure.svg?color=green)](https://python.org)\n[![tests](https://github.com/romainGuiet/napari-figure/workflows/tests/badge.svg)](https://github.com/romainGuiet/napari-figure/actions)\n[![codecov](https://codecov.io/gh/romainGuiet/napari-figure/branch/main/graph/badge.svg)](https://codecov.io/gh/romainGuiet/napari-figure)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-figure)](https://napari-hub.org/plugins/napari-figure)\n\nMaking Figure with napari more easily\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-figure` via [pip]:\n\n    pip install napari-figure\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/romainGuiet/napari-figure.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-figure\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/romainGuiet/napari-figure/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Figure maker"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "imaxt-multiscale-plugin",
    "name": "imaxt-multiscale-plugin",
    "display_name": "IMAXT Multiscale Image Napari Plugin",
    "version": "0.3.1",
    "created_at": "2022-07-27",
    "modified_at": "2023-05-31",
    "authors": [
      "Eduardo Gonzalez Solares"
    ],
    "author_emails": [
      "E.GonzalezSolares@ast.cam.ac.uk"
    ],
    "license": "LGPL-3.0-only",
    "home_pypi": "https://pypi.org/project/imaxt-multiscale-plugin/",
    "home_github": null,
    "home_other": "https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin",
    "summary": "A simple plugin to use with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "xarray",
      "dask",
      "astropy",
      "zarr",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# IMAXT multiscale napari plugin\n\n[![License GNU LGPL v3.0](https://img.shields.io/pypi/l/imaxt-multiscale-plugin.svg?color=green)](https://github.com/eg266/imaxt-multiscale-plugin/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/imaxt-multiscale-plugin.svg?color=green)](https://pypi.org/project/imaxt-multiscale-plugin)\n[![Python Version](https://img.shields.io/pypi/pyversions/imaxt-multiscale-plugin.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/imaxt-multiscale-plugin)](https://napari-hub.org/plugins/imaxt-multiscale-plugin)\n\nA napari plugin to visualize multi-resolution images created with the IMAXT mosaic pipeline.\n\n----------------------------------------------------\n\n## Installation\n\nYou can install `imaxt-multiscale-plugin` via [pip]:\n\n    pip install imaxt-multiscale-plugin\n\n\n## Usage\n\nRun [napari] with the name of the sample to visualize either a local path:\n\n    napari /storage/imaxt/eglez/processed/stpt/20220606_PDX_AB559_GFP_005503_100x15um\n\nor a sample in S3 storage:\n\n    napari s3://imaxtgw/stpt/20220608_DI_PDX_SA535_Tum_5223_04280_100x15um\n    \n## Screenshots\n\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari1.png \"a title\")\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari2.png \"a title\")\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari3.png \"a title\")\n![Alt text](https://gitlab.developers.cam.ac.uk/astronomy/camcead/imaxt/imaxt-multiscale-plugin/-/raw/main/assets/napari4.png \"a title\")\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU LGPL v3.0] license,\n\"imaxt-multiscale-plugin\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [
      ".tif",
      ".npy",
      ".tiff"
    ],
    "contributions_widgets": [
      "Utilities Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-indices",
    "name": "napari-indices",
    "display_name": "indices",
    "version": "0.0.2",
    "created_at": "2023-05-30",
    "modified_at": "2023-05-31",
    "authors": [
      "Emmanuella OKAFOR"
    ],
    "author_emails": [
      "eokafor010@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-indices/",
    "home_github": "https://github.com/Emmanulla0/napari-indices",
    "home_other": null,
    "summary": "Calculer les indices de v√©g√©tation",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "spectral",
      "matplotlib",
      "tifffile",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-indices\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-indices.svg?color=green)](https://github.com/Emmanulla0/napari-indices/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-indices.svg?color=green)](https://pypi.org/project/napari-indices)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-indices.svg?color=green)](https://python.org)\n[![tests](https://github.com/Emmanulla0/napari-indices/workflows/tests/badge.svg)](https://github.com/Emmanulla0/napari-indices/actions)\n[![codecov](https://codecov.io/gh/Emmanulla0/napari-indices/branch/main/graph/badge.svg)](https://codecov.io/gh/Emmanulla0/napari-indices)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-indices)](https://napari-hub.org/plugins/napari-indices)\n\nCalculer les indices de v√©g√©tation\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\nEmmanuella OKAFOR (L3 PA CMI-PSI_Universit√© d'Angers) developed this plugin during her internship with a french team called ImHorPHen (lead by David ROUSSEAU). This plugin realises vegetation indexes computation with hyperspectral images. For the momment, there are five vegetation indexes : NDVI, TCARI, NPCI, SGI, NDGI.\n## Installation\n\nYou can install `napari-indices` via [pip]:\n\n    pip install napari-indices\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/Emmanulla0/napari-indices.git\n\n## Plugin description\n\n\nUsing this plugin requires importing the bands of a hyperspectral image into a **tif file**, in our case, 160 bands. You must launch it by accessing the **Plugins > napari-indices> Vegetation indices** menu.\n\n![Capture d'√©cran 2023-05-29 124720](https://github.com/Emmanuella0/napari-indices/assets/132358490/3b3895df-d0a7-466e-8ada-92bd4b642852)\n\nThen select the vegetation index to be calculated and the different bands to be used, then click the **Run** button to start the calculation. This results in the images corresponding to the calculated indices.\n\n![etape_2](https://github.com/Emmanuella0/napari-indices/assets/132358490/4875f0fc-3435-4875-ba4e-8881cb179b96)\n\n\nThe areas of interest to be analysed must then be defined. To do this, click the **Shapes** button on the Napari interface and choose the **add rectangle** shape from the menu that appears. Using the mouse, it is then possible to draw a rectangle on each of the two areas to be analyzed, for example a tree sheet and a green sheet of paper. \n\n![etape_3](https://github.com/Emmanuella0/napari-indices/assets/132358490/fc8612fe-5deb-4290-b4c3-9cac20acf1ce)\n\n\nTo perform the Fisher ratio calculation and display the histogram, it is necessary to go back to the **Plugins > napari-indices > ExempleQWidget** menu and click the **Click me! **. This action opens a new window displaying the best vegetation index to use, its corresponding Fisher ratio and the histogram of the two selected regions on the image of the vegetation index concerned. A video explaining the plugin is available at: https://uabox.univ-angers.fr/index.php/s/LqB0qs11n3jxZVJ.\n\n![Histogram](https://github.com/Emmanuella0/napari-indices/assets/132358490/be176176-1972-402c-9a01-8e367347d9d8)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-indices\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Emmanulla0/napari-indices/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Example QWidget",
      "Vegetation indices"
    ],
    "contributions_sample_data": [
      "indices"
    ]
  },
  {
    "normalized_name": "guanine-crystal-analysis",
    "name": "guanine-crystal-analysis",
    "display_name": "Guanine Crystal Analysis",
    "version": "0.0.2",
    "created_at": "2022-07-26",
    "modified_at": "2023-05-30",
    "authors": [
      "Mara Lampert"
    ],
    "author_emails": [
      "mara_harriet.lampert@mailbox.tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/guanine-crystal-analysis/",
    "home_github": "https://github.com/biapol/guanine-crystal-analysis",
    "home_other": null,
    "summary": "A plugin for the guanine crystal segmentation, classification and characterization in the zebrafish eye",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "apoc",
      "scikit-image",
      "pandas",
      "napari-simpleitk-image-processing",
      "napari-skimage-regionprops",
      "pyclesperanto-prototype",
      "scikit-learn",
      "napari-workflows",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# guanine-crystal-analysis\n\n[![License BSD-3](https://img.shields.io/pypi/l/guanine-crystal-analysis.svg?color=green)](https://github.com/biopo/guanine-crystal-analysis/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/guanine-crystal-analysis.svg?color=green)](https://pypi.org/project/guanine-crystal-analysis)\n[![Python Version](https://img.shields.io/pypi/pyversions/guanine-crystal-analysis.svg?color=green)](https://python.org)\n[![tests](https://github.com/biopo/guanine-crystal-analysis/workflows/tests/badge.svg)](https://github.com/biopo/guanine-crystal-analysis/actions)\n[![codecov](https://codecov.io/gh/biopo/guanine-crystal-analysis/branch/main/graph/badge.svg)](https://codecov.io/gh/biopo/guanine-crystal-analysis)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/guanine-crystal-analysis)](https://napari-hub.org/plugins/guanine-crystal-analysis)\n\nA plugin for guanine crystal segmentation and classification in the zebrafish eye. More precisely, it provides a workflow that measures on guanine crystal labels and sorts out overlaying partially segmented crystals during classification.\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Usage \n\nThis plugin is suited for users who\n- want to derive size-, shape and intensity-based parameters from individual guanine crystals\n- struggle with partially segmented or overlapping crystals\n- want to investigate further the size and shape of these guanine crystals\n\nThis plugin is not suited for users who \n- are interested in further investigations of intensity of guanine crystals\n\nYou can find the plugin in napari under `Plugins` ‚Üí `guanine-crystal-analysis`\n\n### Image Input\n\nThis plugin can be used on individual 2D slices of z-stacks as the workflow was developed on such input.\nTherefore, the quality of the result might differ on differing input, like crops or maximum projections.\n\n### 1. Normalization\n\nYou can normalize the image selecting `Normalization` where you only need to specify your input image and click on the `Run` button. \n\n![](img/plugin/normalization.png)\n\nNormalizing the image helps to adjust the intensity values and needs to be applied here because the object segmenter is only trained on normalized images.\n\n### 2. Segmentation\n\nWhen selecting `Segmentation`, you need to select the normalized image and a minimum pixel count of label images and click on the `Run` button again.\n![](img/plugin/segmentation.png)\nThis avoids having too small and unhelpful labels and is set by default to 50 pixels. \nFor the training of the model, an [APOC](https://github.com/haesleinhuepf/apoc) pixel classifier was used.\n\n### 3. Analyze Image\n\nUnder `Analyze Image`, you can extract features from your image and label image by selecting them and clicking on the `Run` button.  \n![](img/plugin/analyzeimage.png)\nThe extracted features are a combination of the two libraries [napari-skimage-regionprops](https://github.com/haesleinhuepf/napari-skimage-regionprops) and [napari-simpleitk-image-processing](https://github.com/haesleinhuepf/napari-simpleitk-image-processing). They can be devided into size-, shape-, and intensity-based parameters: \n\n| **size** | **shape**                 | **intensity**  \n|----------|---------------------------|-------------------|\n|![](img/plugin/size.png)      \t|![](img/plugin/shape.png)              \t|![](img/plugin/intensity.png)  \t|\n| area     \t| aspect ratio              \t| maximum intensity \t|\n|          \t| perimeter                 \t| mean intensity    \t|\n|          \t| major-axis-length         \t| minimum intensity \t|\n|          \t| minor-axis-length         \t| median            \t|\n|          \t| circularity               \t| sum               \t|\n|          \t| solidity                  \t| variance          \t|\n|          \t| eccentricity              \t|                   \t|\n|          \t| roundness                 \t|                   \t|\n|          \t| perimeter-on-border       \t|                   \t|\n|          \t| perimeter-on-border-ratio \t|                   \t|\n\nYou can find a glossary with an explanation of these features [in this blog post](https://focalplane.biologists.com/2023/05/03/feature-extraction-in-napari/)\nSome of the guanine crystals are not correctly segmented because of overlay or interference patterns. This problem is addressed with the help of a classification step demonstrated next\n\n### 4. Classify Objects\n\nYou can divide the crystal labels into predicted (blue) and discarded (brown) crystal labels using `Classify Objects`. There you can choose classifiers trained on intensity-, shape- and/or size-based parameters with the help of the checkboxes.\n![](img/plugin/classifyobjects.png)\nFor the training of the model, an [APOC](https://github.com/haesleinhuepf/apoc) object classifier was used.\nIt is recommended to later on not measure the parameters that the classifier was trained on, but other ones.\n\n### 5. Bad Label Exclusion\n\nNow, you can get rid of the discarded (brown) labels for further analysis using `Bad Label Exclusion`. Select the two label images of segmentation and classification result and press the `Run` button again. \n![](img/plugin/badlabelexclusion.png)\nThe result is a label image with only the predicted (blue) labels which are relabeled sequentially. If you want to derive measurements on these predicted labels, you can just use  `Analyze Image` again.\n\n### \"Analyze Deluxe\"\n\nYou can also do all the explained steps in one click using the `Analyze Deluxe` function.\n![](img/plugin/analyzedeluxe.png)\n\n\n## Installation\n\nYou can install `guanine-crystal-analysis` via [pip]:\n\n    pip install guanine-crystal-analysis\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/biopo/guanine-crystal-analysis.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Acknowledgements\nThis project was done in collaboration with the [Rita Mateus Laboratory](https://www.ritamateus.com/). The images shown in the documentation and in the demo jupyter notebooks were acquired there. \nThis project was supported by the Deutsche Forschungsgemeinschaft under Germany‚Äôs Excellence Strategy ‚Äì EXC2068 - Cluster of Excellence \"Physics of Life\" of TU Dresden. \nThis project has been made possible in part by grant number [2021-240341 (Napari plugin accelerator grant)](https://chanzuckerberg.com/science/programs-resources/imaging/napari/improving-image-processing/) from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"guanine-crystal-analysis\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/biopo/guanine-crystal-analysis/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Normalization",
      "Segmentation",
      "Analyze Image",
      "Classify Objects",
      "Bad Label Exclusion",
      "Analyze Deluxe"
    ],
    "contributions_sample_data": [
      "Guanine Crystal Analysis"
    ]
  },
  {
    "normalized_name": "napari-skimage-regionprops",
    "name": "napari-skimage-regionprops",
    "display_name": "napari-skimage-regionprops",
    "version": "0.10.1",
    "created_at": "2021-06-07",
    "modified_at": "2023-05-19",
    "authors": [
      "Marcelo Zoccoler",
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-skimage-regionprops/",
    "home_github": "https://github.com/haesleinhuepf/napari-skimage-regionprops",
    "home_other": null,
    "summary": "A regionprops table widget plugin for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "napari",
      "pandas",
      "napari-tools-menu (>=0.1.19)",
      "napari-workflows",
      "imageio (!=2.22.1)",
      "Deprecated"
    ],
    "package_metadata_description": "# napari-skimage-regionprops (nsr)\n\n\n\n[![License](https://img.shields.io/pypi/l/napari-skimage-regionprops.svg?color=green)](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/LICENSE)\n\n[![PyPI](https://img.shields.io/pypi/v/napari-skimage-regionprops.svg?color=green)](https://pypi.org/project/napari-skimage-regionprops)\n\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-skimage-regionprops.svg?color=green)](https://python.org)\n\n[![tests](https://github.com/haesleinhuepf/napari-skimage-regionprops/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-skimage-regionprops/actions)\n\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops)\n\n[![Development Status](https://img.shields.io/pypi/status/napari-skimage-regionprops.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-skimage-regionprops)](https://napari-hub.org/plugins/napari-skimage-regionprops)\n\n\n\n \n\nA [napari] plugin for measuring properties of labeled objects based on [scikit-image]\n\n\n\n![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/interactive.gif)\n\n\n\n## Usage: measure region properties\n\n\n\nFrom the menu `Tools > Measurement > Regionprops (nsr)` you can open a dialog where you can choose an intensity image, a corresponding label image and the features you want to measure:\n\n\n\n![img.png](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/dialog.png)\n\n\n\nIf you want to interface with the labels and see which table row corresponds to which labeled object, use the label picker and\n\nactivate the `show selected` checkbox.\n\n\n\n![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/interactive.png)\n\n\n\nIf you closed a table and want to reopen it, you can use the menu `Tools > Measurements > Show table (nsr)` to reopen it. \n\nYou just need to select the labels layer the properties are associated with.\n\n\n\nFor visualizing measurements with different grey values, as parametric images, you can double-click table headers.\n\n\n\n![img.png](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/label_value_visualization.gif)\n\n\n\n## Usage: measure point intensities\n\n\n\nAnalogously, also the intensity and coordinates of point layers can be measured using the menu `Tools > Measurement > Measure intensity at point coordinates (nsr)`. \n\nAlso these measurements can be visualized by double-clicking table headers:\n\n\n\n![img.png](measure_point_intensity.png)\n\n\n\n![img_1.png](measure_point_coordinate.png)\n\n\n\n## Working with time-lapse and tracking data\n\n\n\nNote that tables for time-lapse data should include a column named \"frame\", which indicates which slice in\n\ntime the given row refers to. If you want to import your own csv files for time-lapse data make sure to include this column.\n\nIf you have tracking data where each column specifies measurements for a track instead of a label at a specific time point,\n\nthis column must not be added.\n\n\n\nIn case you have 2D time-lapse data you need to convert it into a suitable shape using the function: `Tools > Utilities > Convert 3D stack to 2D time-lapse (time-slicer)`,\n\nwhich can be found in the [napari time slicer](https://www.napari-hub.org/plugins/napari-time-slicer).\n\n\n\nLast but not least, make sure that in case of time-lapse data the label image has labels that are subsquently labeled per timepoint.\n\nE.g. a dataset where label 5 is missing at timepoint 4 may be visualized incorrectly.\n\n\n\n## Usage: multichannel or multi-label data\n\n\n\nIf you want to relate objects from one channels to objects from another channel, you can use `Tools > Measurement tables > Object Features/Properties (scikit-image, nsr)`. \n\nThis plugin module allos you to answer questions like:\n\n  - how many objects I have inside other objects?\n\n  - what is the average intensity of the objects inside other objects?\n\nFor that, you need at least two labeled images in napari. You can relate objects along with their features. \n\nIf intensity features are also wanted, then you also need to provide two intensity images. \n\nBelow, there is a small example on how to use it. \n\nAlso, take a look at [this example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/measure_relationship_to_other_channels_plugin.ipynb).\n\n \n\n ![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/things_inside_things_demo.gif)\n\n\n\n## Usage, programmatically\n\n\n\nYou can also control the tables programmatically. See this \n\n[example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/tables.ipynb) for details on regionprops and\n\n[this example notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/measure_points.ipynb) for details on measuring intensity at point coordinates. For creating parametric map images, see [this notebook](https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/map_measurements.ipynb).\n\n\n\n\n\n## Features\n\nThe user can select categories of features for feature extraction in the user interface. These categories contain measurements from the scikit-image [regionprops list of measurements](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) library:\n\n* size:\n\n  * area\n\n  * bbox_area\n\n  * convex_area\n\n  * equivalent_diameter\n\n* intensity:\n\n  * max_intensity \n\n  * mean_intensity\n\n  * min_intensity\n\n  * standard_deviation_intensity (`extra_properties` implementation using numpy)\n\n* perimeter:\n\n  * perimeter\n\n  * perimeter_crofton\n\n* shape\n\n  * major_axis_length\n\n  * minor_axis_length\n\n  * orientation\n\n  * solidity\n\n  * eccentricity\n\n  * extent\n\n  * feret_diameter_max\n\n  * local_centroid\n\n  * roundness as defined for 2D labels [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html#set)\n\n  * circularity as defined for 2D labels  [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html#set)\n\n  * aspect_ratio as defined for 2D labels [by ImageJ](https://imagej.nih.gov/ij/docs/menus/analyze.html#set)\n\n* position:\n\n  * centroid\n\n  * bbox\n\n  * weighted_centroid\n\n* moments:\n\n  * moments\n\n  * moments_central\n\n  * moments_hu\n\n  * moments_normalized\n\n\n\n \n\n\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n\n## See also\n\n\n\nThere are other napari plugins with similar functionality for extracting features:\n\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)\n\n* [PartSeg](https://www.napari-hub.org/plugins/PartSeg)\n\n* [napari-simpleitk-image-processing](https://www.napari-hub.org/plugins/napari-simpleitk-image-processing)\n\n* [napari-cupy-image-processing](https://www.napari-hub.org/plugins/napari-cupy-image-processing)\n\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\n\n\n\nFurthermore, there are plugins for postprocessing extracted measurements\n\n* [napari-feature-classifier](https://www.napari-hub.org/plugins/napari-feature-classifier)\n\n* [napari-clusters-plotter](https://www.napari-hub.org/plugins/napari-clusters-plotter)\n\n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\n\n\n\n## Installation\n\n\n\nYou can install `napari-skimage-regionprops` via [pip]:\n\n\n\n    pip install napari-skimage-regionprops\n\n\n\nOr if you plan to develop it:\n\n\n\n    git clone https://github.com/haesleinhuepf/napari-skimage-regionprops\n\n    cd napari-skimage-regionprops\n\n    pip install -e .\n\n\n\nIf there is an error message suggesting that git is not installed, run `conda install git`.\n\n\n\n## Contributing\n\n\n\nContributions are very welcome. Tests can be run with [tox], please ensure\n\nthe coverage at least stays the same before you submit a pull request.\n\n\n\n## License\n\n\n\nDistributed under the terms of the [BSD-3] license,\n\n\"napari-skimage-regionprops\" is free and open source software\n\n\n\n## Issues\n\n\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n\n\n[napari]: https://github.com/napari/napari\n\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n\n[@napari]: https://github.com/napari\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[image.sc]: https://image.sc\n\n[napari]: https://github.com/napari/napari\n\n[tox]: https://tox.readthedocs.io/en/latest/\n\n[pip]: https://pypi.org/project/pip/\n\n[PyPI]: https://pypi.org/\n\n[scikit-image]: https://scikit-image.org/\n\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "duplicate_current_frame"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tiledb-bioimg",
    "name": "napari-tiledb-bioimg",
    "display_name": "napari TileDB bioimaging",
    "version": "0.0.1",
    "created_at": "2023-05-12",
    "modified_at": "2023-05-12",
    "authors": [
      "TileDB",
      "Inc."
    ],
    "author_emails": [
      "help@tiledb.io"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tiledb-bioimg/",
    "home_github": "https://github.com/TileDB-Inc/napari-tiledb-bioimg",
    "home_other": null,
    "summary": "Support reading and writing TileDB-Bioimaging image arrays within Napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "dask",
      "tiledb-bioimg (>=0.2.1)",
      "tiledb-cloud ; extra == 'cloud'",
      "napari ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-tiledb-bioimg\n\n[![License MIT](https://img.shields.io/pypi/l/napari-tiledb-bioimg.svg?color=green)](https://github.com/TileDB-Inc/napari-tiledb-bioimg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tiledb-bioimg.svg?color=green)](https://pypi.org/project/napari-tiledb-bioimg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tiledb-bioimg.svg?color=green)](https://python.org)\n[![tests](https://github.com/TileDB-Inc/napari-tiledb-bioimg/workflows/tests/badge.svg)](https://github.com/TileDB-Inc/napari-tiledb-bioimg/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tiledb-bioimg)](https://napari-hub.org/plugins/napari-tiledb-bioimg)\n\nThis plugin supports reading and writing TileDB-BioImaging multi-resolution arrays within Napari.\n\n----------------------------------\n\n## Demo\n\nhttps://github.com/TileDB-Inc/napari-tiledb-bioimg/assets/327706/b408d634-6ad0-4160-8571-18cf8e37b4cc\n\n## Installation\n\n[pending PyPI release!] You can install `napari-tiledb-bioimg` via [pip]:\n\n    pip install napari-tiledb-bioimg\n\n## Quickstart\n\nAfter [ingesting data using `tiledb-bioimg`](https://github.com/TileDB-Inc/TileDB-BioImaging#examples), then:\n\n- Local images can be loaded using Napari's `File -> Open Folder`, and selecting the TileDB array folder. Choose the `napari-tiledb-bioimg` plugin, if prompted.\n\n- Remote arrays (S3, TileDB Cloud) may be loaded using either the `napari` CLI command:\n\n```\nnapari --plugin napari-tiledb-bioimg s3://<bucket>/<path/to/tiledb_array>\n```\n\n- ... or the Napari viewer load command within the Python prompt:\n\n```\n# Within a Napari-enabled Python/IPython prompt, run:\nimport napari\nviewer = napari.Viewer()\n\nviewer.open(\"tiledb://<namespace>/<array name or UUID>\", plugin=\"napari-tiledb-bioimg\")\n```\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with tox or pytest.\n\n### Installation from git:\n\n```\npip install git+https://github.com/TileDB-Inc/napari-tiledb-bioimg.git\n```\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-tiledb-bioimg\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/TileDB-Inc/napari-tiledb-bioimg/issues/new) along with a detailed description.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tdb",
      "*.tiledb",
      "tiledb://*",
      "s3://*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TileDB"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-seedseg",
    "name": "napari-seedseg",
    "display_name": "SeedSeg Segmentation",
    "version": "0.0.2",
    "created_at": "2023-05-09",
    "modified_at": "2023-05-09",
    "authors": [
      "Reza Akbarian Bafghi"
    ],
    "author_emails": [
      "reza.akb98@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-seedseg/",
    "home_github": "https://github.com/rezaakb/napari-seedseg",
    "home_other": null,
    "summary": "A simple plugin for segmentation",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python-headless",
      "scikit-image>=0.19.3",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-seedseg\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-seedseg.svg?color=green)](https://github.com/rezaakb/napari-seedseg/tree/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-seedseg.svg?color=green)](https://pypi.org/project/napari-seedseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-seedseg.svg?color=green)](https://python.org)\n[![tests](https://github.com/rezaakb/napari-seedseg/workflows/tests/badge.svg)](https://github.com/rezaakb/napari-seedseg/actions)\n[![codecov](https://codecov.io/gh/rezaakb/napari-seedseg/branch/main/graph/badge.svg)](https://codecov.io/gh/rezaakb/napari-seedseg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-seedseg)](https://napari-hub.org/plugins/napari-seedseg)\n\nA simple plugin for 2D medical image segmentation. In this project, we are trying to use Flood method for segmentation. \nFlood segmentation, also known as flood fill or region growing, is an image segmentation technique that starts from a seed point and expands to neighboring pixels with similar properties (e.g., intensity, color). In our project, you only can segment one label at the time. Below is a description of the repository's structure and the purpose of each file:\n\n    .\n    ‚îú‚îÄ‚îÄ setup.cfg              # package metadata\n    ‚îú‚îÄ‚îÄ pyproject.toml         # use setuptools\n    ‚îú‚îÄ‚îÄ src/napari_seedseg     \n    ‚îÇ   ‚îú‚îÄ‚îÄ napari.yaml        # Load and stress tests\n    ‚îÇ   ‚îú‚îÄ‚îÄ __init.py__        # Python package metadata files\n    ‚îÇ   ‚îú‚îÄ‚îÄ _widget.py         # Widget contributions\n    ‚îÇ   ‚îú‚îÄ‚îÄ _layers.py         # Layers contributions\n    ‚îÇ   ‚îî‚îÄ‚îÄ _method.py         # Methods contributions\n    ‚îî‚îÄ‚îÄ ...\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-seedseg` via [pip]:\n\n    pip install napari-seedseg\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/rezaakb/napari-seedseg.git\n\n\n## Packages\nIn this project we have used these packages:\n\n    numpy\n    magicgui\n    qtpy\n    opencv-python-headless\n    scikit-image>=0.19.3\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-seedseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/rezaakb/napari-seedseg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SeedSeg"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-amdtrk",
    "name": "napari-amdtrk",
    "display_name": "Amend segmentation and track",
    "version": "1.1.0",
    "created_at": "2023-04-08",
    "modified_at": "2023-05-06",
    "authors": [
      "Yifan Gui"
    ],
    "author_emails": [
      "jeffgui9912@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-amdtrk/",
    "home_github": "https://github.com/Jeff-Gui/napari-amdtrk-plugin",
    "home_other": null,
    "summary": "Manually amend segmentation and track within napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy (<1.24)",
      "magicgui",
      "qtpy",
      "trackpy",
      "pandas",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-amdtrk\n\n[![License MIT](https://img.shields.io/pypi/l/napari-amdtrk.svg?color=green)](https://github.com/Jeff-Gui/napari-amdtrk/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-amdtrk.svg?color=green)](https://pypi.org/project/napari-amdtrk)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-amdtrk.svg?color=green)](https://python.org)\n[![tests](https://github.com/Jeff-Gui/napari-amdtrk/workflows/tests/badge.svg)](https://github.com/Jeff-Gui/napari-amdtrk/actions)\n[![codecov](https://codecov.io/gh/Jeff-Gui/napari-amdtrk/branch/main/graph/badge.svg)](https://codecov.io/gh/Jeff-Gui/napari-amdtrk)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-amdtrk)](https://napari-hub.org/plugins/napari-amdtrk)\n\nAmend segmentation and track within napari manually.\n\n<img src=\"preview.png\" alt=\"overview\" width=\"900\" />\n\n\n### [:eyes: watch a demo video](https://drive.google.com/file/d/1oHPdYcKv-QgOWylm21DnOF1NlVNsRIcL/view)\n\n----------------------------------\n\n### Input data structure\n\nNapari-amdtrk reads an input directory which includes:\n- An intensity image (`tif`) in txyc (or txy) format\n- An object mask (`tif`) in txy format\n- An object table (`csv`) with following essential columns:\n    - frame: time frame\n    - trackId: ID of the track, starting from 1\n    - Center_of_the_object_0: x coordinate\n    - Center_of_the_object_1: y coordinate\n    - continuous_label: the corresponding label (intensity value) of the object in the object mask (You may use `skimage.measure.label` to get it from a binary mask).\n\n- A config file named `config.yaml` (_other names are not allowed_)\n\n    Within the config file, there should be:\n    - intensity_suffix: suffix of the intensity image (e.g., for `foo_GFP.tif`, use `GFP` in the config). For multiple intensity images, separate them with commas (e.g., `GFP, mCherry`)\n    - mask_suffix: suffix of the mask image\n    - track_suffix: suffix of the tracked object table\n    - frame_base: index of the first frame (either `0` or `1`)\n    - stateCol: __optional__ column name for the cell state (e.g., cell cycle phase) in the object table. Leave blank if the object table does not contain it\n\n__Napari-amdtrk will modify mask and track files in place.__ Other files are not affected.\n\n---\n### Quick start\n\n1. Open `napari` GUI.\n2. `File` > `Open folder` > choose `Amend segmentation and track`\n3. `Plugins` > `napari-amdtrk: Amend track widget` > `Run`\n4. In `layer list`, select the `segm` layer to start editing.\n\nPlease check out the demo video [here](https://drive.google.com/file/d/1oHPdYcKv-QgOWylm21DnOF1NlVNsRIcL/view) and the sample data (see below).\n\n----------------------------------\n\n### Sample data\n\nTo load sample data, `File` > `Open Sample` > `napari-amdtrk` > `basic tracks` or `complete cell cycle tracks`.\n\n- basic tracks: simple cell tracks as essential input data.\n- complete cell cycle tracks: cell tracks with additional cell cycle features.\n\nThe above operations will download data to `~/.amd_trk/_sample_data/` (__~230MB__). After downloading is finished, sample data will be loaded.\n\n_Notes_\n- Please cite this repository if using the plugin in your work (try `About` > `Cite this repository` upper right of this homepage).\n  \n- Sample data (cell track videos) have been published with [_pcnaDeep: a fast and robust single-cell tracking method using deep-learning mediated cell cycle profiling_](10.1093/bioinformatics/btac602). We acknowledge Dr Kuan Yoow Chan and members of his lab for generating the data. \n\n----------------------------------\n\n### Keyboard shortcuts\n\n- <kbd>&uarr;</kbd> and <kbd>&darr;</kbd>: toggle different operations\n- <kbd>enter</kbd>: run the operation\n\n- Available to a selected object:\n  - <kbd>control</kbd> + <kbd>9</kbd>: shrink the object mask\n  - <kbd>control</kbd> + <kbd>0</kbd>: expand the object mask\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nPlease install `napari` GUI first:\n\n    python -m pip install \"napari[all]\"\n\nYou can install `napari-amdtrk` via [pip]:\n\n    pip install napari-amdtrk\n\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-amdtrk\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Amend track widget"
    ],
    "contributions_sample_data": [
      "basic tracks",
      "complete cell cycle tracks"
    ]
  },
  {
    "normalized_name": "psfmodels",
    "name": "psfmodels",
    "display_name": "psfmodels",
    "version": "0.3.3",
    "created_at": "2022-04-17",
    "modified_at": "2023-05-06",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "talley.lambert@gmail.com"
    ],
    "license": "GPL-3.0",
    "home_pypi": "https://pypi.org/project/psfmodels/",
    "home_github": "https://github.com/tlambert03/psfmodels",
    "home_other": null,
    "summary": "Scalar and vectorial models of the microscope point spread function (PSF).",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy (>=0.14.0)",
      "typing-extensions",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "flake8-typing-imports ; extra == 'dev'",
      "ipython ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pydocstyle ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "tox-conda ; extra == 'dev'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "jax ; extra == 'testing'",
      "magicgui ; (platform_system != \"Linux\") and extra == 'testing'",
      "qtpy ; (platform_system != \"Linux\") and extra == 'testing'",
      "pyside2 ; (platform_system != \"Linux\" and python_version < \"3.11\") and extra == 'testing'"
    ],
    "package_metadata_description": "# psfmodels\n\n[![PyPI](https://img.shields.io/pypi/v/psfmodels.svg?color=green)](https://pypi.org/project/psfmodels)\n[![Python\nVersion](https://img.shields.io/pypi/pyversions/psfmodels.svg?color=green)](https://python.org)\n[![CI](https://github.com/tlambert03/psfmodels/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/psfmodels/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/tlambert03/psfmodels/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/psfmodels)\n\nPython bindings for scalar and vectorial models of the point spread function.\n\nOriginal C++ code and MATLAB MEX bindings Copyright &copy; 2006-2013, [Francois\nAguet](http://www.francoisaguet.net/software.html), distributed under GPL-3.0\nlicense. Python bindings by Talley Lambert\n\nThis package contains three models:\n\n1. The vectorial model is described in Auget et al 2009<sup>1</sup>. For more\ninformation and implementation details, see Francois' Thesis<sup>2</sup>.\n2. A scalar model, based on Gibson & Lanni<sup>3</sup>.\n3. A gaussian approximation (both paraxial and non-paraxial), using paramters from Zhang et al (2007)<sup>4</sup>.\n\n<small>\n\n<sup>1</sup> [F. Aguet et al., (2009) Opt. Express 17(8), pp.\n6829-6848](https://doi.org/10.1364/OE.17.006829)\n\n<sup>2</sup> [F. Aguet. (2009) Super-Resolution Fluorescence Microscopy Based on\nPhysical Models. Swiss Federal Institute of Technology Lausanne, EPFL Thesis no.\n4418](http://bigwww.epfl.ch/publications/aguet0903.html)\n\n<sup>3</sup> [F. Gibson and F. Lanni (1992) J. Opt. Soc. Am. A, vol. 9, no. 1, pp. 154-166](https://opg.optica.org/josaa/abstract.cfm?uri=josaa-9-1-154)\n\n<sup>4</sup> [Zhang et al (2007). Appl Opt\n. 2007 Apr 1;46(10):1819-29.](https://doi.org/10.1364/AO.46.001819)\n\n</small>\n\n### see also:\n\nFor a different (faster) scalar-based Gibson‚ÄìLanni PSF model, see the\n[MicroscPSF](https://github.com/MicroscPSF) project, based on [Li et al\n(2017)](https://doi.org/10.1364/JOSAA.34.001029) which has been implemented in\n[Python](https://github.com/MicroscPSF/MicroscPSF-Py),\n[MATLAB](https://github.com/MicroscPSF/MicroscPSF-Matlab), and\n[ImageJ/Java](https://github.com/MicroscPSF/MicroscPSF-ImageJ)\n\n## Install\n\n```sh\npip install psfmodels\n```\n\n### from source\n\n```sh\ngit clone https://github.com/tlambert03/PSFmodels.git\ncd PSFmodels\npip install -e \".[dev]\"  # will compile c code via pybind11\n```\n\n## Usage\n\nThere are two main functions in `psfmodels`: `vectorial_psf` and `scalar_psf`.\nAdditionally, each version has a helper function called `vectorial_psf_centered`\nand `scalar_psf_centered` respectively. The main difference is that the `_psf`\nfunctions accept a vector of Z positions `zv` (relative to coverslip) at which\nPSF is calculated. As such, the point source may or may not actually be in the\ncenter of the rendered volume. The `_psf_centered` variants, by contrast, do\n_not_ accecpt `zv`, but rather accept `nz` (the number of z planes) and `dz`\n(the z step size in microns), and always generates an output volume in which the\npoint source is positioned in the middle of the Z range, with planes equidistant\nfrom each other. All functions accept an argument `pz`, specifying the position\nof the point source relative to the coverslip. See additional keyword arguments\nbelow\n\n_Note, all output dimensions (`nx` and `nz`) should be odd._\n\n```python\nimport psfmodels as psfm\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import PowerNorm\n\n# generate centered psf with a point source at `pz` microns from coverslip\n# shape will be (127, 127, 127)\npsf = psfm.make_psf(127, 127, dxy=0.05, dz=0.05, pz=0)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(psf[nz//2], norm=PowerNorm(gamma=0.4))\nax2.imshow(psf[:, nx//2], norm=PowerNorm(gamma=0.4))\nplt.show()\n```\n\n![Image of PSF](fig.png)\n\n```python\n# instead of nz and dz, you can directly specify a vector of z positions\nimport numpy as np\n\n# generate 31 evenly spaced Z positions from -3 to 3 microns\npsf = psfm.make_psf(np.linspace(-3, 3, 31), nx=127)\npsf.shape  # (31, 127, 127)\n```\n\n**all** PSF functions accept the following parameters. Units should be provided\nin microns unless otherwise stated. Python API may change slightly in the\nfuture.  See function docstrings as well.\n\n```\nnx (int):       XY size of output PSF in pixels, must be odd.\ndxy (float):    pixel size in sample space (microns) [default: 0.05]\npz (float):     depth of point source relative to coverslip (in microns) [default: 0]\nti0 (float):    working distance of the objective (microns) [default: 150.0]\nni0 (float):    immersion medium refractive index, design value [default: 1.515]\nni (float):     immersion medium refractive index, experimental value [default: 1.515]\ntg0 (float):    coverslip thickness, design value (microns) [default: 170.0]\ntg (float):     coverslip thickness, experimental value (microns) [default: 170.0]\nng0 (float):    coverslip refractive index, design value [default: 1.515]\nng (float):     coverslip refractive index, experimental value [default: 1.515]\nns (float):     sample refractive index [default: 1.47]\nwvl (float):    emission wavelength (microns) [default: 0.6]\nNA (float):     numerical aperture [default: 1.4]\n```\n\n## Comparison with other models\n\nWhile these models are definitely slower than the one implemented in [Li et al\n(2017)](https://doi.org/10.1364/JOSAA.34.001029) and\n[MicroscPSF](https://github.com/MicroscPSF), there are some interesting\ndifferences between the scalar and vectorial approximations, particularly with\nhigher NA lenses, non-ideal sample refractive index, and increasing spherical\naberration with depth from the coverslip.\n\nFor an interactive comparison, see the [examples.ipynb](notebooks/examples.ipynb) Jupyter\nnotebook.\n\n## Lightsheet PSF utility function\n\nThe `psfmodels.tot_psf()` function provides a quick way to simulate the total\nsystem PSF (excitation x detection) as might be observed on a light sheet\nmicroscope (currently, only strictly orthogonal illumination and detection are\nsupported).  See the [lightsheet.ipynb](notebooks/lightsheet.ipynb) Jupyter notebook for\nexamples.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PSF Generator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "palmari",
    "name": "palmari",
    "display_name": "Palmari",
    "version": "0.3.0",
    "created_at": "2022-05-06",
    "modified_at": "2023-05-01",
    "authors": [
      "Hippolyte Verdier"
    ],
    "author_emails": [
      "hverdier@pasteur.fr"
    ],
    "license": "\"CeCILL\"",
    "home_pypi": "https://pypi.org/project/palmari/",
    "home_github": "https://github.com/hippover/palmari",
    "home_other": null,
    "summary": "Palmari provides a plugin to analyze PALM movies, as well as microscope recordings of other SMLM-based SPT modalities. Set up your pipeline on one file, run it on a folder !",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "click",
      "dask (>=2022.1.0)",
      "dask-image (>=2021.12.0)",
      "imageio-ffmpeg",
      "magicgui (>=0.5.0)",
      "matplotlib (>=3.5)",
      "munkres",
      "napari",
      "napari-aicsimageio",
      "numpy",
      "pandas",
      "pyyaml",
      "qtpy",
      "scikit-image (>=0.18.3)",
      "scikit-learn",
      "toml",
      "tqdm",
      "trackpy (>=0.5.0)",
      "tox ; extra == 'testing'",
      "PyQt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "package_metadata_description": "# Palmari\n\n[![Documentation Status](https://readthedocs.org/projects/palmari/badge/?version=latest)](https://palmari.readthedocs.io/en/latest/?badge=latest)\n[![Python Version](https://img.shields.io/pypi/pyversions/palmari.svg?color=green)](https://python.org)\n[![tests](https://github.com/hippover/palmari/workflows/tests/badge.svg)](https://github.com/hippover/palmari/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/palmari)](https://napari-hub.org/plugins/palmari)\n[![License](https://img.shields.io/pypi/l/palmari.svg?color=green)](https://github.com/hippover/palmari/raw/main/LICENSE)\n\nA processing pipeline for PALM movies analysis (pre-processing, localization, drift correction, tracking).\n\nCheck out the [documentation] to get started.\n\n![napari_plugin](https://github.com/hippover/palmari/raw/main/docs/images/plugin_steps.png \"Fine-tune your pipelines on a movie, run it on a batch easily !\")\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `palmari` via [pip]:\n\n    pip install palmari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hippover/palmari.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [CeCILL] license.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hippover/palmari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[CeCILL]: http://cecill.info/index.en.html\n[documentation]: https://palmari.readthedocs.io/en/latest/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Run Palmari on file...",
      "Run Palmari on folder..."
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "koopa-viz",
    "name": "koopa-viz",
    "display_name": "Koopa",
    "version": "0.0.5",
    "created_at": "2022-09-29",
    "modified_at": "2023-04-28",
    "authors": [
      "Bastian Eichenberger"
    ],
    "author_emails": [
      "bastian@eichenbergers.ch"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/koopa-viz/",
    "home_github": "https://github.com/bbquercus/koopa",
    "home_other": null,
    "summary": "Vizualization plugin for koopa image analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "pyarrow",
      "qtpy",
      "tifffile",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "[![License MIT](https://img.shields.io/pypi/l/koopa-viz.svg?color=green)](https://github.com/bbquercus/koopa/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/koopa-viz.svg?color=green)](https://pypi.org/project/koopa-viz)\n[![Python Version](https://img.shields.io/pypi/pyversions/koopa-viz.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/koopa-viz)](https://napari-hub.org/plugins/koopa-viz)\n\n# koopa-viz\n\nVizualization plugin for koopa image analysis\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\nMore information can be found on the official [GitHub repo].\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[GitHub repo]: https://github.com/bbquercus/koopa\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[file an issue]: https://github.com/bbquercus/koopa/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Koopa"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "vessel-express-napari",
    "name": "vessel-express-napari",
    "display_name": "vessel-express-napari",
    "version": "0.0.9",
    "created_at": "2022-05-17",
    "modified_at": "2023-04-28",
    "authors": [
      "Lennart Kowitz"
    ],
    "author_emails": [
      "lennart.kowitz@isas.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/vessel-express-napari/",
    "home_github": "https://github.com/MMV-Lab/vessel-express-napari",
    "home_other": null,
    "summary": "A simple plugin for 3D vessel segmentation",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "itk",
      "scikit-image",
      "aicssegmentation"
    ],
    "package_metadata_description": "# vessel-express-napari\n\n[![License](https://img.shields.io/pypi/l/vessel-express-napari.svg?color=green)](https://github.com/MMV-Lab/vessel-express-napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/vessel-express-napari.svg?color=green)](https://pypi.org/project/vessel-express-napari/)\n[![Python Version](https://img.shields.io/pypi/pyversions/vessel-express-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/MMV-Lab/vessel-express-napari/workflows/tests/badge.svg)](https://github.com/MMV-Lab/vessel-express-napari/actions)\n[![codecov](https://codecov.io/gh/MMV-Lab/vessel-express-napari/branch/main/graph/badge.svg?token=mJPpDiioxu)](https://codecov.io/gh/MMV-Lab/vessel-express-napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/vessel-express-napari)](https://www.napari-hub.org/plugins/vessel-express-napari)\n\nA simple plugin for 3D vessel segmentation of LSFM images\n\nThis [napari] plugin can be used to optimize the segmentation parameters for the [main VesselExpress software platform](https://github.com/RUB-Bioinf/VesselExpress).\n\n----------------------------------\n\n\n## Installation\n\nThe easiest way to install the plugin is to open napari, go to Plugins, then Install/Uninstall plugins. You will be able to find the plugin by name \"vessel-express-napari\". \n\nOr, you can install `vessel-express-napari` via [pip]:\n\n    pip install vessel-express-napari\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/MMV-Lab/vessel-express-napari.git\n\n\n## Documentation\n\nWe provide a [quick start guide] to explain the important pieces of this plugin. Suggestions and feature quests are very welcomed. \n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"vessel-express-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/MMV-Lab/vessel-express-napari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[quick start guide]: https://github.com/MMV-Lab/vessel-express-napari/blob/main/quick_start.md\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "ParameterTuning",
      "Evaluation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-napari",
    "name": "napari-napari",
    "display_name": "Napari Napari",
    "version": "0.0.1",
    "created_at": "2023-04-23",
    "modified_at": "2023-04-23",
    "authors": [
      "Jordao Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-napari/",
    "home_github": "https://github.com/jookuma/napari-napari",
    "home_other": null,
    "summary": "A napari viewer of the napari viewer",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# [napari-napari](https://github.com/jookuma/napari-napari)\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-napari.svg?color=green)](https://github.com/jookuma/napari-napari/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-napari.svg?color=green)](https://pypi.org/project/napari-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/jookuma/napari-napari/workflows/tests/badge.svg)](https://github.com/jookuma/napari-napari/actions)\n[![codecov](https://codecov.io/gh/jookuma/napari-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/jookuma/napari-napari)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-napari)](https://napari-hub.org/plugins/napari-napari)\n\nA viewer of the napari viewer.\n\nhttps://user-images.githubusercontent.com/21022743/233817006-67ab4165-0b9a-46aa-9731-5964448252de.mp4\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-napari` via [pip]:\n\n    pip install napari-napari\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jookuma/napari-napari.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jookuma/napari-napari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-zulip",
    "name": "napari-zulip",
    "display_name": "napari-zulip",
    "version": "0.0.2",
    "created_at": "2023-04-22",
    "modified_at": "2023-04-22",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czi@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-zulip/",
    "home_github": "https://github.com/kephale/napari-zulip",
    "home_other": null,
    "summary": "A simple plugin for interacting with Zulip from napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "zulip",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-zulip\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-zulip.svg?color=green)](https://github.com/kephale/napari-zulip/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-zulip.svg?color=green)](https://pypi.org/project/napari-zulip)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-zulip.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-zulip/workflows/tests/badge.svg)](https://github.com/kephale/napari-zulip/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-zulip/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-zulip)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-zulip)](https://napari-hub.org/plugins/napari-zulip)\n\nA simple plugin for interacting with Zulip from napari.\n\n![An example screenshot of napari-zulip in action. It shows the plugin napari-boids and a filtered noise image, as well as a docked version of the napari-zulip plugin](./resources/demo_screenshot.png)  \n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-zulip` via [pip]:\n\n    pip install napari-zulip\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-zulip.git\n\n### Setting it up\n\nThe plugin is going to look for a file in `<home directory>/.zulip.d/napari.zulipchat.com.zuliprc`.\n\n**If you want to use this on a different zulip then adjust the `napari.zulipchat.com` to whatever the correct domain should be.**\n\n#### How to generate a `.zuliprc` file\n\nIn the Zulip app:\n- Select `Menu`\n- Select `Personal settings`\n- Select `Account & privacy`\n- Click on `Show/change your API key`\n- Enter your password\n- Click `Download .zuliprc` \n- Save the file as `<home directory>/.zulip.d/napari.zulipchat.com.zuliprc` (or change the domain name if using a different Zulip server)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-zulip\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-zulip/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Send the current screenshot to Zulip"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "test-detect-spots",
    "name": "test-detect-spots",
    "display_name": "napari Detect Spots",
    "version": "0.0.1",
    "created_at": "2023-04-20",
    "modified_at": "2023-04-20",
    "authors": [
      "Kevin Lai"
    ],
    "author_emails": [
      "klai@chanzuckerberg.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/test-detect-spots/",
    "home_github": null,
    "home_other": "None",
    "summary": "A dummy plugin to learn how to create plugins",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# test-detect-spots\n\n[![License BSD-3](https://img.shields.io/pypi/l/test-detect-spots.svg?color=green)](https://github.com/klai95/test-detect-spots/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/test-detect-spots.svg?color=green)](https://pypi.org/project/test-detect-spots)\n[![Python Version](https://img.shields.io/pypi/pyversions/test-detect-spots.svg?color=green)](https://python.org)\n[![tests](https://github.com/klai95/test-detect-spots/workflows/tests/badge.svg)](https://github.com/klai95/test-detect-spots/actions)\n[![codecov](https://codecov.io/gh/klai95/test-detect-spots/branch/main/graph/badge.svg)](https://codecov.io/gh/klai95/test-detect-spots)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/test-detect-spots)](https://napari-hub.org/plugins/test-detect-spots)\n\nA dummy plugin to learn how to create plugins\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `test-detect-spots` via [pip]:\n\n    pip install test-detect-spots\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"test-detect-spots\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Detect Spots Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-segment-anything",
    "name": "napari-segment-anything",
    "display_name": "Segment Anything",
    "version": "0.1.4",
    "created_at": "2023-04-05",
    "modified_at": "2023-04-19",
    "authors": [
      "Jordao Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-segment-anything/",
    "home_github": "https://github.com/jookuma/napari-segment-anything",
    "home_other": null,
    "summary": "Napari plugin of Segment Anything Model (SAM)",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "torch",
      "torchvision",
      "segment-anything",
      "qtpy",
      "magicgui",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-segment-anything\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-segment-anything.svg?color=green)](https://github.com/jookuma/napari-segment-anything/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-segment-anything.svg?color=green)](https://pypi.org/project/napari-segment-anything)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-segment-anything.svg?color=green)](https://python.org)\n[![tests](https://github.com/jookuma/napari-segment-anything/workflows/tests/badge.svg)](https://github.com/jookuma/napari-segment-anything/actions)\n[![codecov](https://codecov.io/gh/jookuma/napari-segment-anything/branch/main/graph/badge.svg)](https://codecov.io/gh/jookuma/napari-segment-anything)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-segment-anything)](https://napari-hub.org/plugins/napari-segment-anything)\n\nNapari plugin of [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything)\n\nDownload the network weights [here](https://github.com/facebookresearch/segment-anything#model-checkpoints)\n\n\nhttps://user-images.githubusercontent.com/21022743/230456433-2fa7bc40-a735-4d73-8d87-ecf776bbe2be.mp4\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-segment-anything` via [pip]:\n\n```bash\npip install napari-segment-anything\n```\n\nWe recommend installing the latest development version because this package is being developed:\n\n```bash\npip install git+https://github.com/jookuma/napari-segment-anything.git\n```\n\n**IMPORTANT**: `napari` won't work if you don't have `pyqt5` or `pyside2` installed.\n\n## Instructions\n\n### Opening napari-segment-anything\n\nThis software is napari plugin, so once you have napari installed you can open it using the command line:\n\n```bash\nnapari <your image path> -w napari-segment-anything 'Segment Anything'\n```\n\nI noticed that sometimes napari fails to load the plugin widget from the command line, go to step 2 from below to load it.\n\nIf you prefer the user interface you need to:\n\n1) Drag and drop your image into napari to load it;\n2) Go to the \"Plugins\" file menu in the upper right corner and select the \"Segment Anything\" plugin.\n3) Follow the instructions below for usage.\n\n**IMPORTANT**: If you get an error make sure you have `pyqt5` or `pyside2` installed.\n\n### Usage\n\n- Interactions are done on the \"SAM points\" and \"SAM box\" layers using the existing functionalities of napari. Only rectangle shapes trigger the network prediction.\n- For points supervision, left clicks are positive cues (object) and right clicks are negative (background).\n- Press the \"Confirm Annot.\" button (or the \"C\" key) to propagate the current segmentation mask to the label image.\n- Use the napari labels layer features to delete or edit already confirmed labels.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-segment-anything\" is a free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jookuma/napari-segment-anything/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Segment Anything"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-vodex",
    "name": "napari-vodex",
    "display_name": "VoDEx",
    "version": "1.0.12",
    "created_at": "2023-01-04",
    "modified_at": "2023-04-17",
    "authors": [
      "Anna Nadtochiy"
    ],
    "author_emails": [
      "lemonjustgithub@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-vodex/",
    "home_github": "https://github.com/LemonJust/napari-vodex",
    "home_other": null,
    "summary": "A napari plugin for VoDEx : Volumetric Data and Experiment Manager. Allows to load volumetric data based on experimental conditions.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "vodex (>=1.0.12)",
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-vodex\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vodex.svg?color=green)](https://github.com/LemonJust/napari-vodex/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-vodex.svg?color=green)](https://pypi.org/project/napari-vodex)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vodex.svg?color=green)](https://python.org)\n[![tests](https://github.com/LemonJust/napari-vodex/workflows/tests/badge.svg)](https://github.com/LemonJust/napari-vodex/actions)\n[![codecov](https://codecov.io/gh/LemonJust/napari-vodex/branch/main/graph/badge.svg)](https://codecov.io/gh/LemonJust/napari-vodex)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vodex)](https://napari-hub.org/plugins/napari-vodex)\n\nA plugin to load volumetric data based on experimental conditions.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-vodex` via [pip]:\n\n    pip install napari-vodex\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/LemonJust/napari-vodex.git\n\n## How-To Guide\n\nTo get started with napari_vodex, please see details and examples in [How-To Guide](https://lemonjust.github.io/vodex/napari/how-to/) .\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-vodex\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/LemonJust/napari-vodex/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Vodex Data Loader"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-hello",
    "name": "napari_hello",
    "display_name": "napari_hello",
    "version": "0.1.0",
    "created_at": "2023-04-16",
    "modified_at": "2023-04-16",
    "authors": [
      "Your Name"
    ],
    "author_emails": [
      "your.email@example.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-hello/",
    "home_github": null,
    "home_other": "None",
    "summary": "My napari plugin",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "napari"
    ],
    "package_metadata_description": null,
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spreadsheet",
    "name": "napari-spreadsheet",
    "display_name": "Spreadsheet",
    "version": "0.0.4",
    "created_at": "2022-08-28",
    "modified_at": "2023-04-16",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin-sc@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-spreadsheet/",
    "home_github": "https://github.com/hanjinliu/napari-spreadsheet",
    "home_other": null,
    "summary": "A spreadsheet widget for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "napari",
      "numpy",
      "pandas",
      "qtpy",
      "tabulous (>=0.5.0)",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-spreadsheet\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-spreadsheet.svg?color=green)](https://github.com/hanjinliu/napari-spreadsheet/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spreadsheet.svg?color=green)](https://pypi.org/project/napari-spreadsheet)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spreadsheet.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-spreadsheet/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-spreadsheet/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-spreadsheet/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-spreadsheet)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spreadsheet)](https://napari-hub.org/plugins/napari-spreadsheet)\n\nLet's replace Microsoft Excel or Google Spreadsheet with `napari-spreadsheet` for your daily image analysis.\n\n### Highlights\n\n- Convert layer features to a spreadsheet.\n- Update layer features from a spreadsheet.\n- Send spreadsheet data to the namespace of napari's console directly.\n\n![](https://github.com/hanjinliu/napari-spreadsheet/blob/main/images/image.png)\n\nThis plugin is largely dependent on [tabulous](https://github.com/hanjinliu/tabulous). To know more about the user interface, please see the [documentation](https://hanjinliu.github.io/tabulous/main/user_interface.html).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-spreadsheet` via [pip]:\n\n    pip install napari-spreadsheet\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-spreadsheet.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-spreadsheet\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hanjinliu/napari-spreadsheet/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.xlsx",
      "*.txt",
      "*.csv",
      "*.dat"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Spreadsheet"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-griottes",
    "name": "napari-griottes",
    "display_name": "Griottes",
    "version": "0.4.1",
    "created_at": "2023-03-09",
    "modified_at": "2023-04-14",
    "authors": [
      "Andrey Aristov"
    ],
    "author_emails": [
      "aaristov@pasteur.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-griottes/",
    "home_github": "https://github.com/aaristov/napari-griottes",
    "home_other": null,
    "summary": "Create graphs",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "griottes",
      "networkx",
      "numpy",
      "pandas (<2)"
    ],
    "package_metadata_description": "# napari-griottes\n\n[![License](https://img.shields.io/pypi/l/napari-griottes.svg?color=green)](https://github.com/BaroudLab/napari-griottes/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-griottes.svg?color=green)](https://pypi.org/project/napari-griottes)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-griottes.svg?color=green)](https://python.org)\n[![tests](https://github.com/BaroudLab/napari-griottes/workflows/tests/badge.svg)](https://github.com/BaroudLab/napari-griottes/actions)\n[![codecov](https://codecov.io/gh/BaroudLab/napari-griottes/branch/main/graph/badge.svg)](https://codecov.io/gh/BaroudLab/napari-griottes)\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-griottes)](https://napari-hub.org/plugins/napari-griottes)\n\n\n\nUse [üçí  `Griottes` üçí](https://github.com/BaroudLab/Griottes) in napari!\n\n----------------------------------\n\n\n\nhttps://user-images.githubusercontent.com/11408456/224119160-c381091d-8275-449e-9cf4-679ab474acd2.mp4\n\n\n\n\n## Installation\n\nInstall from napari\n\n![image](https://user-images.githubusercontent.com/11408456/224108834-f484ba37-50f4-415e-bdfb-509c6c5b88c4.png)\n\n\nYou can install `napari-griottes` via [pip]:\n\n    pip install napari-griottes\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/BaroudLab/napari-griottes.git\n\n\n\n## Usage\n\n### Starting with labels:\n\n1. Open the plugin in Plugins/napari-griottes\n2. Make sure the layer with labels is selected\n3. Click Run once to get centers\n4. Click Run second time to get graph\n5. Select the right kind of graph in the drop-down menu\n6. Adjust the distance\n7. Adjust thickness\n\n![Screenshot from three labels geometric contact mp4](https://user-images.githubusercontent.com/11408456/167371516-05db2ba5-cdfc-47c4-a488-8f46afd0ae5b.png)\n\n\n\nhttps://user-images.githubusercontent.com/11408456/167825581-47c39884-34cf-4b5c-ad84-a4572217559d.mp4\n\n\n\n### Starting with Segmented cells\n\n1. Open sample data: File / Open Sample / napari-griottes / Zebrafish 2D with labels\n2. Select the top layer and covert it to labels (right click - Convert to labels)\n3. Run the plugin once to get the centers of labels\n4. Run the plugin twice to get the connections\n5. Proceed with graph creation\n\n\n![Screenshot from cells graphs mp4](https://user-images.githubusercontent.com/11408456/167372895-3c9036b9-af50-4575-bcf3-1805eb261bd7.png)\n\n\n\n\nhttps://user-images.githubusercontent.com/11408456/168237170-b43afd5a-26a4-4cdc-bc42-d3f46f138536.mp4\n\n\n### Saving and recovering the graph\n\nAny graph you see in napari can be saved in .json format.\n1. Select he layers with connections\n2. Click File/Save Selected Layer\n3. Choose Griottes in drop-down menu\n4. Save\n\nIn order to recover a previously saved graph in napari, you can simply drag-n-drop your file into napari, or use file open fialog.\n\n\n\nhttps://user-images.githubusercontent.com/11408456/167845853-e7071199-3f58-4d11-8d7b-c1358a150e6b.mp4\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-griottes\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/BaroudLab/napari-griottes/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.json",
      "*.csv",
      "*.griottes",
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".json"
    ],
    "contributions_widgets": [
      "Make graph",
      "Save graph"
    ],
    "contributions_sample_data": [
      "Cell properties table 3D",
      "Zebrafish 2D with labels"
    ]
  },
  {
    "normalized_name": "napari-ui-tracer",
    "name": "napari-ui-tracer",
    "display_name": "Napari UI tracer",
    "version": "0.1.2",
    "created_at": "2023-03-14",
    "modified_at": "2023-04-13",
    "authors": [
      "Daniel Althviz"
    ],
    "author_emails": [
      "dalthviz@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-ui-tracer/",
    "home_github": "https://github.com/dalthviz/napari-ui-tracer",
    "home_other": null,
    "summary": "A plugin to help understand Napari UI components and check their source code definition",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "qtpy (>=2.3.0)",
      "pre-commit ; extra == 'pre-commit'",
      "pyqt5 ; extra == 'pyqt5'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-ui-tracer\n\n[![License MIT](https://img.shields.io/pypi/l/napari-ui-tracer.svg?color=green)](https://github.com/dalthviz/napari-ui-tracer/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ui-tracer.svg?color=green)](https://python.org)\n[![PyPI](https://img.shields.io/pypi/v/napari-ui-tracer.svg?color=green)](https://pypi.org/project/napari-ui-tracer)\n[![PyPI download month](https://img.shields.io/pypi/dm/napari-ui-tracer.svg?color=green)](https://pypi.org/project/napari-ui-tracer/)\n[![conda version](https://img.shields.io/conda/vn/conda-forge/napari-ui-tracer.svg?color=blue)](https://anaconda.org/conda-forge/napari-ui-tracer)\n[![conda download count](https://img.shields.io/conda/d/conda-forge/napari-ui-tracer.svg?color=blue)](https://anaconda.org/conda-forge/napari-ui-tracer)\n[![tests](https://github.com/dalthviz/napari-ui-tracer/workflows/tests/badge.svg)](https://github.com/dalthviz/napari-ui-tracer/actions)\n[![codecov](https://codecov.io/gh/dalthviz/napari-ui-tracer/branch/main/graph/badge.svg?token=E6je6vXOSA)](https://codecov.io/gh/dalthviz/napari-ui-tracer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-ui-tracer)](https://napari-hub.org/plugins/napari-ui-tracer)\n\nA plugin to help understand Napari UI components and locate their code definition\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n![GIF showing Napari UI tracer's functionality](https://raw.githubusercontent.com/dalthviz/napari-ui-tracer/main/images/napari-ui-tracer.gif)\n\n## Installation\n\nYou can install `napari-ui-tracer` via [pip]:\n\n    pip install napari-ui-tracer\n\nOr via [conda]:\n\n    conda install -c conda-forge napari-ui-tracer\n\nTo install latest development version :\n\n    pip install git+https://github.com/dalthviz/napari-ui-tracer.git\n\n## Usage\n\n1. Show the plugin inside the napari interface:\n\n    * You can launch napari with the plugin visible running:\n\n            napari -w napari-ui-tracer\n\n    * Or select it from `Plugins > Napari UI tracer widget`\n\n2. Check the `Enable Qt event filter` checkbox:\n    * Use `Ctrl/Cmd + Mouse button right click` to see the information available about any widget inside napari\n    * An option to show objects documentation (object class docstring) can be used by checking the `Show object documentation` checkbox\n\n3. Check the `Enable application events logging` checkbox:\n    * A log like information with the events generated when interacting with the application will appear\n    * Some configuration options are available:\n        * `Stack depth`: Stack depth to show. Default to 20\n        * `Allowed nested events`: How many sub-emit nesting levels to show (i.e. events that get triggered by other events). Default to 0\n\n4. If you want to explore the related widget or event module source file, click the link in the output section of the plugin (the module file will open if you have a registered program to open such kind of file)\n\n## Contributing\n\nContributions are very welcome. Pre-commit is used for formatting. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-ui-tracer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/dalthviz/napari-ui-tracer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[conda]: https://docs.conda.io/projects/conda/en/stable/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Napari UI tracer widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-blender-bridge",
    "name": "napari-blender-bridge",
    "display_name": "napari-blender-bridge",
    "version": "0.2.0",
    "created_at": "2023-04-07",
    "modified_at": "2023-04-10",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "GNU GPL v3.0",
    "home_pypi": "https://pypi.org/project/napari-blender-bridge/",
    "home_github": "https://github.com/haesleinhuepf/napari-blender-bridge",
    "home_other": null,
    "summary": "Transfer surface layers between Napari and Blender",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "napari-process-points-and-surfaces (>=0.4.2)",
      "vedo"
    ],
    "package_metadata_description": "# napari-blender-bridge\n\n\n\n[![License](https://img.shields.io/pypi/l/napari-blender-bridge.svg?color=green)](https://github.com/haesleinhuepf/napari-blender-bridge/raw/master/LICENSE)\n\n[![PyPI](https://img.shields.io/pypi/v/napari-blender-bridge.svg?color=green)](https://pypi.org/project/napari-blender-bridge)\n\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-blender-bridge.svg?color=green)](https://python.org)\n\n[![tests](https://github.com/haesleinhuepf/napari-blender-bridge/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-blender-bridge/actions)\n\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-blender-bridge/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-blender-bridge)\n\n[![Development Status](https://img.shields.io/pypi/status/napari-blender-bridge.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blender-bridge)](https://napari-hub.org/plugins/napari-blender-bridge)\n\n\n\nTransfer surface layers between Napari and Blender. This plugin is young and has just limited functionality. Contributions are welcome.\n\n\n\n\n\n![img.png](https://github.com/haesleinhuepf/napari-blender-bridge/raw/main/docs/easter.gif)\n\n\n\n## Usage\n\n\n\nThis plugin has its own submenu with all functionality under `Tools > Blender`. You can start up Blender, send a surface layer to Blender, retrieve all meshes back as one surface layer and shut down Blender.\n\n\n\n## Installation instructions\n\n\n\n* Download and install [Blender 3.5](https://www.blender.org/download/). \n\n* Start Blender and click the menu `Edit > Preferences`. Activate `Developer extras`.\n\n\n\n![img.png](https://github.com/haesleinhuepf/napari-blender-bridge/raw/main/docs/blender_preferences.png)\n\n\n\n* It is recommended to run this plugin in a conda environment together with [devbio-napari](https://github.com/haesleinhuepf/devbio-napari), \n\n[vedo](https://vedo.embl.es/) and [napari-process-points-and-surfaces](https://github.com/haesleinhuepf/napari-process-points-and-surfaces).\n\nTo install these, please run these commands line-by-line:\n\n```\n\nmamba create --name napari-blender-env python=3.9 devbio-napari vedo -c conda-forge\n\nmamba activate napari-blender-env\n\npip install napari-process-points-and-surfaces napari-blender-bridge\n\n```\n\n\n\n## Similar and related plugins\n\n\n\nThere are other plugins for working with surface meshes:\n\n* [napari-stress](https://github.com/campaslab/napari-stress)\n\n* [napari-pymeshlab](https://github.com/zacsimile/napari-pymeshlab)\n\n* [napari-process-points-and-surfaces](https://github.com/haesleinhuepf/napari-process-points-and-surfaces)\n\n\n\n## Contributing\n\n\n\nContributions are very welcome. Tests can be run with [tox], please ensure\n\nthe coverage at least stays the same before you submit a pull request.\n\n\n\n## License\n\n\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\n\"napari-blender-bridge\" is free and open source software\n\n\n\n## Issues\n\n\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n\n[napari]: https://github.com/napari/napari\n\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n\n[@napari]: https://github.com/napari\n\n[MIT]: http://opensource.org/licenses/MIT\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n\n[cookiecutter-napari-plugin]: https://github.com/haesleinhuepf/cookiecutter-napari-assistant-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-blender-bridge/issues\n\n[napari]: https://github.com/napari/napari\n\n[tox]: https://tox.readthedocs.io/en/latest/\n\n[pip]: https://pypi.org/project/pip/\n\n[PyPI]: https://pypi.org/\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-process-points-and-surfaces",
    "name": "napari-process-points-and-surfaces",
    "display_name": "napari-process-points-and-surfaces",
    "version": "0.5.0",
    "created_at": "2022-02-05",
    "modified_at": "2023-04-09",
    "authors": [
      "Robert Haase",
      "Johannes Soltwedel"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-process-points-and-surfaces/",
    "home_github": "https://github.com/haesleinhuepf/napari-process-points-and-surfaces",
    "home_other": null,
    "summary": "Process and analyze surfaces using open3d and vedo in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "napari-tools-menu (>=0.1.14)",
      "napari-time-slicer (>=0.4.5)",
      "napari-workflows (>=0.2.3)",
      "vedo (>=2022.4.1)",
      "napari-skimage-regionprops (>=0.5.5)",
      "pandas",
      "imageio (!=2.22.1)",
      "stackview (>=0.5.2)"
    ],
    "package_metadata_description": "# napari-process-points-and-surfaces (nppas)\n\n[![License](https://img.shields.io/pypi/l/napari-process-points-and-surfaces.svg?color=green)](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-process-points-and-surfaces.svg?color=green)](https://pypi.org/project/napari-process-points-and-surfaces)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-process-points-and-surfaces.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-process-points-and-surfaces/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-process-points-and-surfaces)\n[![Development Status](https://img.shields.io/pypi/status/napari-process-points-and-surfaces.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-process-points-and-surfaces)](https://napari-hub.org/plugins/napari-process-points-and-surfaces)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7654555.svg)](https://doi.org/10.5281/zenodo.7654555)\n\nProcess and analyze surfaces using [vedo](https://vedo.embl.es/) in [napari].\n\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/graphical_abstract.gif)\nThe nppas gastruloid example is derived from [AV Luque and JV Veenvliet (2023)](https://zenodo.org/record/7603081) which is licensed [CC-BY](https://creativecommons.org/licenses/by/4.0/legalcode) and can be downloaded from here: https://zenodo.org/record/7603081\n\n## Usage\n\nYou find menus for surface generation, smoothing and analysis in the menu `Tools > Surfaces` and `Tools > Points`. \nFor detailed explanation of the underlying algorithms, please refer to the [vedo](https://vedo.embl.es/) documentation.\n\nFor processing meshes in Python scripts, see the [demo notebook](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/docs/demo.ipynb). \nThere you also learn how this screenshot is made:\n\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/screenshot5.png)\n\nFor performing quantitative measurements of surface in Python scripts, see the [demo notebook](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/blob/main/docs/quality_measurements.ipynb). \nThere you also learn how this screenshot is made:\n\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/screenshot6.png)\n\n### Surface measurements and annotations\n\nUsing the menu `Tools > Measurement tables > Surface quality table (vedo, nppas)` you can derive quantiative measurements of\nthe vertices in a given surface layer. \n\n![img_1.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_measurements2.png)\n\nTo differentiate regions when analyzing those measurements it is recommended to use the menu `Tools > Surfaces > Annotate surface manually (nppas)`\nafter measurements have been made. This tool allows you to draw annotation label values on the surface. \nIt is recommended to do activate a colorful colormap such as `hsv` before starting to draw annotations. \nFurthermore, set the maximum of the contrast limit range to the number of regions you want to annotate + 1.\nAnnotations can be drawn as freehand lines and circles.\n\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_annotation2.png)\n\nAfter measurements and annotations were done, you can save the annotation in the same measurement table using the menu\n`Tools > Measurement tables > Surface quality/annotation to table (nppas)`\n\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/surface_annotation_in_table2.png)\n\nFor classifying surface vertices using machine learning, please refer to the [napari APOC](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification) documentation.\n\n### Measurement visualization\n\nTo visualize measurements on the surface, just double-click on the table column headers.\n\n![img.png](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/raw/main/docs/quality_measurements.gif)\n\n## Installation\n\nYou can install `napari-process-points-and-surfaces` via mamba/conda and pip:\n\n```\nmamba install vedo vtk libnetcdf=4.7.4 -c conda-forge\npip install napari-process-points-and-surfaces\n```\n\n### Troubleshooting: Open3d installation\n\nSince version 0.4.0, `nppas` does no longer depend on [open3d](http://www.open3d.org/). \nSome deprecated functions still use Open3d though. \nFollow the installation instructions in the [open3d documentation](http://www.open3d.org/docs/release/getting_started.htm) to install it and keep using those functions.\nAlso consider updating code and no longer using these deprecated functions. \nSee [release notes](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/releases/tag/0.4.0) for details.\n\n## See also\n\nThere are other napari plugins with similar / overlapping functionality\n* [morphometrics](https://www.napari-hub.org/plugins/morphometrics)  \n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\n* [napari-pymeshlab](https://www.napari-hub.org/plugins/napari-pymeshlab)\n* [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant)\n* [napari-stress](https://www.napari-hub.org/plugins/napari-stress)\n\nAnd there is software for doing similar things:\n* [meshlab](https://www.meshlab.net/)\n* [paraview](https://www.paraview.org/)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-process-points-and-surfaces\" is free and open source software\n\n## Acknowledgements\n\nSome code snippets and example data were taken from the [vedo](https://vedo.embl.es/) and [open3d](http://www.open3d.org/) \nrepositories and documentation. See [thirdparty licenses](https://github.com/haesleinhuepf/napari-process-points-and-surfaces/tree/main/licenses_third_party) for licensing details.\nThe Standford Bunny example dataset has been taken from [The Stanford 3D Scanning Repository](http://graphics.stanford.edu/data/3Dscanrep/).\nThe nppas gastruloid example is derived from [AV Luque and JV Veenvliet (2023)](https://zenodo.org/record/7603081) which is licensed [CC-BY](https://creativecommons.org/licenses/by/4.0/legalcode) and can be downloaded from here: https://zenodo.org/record/7603081\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-process-points-and-surfaces/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[image.sc]: https://image.sc\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SurfaceAnnotationWidget",
      "convex_hull",
      "filter_smooth_simple",
      "filter_smooth_laplacian",
      "filter_smooth_taubin",
      "simplify_vertex_clustering",
      "simplify_quadric_decimation",
      "subdivide_loop",
      "labels_to_centroids",
      "sample_points_uniformly",
      "sample_points_poisson_disk",
      "voxel_down_sample",
      "points_to_labels",
      "points_to_convex_hull_surface",
      "surface_from_point_cloud_alpha_shape",
      "surface_from_point_cloud_ball_pivoting",
      "label_to_surface",
      "largest_label_to_surface",
      "add_quality",
      "add_curvature_scalars",
      "add_spherefitted_curvature"
    ],
    "contributions_sample_data": [
      "Standford bunny (nppas)",
      "Ellipsoid (nppas)",
      "Gastruloid (AV Luque and JV Veenvliet (2023), nppas)"
    ]
  },
  {
    "normalized_name": "napari-obj",
    "name": "napari-obj",
    "display_name": "obj file reader",
    "version": "1.0.0",
    "created_at": "2023-04-05",
    "modified_at": "2023-04-05",
    "authors": [
      "L√©o Guignard"
    ],
    "author_emails": [
      "leo.guignard@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-obj/",
    "home_github": "https://github.com/guignardlab/napari-obj",
    "home_other": null,
    "summary": "A plugin to read .obj files",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-obj\n\n<!-- [![License MIT](https://img.shields.io/pypi/l/napari-obj.svg?color=green)](https://github.com/guignardlab/napari-obj/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-obj.svg?color=green)](https://pypi.org/project/napari-obj)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-obj.svg?color=green)](https://python.org)\n[![tests](https://github.com/guignardlab/napari-obj/workflows/tests/badge.svg)](https://github.com/guignardlab/napari-obj/actions)\n[![codecov](https://codecov.io/gh/guignardlab/napari-obj/branch/main/graph/badge.svg)](https://codecov.io/gh/guignardlab/napari-obj)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-obj)](https://napari-hub.org/plugins/napari-obj) -->\n\nA plugin to read .obj files\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-obj` via [pip]:\n\n    pip install napari-obj\n\nTo install latest development version :\n\n    pip install git+https://github.com/guignardlab/napari-obj.git\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-obj\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guignardlab/napari-obj/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.obj"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-features-selector",
    "name": "napari-features-selector",
    "display_name": "Features Selection GA",
    "version": "0.0.4",
    "created_at": "2023-03-28",
    "modified_at": "2023-03-29",
    "authors": [
      "Sanjeev Kumar"
    ],
    "author_emails": [
      "kumar.san96@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-features-selector/",
    "home_github": "https://github.com/kumar-sanjeeev/napari-features-selector",
    "home_other": null,
    "summary": "A lightweight widget for features selection.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "scikit-learn",
      "sklearn-genetic-opt",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-features-selector\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-features-selector.svg?color=green)](https://github.com/kumar-sanjeeev/napari-features-selector/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-features-selector.svg?color=green)](https://pypi.org/project/napari-features-selector)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-features-selector.svg?color=green)](https://python.org)\n[![tests](https://github.com/kumar-sanjeeev/napari-features-selector/workflows/tests/badge.svg)](https://github.com/kumar-sanjeeev/napari-features-selector/actions)\n[![codecov](https://codecov.io/gh/kumar-sanjeeev/napari-features-selector/branch/main/graph/badge.svg)](https://codecov.io/gh/kumar-sanjeeev/napari-features-selector)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-features-selector)](https://napari-hub.org/plugins/napari-features-selector)\n\n\nAn interactive plugin that enables users to choose the important/relevant features from a set of multiple features. These selected features can then be applied to various tasks like object detection, segmentation, classification, among others.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-features-selector` via [pip]:\n\n    pip install napari-features-selector\n\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-features-selector\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kumar-sanjeeev/napari-features-selector/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Feature Selection using - GA [Genetic Algorithm]"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ndtiffs",
    "name": "napari-ndtiffs",
    "display_name": "napari-ndtiffs",
    "version": "0.2.1",
    "created_at": "2021-06-24",
    "modified_at": "2023-03-29",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "Talley Lambert <talley.lambert@gmail.com>"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-ndtiffs/",
    "home_github": "https://github.com/tlambert03/napari-ndtiffs",
    "home_other": null,
    "summary": "napari plugin for nd tiff folders with OpenCl deskew",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "dask[array]",
      "napari-plugin-engine>=0.1.4",
      "numpy",
      "python-dateutil",
      "scipy",
      "tifffile",
      "black; extra == 'dev'",
      "ipython; extra == 'dev'",
      "mypy; extra == 'dev'",
      "pdbpp; extra == 'dev'",
      "rich; extra == 'dev'",
      "ruff; extra == 'dev'",
      "pyopencl; extra == 'opencl'",
      "pyopencl; extra == 'test'",
      "pytest; extra == 'test'",
      "pytest-cov; extra == 'test'"
    ],
    "package_metadata_description": "# napari-ndtiffs\n\n[![License](https://img.shields.io/pypi/l/napari-ndtiffs.svg?color=green)](https://raw.githubusercontent.com/tlambert03/napari-ndtiffs/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ndtiffs.svg?color=green)](https://pypi.org/project/napari-ndtiffs)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ndtiffs.svg?color=green)](https://python.org)\n[![tests](https://github.com/tlambert03/napari-ndtiffs/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-ndtiffs/actions)\n[![codecov](https://codecov.io/gh/tlambert03/napari-ndtiffs/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-ndtiffs)\n\nnapari plugin for nd tiff folders with optional CUDA or OpenCL-based deskewing.\n\nBuilt-in support for folders of (skewed) lattice light sheet tiffs.\n\n![napari-ndtiffs demo](https://github.com/tlambert03/napari-ndtiffs/raw/main/demo.gif)\n\n----------------------------------\n\n*This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.*\n\n## Features\n\n- Drag and drop a folder of tiffs onto napari window to view easily \n  - (currently designed to detect  lattice light sheet tiffs, but easily\n    adjustable)\n- If lattice `Settings.txt` file is found, will deskew automatically (only if\n  necessary)\n- Lazily loads dataset on demand.  quickly load preview your data.\n- Handles `.zip` archives as well!  Just directly compress your tiff folder,\n  then drop it into napari.\n- All OpenCL deskewing, works on GPU as well as CPU, falls back to scipy if\n  [PyOpenCL] is unavailable.\n- CuPy-based deskewing will work for cards with NVIDIA GPUs that support CUDA.\n  CuPy 8.x releases should work, although CuPy >= 9 is recommended. If [CuPy]\n  is unavailable, the [PyOpenCL] implementation is used instead.\n\nIt would not be hard to support arbitrary filenaming patterns!  If you have a\nfolder of tiffs with a consistent naming scheme and would like to take advantage\nof this plugin, feel free to open an issue!\n\n## Installation\n\nYou can install `napari-ndtiffs` via [pip]:\n\n```shell\npip install napari-ndtiffs\n```\n\nTo also install PyOpenCL (for faster deskewing):\n\n```shell\npip install napari-ndtiffs[opencl]\n```\n\nOn NVIDIA GPUs with CUDA support, the [CuPy] implementation may be faster than\n[PyOpenCL]. CuPy also has experimental support for AMD GPUs via HIP/ROCm. See\nthe CuPy [installation instructions](https://docs.cupy.dev/en/stable/install.html)\n\n\n## Usage\n\nIn most cases, just drop your folder onto napari, or use `viewer.open(\"path\")`\n\n### Overriding parameters\n\nYou can control things like voxel size and deskewing angle as follows:\n\n```python\nfrom napari_ndtiffs import parameter_override\nimport napari\n\nviewer = napari.Viewer()\nwith parameter_override(angle=45, name=\"my image\"):\n    viewer.open(\"path/to/folder\", plugin=\"ndtiffs\")\n```\n\nValid keys for `parameter_override` include:\n\n- **dx**: (`float`) the pixel size, in microns\n- **dz**: (`float`)the z step size, in microns\n- **deskew**: (`bool`) whether or not to deskew, (by default, will deskew if angle > 0, or if a lattice metadata file is detected that requires deskewing) \n- **angle**: (`float`) the angle of the light sheet relative to the coverslip\n- **padval**: (`float`) the value with which to pad the image edges when deskewing (default is 0)\n- **contrast_limits**: (`2-tuple of int`) (min, max) contrast_limits to use when viewing the image\n- **name**: (`str`) an optional name for the image\n\n### Sample data\n\nTry it out with test data: [download sample data](https://www.dropbox.com/s/up4ywrn2sckjunc/lls_mitosis.zip?dl=1)\n\nYou can unzip if you like, or just drag the zip file onto the napari window.\n\nOr, from command line, use:\n\n```bash\nnapari path/to/lls_mitosis.zip\n```\n\n## Debugging\n\nTo monitor file io and deskew activity, enter the following in the napari console:\n\n```python\nimport logging\nlogging.getLogger('napari_ndtiffs').setLevel('DEBUG')\n```\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-ndtiffs\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/tlambert03/napari-ndtiffs/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[PyOpenCL]: https://documen.tician.de/pyopencl/\n[CuPy]: https://docs.cupy.dev/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cell-centroid-annotator",
    "name": "napari-cell-centroid-annotator",
    "display_name": "3D cell centroid annotator",
    "version": "0.0.1",
    "created_at": "2023-03-23",
    "modified_at": "2023-03-23",
    "authors": [
      "Tim Van De Looverbosch"
    ],
    "author_emails": [
      "tim.vandelooverbosch@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-cell-centroid-annotator/",
    "home_github": "https://github.com/tim-vdl/napari-cell-centroid-annotator",
    "home_other": null,
    "summary": "A simple plugin to annotate cell centroids in 3D images",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "tifffile",
      "pathlib",
      "napari",
      "napari-plugin-engine",
      "napari-layer-table",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-cell-centroid-annotator\n\n[![License MIT](https://img.shields.io/pypi/l/napari-cell-centroid-annotator.svg?color=green)](https://github.com/tim-vdl/napari-cell-centroid-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cell-centroid-annotator.svg?color=green)](https://pypi.org/project/napari-cell-centroid-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cell-centroid-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/tim-vdl/napari-cell-centroid-annotator/workflows/tests/badge.svg)](https://github.com/tim-vdl/napari-cell-centroid-annotator/actions)\n[![codecov](https://codecov.io/gh/tim-vdl/napari-cell-centroid-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/tim-vdl/napari-cell-centroid-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cell-centroid-annotator)](https://napari-hub.org/plugins/napari-cell-centroid-annotator)\n\nA simple plugin to annotate cell centroids in 3D images\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-cell-centroid-annotator` via [pip]:\n\n    pip install napari-cell-centroid-annotator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/tim-vdl/napari-cell-centroid-annotator.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-cell-centroid-annotator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/tim-vdl/napari-cell-centroid-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.tif",
      "*.npy",
      "*.csv"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Centroid Annotation Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-listener",
    "name": "napari-listener",
    "display_name": "Napari Listener",
    "version": "0.1.0b1.post1",
    "created_at": "2023-03-21",
    "modified_at": "2023-03-21",
    "authors": [
      "Ashley Anderson"
    ],
    "author_emails": [
      "aandersoniii@chanzuckerberg.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-listener/",
    "home_github": "https://github.com/aganders3/napari-listener",
    "home_other": null,
    "summary": "Control napari via local socket.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-listener\n\n[![License MIT](https://img.shields.io/pypi/l/napari-listener.svg?color=green)](https://github.com/aganders3/napari-listener/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-listener.svg?color=green)](https://pypi.org/project/napari-listener)\n[![tests](https://github.com/aganders3/napari-listener/workflows/tests/badge.svg)](https://github.com/aganders3/napari-listener/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-listener)](https://napari-hub.org/plugins/napari-listener)\n\nOpens a socket to listen for commands to control napari from other processes.\nThis can be useful for controlling napari programmatically from other\napplications, or for improving general OS integration (e.g. opening data from a\nfile or UrL in a running instance of napari).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s\n[cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-listener` via [pip]:\n\n    pip install napari-listener\n\n## Usage\n\nOnce installed, `napari-listener` can be started from the `napari > Plugins >\nStart Listening` menu. You will see a new docked widget that displays the\naddress and port for the listener.\n\nThe listener is a TCP server that expects app-model command IDs. It will\nexecute any valid app-model command, but `napari-listener` registers its own\nadditional commands for demonstration purposes in\nhttps://github.com/aganders3/napari-listener/blob/main/src/napari_listener/_actions.py.\n\nYou can test `napari-listener` using a TCP client such as\n[netcat](https://linux.die.net/man/1/nc) or\n[curl](https://curl.se/docs/manpage.html) to send an app-model command (and\noptional args). For example:\n\n```shell\n% nc 127.0.0.1 40256 <<< \"napari:open-file /path/to/local/file\"\n```\n\n<img src=\"https://raw.githubusercontent.com/aganders3/napari-listener/main/napari-listener-demo.gif\" alt=\"quick demo of napari-listener\">\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-listener\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed\ndescription.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Start Listening"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-buds",
    "name": "napari-buds",
    "display_name": "napari BudAnnotation",
    "version": "0.1.6",
    "created_at": "2022-08-16",
    "modified_at": "2023-03-17",
    "authors": [
      "Sander van Otterdijk"
    ],
    "author_emails": [
      "scvanotterdijk@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-buds/",
    "home_github": "https://github.com/SanderSMFISH/napari-buds",
    "home_other": null,
    "summary": "Random-forest automated bud annotation",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "napari",
      "magic-class",
      "scipy",
      "scikit-learn",
      "scikit-image",
      "matplotlib",
      "joblib",
      "imageio-ffmpeg",
      "stackview",
      "jupyterlab",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-buds\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-buds.svg?color=green)](https://github.com/SanderSMFISH/napari-buds/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-buds.svg?color=green)](https://pypi.org/project/napari-buds)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-buds.svg?color=green)](https://python.org)\n[![tests](https://github.com/SanderSMFISH/napari-buds/workflows/tests/badge.svg)](https://github.com/SanderSMFISH/napari-buds/actions)\n[![codecov](https://codecov.io/gh/SanderSMFISH/napari-buds/branch/main/graph/badge.svg)](https://codecov.io/gh/SanderSMFISH/napari-buds)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-buds)](https://napari-hub.org/plugins/napari-buds)\n\nRandom-forest automated bud annotation\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nmake sure you already have installed napari. \n\nNext, You can install `napari-buds` via [pip]:\n\n    pip install napari-buds\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/SanderSMFISH/napari-buds.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Documentation\nNapari-Buds is a random forest based mother-bud annotation plugin for Napari devevoped by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. **Before installation make sure you have a working version of napari installed (pip install \"napari[all]\").** Napari-Buds is a random forest based mother-bud annotation plugin for Napari developed by the TutucciLab (https://www.tutuccilab.com/) of the systems biology group at the Vrije Universiteit van Amsterdam. Mother-bud annotation requires single or multichannel 2D images of budding yeast and a fluorescent marker that localizes to the bud. In the example dataset provided smFISH DNA-probes were used as localized bud marker.The GUI layout for random forest based classification was inspired by ImageJ 'plugin Weka Segmentation' [1]. \n\nPlease follow the workflow described underneath to perform mother-bud annotation:\n\n1. Open images in napari and create empty label layer.\nFor multichannel images each channel should be provided seperately to napari.\nAn example (jupyter) notebook (Open Test Images Napari.ipynb) for loading test data in napari is provided in the notebooks folder. \nExample dataset can be downloaded from https://zenodo.org/record/7004556#.YwM1_HZBztU. \n    \n2. If multichannel images are unaligned the  translate widget under Plugins>napari-buds>Translate can be used. \nSelect which layer should be translated to align to the layers in widget menu. Then use the aswd keys to translate (move) the selected layer. \nTo register changes and update coordinates of the translated image in napari press t. \n    \n### Random forest classification\n3. To open the mother-bud annotation plugin go to Plugins>napari-buds>bud annotation.\n    \n4. To train a random forest classifier, in the created label layer draw examples of cells, buds and background (see tutorial gif below). \nIn the Define Label segment of the widget you define which label value (class #label_value) corresponds to cells, buds and background. \nCurrently, cells and backgrounds and buds **have to be defined in the Define Label segment**  if you want to be able to segment the classification as well.\nIn the segment **Layers to extract Features from** we can select which layers will be used in training the random forest classifier. \nNext press **Train classifier**. After training is completed a result layer is added to layer list. \nInspect the results carefully to asses classifier performance. The trained classifier can be saved using the **save classifier** button.\nPreviously trained classifier can be loaded by pressing **Load classifier**. Loaded classifier can applied to new images by pressing **Classify**, resulting again in a results layer.\nIt is possible to change the random forest parameters with **the Set random forest parameters** button and changing the values in the pop up menu.\nPress **Run** to register changed settings. For an example of the parameters used see: \nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and \nhttps://scikit-image.org/docs/stable/auto_examples/segmentation/plot_trainable_segmentation.html. \n    \n5. Next, we want to perfom watershed segmentation using the result layer. However, for watershed segmentation seeds (also called markers) are required\n(for an explanation of watershed segmenation see: https://en.wikipedia.org/wiki/Watershed_(image_processing)). \nTo define the seeds we can either simply threshold on one of the supplied image layers or we can use distance tranform (https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html#sphx-glr-auto   examples-segmentation-plot-watershed-py).The resulting seeds layer can be adjusted manually by editing in napari.\nA good seeds layers correspond to each cell having a single seed (buds are not single cells). To perform watershed segmentation press the **Segment** button.\n    \n6. Carefully inspect the resulting cell mask and bud layer. Correct the mistakes in both layers. \nBud label values should correspond to the label value of the cell mask of mother cell. To verify mother bud relations were drawn correctly\npress **Draw Mother-Bud relations**. If Mother-Bud relations are correct, you can save both label layers. Mother and buds simply share the same label number.\nThus, either the mother or bud layer can be manually corrected for mistakes. Corrections can be checked by clicking **Draw Mother-Bud relations** again. \nmother and buds layer can be saved manually in napari. When using Jupyter notebook mother and bud layers can be saved as shown in Open Test Images Napari.ipynb.\n\n7. An example notebook for dataextraction of the created cell and bud masks can be found in the example notebooks folder (Extract_Mother_Buds_relations_from_Masks_and_intergrate_FQ_spot_data.ipynb).This notebooks relates RNA spots (smFISH data found on zenodo) to the mother or bud compartment. \n\n\nSee video for clarification:\n\n![Watch the video](https://github.com/SanderSMFISH/napari-buds/blob/main/videos/Napari_bud_gif.gif)\n\n## Similar Napari plugins \n\n1-napari-accelerated-pixel-and-object-classification (APOC) by Robert Haase.\n\n2-napari-feature-classifier.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-buds\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n### Known Issues\n\nIf window geometry of the window is unable to be set, this might lead to issues in the display of the widget. For example, part of the widget might fall of the screen.\nIn these cases, it might help to adjust in your display setting the display scaling to a lower setting. \n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/SanderSMFISH/napari-buds/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n## References\n1. Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K. W., Schindelin, J., Cardona, A., & Sebastian Seung, H. (2017). Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification. Bioinformatics, 33(15), 2424‚Äì2426. doi:10.1093/bioinformatics/btx180\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Translate image layers (controls ASWD, T = register)",
      "bud annotation"
    ],
    "contributions_sample_data": [
      "napari BudAnnotation"
    ]
  },
  {
    "normalized_name": "napari-vemseg",
    "name": "napari-vemseg",
    "display_name": "napari-vemseg",
    "version": "0.0.9",
    "created_at": "2023-03-14",
    "modified_at": "2023-03-15",
    "authors": [
      "Matous Elphick"
    ],
    "author_emails": [
      "matous.elphick@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-vemseg/",
    "home_github": null,
    "home_other": "None",
    "summary": "A simple plugin for semi-automated segmentation of volume electron microscopy images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "torch",
      "numpy",
      "scikit-image",
      "magicgui",
      "qtpy",
      "napari",
      "tqdm",
      "apoc",
      "superqt",
      "setuptools",
      "vemseg"
    ],
    "package_metadata_description": "\n<img width=\"250\"  src=\"https://github.com/MatousE/napari-vemseg/blob/main/images/VEMSEG-FINAL.svg\"> \n\n# napari-vemseg\n\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vemseg.svg?color=green)](https://github.com/MatousE/napari-vemseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-vemseg.svg?color=green)](https://pypi.org/project/napari-vemseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vemseg.svg?color=green)](https://python.org)\n[![tests](https://github.com/MatousE/napari-vemseg/workflows/tests/badge.svg)](https://github.com/MatousE/napari-vemseg/actions)\n[![codecov](https://codecov.io/gh/MatousE/napari-vemseg/branch/main/graph/badge.svg)](https://codecov.io/gh/MatousE/napari-vemseg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vemseg)](https://napari-hub.org/plugins/napari-vemseg)\n\nA simple plugin to do semi-automated segmentation within napari using vemseg built on\nXGBoost.\n\n## Installation\n### `conda` Installation and Environment Creation\nTo start with a conda environment must be created.\n```\nconda create -n vemseg-env python=3.8\nconda activate vemseg-env\n```\n### `napari` Instillation\nWe must then install [napari]:\n\n```\npip install \"napari[all]\"\n```\n### `napari-vemseg` Instillation\nYou can finally install `napari-vemseg` via [pip]:\n```\nconda install pyopencl\npip install napari-vemseg\n```\n## Usage\n### Train VEMClassifier\n\n### Predict Using Pretrained VEMClassifier\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-vemseg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "VEMseg Train Pixel Classifier",
      "VEMseg Predict Pixel Classifier",
      "PHH Mitochondria Predict"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-live-flim",
    "name": "napari-live-flim",
    "display_name": "Napari Live Flim",
    "version": "0.1.1",
    "created_at": "2022-11-09",
    "modified_at": "2023-03-14",
    "authors": [
      "Kevin Tan"
    ],
    "author_emails": [
      "kktangent@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-live-flim/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin for real-time FLIM analysis",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "dataclasses-json",
      "flimlib",
      "magicgui",
      "matplotlib",
      "numpy",
      "qtpy",
      "scipy",
      "superqt",
      "vispy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-live-flim\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-live-flim.svg?color=green)](https://github.com/uw-loci/napari-live-flim/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-live-flim.svg?color=green)](https://pypi.org/project/napari-live-flim)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-live-flim.svg?color=green)](https://python.org)\n[![tests](https://github.com/uw-loci/napari-live-flim/workflows/tests/badge.svg)](https://github.com/uw-loci/napari-live-flim/actions)\n[![codecov](https://codecov.io/gh/uw-loci/napari-live-flim/branch/main/graph/badge.svg)](https://codecov.io/gh/uw-loci/napari-live-flim)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-live-flim)](https://napari-hub.org/plugins/napari-live-flim)\n\nA plugin for real-time FLIM analysis\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Required dependencies\n\n- [OpenScan] TCSPC module and all dependencies.\n    - Verify FLIM electronics are compatible with OpenScan\n- Python and the [napari] package and all dependencies\n\nYou can install `napari` via [pip]:\n\n    pip install napari[all]\n\n## Installation\n\nYou can install `napari-live-flim` via [pip]:\n\n    pip install napari-live-flim\n\n## Usage\n\n1. In MicroManager, set a port number in the device property setting named `OpenScanFLIM-BH-TCSPC-SendFLIMHistogramsToUDPPort`\n2. In Napari, select **Plugins > FLIM Viewer (napari-live-flim)** to run the plugin. Enter the same port number to connect to OpenScan.\n3. Begin acquisition within MicroManager.\n4. Interact with the FLIM data in real-time within napari.\n    - Modify the FLIM Parameters and Display Filters settings as desired.\n    - Add selections to the Lifetime Image or Phasor Plot by clicking the relevant New Selection buttons.\n    - Manipulate the selections with the mouse cursor and modify the selection layer with the layer controls.\n    - Click the Snapshot button during acquisition to take a snapshot.\n    - Use the scroll bar under the Lifetime Image to recall a specific snapshot.\n5. Stop scanning within MicroManager to end acquisition.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-live-flim\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/uw-loci/napari-live-flim/issues\n[OpenScan]: https://github.com/openscan-lsm/OpenScan\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "FLIM Viewer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-meshio",
    "name": "napari-meshio",
    "display_name": "meshio",
    "version": "0.0.1",
    "created_at": "2023-03-07",
    "modified_at": "2023-03-07",
    "authors": [
      "Genevieve Buckley"
    ],
    "author_emails": [
      "yourname@example.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-meshio/",
    "home_github": "https://github.com/GenevieveBuckley/napari-meshio",
    "home_other": null,
    "summary": "I/O for mesh files.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "meshio",
      "pooch",
      "rich",
      "mkdocs ; extra == 'testing'",
      "mkdocs-gen-files ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-meshio\n\n[![License MIT](https://img.shields.io/pypi/l/napari-meshio.svg?color=green)](https://github.com/GenevieveBuckley/napari-meshio/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-meshio.svg?color=green)](https://pypi.org/project/napari-meshio)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-meshio.svg?color=green)](https://python.org)\n[![tests](https://github.com/GenevieveBuckley/napari-meshio/workflows/tests/badge.svg)](https://github.com/GenevieveBuckley/napari-meshio/actions)\n[![codecov](https://codecov.io/gh/GenevieveBuckley/napari-meshio/branch/main/graph/badge.svg)](https://codecov.io/gh/GenevieveBuckley/napari-meshio)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-meshio)](https://napari-hub.org/plugins/napari-meshio)\n\nThis napari plugin uses [meshio](https://github.com/nschloe/meshio) to read and write mesh files to surfaces in napari.\n\n![Screenshot: Stanford bunny example data in napari](assets/bunny-screenshot.png)\n\n*Image caption: screenshot of the [Stanford bunny](http://graphics.stanford.edu/data/3Dscanrep/) example surface mesh open in napari.*\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n- [Installation](#installation)\n- [How to use napari-meshio](#how-to-use-napari-meshio)\n    - [Read surface data from file](#read-surface-data-from-file)\n    - [Open example surface data](#open-example-surface-data)\n    - [Save surface data](#save-surface-data)\n    - [Supported mesh file formats](#supported-mesh-file-formats)\n- [Contributing](#contributing)\n- [License](#license)\n- [Issues](#issues)\n\n## Installation\n\nYou can install `napari-meshio` via [pip]:\n\n    pip install napari-meshio\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/GenevieveBuckley/napari-meshio.git\n\n\n## How to use napari-meshio\n\n### Read surface data from file\n\nDrag and drop the file onto the napari viewer.\n\n*Note: [Here](https://people.sc.fsu.edu/~jburkardt/data/ply/ply.html) are a number of `.ply` example files you can download to try, like [this airplane](https://people.sc.fsu.edu/~jburkardt/data/ply/airplane.ply) (see [image](https://people.sc.fsu.edu/~jburkardt/data/ply/airplane.png)).*\n\n### Open example surface data\n\nLaunch the napari viewer, then open one of the sample datasets (eg: the [Stanford bunny](http://graphics.stanford.edu/data/3Dscanrep/)) from the file menu:\n\n`File` > `Open Sample` > `napari-meshio` > `bunny`\n\nOr, open sample data from python with:\n\n```python\nimport napari\n\nviewer = napari.Viewer(ndisplay=3)\nviewer.open_sample('napari-meshio', 'bunny')\n```\n\n### Save surface data\n\nTo save a surface layer, click the layer name to select it, and then choose save from the file menu:\n\n`File` > `Save selected layer(s)`\n\nYou can also use keyboard shortcuts to save the selected surface layer:\n- Windows/Linux: `Control` + `S`\n- Mac: `‚åò` + `S`\n\nOr, save surface layers from python with:\n```python\nfilename = \"bunny.stl\"\nviewer.layers['bunny'].save(filename)\n```\n*Note: this code example assumes you have the Stanford bunny example dataset loaded.*\n\nA [wide variety of surface mesh file formats are supported](#supported-mesh-file-formats) by\n[meshio](https://github.com/nschloe/meshio).\nIf no file extension is provided when saving a surface layer,\nthe default is the `.ply` polygon file format.\n\n### Supported mesh file formats\n\n*Note: Only triangular mesh faces are supported by napari.*\n\nThe [meshio](https://github.com/nschloe/meshio) library documentation describes the supported file formats:\n\n> There are various mesh formats available for representing unstructured meshes.\nmeshio can read and write all of the following and smoothly converts between them:\n>\n>> [Abaqus](http://abaqus.software.polimi.it/v6.14/index.html) (`.inp`),\n>> ANSYS msh (`.msh`),\n>> [AVS-UCD](https://lanl.github.io/LaGriT/pages/docs/read_avs.html) (`.avs`),\n>> [CGNS](https://cgns.github.io/) (`.cgns`),\n>> [DOLFIN XML](https://manpages.ubuntu.com/manpages/jammy/en/man1/dolfin-convert.1.html) (`.xml`),\n>> [Exodus](https://nschloe.github.io/meshio/exodus.pdf) (`.e`, `.exo`),\n>> [FLAC3D](https://www.itascacg.com/software/flac3d) (`.f3grid`),\n>> [H5M](https://www.mcs.anl.gov/~fathom/moab-docs/h5mmain.html) (`.h5m`),\n>> [Kratos/MDPA](https://github.com/KratosMultiphysics/Kratos/wiki/Input-data) (`.mdpa`),\n>> [Medit](https://people.sc.fsu.edu/~jburkardt/data/medit/medit.html) (`.mesh`, `.meshb`),\n>> [MED/Salome](https://docs.salome-platform.org/latest/dev/MEDCoupling/developer/med-file.html) (`.med`),\n>> [Nastran](https://help.autodesk.com/view/NSTRN/2019/ENU/?guid=GUID-42B54ACB-FBE3-47CA-B8FE-475E7AD91A00) (bulk data, `.bdf`, `.fem`, `.nas`),\n>> [Netgen](https://github.com/ngsolve/netgen) (`.vol`, `.vol.gz`),\n>> [Neuroglancer precomputed format](https://github.com/google/neuroglancer/tree/master/src/neuroglancer/datasource/precomputed#mesh-representation-of-segmented-object-surfaces),\n>> [Gmsh](https://gmsh.info/doc/texinfo/gmsh.html#File-formats) (format versions 2.2, 4.0, and 4.1, `.msh`),\n>> [OBJ](https://en.wikipedia.org/wiki/Wavefront_.obj_file) (`.obj`),\n>> [OFF](https://segeval.cs.princeton.edu/public/off_format.html) (`.off`),\n>> [PERMAS](https://www.intes.de) (`.post`, `.post.gz`, `.dato`, `.dato.gz`),\n>> [PLY](<https://en.wikipedia.org/wiki/PLY_(file_format)>) (`.ply`),\n>> [STL](<https://en.wikipedia.org/wiki/STL_(file_format)>) (`.stl`),\n>> [Tecplot .dat](http://paulbourke.net/dataformats/tp/),\n>> [TetGen .node/.ele](https://wias-berlin.de/software/tetgen/fformats.html),\n>> [SVG](https://www.w3.org/TR/SVG/) (2D output only) (`.svg`),\n>> [SU2](https://su2code.github.io/docs_v7/Mesh-File/) (`.su2`),\n>> [UGRID](https://www.simcenter.msstate.edu/software/documentation/ug_io/3d_grid_file_type_ugrid.html) (`.ugrid`),\n>> [VTK](https://vtk.org/wp-content/uploads/2015/04/file-formats.pdf) (`.vtk`),\n>> [VTU](https://vtk.org/Wiki/VTK_XML_Formats) (`.vtu`),\n>> [WKT](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) ([TIN](https://en.wikipedia.org/wiki/Triangulated_irregular_network)) (`.wkt`),\n>> [XDMF](https://xdmf.org/index.php/XDMF_Model_and_Format) (`.xdmf`, `.xmf`).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-meshio\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/GenevieveBuckley/napari-meshio/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.vol.gz",
      "*.obj",
      "*.ugrid",
      "*.vtu",
      "*.msh",
      "*.mdpa",
      "*.e",
      "*.dato",
      "*.avs",
      "*.dat",
      "*.ply",
      "*.ele",
      "*.xmf",
      "*.post",
      "*.su2",
      "*.h5m",
      "*.wkt",
      "*.dato.gz",
      "*.f3grid",
      "*.vol",
      "*.xml",
      "*.stl",
      "*.med",
      "*.exo",
      "*.node",
      "*.inp",
      "*.bdf",
      "*.fem",
      "*.nas",
      "*.vtk",
      "*.off",
      "*.cgns",
      "*.meshb",
      "*.mesh",
      "*.post.gz",
      "*.xdmf"
    ],
    "contributions_writers_filename_extensions": [
      ".node",
      ".ele",
      ".mdpa",
      ".post.gz",
      ".vtu",
      ".fem",
      ".vol",
      ".off",
      ".ply",
      ".vtk",
      ".dat",
      ".stl",
      ".med",
      ".post",
      ".dato.gz",
      ".su2",
      ".vol.gz",
      ".exo",
      ".meshb",
      ".inp",
      ".wkt",
      ".avs",
      ".bdf",
      ".ugrid",
      ".xml",
      ".e",
      ".f3grid",
      ".msh",
      ".xdmf",
      ".mesh",
      ".obj",
      ".dato",
      ".xmf",
      ".nas",
      ".cgns",
      ".h5m"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "bunny"
    ]
  },
  {
    "normalized_name": "napari-gruvbox",
    "name": "napari-gruvbox",
    "display_name": "napari Gruvbox",
    "version": "0.1.1",
    "created_at": "2022-12-14",
    "modified_at": "2023-03-06",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "brisvag@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-gruvbox/",
    "home_github": "https://github.com/brisvag/napari-gruvbox",
    "home_other": null,
    "summary": "Gruvbox theme for napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "pygments (>=2.9)"
    ],
    "package_metadata_description": "# napari-gruvbox\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-gruvbox.svg?color=green)](https://github.com/brisvag/napari-gruvbox/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-gruvbox.svg?color=green)](https://pypi.org/project/napari-gruvbox)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-gruvbox.svg?color=green)](https://python.org)\n[![tests](https://github.com/brisvag/napari-gruvbox/workflows/tests/badge.svg)](https://github.com/brisvag/napari-gruvbox/actions)\n[![codecov](https://codecov.io/gh/brisvag/napari-gruvbox/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-gruvbox)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-gruvbox)](https://napari-hub.org/plugins/napari-gruvbox)\n\nGruvbox theme for napari. Colors are taken from the palette in https://github.com/morhetz/gruvbox.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-gruvbox` via [pip]:\n\n    pip install napari-gruvbox\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/brisvag/napari-gruvbox.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-gruvbox\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/brisvag/napari-gruvbox/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-j",
    "name": "napari-J",
    "display_name": "napari-J",
    "version": "0.3",
    "created_at": "2022-01-07",
    "modified_at": "2023-03-03",
    "authors": [
      "Volker Baecker"
    ],
    "author_emails": [
      "volker.baecker@mri.cnrs.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-j/",
    "home_github": "https://github.com/MontpellierRessourcesImagerie/napari-J",
    "home_other": null,
    "summary": "A plugin to exchange data with FIJI and to use FIJI image analysis from napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "JPype1 (>=1.2.1)",
      "matplotlib",
      "imageio-ffmpeg",
      "matplotlib ; extra == 'testing'",
      "imageio-ffmpeg ; extra == 'testing'",
      "python-matplotlib-qt5 ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-J\n\n[![License](https://img.shields.io/pypi/l/napari-J.svg?color=green)](https://github.com/MontpellierRessourcesImagerie/napari-J/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-J.svg?color=green)](https://pypi.org/project/napari-J)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-J.svg?color=green)](https://python.org)\n[![tests](https://github.com/MontpellierRessourcesImagerie/napari-J/workflows/tests/badge.svg)](https://github.com/MontpellierRessourcesImagerie/napari-J/actions)\n[![codecov](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-J/branch/master/graph/badge.svg)](https://codecov.io/gh/MontpellierRessourcesImagerie/napari-J)\n\nA plugin to exchange data with FIJI and to use FIJI image analysis from napari.\nCurrent features are:\n\n * get the active image from FIJI\n * send a screenshot to FIJI\n * get a set of points from the FIJI results table\n * filter the points in napari\n * send the filtered points back to FIJI\n \nKnown problems:\n\n* Crashes on linux  when the file-dialog is opened. Workaround: Set the option ``Use JFileChooser to open/save`` from the ``Edit>Options>Input/Output`` menu.\n* 03.05.2022 - Currently you need to have the range of the quality values for point between 0 and 255, in the new version they can have any range, but we are waiting for the bug in napari 0.4.15 to be fixed to release this. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-J` via [pip]:\n\n    pip install napari-J\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-J\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/MontpellierRessourcesImagerie/napari-J/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MontpellierRessourcesImagerie/napari-J/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Connection",
      "Image",
      "Points"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-large-image-importer",
    "name": "napari-large-image-importer",
    "display_name": "NLII",
    "version": "0.0.2",
    "created_at": "2023-02-21",
    "modified_at": "2023-02-21",
    "authors": [
      "Hiroki Kawai"
    ],
    "author_emails": [
      "h.kawai888@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-large-image-importer/",
    "home_github": "https://github.com/hiroalchem/napari-large-image-importer",
    "home_other": null,
    "summary": "Napari plugin for easy, memory-efficient import of large images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tifffile",
      "zarr",
      "dask",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-large-image-importer\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-large-image-importer.svg?color=green)](https://github.com/hiroalchem/napari-large-image-importer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-large-image-importer.svg?color=green)](https://pypi.org/project/napari-large-image-importer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-large-image-importer.svg?color=green)](https://python.org)\n[![tests](https://github.com/hiroalchem/napari-large-image-importer/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-large-image-importer/actions)\n[![codecov](https://codecov.io/gh/hiroalchem/napari-large-image-importer/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-large-image-importer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-large-image-importer)](https://napari-hub.org/plugins/napari-large-image-importer)\n\nNapari plugin for easy, memory-efficient import of large images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-large-image-importer` via [pip]:\n\n    pip install napari-large-image-importer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hiroalchem/napari-large-image-importer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-large-image-importer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hiroalchem/napari-large-image-importer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "NLII QWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-qrcode",
    "name": "napari-qrcode",
    "display_name": "QR-Code",
    "version": "0.0.1",
    "created_at": "2023-02-21",
    "modified_at": "2023-02-21",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czi@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-qrcode/",
    "home_github": "https://github.com/kephale/napari-qrcode",
    "home_other": null,
    "summary": "A napari plugin to generate QR-Codes",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "qrcode",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-qrcode\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-qrcode.svg?color=green)](https://github.com/kephale/napari-qrcode/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-qrcode.svg?color=green)](https://pypi.org/project/napari-qrcode)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-qrcode.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-qrcode/workflows/tests/badge.svg)](https://github.com/kephale/napari-qrcode/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-qrcode/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-qrcode)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-qrcode)](https://napari-hub.org/plugins/napari-qrcode)\n\nA napari plugin to generate QR-Codes\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-qrcode` via [pip]:\n\n    pip install napari-qrcode\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-qrcode.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-qrcode\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-qrcode/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Generate a QR code"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-roi",
    "name": "napari-roi",
    "display_name": "napari-roi",
    "version": "0.1.8",
    "created_at": "2021-11-19",
    "modified_at": "2023-02-17",
    "authors": [
      "Jonas Windhager"
    ],
    "author_emails": [
      "jonas@windhager.io"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-roi/",
    "home_github": "https://github.com/BodenmillerGroup/napari-roi",
    "home_other": null,
    "summary": "Select regions of interest (ROIs) using napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "qtpy"
    ],
    "package_metadata_description": "# napari-roi\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-roi)](https://napari-hub.org/plugins/napari-roi)\n[![PyPI](https://img.shields.io/pypi/v/napari-roi.svg?color=green)](https://pypi.org/project/napari-roi)\n[![License](https://img.shields.io/pypi/l/napari-roi.svg?color=green)](https://github.com/BodenmillerGroup/napari-roi/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-roi.svg?color=green)](https://python.org)\n[![Issues](https://img.shields.io/github/issues/BodenmillerGroup/napari-roi)](https://github.com/BodenmillerGroup/napari-roi/issues)\n[![Pull requests](https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-roi)](https://github.com/BodenmillerGroup/napari-roi/pulls)\n\nSelect regions of interest (ROIs) using napari\n\n## Installation\n\nYou can install napari-roi via [pip](https://pypi.org/project/pip/):\n\n    pip install napari-roi\n\nAlternatively, you can install napari-roi via [conda](https://conda.io/):\n\n    conda install -c conda-forge napari-roi\n\n## Usage\n\nThe *napari-roi* plugin can be opened from within napari (`napari -> napari-roi: regions of interest`) and operates on napari *Shapes* layers.\n\nROIs can be added to any napari *Shapes* layer, either by drawing a standard napari shape (e.g. rectangle), or by adding a rectangular ROI of specified size using the `Add ROI` functionality in the *napari-roi* widget. Each ROI is associated with a name, a position (X/Y origin), and a size (width/height). The location of the X/Y origin of all ROIs can be chosen in the *napari-roi* widget. Note that any shape supported by napari (e.g. ellipse, rectangle, polygon, line, path) can serve as an ROI; for non-rectangular shapes, *napari-roi* computes rectangular bounding boxes aligned with the napari coordinate system to determine their positions and sizes. ROIs can be edited or deleted by modifying the corresponding shapes in napari, or by editing the corresponding row in the *napari-roi* widget.\n\nAll ROIs in the current *Shapes* layer can be saved to a comma-separated values (CSV) file using the `Save` functionality in the *napari-roi* widget. When the `Autosave` option is checked, the file is automatically updated on every ROI change. Note that the selected file is specific to the current *Shapes* layer; ROIs from different *Shapes* layers cannot be saved to the same file. ROIs can be loaded from a previously saved file and added to the current *Shapes* layer by opening the file in the *napari-roi* widget.\n\nCSV files saved using *napari-roi* adhere to the following format:\n\n| Columns | Description |\n| --- | --- |\n| `Name` | ROI name |\n| `X`, `Y` | Position (X/Y origin) |\n| `W`, `H` | Size (width/height) |\n\n## Authors\n\nCreated and maintained by [Jonas Windhager](mailto:jonas@windhager.io) until February 2023.\n\nMaintained by [Milad Adibi](mailto:milad.adibi@uzh.ch) from February 2023.\n\n## Contributing\n\n[Contributing](https://github.com/BodenmillerGroup/napari-roi/blob/main/CONTRIBUTING.md)\n\n## Changelog\n\n[Changelog](https://github.com/BodenmillerGroup/napari-roi/blob/main/CHANGELOG.md)\n\n## License\n\n[MIT](https://github.com/BodenmillerGroup/napari-roi/blob/main/LICENSE)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "regions of interest"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tyssue",
    "name": "napari-tyssue",
    "display_name": "napari tyssue",
    "version": "0.1.2",
    "created_at": "2022-10-20",
    "modified_at": "2023-02-17",
    "authors": [
      "Kyle Harrington"
    ],
    "author_emails": [
      "czi@kyleharrington.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-tyssue/",
    "home_github": "https://github.com/kephale/napari-tyssue",
    "home_other": null,
    "summary": "A napari plugin for use with the tyssue library",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tyssue",
      "quantities",
      "pooch",
      "tables",
      "imageio-ffmpeg",
      "invagination (==0.0.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-tyssue\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-tyssue.svg?color=green)](https://github.com/kephale/napari-tyssue/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tyssue.svg?color=green)](https://pypi.org/project/napari-tyssue)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tyssue.svg?color=green)](https://python.org)\n[![tests](https://github.com/kephale/napari-tyssue/workflows/tests/badge.svg)](https://github.com/kephale/napari-tyssue/actions)\n[![codecov](https://codecov.io/gh/kephale/napari-tyssue/branch/main/graph/badge.svg)](https://codecov.io/gh/kephale/napari-tyssue)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tyssue)](https://napari-hub.org/plugins/napari-tyssue)\n\nA napari plugin for use with the tyssue library\n\n![napari-tyssue demo of apoptosis model](https://github.com/kephale/napari-tyssue/raw/main/assets/napari_tyssue_apoptosis.gif)\n\n\nExample video of apoptosis demo simulation created based on the\napoptosis demo from\n[tyssue-demo](https://github.com/DamCB/tyssue-demo).\n\n![napari-tyssue demo of invagination model](https://github.com/kephale/napari-tyssue/raw/main/assets/napari_tyssue_invagination_3x.gif)\n\n\nExample video of apoptosis demo simulation created based on work under\nrevision by Suzanne group at U Toulouse entitled\n\"Epithelio-mesenchymal transition generates an apico-basal driving\nforce required for tissue remodeling\" [available here](https://github.com/DamCB/invagination).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou are better off using conda. You will need pytables, and ideally CGAL.\n\nYou can install `napari-tyssue` via [pip]:\n\n    pip install napari-tyssue\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/kephale/napari-tyssue.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-tyssue\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kephale/napari-tyssue/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari-tyssue apoptosis",
      "napari-tyssue invagination"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napping",
    "name": "napping",
    "display_name": "napping",
    "version": "0.2.4",
    "created_at": "2021-02-02",
    "modified_at": "2023-02-17",
    "authors": [
      "Jonas Windhager"
    ],
    "author_emails": [
      "jonas.windhager@uzh.ch"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napping/",
    "home_github": "https://github.com/BodenmillerGroup/napping",
    "home_other": null,
    "summary": "Control point mapping and coordination transformation using napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "imagecodecs",
      "imageio",
      "napari[all] (>=0.4.16)",
      "numpy",
      "pandas",
      "qtpy",
      "scikit-image"
    ],
    "package_metadata_description": "# napping\n\n[![PyPI](https://img.shields.io/pypi/v/napping.svg?color=green)](https://pypi.org/project/napping)\n[![License](https://img.shields.io/pypi/l/napping.svg?color=green)](https://github.com/BodenmillerGroup/napping/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napping.svg?color=green)](https://python.org)\n[![Issues](https://img.shields.io/github/issues/BodenmillerGroup/napping)](https://github.com/BodenmillerGroup/napping/issues)\n[![Pull requests](https://img.shields.io/github/issues-pr/BodenmillerGroup/napping)](https://github.com/BodenmillerGroup/napping/pulls)\n\nControl point mapping and coordinate transformation using napari\n\n## Installation\n\nYou can install `napping` via [pip](https://pypi.org/project/pip/):\n\n    pip install napping\n\nTo install latest development version:\n\n    pip install git+https://github.com/BodenmillerGroup/napping.git\n\n## Usage\n\nRun `napping` for control point mapping and coordinate transformation\n\n## Authors\n\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\n\n## Contributing\n\n[Contributing](https://github.com/BodenmillerGroup/napping/blob/main/CONTRIBUTING.md)\n\n## Changelog\n\n[Changelog](https://github.com/BodenmillerGroup/napping/blob/main/CHANGELOG.md)\n\n## License\n\n[MIT](https://github.com/BodenmillerGroup/napping/blob/main/LICENSE.md)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-layer-details-display",
    "name": "napari-layer-details-display",
    "display_name": "napari-layer-details-display",
    "version": "0.1.5",
    "created_at": "2021-11-13",
    "modified_at": "2023-02-15",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-layer-details-display/",
    "home_github": "https://github.com/haesleinhuepf/napari-layer-details-display",
    "home_other": null,
    "summary": "A display for layer information and properties",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# napari-layer-details-display\n\n[![License](https://img.shields.io/pypi/l/napari-layer-details-display.svg?color=green)](https://github.com/haesleinhuepf/napari-layer-details-display/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-layer-details-display.svg?color=green)](https://pypi.org/project/napari-layer-details-display)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-layer-details-display.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-layer-details-display/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-layer-details-display/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-layer-details-display/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-layer-details-display)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-layer-details-display)](https://napari-hub.org/plugins/napari-layer-details-display)\n\nA display for layer information and properties\n\n![img.png](https://github.com/haesleinhuepf/napari-layer-details-display/raw/main/images/screenshot.png)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-layer-details-display` via [pip]:\n\n    pip install napari-layer-details-display\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/haesleinhuepf/napari-layer-details-display.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-layer-details-display\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-layer-details-display/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "LayerDetailsDisplay"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pointslayer-projection",
    "name": "napari-pointslayer-projection",
    "display_name": "Projection of Points layers",
    "version": "0.0.2",
    "created_at": "2023-02-14",
    "modified_at": "2023-02-14",
    "authors": [
      "Niklas Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-pointslayer-projection/",
    "home_github": "https://github.com/gatoniel/napari-pointslayer-projection",
    "home_other": null,
    "summary": "This plugin creates a 2d projection of all your points.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-pointslayer-projection\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-pointslayer-projection.svg?color=green)](https://github.com/gatoniel/napari-pointslayer-projection/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pointslayer-projection.svg?color=green)](https://pypi.org/project/napari-pointslayer-projection)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pointslayer-projection.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-pointslayer-projection/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-pointslayer-projection/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-pointslayer-projection/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-pointslayer-projection)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pointslayer-projection)](https://napari-hub.org/plugins/napari-pointslayer-projection)\n\nThis plugin creates a 2d projection of all your points.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-pointslayer-projection` via [pip]:\n\n    pip install napari-pointslayer-projection\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-pointslayer-projection.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-pointslayer-projection\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-pointslayer-projection/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Project points into 2d"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-imsmicrolink",
    "name": "napari-imsmicrolink",
    "display_name": "napari-imsmicrolink",
    "version": "0.1.9",
    "created_at": "2021-12-14",
    "modified_at": "2023-02-10",
    "authors": [
      "Nathan Heath Patterson"
    ],
    "author_emails": [
      "heath.patterson@vanderbilt.edu"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-imsmicrolink/",
    "home_github": "https://github.com/nhpatterson/napari-imsmicrolink",
    "home_other": null,
    "summary": "Plugin to perform IMS to microscopy registration using laser ablation marks.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "tifffile",
      "dask",
      "zarr (>=2.10.3)",
      "qtpy",
      "aicsimageio[bioformats]",
      "bioformats-jar",
      "SimpleITK",
      "pandas",
      "h5py",
      "opencv-python",
      "czifile",
      "imagecodecs",
      "napari[all]"
    ],
    "package_metadata_description": "# napari-imsmicrolink\n![microlink-logo-update](https://user-images.githubusercontent.com/17855764/146078168-dd557089-ff10-46d6-b24d-268f5d21a9ee.png)\n\n[![License](https://img.shields.io/pypi/l/napari-imsmicrolink.svg?color=green)](https://github.com/nhpatterson/napari-imsmicrolink/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-imsmicrolink.svg?color=green)](https://pypi.org/project/napari-imsmicrolink)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-imsmicrolink.svg?color=green)](https://python.org)\n[![tests](https://github.com/nhpatterson/napari-imsmicrolink/workflows/tests/badge.svg)](https://github.com/nhpatterson/napari-imsmicrolink/actions)\n\n[napari] plugin to perform MALDI IMS - microscopy registration using laser ablation marks as described in [Anal. Chem. 2018, 90, 21, 12395‚Äì12403](https://pubs.acs.org/doi/abs/10.1021/acs.analchem.8b02884). This plugin is a work-in-progress but is mostly functional.\n\n__N.B.__ This tool is __NOT__ a general purpose registration framework to find transforms between IMS (MALDI or otherwise)\nand microscopy. It is built to align MALDI IMS pixels to their corresponding laser ablation marks as captured by microscopy AFTER the IMS experiment. \nThis approach has the advantage of providing direct evidence of registration performance as IMS pixels are aligned \nto their _explicit spatial origin_ in microscopy space, improving overall accuracy and confidence of microscopy-driven IMS \ndata analysis.\n\n## Installation\n\nYou can install `napari-imsmicrolink` via [pip]:\n\n    pip install napari-imsmicrolink\n\n### Typical experiment workflow\n1. Acquire pre-IMS microscopy (autofluorescence, brightfield) - _optional_\n2. Perform normal IMS sample preparation.\n3. Acquire post-IMS microscopy (autofluorescence, brightfield) with matrix still on sample\nthat reveals laser ablation marks.\n\n4. Gather IMS data that contains XY integer coordinates for the IMS experiment\n   (.imzML, Bruker spotlist (.txt, .csv), Bruker peaks.sqlite (_FTICR_),\n   Bruker .tsf (TIMS qTOF only))\n\n5. Run `napari-imsmicrolink` with data 3 and 4\n\n6. Once registered, use `wsireg` to align other microscopy modalities to IMS-registered post-IMS\nmicroscopy\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-imsmicrolink\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/nhpatterson/napari-imsmicrolink/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "IMS MicroLink"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nibabel",
    "name": "napari-nibabel",
    "display_name": "Napari NiBabel",
    "version": "0.1.0",
    "created_at": "2023-02-10",
    "modified_at": "2023-02-10",
    "authors": [
      "Ashley Anderson"
    ],
    "author_emails": [
      "aandersoniii@chanzuckerberg.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-nibabel/",
    "home_github": null,
    "home_other": "None",
    "summary": "Read access to some common neuroimaging file formats",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "nibabel[dicom,dicomfs,spm]",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-nibabel\n\n[![License MIT](https://img.shields.io/pypi/l/napari-nibabel.svg?color=green)](https://github.com/aganders3/napari-nibabel/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nibabel.svg?color=green)](https://pypi.org/project/napari-nibabel)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nibabel.svg?color=green)](https://python.org)\n[![tests](https://github.com/aganders3/napari-nibabel/workflows/tests/badge.svg)](https://github.com/aganders3/napari-nibabel/actions)\n[![codecov](https://codecov.io/gh/aganders3/napari-nibabel/branch/main/graph/badge.svg)](https://codecov.io/gh/aganders3/napari-nibabel)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nibabel)](https://napari-hub.org/plugins/napari-nibabel)\n\nRead access to some common neuroimaging file formats, thanks to the\n[NiBabel](https://nipy.org/nibabel/) and [pydicom](https://pydicom.github.io/)\nlibraries.\n\nAlso check out [napari-medical-image-formats](https://www.napari-hub.org/plugins/napari-medical-image-formats)!\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-nibabel` via [pip]:\n\n    pip install napari-nibabel\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-nibabel\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.nii.gz",
      "*.nii",
      "*.hdr",
      "*.dcm",
      "*.par",
      "*.gii"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "smo",
    "name": "smo",
    "display_name": "smo",
    "version": "2.0.2",
    "created_at": "2021-11-09",
    "modified_at": "2023-02-06",
    "authors": [
      "Mauro Silberberg"
    ],
    "author_emails": [
      "maurosilber@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/smo/",
    "home_github": "https://github.com/maurosilber/smo",
    "home_other": null,
    "summary": "Implementation of the Silver Mountain Operator (SMO) for the estimation of background distributions.",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "typing-extensions ; python_version < \"3.9\"",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "tox ; extra == 'dev'"
    ],
    "package_metadata_description": "![PyPi](https://img.shields.io/pypi/pyversions/smo.svg)\n[![License](https://img.shields.io/github/license/maurosilber/smo)](https://opensource.org/licenses/MIT)\n[![PyPi](https://img.shields.io/pypi/v/smo.svg)](https://pypi.python.org/pypi/smo)\n[![Conda](https://img.shields.io/conda/pn/conda-forge/smo)](https://anaconda.org/conda-forge/smo)\n\n# SMO\n\nSMO is a Python package that implements the Silver Mountain Operator (SMO), which allows to recover an unbiased estimation of the background intensity distribution in a robust way.\n\nWe provide an easy to use Python package and plugins for some of the major image processing softwares: [napari](https://napari.org), [CellProfiler](https://cellprofiler.org), and [ImageJ](https://imagej.net) / [FIJI](https://fiji.sc). See Plugins section below.\n\n## Citation\n\nTo learn more about the theory behind SMO, you can read the [pre-print in BioRxiv](https://doi.org/10.1101/2021.11.09.467975).\n\nIf you use this software, please cite that pre-print.\n\n## Usage\n\nTo obtain a background-corrected image, it is as straightforward as:\n\n```python\nimport skimage.data\nfrom smo import SMO\n\nimage = skimage.data.human_mitosis()\nsmo = SMO(sigma=0, size=7, shape=(1024, 1024))\nbackground_corrected_image = smo.bg_corrected(image)\n```\n\nwhere we used a sample image from `scikit-image`.\nBy default,\nthe background correction subtracts the median value of the background distribution.\nNote that the background regions will end up with negative values,\nbut with a median value of 0.\n\nA notebook explaining in more detail the meaning of the parameters and other possible uses for SMO is available here: [smo/examples/usage.ipynb](https://github.com/maurosilber/SMO/blob/main/smo/examples/usage.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maurosilber/SMO/blob/main/smo/examples/usage.ipynb).\n\n## Installation\n\nIt can be installed with `pip` from PyPI:\n\n```\npip install smo\n```\n\nor with `conda` from the conda-forge channel:\n\n```\nconda install -c conda-forge smo\n```\n\n## Plugins\n### Napari\n\nA [napari](https://napari.org) plugin is available.\n\nTo install:\n\n- Option 1: in napari, go to `Plugins > Install/Uninstall Plugins...` in the top menu, search for `smo` and click on the install button.\n\n- Option 2: just `pip` install this package in the napari environment.\n\nIt will appear in the `Plugins` menu.\n\n### CellProfiler\n\nA [CellProfiler](https://cellprofiler.org) plugin in available in the [smo/plugins/cellprofiler](smo/plugins/cellprofiler) folder.\n\n![](images/CellProfiler_SMO.png)\n\nTo install, save [this file](https://raw.githubusercontent.com/maurosilber/SMO/main/smo/plugins/cellprofiler/smo.py) into your CellProfiler plugins folder. You can find (or change) the location of your plugins directory in `File > Preferences > CellProfiler plugins directory`.\n\n### ImageJ / FIJI\n\nAn [ImageJ](https://imagej.net) / [FIJI](https://fiji.sc) plugin is available in the [smo/plugins/imagej](smo/plugins/imagej) folder.\n\n![](images/ImageJ_SMO.png)\n\nTo install, download [this file](https://raw.githubusercontent.com/maurosilber/SMO/main/smo/plugins/imagej/smo.py) and:\n\n- Option 1: in the ImageJ main window, click on `Plugins > Install... (Ctrl+Shift+M)`, which opens a file chooser dialog. Browse and select the downloaded file. It will prompt to restart ImageJ for changes to take effect.\n\n- Option 2: copy into your ImageJ plugins folder (`File > Show Folder > Plugins`).\n\nTo use the plugin, type `smo` on the bottom right search box:\n\n![](images/ImageJ_MainWindow.png)\n\nselect `smo` in the `Quick Search` window and click on the `Run` button.\n\n![](images/ImageJ_QuickSearch.png)\n\nNote: the ImageJ plugin does not check that saturated pixels are properly excluded.\n\n## Development\n\nCode style is enforced via pre-commit hooks. To set up a development environment, clone the repository, optionally create a virtual environment, install the [dev] extras and the pre-commit hooks:\n\n```\ngit clone https://github.com/maurosilber/SMO\ncd SMO\nconda create -n smo python pip numpy scipy\npip install -e .[dev]\npre-commit install\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "smo_image",
      "smo_probability",
      "background_correction",
      "background_probability"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "morphometrics",
    "name": "morphometrics",
    "display_name": "morphometrics",
    "version": "0.0.8",
    "created_at": "2022-03-17",
    "modified_at": "2023-02-02",
    "authors": [
      "Kevin Yamauchi"
    ],
    "author_emails": [
      "kevin.yamauchi@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/morphometrics/",
    "home_github": "https://github.com/kevinyamauchi/morphometrics",
    "home_other": null,
    "summary": "A plugin for quantifying shape and neighborhoods from images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "glasbey",
      "imageio (!=2.11.0,!=2.22.1,>=2.5.0)",
      "leidenalg",
      "napari-skimage-regionprops",
      "napari",
      "numba",
      "numpy",
      "qtpy",
      "pandas",
      "pooch",
      "pyclesperanto-prototype (>=0.8.0)",
      "pymeshfix",
      "pyqtgraph",
      "scanpy",
      "scikit-image (>0.19.0)",
      "scikit-learn (>=0.24.2)",
      "tqdm",
      "trimesh[easy]",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'"
    ],
    "package_metadata_description": "# morphometrics\n\n[![License](https://img.shields.io/pypi/l/morphometrics.svg?color=green)](https://github.com/morphometrics/morphometrics/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/morphometrics.svg?color=green)](https://pypi.org/project/morphometrics)\n[![Python Version](https://img.shields.io/pypi/pyversions/morphometrics.svg?color=green)](https://python.org)\n[![tests](https://github.com/morphometrics/morphometrics/workflows/tests/badge.svg)](https://github.com/morphometrics/morphometrics/actions)\n[![codecov](https://codecov.io/gh/morphometrics/morphometrics/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphometrics)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphometrics)](https://napari-hub.org/plugins/morphometrics)\n\nA plugin for quantifying shape and neighborhoods from images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\n### conda environment file\nYou can install `morphometrics` via our conda environment file. To do so, first install anaconda or miniconda on\nyour computer. Then, download the [`environment.yml file`](https://raw.githubusercontent.com/kevinyamauchi/morphometrics/master/environment.yml) (right click the link and \"Save as...\"). In your terminal,\nnavigate to the directory you downloaded the `environment.yml` file to:\n\n```bash\ncd <path/to/downloaded/environment.yml>\n```\n\nThen create the `morphometrics` environment using\n\n```bash\nconda env create -f environment.yml\n```\n\nOnce the environment has been created, you can activate it and use `morphometrics` as described below.\n\n```bash\nconda activate morphometrics\n```\n\nIf you are on Mac OS or Linux install the following:\n\nMac:\n\n```bash\nconda install -c conda-forge ocl_icd_wrapper_apple\n```\n\nLinux:\n\n```bash\nconda install -c conda-forge ocl-icd-system\n```\n\n\n### Development installation\n\nTo install latest development version :\n\n    pip install git+https://github.com/kevinyamauchi/morphometrics.git\n\n## Example applications\n<table border=\"0\">\n<tr><td>\n\n\n<img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/surface_distance_measurement.gif\"\nwidth=\"300\"/>\n\n</td><td>\n\n[measure the distance between surfaces](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/surface_distance_measurement.ipynb)\n\n</td></tr><tr><td>\n\n<img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/region_props_plugin.png\"\nwidth=\"300\"/>\n\n</td><td>\n\n[napari plugin for measuring properties of segmented objects (regionprops)](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/measure_with_widget.py)\n\n</td></tr><tr><td>\n\n<img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/object_classification.png\"\nwidth=\"300\"/>\n\n</td><td>\n\n[object classification](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/object_classification.ipynb)\n\n</td></tr><tr><td>\n\n<img src=\"https://github.com/kevinyamauchi/morphometrics/raw/main/resources/mesh_object.png\"\nwidth=\"300\"/>\n\n</td><td>\n\n[mesh binary mask](https://github.com/kevinyamauchi/morphometrics/blob/main/examples/mesh_binary_mask.ipynb)\n\n\n</td></tr></table>\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"morphometrics\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/kevinyamauchi/morphometrics/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Measure region properties",
      "Annotate clustered features",
      "Curate labels"
    ],
    "contributions_sample_data": [
      "simple labeled cube",
      "random 3D intensity image",
      "3D cylinders and spheres"
    ]
  },
  {
    "normalized_name": "napari-nd2-folder-viewer",
    "name": "napari-nd2-folder-viewer",
    "display_name": "napari nd2 folder viewer",
    "version": "0.0.13",
    "created_at": "2022-08-02",
    "modified_at": "2023-02-01",
    "authors": [
      "Niklas Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-nd2-folder-viewer/",
    "home_github": "https://github.com/gatoniel/napari-nd2-folder-viewer",
    "home_other": null,
    "summary": "Look through separate nd2 files in one viewer.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pyyaml",
      "marshmallow",
      "desert",
      "nd2 (>=0.4.3)",
      "dask",
      "pandas",
      "openpyxl",
      "julian",
      "napari-animation",
      "scikit-learn",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-nd2-folder-viewer\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-nd2-folder-viewer.svg?color=green)](https://github.com/gatoniel/napari-nd2-folder-viewer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nd2-folder-viewer.svg?color=green)](https://pypi.org/project/napari-nd2-folder-viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nd2-folder-viewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-nd2-folder-viewer/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-nd2-folder-viewer/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-nd2-folder-viewer/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-nd2-folder-viewer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nd2-folder-viewer)](https://napari-hub.org/plugins/napari-nd2-folder-viewer)\n\nLook through separate nd2 files in one viewer.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-nd2-folder-viewer` via [pip]:\n\n    pip install napari-nd2-folder-viewer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-nd2-folder-viewer.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-nd2-folder-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-nd2-folder-viewer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Load nd2 folder"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-macrokit",
    "name": "napari-macrokit",
    "display_name": "napari macro-kit",
    "version": "0.0.1",
    "created_at": "2023-01-27",
    "modified_at": "2023-01-27",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin-sc@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-macrokit/",
    "home_github": "https://github.com/hanjinliu/napari-macrokit",
    "home_other": null,
    "summary": "Executable script generation for napari plugins",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "macro-kit (>=0.4.0)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-macrokit\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-macrokit.svg?color=green)](https://github.com/hanjinliu/napari-macrokit/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-macrokit.svg?color=green)](https://pypi.org/project/napari-macrokit)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-macrokit.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-macrokit/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-macrokit/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-macrokit/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-macrokit)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-macrokit)](https://napari-hub.org/plugins/napari-macrokit)\n\nExecutable script generation for napari plugins.\n\n![](https://github.com/hanjinliu/napari-macrokit/blob/main/images/example.gif)\n&uarr; [Example](https://github.com/hanjinliu/napari-macrokit/blob/main/examples/regionprops.py) showing the real-time recording of GUI operation.\n\nThis napari plugin aims at making image analysis reproducible with arbitrary input/output types.\n\n## Usage\n\nCreate a macro object, decorate functions with `record` method and run!\n\n```python\nfrom napari_macrokit import get_macro\n\nmacro = get_macro(\"my-plugin-specifier\")  # get macro object\n\n# define a function\n@macro.record\ndef add(a: float, b: float) -> float:\n    return a + b\n\n# run\nresult = add(3.2, 5.4)\nadd(result, 1.0)\n\nmacro\n\n# Out:\n# >>> float0 = add(3.2, 5.4)\n# >>> float1 = add(float0, 1.0)\n```\n\n## Record GUI Operations\n\nYou can use recordable functions in your widgets to keep tracks of GUI operations.\nMore simply, you can double-decorate functions with `record` and `magicgui`.\n\n```python\nimport numpy as np\nfrom magicgui import magicgui\nimport napari\nfrom napari.types import ImageData\nfrom napari_macrokit import get_macro\n\nmacro = get_macro(\"my-plugin-specifier\")  # get macro object\n\n# define recordable magicgui\n@magicgui\n@macro.record\ndef add(image: ImageData, b: float) -> ImageData:\n    return image + b\n\nviewer = napari.Viewer()  # launch a viewer\nviewer.add_image(np.random.random((100, 100)))  # image data\nviewer.window.add_dock_widget(add)  # add magicgui to the viewer\n```\n\nRunning add twice in GUI and you'll find macro updated like below.\n\n```python\nmacro\n# Out\n# >>> image0 = add(viewer.layers['Image'].data, 0.06)\n# >>> image1 = add(image0, 0.12)\n```\n\n## Combining Plugins\n\nSuppose you have two modules that use `napari-macrokit`.\n\n```python\n# napari_module_0.py\n\nfrom napari.types import ImageData\nfrom scipy import ndimage as ndi\nfrom napari_macrokit import get_macro\n\nmacro = get_macro(\"napari-module-0\")\n\n@macro.record\ndef gaussian_filter(image: ImageData, sigma: float) -> ImageData:\n    return ndi.gaussian_filter(image, sigma=sigma)\n\n@macro.record\ndef threshold(image: ImageData, value: float) -> ImageData:\n    return image > value\n```\n\n```python\n# napari_module_1.py\n\nfrom napari.types import ImageData\nimport numpy as np\nfrom napari_macrokit import get_macro\nmacro = get_macro(\"napari-module-1\")\n\n@macro.record\ndef estimate_background(image: ImageData) -> float:\n    return np.percentile(image, 10.0)\n\n```\n\nYou can use functions from both modules to build an analysis workflow by collecting existing macro objects with `collect_macro` function. All the recordable actions in the modules will also be recorded to the returned macro object.\n\n```python\nimport numpy as np\nfrom napari_macrokit import collect_macro\nfrom napari_module_0 import gaussian_filter, threshold\nfrom napari_module_1 import estimate_background\n\n# global_macro will record all the macro available at this point\nglobal_macro = collect_macro()\n\n# start image analysis!\nimage = np.random.random((100, 100))\n\nout = gaussian_filter(image, 2.0)\nthresh = estimate_background(out)\nbinary = threshold(out, thresh)\n\nmacro\n# Out\n# >>> image0 = gaussian_filter(arr0, 2.0)\n# >>> float0 = estimate_background(image0)\n# >>> image1 = threshold(image1, float0)\n```\n\n---------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-macrokit` via [pip]:\n\n    pip install napari-macrokit\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-macrokit.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-macrokit\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hanjinliu/napari-macrokit/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "napari-macrokit"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-result-stack",
    "name": "napari-result-stack",
    "display_name": "Result stack",
    "version": "0.0.1",
    "created_at": "2023-01-27",
    "modified_at": "2023-01-27",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin-sc@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-result-stack/",
    "home_github": "https://github.com/hanjinliu/napari-result-stack",
    "home_other": null,
    "summary": "Widgets and type annotations for storing function results of any types",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-result-stack\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-result-stack.svg?color=green)](https://github.com/hanjinliu/napari-result-stack/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-result-stack.svg?color=green)](https://pypi.org/project/napari-result-stack)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-result-stack.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-result-stack/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-result-stack/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-result-stack/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-result-stack)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-result-stack)](https://napari-hub.org/plugins/napari-result-stack)\n\nWidgets and type annotations for storing function results of any types.\n\n## `Stored` type\n\nType `Stored[T]` is equivalent to `T` for the type checker, but `magicgui` is aware of this annotation and behaves as a \"storage\" for the `T` instances.\n\n```python\nfrom pathlib import Path\nimport pandas as pd\nfrom magicgui import magicgui\nfrom napari_result_stack import Stored\n\n# Returned values will be stored in a result stack.\n@magicgui\ndef provide_data(path: Path) -> Stored[pd.DataFrame]:\n    return pd.read_csv(path)\n\n# You can choose one of the values stored in the result stack\n# for the argument `df` from a ComboBox widget.\n@magicgui\ndef print_data(df: Stored[pd.DataFrame]):\n    print(df)\n```\n\n![](https://github.com/hanjinliu/napari-result-stack/blob/main/images/demo-0.gif)\n\n- Different types use different storage. e.g. `Stored[int]` and `Stored[str]` do not share the same place.\n- Even for the same type, you can specify the second key to split the storage. e.g. `Stored[int]`, `Stored[int, 0]` and `Stored[int, \"my-plugin-name\"]` use the distinct storages.\n\n## Manually store variables\n\n`Stored.valuesof[T]` is a `list`-like object that returns a view of the values stored in `Stored[T]`. This value view is useful if you want to store values outside `@magicgui`.\n\n```python\nfrom magicgui.widgets import PushButton\nfrom datetime import datetime\nfrom napari_result_stack import Stored\n\nbutton = PushButton(text=\"Click!\")\n\n@button.changed.connect\ndef _record_now():\n    Stored.valuesof[datetime].append(datetime.now())\n\n```\n\n## Result stack widget\n\n`napari-result-stack` provides a plugin widget that is helpful to inspect all the stored values.\n\n![](https://github.com/hanjinliu/napari-result-stack/blob/main/images/demo-1.gif)\n\n\n<details><summary>Show code</summary><div>\n\n```python\nfrom napari_result_stack import Stored\nfrom magicgui import magicgui\nimport numpy as np\nimport pandas as pd\n\n@magicgui\ndef f0() -> Stored[pd.DataFrame]:\n    return pd.DataFrame(np.random.random((4, 3)))\n\n@magicgui\ndef f1(x: Stored[pd.DataFrame]) -> Stored[float]:\n    return np.mean(np.array(x))\n\nviewer.window.add_dock_widget(f0, name=\"returns a DataFrame\")\nviewer.window.add_dock_widget(f1, name=\"mean of a DataFrame\")\n```\n\n---\n</div></details>\n\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-result-stack` via [pip]:\n\n    pip install napari-result-stack\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-result-stack.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-result-stack\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hanjinliu/napari-result-stack/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Result stack"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-solarized",
    "name": "napari-solarized",
    "display_name": "Napari Solarized",
    "version": "0.1.1",
    "created_at": "2023-01-23",
    "modified_at": "2023-01-23",
    "authors": [
      "Ashley Anderson"
    ],
    "author_emails": [
      "aandersoniii@chanzuckerberg.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-solarized/",
    "home_github": "https://github.com/aganders3/napari-solarized",
    "home_other": null,
    "summary": "Solarized themes for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "npe2 ; extra == 'testing'",
      "typer ; extra == 'testing'",
      "importlib-resources ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-solarized\n\nSolarized (-ish) themes for napari, based on [solarized](https://ethanschoonover.com/solarized/).\n\n![solarized dark screenshot](https://raw.githubusercontent.com/aganders3/napari-solarized/main/screenshot_dark.png)\n![solarized light screenshot](https://raw.githubusercontent.com/aganders3/napari-solarized/main/screenshot_light.png)\n\n[![License MIT](https://img.shields.io/pypi/l/napari-solarized.svg?color=green)](https://github.com/aganders3/napari-solarized/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-solarized.svg?color=green)](https://pypi.org/project/napari-solarized)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-solarized.svg?color=green)](https://python.org)\n[![tests](https://github.com/aganders3/napari-solarized/workflows/tests/badge.svg)](https://github.com/aganders3/napari-solarized/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-solarized)](https://napari-hub.org/plugins/napari-solarized)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-solarized` via [pip]:\n\n    pip install napari-solarized\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/aganders3/napari-solarized.git\n\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-solarized\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/aganders3/napari-solarized/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-file-watcher",
    "name": "napari-file-watcher",
    "display_name": "napari-file-watcher",
    "version": "0.1.1",
    "created_at": "2023-01-16",
    "modified_at": "2023-01-16",
    "authors": [
      "Xavier Casas Moreno"
    ],
    "author_emails": [
      "xaviercm@kth.se"
    ],
    "license": "GPL-3.0",
    "home_pypi": "https://pypi.org/project/napari-file-watcher/",
    "home_github": "https://github.com/kasasxav/napari-file-watcher",
    "home_other": null,
    "summary": "A napari plugin for file watching",
    "categories": [],
    "package_metadata_requires_python": null,
    "package_metadata_requires_dist": [
      "napari",
      "ome-zarr",
      "zarr",
      "h5py",
      "PyQt5",
      "qtpy",
      "QScintilla"
    ],
    "package_metadata_description": "# File watcher plugin for napari (napari-file-watcher)\n\n\nThis plugin contains two widgets: file watcher and script editor.\n\n\n## Usage: file watcher\n\nThe file watcher monitors a folder and displays its images (tiff, ome-zarr or hdf5) as napari layers, watch the following video for a demo:\n\n[![IMAGE ALT TEXT](http://img.youtube.com/vi/lFRVwlHgJ-Y/0.jpg)](https://www.youtube.com/watch?v=lFRVwlHgJ-Y \"Demo napari-file-watcher\")\n\nInstructions:\n\n1. Select the folder you want to monitor by pressing \"Browse\".\n2. Select the extension of the files: \"zarr\", \"hdf5\" or \"tiff\".\n3. Click \"Watch and run\" to display the current items & the newly arrived as napari layers.\n4. If you click in one of the files of the list, the metadata will show (for hdf5 and zarr)\n\n## Usage: scripting editor\n\nThe script editor is for developing scripts and saving them in the filesystem. \nWe have used this widget in the context of developing scripts for microscopy control software that implements another file watcher.\n\nInstructions:\n\n1. Select the folder where you want to save your scripts in \"Browse\".\n2. Type the name of the script in the edit box below.\n3. Click \"Add\" for saving it into the folder after typing, or \"Open\" to display an existing file.\n\n## Installation\n\nYou can install `napari-file-watcher` via [pip]:\n\n    pip install napari-file-watcher\n\nOr if you plan to develop it:\n\n    git clone https://github.com/kasasxav/napari-file-watcher\n    cd napari-file-watcher\n    pip install -e .\n\nIf there is an error message suggesting that git is not installed, run `conda install git`.\n\n## Contributing\n\nContributions are welcome, tests are run with pytest.\n\n## Issues\n\nIssues can be reported at: https://github.com/kasasxav/napari-file-watcher/issues\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "File watcher",
      "Scripting editor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pystackreg",
    "name": "napari-pystackreg",
    "display_name": "napari pystackreg",
    "version": "0.1.4",
    "created_at": "2022-07-07",
    "modified_at": "2023-01-15",
    "authors": [
      "Gregor Lichtner"
    ],
    "author_emails": [
      "gregor.lichtner@med.uni-greifswald.de"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-pystackreg/",
    "home_github": "https://github.com/glichtner/napari-pystackreg",
    "home_other": null,
    "summary": "Robust image registration for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "numpy",
      "pystackreg (>=0.2.6)",
      "qtpy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-pystackreg\n\n[![License](https://img.shields.io/pypi/l/napari-pystackreg.svg?color=green)](https://github.com/glichtner/napari-pystackreg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pystackreg.svg?color=green)](https://pypi.org/project/napari-pystackreg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pystackreg.svg?color=green)](https://python.org)\n[![tests](https://github.com/glichtner/napari-pystackreg/workflows/tests/badge.svg)](https://github.com/glichtner/napari-pystackreg/actions)\n[![codecov](https://codecov.io/gh/glichtner/napari-pystackreg/branch/main/graph/badge.svg)](https://codecov.io/gh/glichtner/napari-pystackreg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pystackreg)](https://napari-hub.org/plugins/napari-pystackreg)\n\nRobust image registration for napari.\n\n## Summary\nnapari-pystackreg offers the image registration capabilities of the python package\n[pystackreg](https://github.com/glichtner/pystackreg) for napari.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/napari-pystackreg.gif)\n\n## Description\n\npyStackReg is used to align (register) one or more images to a common reference image, as is required usually in\ntime-resolved fluorescence or wide-field microscopy.\nIt is directly ported from the source code of the ImageJ plugin ``TurboReg`` and provides additionally the\nfunctionality of the ImageJ plugin ``StackReg``, both of which were written by Philippe Thevenaz/EPFL\n(available at http://bigwww.epfl.ch/thevenaz/turboreg/).\n\npyStackReg provides the following five types of distortion:\n\n- Translation\n- Rigid body (translation + rotation)\n- Scaled rotation (translation + rotation + scaling)\n- Affine (translation + rotation + scaling + shearing)\n- Bilinear (non-linear transformation; does not preserve straight lines)\n\npyStackReg supports the full functionality of StackReg plus some additional options, e.g., using different reference\nimages and having access to the actual transformation matrices (please see the examples below). Note that pyStackReg\nuses the high quality (i.e. high accuracy) mode of TurboReg that uses cubic spline interpolation for transformation.\n\nPlease note: The bilinear transformation cannot be propagated, as a combination of bilinear transformations does not\ngenerally result in a bilinear transformation. Therefore, stack registration/transform functions won't work with\nbilinear transformation when using \"previous\" image as reference image. You can either use another reference (\n\"first\" or \"mean\" for first or mean image, respectively), or try to register/transform each image of the stack\nseparately to its respective previous image (and use the already transformed previous image as reference for the\nnext image).\n\n## Installation\n\nYou can install ``napari-pystackreg`` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/):\n\n    pip install napari-pystackreg\n\nYou can also install ``napari-pystackreg`` via [conda](https://docs.conda.io/en/latest/):\n\n    conda install -c conda-forge napari-pystackreg\n\nOr install it via napari's plugin installer.\n\n    Plugins > Install/Uninstall Plugins... > Filter for \"napari-pystackreg\" > Install\n\nTo install latest development version:\n\n    pip install git+https://github.com/glichtner/napari-pystackreg.git\n\n## Usage\n\n\n### Open Plugin User Interface\n\nStart up napari, e.g. from the command line:\n\n    napari\n\nThen, load an image stack (e.g. via ``File > Open Image...``) that you want to register. You can also use the example\nstack provided by the pluging (``File > Open Sample > napari-pystackreg: PC12 moving example``).\nThen, select the ``napari-pystackreg`` plugin from the ``Plugins > napari-pystackreg: pystackreg`` menu.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-initial.png)\n\n### User Interface Options\nA variety of options are available to control the registration process:\n\n* `Image Stack`: The image layer that should be registered/transformed.\n* `Transformation`: The type of transformation that should be applied.\n  - `Translation`: translation\n  - `Rigid body`: translation + rotation\n  - `Scaled rotation`: translation + rotation + scaling\n  - `Affine`: translation + rotation + scaling + shearing\n  - `Bilinear`: non-linear transformation; does not preserve straight lines\n* `Reference frame:` The reference image for registration.\n  - `Previous frame`: Aligns each frame (image) to its previous frame in the stack\n  - `Mean (all frames)`: Aligns each frame (image) to the average of all images in the stack\n  - `Mean (first n frames)`: Aligns each frame (image) to the mean of the first n frames in the stack. n is a tuneable parameter.\n* `Moving-average stack before register`: Apply a moving average to the stack before registration. This can be useful to\n  reduce noise in the stack (if the signal-to-noise ratio is very low). The moving average is applied to the stack only\n  for determining the transformation matrices, but not for the actual transforming of the stack.\n* `Transformation matrix file`: Transformation matrices can be saved to or loaded from a file for permanent storage.\n\n### Reference frame\nThe reference frame is the frame to which the other frames are aligned. The default option is to use the\n`Previous frame`, which will register each frame to its respective previous frame in the stack. Alternatively, the\nreference frame can be set to the mean of all frames in the stack (`Mean (all frames)`) or the mean of the first n\nframes in the stack (`Mean (first n frames)`). The latter option can be useful if the first frames in the stack are more\nstable than the later frames (e.g. if the first frames are taken before the sample is moved). When selecting the\n`Mean (first n frames)` option, the number of frames to use for the mean can be set via the spinbox below the option.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-reference-mean-n.png)\n\n### Moving average before registration\nTo increase registration performance with low signal-to-noise ratio stacks, a moving average can be applied to the\nstack before registration. The moving average is applied to the stack only for determining the\ntransformation matrices, but not for the actual transforming of the stack. That means that the transformed stack will\nstill contain the original frames (however registered), but not the averaged frames.\n\nWhen selecting the `Moving-average stack before register` option, the number of frames to use for the moving average can\nbe set via the spinbox below the option.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-moving-average.png)\n\n### Transformation matrix file\nThe transformation matrices can be saved to or loaded from a file for permanent storage. This can be useful if you want\nto apply the same transformation to another stack (e.g. a different channel of the same sample). The transformation\nmatrices are saved as a numpy array in a binary file (``.npy``). The file can be loaded via the `Load` button and saved\nvia the `Save` button.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-register-tmat.png)\n\n### Register/Transform\nTo perform the actual registration and transformation steps, click the `Register` and `Transform` buttons, respectively.\n\nThe `Register` button will register the stack to the reference by determining the appropriate transformation matrices,\nwithout actually transforming the stack. The transformation matrices can be saved to a file via the `Save` button in the\n`Transformation matrix file` section.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-registered.png)\n\nThe `Transform` button (1) will transform the stack to the reference by applying the transformation matrices that are\ncurrently loaded to the stack selected in `Image Stack`. For the button to become active, either the transformation\nmatrices have to be loaded from a file via the `Load` button in the `Transformation matrix file` section, or the\n`Register` button has to be clicked first to determine the transformation matrices.\n\nThe `Transform` button will also add a new image layer to the napari viewer (2) with the transformed stack. The name of the\nnew layer will be the name of the original stack with the prefix `Registered`.\n\n![](https://github.com/glichtner/napari-pystackreg/raw/main/docs/ui-transformed.png)\n\nFinally, the `Register & Transform` button will perform both the registration and transformation steps in one go.\n\n----------------------------------\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-pystackreg\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Acknowledgments\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/glichtner/napari-pystackreg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "pystackreg"
    ],
    "contributions_sample_data": [
      "PC12 moving example"
    ]
  },
  {
    "normalized_name": "napari-plugin-search",
    "name": "napari-plugin-search",
    "display_name": "napari-plugin-search",
    "version": "0.1.4",
    "created_at": "2021-08-21",
    "modified_at": "2023-01-04",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-plugin-search/",
    "home_github": "https://github.com/haesleinhuepf/napari-plugin-search",
    "home_other": null,
    "summary": "Find napari plugins",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# napari-plugin-search\n\n[![License](https://img.shields.io/pypi/l/napari-plugin-search.svg?color=green)](https://github.com/haesleinhuepf/napari-plugin-search/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-plugin-search.svg?color=green)](https://pypi.org/project/napari-plugin-search)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plugin-search.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-plugin-search/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plugin-search/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-plugin-search/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plugin-search)\n\nFind napari plugins\n![img.png](https://github.com/haesleinhuepf/napari-plugin-search/raw/main/docs/napari-plugin-search-screencast.gif)\n\n## Usage\nEnter the name of the plugin you are searching for and use the `up` and `down` arrow keys to navigate between them. \nHit `Enter` to start a plugin.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-plugin-search` via [pip]:\n\n    pip install napari-plugin-search\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-plugin-search\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-plugin-search/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[image.sc]: https://image.sc\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PluginSearch"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-hierarchical",
    "name": "napari-hierarchical",
    "display_name": "napari-hierarchical",
    "version": "0.1.0",
    "created_at": "2022-12-21",
    "modified_at": "2022-12-21",
    "authors": [
      "Jonas Windhager"
    ],
    "author_emails": [
      "jonas.windhager@uzh.ch"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-hierarchical/",
    "home_github": "https://github.com/BodenmillerGroup/napari-hierarchical",
    "home_other": null,
    "summary": "Hierarchical file format support for napari",
    "categories": [],
    "package_metadata_requires_python": "<3.11,>=3.8",
    "package_metadata_requires_dist": [
      "napari (<0.4.18,>=0.4.17)",
      "pluggy",
      "qtpy",
      "dask ; extra == 'all'",
      "h5py ; extra == 'all'",
      "readimc ; extra == 'all'",
      "s3fs ; extra == 'all'",
      "zarr ; extra == 'all'",
      "dask ; extra == 'hdf5'",
      "h5py ; extra == 'hdf5'",
      "dask ; extra == 'imc'",
      "readimc ; extra == 'imc'",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "dask ; extra == 'zarr'",
      "s3fs ; extra == 'zarr'",
      "zarr ; extra == 'zarr'"
    ],
    "package_metadata_description": "# napari-hierarchical\n\n[![License MIT](https://img.shields.io/pypi/l/napari-hierarchical.svg?color=green)](https://github.com/BodenmillerGroup/napari-hierarchical/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-hierarchical.svg?color=green)](https://pypi.org/project/napari-hierarchical)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-hierarchical.svg?color=green)](https://python.org)\n[![tests](https://github.com/BodenmillerGroup/napari-hierarchical/workflows/tests/badge.svg)](https://github.com/BodenmillerGroup/napari-hierarchical/actions)\n[![codecov](https://codecov.io/gh/BodenmillerGroup/napari-hierarchical/branch/main/graph/badge.svg)](https://codecov.io/gh/BodenmillerGroup/napari-hierarchical)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-hierarchical)](https://napari-hub.org/plugins/napari-hierarchical)\n\nHierarchical file format support for napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-hierarchical` via [pip]:\n\n    pip install \"napari-hierarchical[all]\"\n\nTo install latest development version :\n\n    pip install \"git+https://github.com/BodenmillerGroup/napari-hierarchical.git#egg=napari-hierarchical[all]\"\n\n## Usage\n\nThe plugin enables the reading, editing and writing of container formats. In the plugin, *groups* represent hierarchically structured collections of *arrays*. Each group can hold zero or more arrays and can have zero or more child groups (hierarchical structure). An array is a logical representation of (image) data on disk and directly corresponds to a napari layer when loaded.\n\nFiles can be opened through napari (e.g. `File -> Open File(s)` menu, `Viewer.open(...)` function), as the plugin implements napari's file reader hook. Upon opening a hierarchically structured file, the *Groups* and *Arrays* widgets are displayed. The *Groups* widget allows to browse and restructure the groups tree, while the *Arrays* widget groups arrays from the selected groups by file format-specific metadata (e.g. channel name for MCD files). Selecting arrays also selects the corresponding napari layers, allowing to adjust their properties.\n\nArrays can be loaded individually by toggling their *loaded* state (circular button), which will add napari layers for the corresponding arrays. Similarly, loaded arrays can be shown or hidden by toggling their *visible* state (eye button), which will toggle the visibility of the associated napari layers. The loaded/visible states of groups (collections of arrays) can be toggled in a similar fashion. Arrays are always loaded into memory (no memory mapping), to allow for editing the tree structure. Loaded root groups can be exported to supported hierarchical file formats.\n\nCurrently, reading/writing of HDF5 and Zarr (not: OME-NGFF) files are supported out of the box, as well as reading imaging mass cytometry (IMC) data (i.e., MCD files). For these file formats, sample data is available through the plugin. Additional readers/writers can be implemented using a pluggy-based interface, similar to the first generation `napari-plugin-engine`.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-hierarchical\" is free and open source software\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/BodenmillerGroup/napari-hierarchical/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Groups",
      "Arrays"
    ],
    "contributions_sample_data": [
      "IMC mock data (Bodenmiller group, University of Zurich)",
      "3D confocal time-lapse imaging of human condensins (Walther et al., 2018)",
      "3D confocal imaging of a pollen grain (Computer Vision Group, University of Freiburg)"
    ]
  },
  {
    "normalized_name": "napari-bud-cell-segmenter",
    "name": "napari-bud-cell-segmenter",
    "display_name": "Bud Cell Segmenter",
    "version": "0.1.4",
    "created_at": "2022-11-14",
    "modified_at": "2022-12-16",
    "authors": [
      "Aurelien Maillot"
    ],
    "author_emails": [
      "aurelien.maillot@protonmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-bud-cell-segmenter/",
    "home_github": "https://github.com/AurelienMaillot/napari-bud-cell-segmenter",
    "home_other": null,
    "summary": "A plugin to segment embryonic mammary bud cells and detect 2 RNA probes",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "scikit-image",
      "napari",
      "tifffile",
      "matplotlib",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-bud-cell-segmenter\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-bud-cell-segmenter.svg?color=green)](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bud-cell-segmenter.svg?color=green)](https://pypi.org/project/napari-bud-cell-segmenter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bud-cell-segmenter.svg?color=green)](https://python.org)\n[![tests](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/workflows/tests/badge.svg)](https://github.com/AurelienMaillot/napari-bud-cell-segmenter/actions)\n[![codecov](https://codecov.io/gh/AurelienMaillot/napari-bud-cell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AurelienMaillot/napari-bud-cell-segmenter)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bud-cell-segmenter)](https://napari-hub.org/plugins/napari-bud-cell-segmenter)\n\nA plugin to segment embryonic mammary bud cells and detect 2 RNA probes\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-bud-cell-segmenter` via [pip]:\n\n    pip install napari-bud-cell-segmenter\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/AurelienMaillot/napari-bud-cell-segmenter.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bud-cell-segmenter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/AurelienMaillot/napari-bud-cell-segmenter/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Load Data",
      "Preprocessing",
      "Segmentation",
      "Manual Corrections Widget",
      "Blob detection",
      "Statistics",
      "Visualization",
      "Export Data"
    ],
    "contributions_sample_data": [
      "Bud Cell Segmenter"
    ]
  },
  {
    "normalized_name": "misic-napari",
    "name": "misic-napari",
    "display_name": "misic-napari",
    "version": "0.2.3",
    "created_at": "2021-12-07",
    "modified_at": "2022-12-15",
    "authors": [
      "S. Panigrahi & L. Espinosa",
      "IAM",
      "LCB"
    ],
    "author_emails": [
      "spanigrahi@imm.cnrs.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/misic-napari/",
    "home_github": "https://github.com/pswap/misic",
    "home_other": null,
    "summary": "Segmentation of bacteria agnostic to imaging modality",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "tensorflow",
      "termcolor"
    ],
    "package_metadata_description": "# misic-napari\n\n<!-- [![License](https://img.shields.io/pypi/l/misic-napari-plugin.svg?color=green)](https://github.com/pswap/misic-napari-plugin/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/misic-napari-plugin.svg?color=green)](https://pypi.org/project/misic-napari-plugin)\n[![Python Version](https://img.shields.io/pypi/pyversions/misic-napari-plugin.svg?color=green)](https://python.org)\n[![tests](https://github.com/pswap/misic-napari-plugin/workflows/tests/badge.svg)](https://github.com/pswap/misic-napari-plugin/actions)\n[![codecov](https://codecov.io/gh/pswap/misic-napari-plugin/branch/master/graph/badge.svg)](https://codecov.io/gh/pswap/misic-napari-plugin) -->\n\n----------------------------------\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\nA napari plugin for [MiSiC](https://elifesciences.org/articles/65151). Segmentation of bacteria in dense colonies. \nThe plugin provides acces to preprocessing of the image like scaling, gamma correction, sharpness and noise variance that can improve the segmentation of bacteria irrespective of the imaging modality.\n\n## Install Napari\nInstall napari either the bundled app or through [pip/conda]\nhttps://napari.org/#installation\n\n## Installation\n\nInstall `misic-napari` through plugin manager in napari.\n\nOr\n\nYou can install `misic-napari` via [pip] in the napari console:\n\n    pip install misic-napari\n\n## Tutorial\nNote: \nThe image should be in the format [n,row,col] or [row,col], i.e., a single image or a stack. Hyper-stacks are not supported yet. \n\n#### get_width\n\n\nCreates a Shapes layer with name 'cell-width' where the cell width can be hand drawn using line drawing tools in the shapes layer. This need not be precise and can be adjusted later. Click `get_cell_width` to obtain the desired mean cell width. This will be used to scale the image accordingly before segmentation.\n \n#### segment\n\nThis can be used to quickly set the parameters that can be later used to segment the whole stack.\n\n```\nuse_roi\n```\nA square ROI of side 256 is created by default for quickly checking adjusting the segmentation parameters. The roi can be resized or moved in the `roi` shapes layer.\n\n```\nlight_background\n```\nTrue; for phase-contrast images.\n\nFalse; for bright-field and fluorescence images.\n\n```\nuse_local_noise\n```\nIf checked, this adds noise to image with local variance. In this case, a noise_var of around 0.1 works well. If unchecked, this adds noise with global variance of noise_var/100. Adding may help in removing false positives.\n\n```\ngaussian_laplace\n```\nUseful when segmenting fluorescence images. \n\n```\nadjust_scale\n```\nFine-tuning the scale around ([0.8,1.2]) the scale obtained from cell-width determined in `get_cell_width`.\n\n```\nnoise_var\n```\nAmount of noise to be added to the image at the preprocessing step. This helps reduce the False Positives and, in many cases, to separate cells effectively. \n```\ngamma\n```\ngamma correction \n\n```\nsharpness_scale and sharpness_amount\n```\nUnsharp mask based sharpness with sigma = sharpness_scale and amount = sharpness_amount\n\n\n\n### segment_stack\nSegments the entire stack using the parameters that were obtained in \"segment\".\n\n\n### save\nThe parameters can be saved in a json file. \n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"misic-napari\" is free and open source software\n\n## Cite\n```\n@article {10.7554/eLife.65151,\narticle_type = {journal},\ntitle = {Misic, a general deep learning-based method for the high-throughput cell segmentation of complex bacterial communities},\nauthor = {Panigrahi, Swapnesh and Murat, Doroth√©e and Le Gall, Antoine and Martineau, Eug√©nie and Goldlust, Kelly and Fiche, Jean-Bernard and Rombouts, Sara and N√∂llmann, Marcelo and Espinosa, Leon and Mignot, T√¢m},\neditor = {Xiao, Jie and Storz, Gisela and Hensel, Zach},\nvolume = 10,\nyear = 2021,\nmonth = {sep},\npub_date = {2021-09-09},\npages = {e65151},\ncitation = {eLife 2021;10:e65151},\ndoi = {10.7554/eLife.65151},\nurl = {https://doi.org/10.7554/eLife.65151},\nabstract = {Studies of bacterial communities, biofilms and microbiomes, are multiplying due to their impact on health and ecology. Live imaging of microbial communities requires new tools for the robust identification of bacterial cells in dense and often inter-species populations, sometimes over very large scales. Here, we developed MiSiC, a general deep-learning-based 2D segmentation method that automatically segments single bacteria in complex images of interacting bacterial communities with very little parameter adjustment, independent of the microscopy settings and imaging modality. Using a bacterial predator-prey interaction model, we demonstrate that MiSiC enables the analysis of interspecies interactions, resolving processes at subcellular scales and discriminating between species in millimeter size datasets. The simple implementation of MiSiC and the relatively low need in computing power make its use broadly accessible to fields interested in bacterial interactions and cell biology.},\nkeywords = {Deep learning, image analysis, microscopy, myxococcus xanthus, biofilms},\njournal = {eLife},\nissn = {2050-084X},\npublisher = {eLife Sciences Publications, Ltd},\n}\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "get_width",
      "segment",
      "segment_stack",
      "save_params"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "partseg-smfish",
    "name": "PartSeg-smfish",
    "display_name": "PartSeg-smfish",
    "version": "0.1.3",
    "created_at": "2022-10-24",
    "modified_at": "2022-12-06",
    "authors": [
      "Grzegorz Bokota"
    ],
    "author_emails": [
      "g.bokota@cent.uw.edu.pl"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/partseg-smfish/",
    "home_github": "https://github.com/4DNucleome/PartSeg-smfish",
    "home_other": null,
    "summary": "PartSeg and napari plugin for smfish data",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "PartSeg (>=0.13.0)",
      "numpy",
      "napari",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "package_metadata_description": "# PartSeg-smfish\n\n[![License BSD-3](https://img.shields.io/pypi/l/PartSeg-smfish.svg?color=green)](https://github.com/4DNucleome/PartSeg-smfish/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/PartSeg-smfish.svg?color=green)](https://pypi.org/project/PartSeg-smfish)\n[![Python Version](https://img.shields.io/pypi/pyversions/PartSeg-smfish.svg?color=green)](https://python.org)\n[![tests](https://github.com/4DNucleome/PartSeg-smfish/workflows/tests/badge.svg)](https://github.com/4DNucleome/PartSeg-smfish/actions)\n[![codecov](https://codecov.io/gh/4DNucleome/PartSeg-smfish/branch/main/graph/badge.svg)](https://codecov.io/gh/4DNucleome/PartSeg-smfish)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/PartSeg-smfish)](https://napari-hub.org/plugins/PartSeg-smfish)\n\nPartSeg and napari plugin for smfish data\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `PartSeg-smfish` via [pip]:\n\n    pip install PartSeg-smfish\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/4DNucleome/PartSeg-smfish.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"PartSeg-smfish\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/4DNucleome/PartSeg-smfish/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Verify points",
      "Find single points",
      "Copy labels",
      "Gauss background estimate",
      "Laplacian check",
      "Laplacian estimate",
      "Maximum projection"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-affinities",
    "name": "napari-affinities",
    "display_name": "napari Affinities",
    "version": "0.1.3",
    "created_at": "2022-06-12",
    "modified_at": "2022-11-27",
    "authors": [
      "William Patton"
    ],
    "author_emails": [
      "will.hunter.patton@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-affinities/",
    "home_github": "https://github.com/pattonw/napari-affinities",
    "home_other": null,
    "summary": "A plugin for creating, visualizing, and processing affinities",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "zarr",
      "magicgui",
      "bioimageio.core",
      "gunpowder",
      "matplotlib",
      "torch",
      "lsds"
    ],
    "package_metadata_description": "# napari-affinities\n\n[![License](https://img.shields.io/pypi/l/napari-affinities.svg?color=green)](https://github.com/pattonw/napari-affinities/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-affinities.svg?color=green)](https://pypi.org/project/napari-affinities)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-affinities.svg?color=green)](https://python.org)\n[![tests](https://github.com/pattonw/napari-affinities/workflows/tests/badge.svg)](https://github.com/pattonw/napari-affinities/actions)\n[![codecov](https://codecov.io/gh/pattonw/napari-affinities/branch/main/graph/badge.svg)](https://codecov.io/gh/pattonw/napari-affinities)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-affinities)](https://napari-hub.org/plugins/napari-affinities)\n\nA plugin for creating, visualizing, and processing affinities\n\n---\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou will need a conda environment for everything to run\nsmoothly. Supported python versions are 3.7, 3.8, 3.9.\n\n### pip\nYou can install `napari-affinities` via [pip]:\n\n    `pip install napari-affinities`\n\nTo install latest development version :\n\n    `pip install git+https://github.com/pattonw/napari-affinities.git`\n\nInstall torch according to your system [(follow the instructions here)](https://pytorch.org/get-started/locally/). For example with cuda 10.2 available, run:\n\n    conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n\nInstall conda requirements:\n\n    conda install -c conda-forge affogato\n\n### conda\n\nIf you install via conda, there are fewer steps since\naffogato and pytorch will be installed for you.\n\nYou can install `napari-affinities` via [conda]:\n\n    `conda install -c conda-forge napari-affinities`\n\n### Download example model:\n\n#### 2D:\n\n[epithelial example model](https://oc.embl.de/index.php/s/zfWMKu7HoQnSJLs)\nPlace the model zip file wherever you want. You can open it in the plugin with the \"load from file\" button.\n\n#### 3D\n\n[lightsheet example model](https://owncloud.gwdg.de/index.php/s/LsShICsOcilqPRs)\nUnpack the tar file into test data (`lightsheet_nuclei_test_data` (an hdf5 file)) and model (`LightsheetNucleusSegmentation.zip` (a bioimageio model)).\nMove the data into sample_data which will enable you to load the \"Lightsheet Sample\" data in napari.\nPlace the model zip file anywhere you want. You can open it in the plugin with the \"load from file\" button.\n\n##### Workarounds to be fixed:\n\n1. you need to update the `rdf.yaml` in the `LightsheetNucleusSegmentation.zip` with the following:\n   - \"shape\" for \"input0\" should be updated with a larger minimum input size and \"output0\" should be updated with a larger halo. If not fixed, there will be significant tiling artifacts.\n   - (Optional) \"output0\" should be renamed to affinities. The plugin supports multiple outputs and relies on names for figuring out which one is which. If unrecognized names are provided we assume the outputs are ordered (affinities, fgbg, lsds) but this is less reliable than explicit names.\n2. This model also generates foreground in the same array as affinities, i.e. a 10 channel output `(fgbg, [-1, 0, 0], [0, -1, 0], [0, 0, -1], [-2, 0, 0], ...)`. Although predictions will work, post processing such as mutex watershed will break unless you manually separate the first channel.\n\n## Use\n\nRequirements for the model:\n\n1. Bioimageio packaged pytorch model\n2. Outputs with names \"affinities\", \"fgbg\"(optional) or \"lsds\"(optional)\n   - if these names are not used, it will be assumed that the outputs are affinities, fgbg, then lsds in that order\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-affinities\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[mit]: http://opensource.org/licenses/MIT\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/pattonw/napari-affinities/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[pypi]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Mutex Watershed Widget",
      "Affinities Widget"
    ],
    "contributions_sample_data": [
      "Epithelial sample",
      "Lightsheet sample"
    ]
  },
  {
    "normalized_name": "napari-validate-random-label-predictions",
    "name": "napari-validate-random-label-predictions",
    "display_name": "napari validate random label predictions",
    "version": "0.0.1",
    "created_at": "2022-11-23",
    "modified_at": "2022-11-23",
    "authors": [
      "Niklas Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-validate-random-label-predictions/",
    "home_github": "https://github.com/gatoniel/napari-validate-random-label-predictions",
    "home_other": null,
    "summary": "Validate separate instances of label predictions manually",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-validate-random-label-predictions\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-validate-random-label-predictions.svg?color=green)](https://github.com/gatoniel/napari-validate-random-label-predictions/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-validate-random-label-predictions.svg?color=green)](https://pypi.org/project/napari-validate-random-label-predictions)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-validate-random-label-predictions.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-validate-random-label-predictions/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-validate-random-label-predictions/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-validate-random-label-predictions/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-validate-random-label-predictions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-validate-random-label-predictions)](https://napari-hub.org/plugins/napari-validate-random-label-predictions)\n\nValidate separate instances of label predictions manually\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-validate-random-label-predictions` via [pip]:\n\n    pip install napari-validate-random-label-predictions\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-validate-random-label-predictions.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-validate-random-label-predictions\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-validate-random-label-predictions/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Validate Labels"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-owncloud",
    "name": "napari-owncloud",
    "display_name": "napari-owncloud",
    "version": "0.1.2",
    "created_at": "2022-11-16",
    "modified_at": "2022-11-19",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-owncloud/",
    "home_github": "https://github.com/haesleinhuepf/napari-owncloud",
    "home_other": null,
    "summary": "Browse folders and images in owncloud / nextcloud servers and open them using just a double-click!",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "pyocclient"
    ],
    "package_metadata_description": "# napari-owncloud\n\n[![License](https://img.shields.io/pypi/l/napari-owncloud.svg?color=green)](https://github.com/haesleinhuepf/napari-owncloud/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-owncloud.svg?color=green)](https://pypi.org/project/napari-owncloud)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-owncloud.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-owncloud/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-owncloud/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-owncloud/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-owncloud)\n\n## Usage\n\nBrowse folders and images in [owncloud](https://owncloud.com/) / [nextcloud](https://nextcloud.com/) servers and open them using just a double-click! \n\nLogin to an owncloud or nextcloud server by clicking the menu `Tools > Utilities > Browse owncloud / nextcloud storage`\n\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/login.png)\n\nYou can then navigate through folders by double-clicking `folder/` items in the list.\nYou can also open images by double-clicking them. Alternatively, use the arrow-up and arrow-down key to navigate the list and hit ENTER to open an image or folder.\n\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/browse.png)\n\nStore images in your cloud storage using the button `Save / upload current layer`. Note: Currently, only single selected layers can be saved.\n\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/upload.png)\n\n[Demo](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/demo.mp4)\n\n![](https://github.com/haesleinhuepf/napari-owncloud/raw/main/docs/demo.gif)\n\n## Installation\n\nYou can install `napari-owncloud` via [pip]:\n\n    pip install napari-owncloud\n\n## Related plugins\n\nThere are other napari plugins that allow you browsing local and online image storage\n* [napari-omero](https://www.napari-hub.org/plugins/napari-omero)\n* [napari-folder-browser](https://www.napari-hub.org/plugins/napari-folder-browser)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-owncloud\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-owncloud/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[image.sc]: https://image.sc\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "OwncloudBrowser"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pssr",
    "name": "napari-pssr",
    "display_name": "napari PSSR",
    "version": "0.1",
    "created_at": "2022-11-18",
    "modified_at": "2022-11-18",
    "authors": [
      "William Patton"
    ],
    "author_emails": [
      "will.hunter.patton@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-pssr/",
    "home_github": "https://github.com/pattonw/napari-pssr",
    "home_other": null,
    "summary": "A plugin for training and applying pssr",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "zarr",
      "magicgui",
      "bioimageio.core",
      "gunpowder",
      "matplotlib",
      "torch",
      "napari"
    ],
    "package_metadata_description": "# napari-pssr\n\n[![License](https://img.shields.io/pypi/l/napari-pssr.svg?color=green)](https://github.com/pattonw/napari-pssr/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pssr.svg?color=green)](https://pypi.org/project/napari-pssr)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pssr.svg?color=green)](https://python.org)\n[![tests](https://github.com/pattonw/napari-pssr/workflows/tests/badge.svg)](https://github.com/pattonw/napari-pssr/actions)\n[![codecov](https://codecov.io/gh/pattonw/napari-pssr/branch/main/graph/badge.svg)](https://codecov.io/gh/pattonw/napari-pssr)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pssr)](https://napari-hub.org/plugins/napari-pssr)\n\nA plugin for training and applying pssr\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-pssr` via [pip]:\n\n    pip install napari-pssr\n\nSome libraries need to be updated to the most recent version to get all features.\nThese will be updated once they are released on pypi\n    \n    pip install git+https://github.com/bioimage-io/core-bioimage-io-python\",\n    pip install git+https://github.com/funkey/gunpowder.git@patch-1.2.3\",\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/pattonw/napari-pssr.git\n\n## Model download\n\nA sample model can be downloaded from `https://github.com/pattonw/model-specs/tree/main/pssr`. This model comes with some restrictive dependencies. To use follow these steps.\n1) install this plugin following the directions provided above\n2) install bioimageio.core via `pip install bioimageio.core` or `conda install -c conda-forge bioimageio.core`\n3) `pip install fastai==1.0.55 tifffile libtiff czifile scikit-image`\n4) `pip uninstall torch torchvision` (may need multiple runs)\n5) `conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch`\n6) `pip install pillow==6.1.0`\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-pssr\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/pattonw/napari-pssr/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PSSR Widget"
    ],
    "contributions_sample_data": [
      "8nm EM",
      "2nm EM",
      "large 8nm EM"
    ]
  },
  {
    "normalized_name": "napari-steinpose",
    "name": "napari-steinpose",
    "display_name": "napari Steinpose",
    "version": "0.1.0",
    "created_at": "2022-11-17",
    "modified_at": "2022-11-17",
    "authors": [
      "Guillaume Witz"
    ],
    "author_emails": [
      "guillaume.witz@unibe.ch"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-steinpose/",
    "home_github": "https://github.com/guiwitz/napari-steinpose",
    "home_other": null,
    "summary": "A plugin to process Imaging Mass Cytometry data with cellpose and steinbock",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "torch (==1.11.0)",
      "cellpose",
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "readimc",
      "steinbock",
      "pandas",
      "aicsimageio",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest-order ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-steinpose\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-steinpose.svg?color=green)](https://github.com/guiwitz/napari-steinpose/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-steinpose.svg?color=green)](https://pypi.org/project/napari-steinpose)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-steinpose.svg?color=green)](https://python.org)\n[![tests](https://github.com/guiwitz/napari-steinpose/workflows/tests/badge.svg)](https://github.com/guiwitz/napari-steinpose/actions)\n[![codecov](https://codecov.io/gh/guiwitz/napari-steinpose/branch/main/graph/badge.svg)](https://codecov.io/gh/guiwitz/napari-steinpose)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-steinpose)](https://napari-hub.org/plugins/napari-steinpose)\n\nThis napari plugin allows to segment and extract information from Imaging Mass Cytometry data by combining the [cellpose](http://www.cellpose.org/) and [steinbock](https://bodenmillergroup.github.io/steinbock/v0.14.2/) tools.\n\n## Installation\n\nIn order to use this plugin, whe highly recommend to create a specific environment and to install the required software in it. You can create a conda environment using:\n\n    conda create -n steinpose python=3.8.5 napari -c conda-forge\n\nThen activate it and install the plugin:\n    \n    conda activate steinpose\n    pip install napari-steinpose\n\n### Potential issue with PyTorch\n\nCellpose and therefore the plugin and napari can crash without warning in some cases with ```torch==1.12.0```. This can be fixed by reverting to an earlier version using:\n    \n    pip install torch==1.11.0\n\n### GPU\n\nIn order to use a GPU:\n\n1. Uninstall the PyTorch version that gets installed by default with Cellpose:\n\n        pip uninstall torch\n\n2. Make sure your have up-to-date drivers for your NVIDIA card installed.\n\n3. Re-install a GPU version of PyTorch via conda using a command that you can find [here](https://pytorch.org/get-started/locally/) (this takes care of the cuda toolkit, cudnn etc. so **no need to install manually anything more than the driver**). The command will look like this:\n\n        conda install pytorch torchvision cudatoolkit=11.3 -c pytorch\n\n### Plugin Updates\n\nTo update the plugin, you only need to activate the existing environment and install the new version:\n\n    conda activate steinpose\n    pip install git+https://github.com/guiwitz/napari-steinpose.git -U\n\n## Usage\n\nHere is a short summary on how to proceed to use the plugin. For more detailed information, please visit [this page](https://guiwitz.github.io/napari-steinpose).\n\n### Load data\n\nUsing the \"Select data folder\" button, select a folder containing your .mcd files. The contents of the folder will appear in the List of images box. When you select one of the files it is loaded in the viewer. Using the ROI spinpox, you can change the roi (or acquisition) to be visualized.\n### Segmentation\n\n1. In the channels tab, choose the combination of channels to use to define images to segment. You can choose what type of projection (mean, min etc.) is used to combine channels. You can either select channels defining both cells and nuclei or just a single channel. **Note that if you want to just segment nuclei, you need to select them as \"cell channel\".**\n\n2. To save the output, select a folder using the \"Select output folder\" button.\n\n3. In the segmentation tab, pick a cellpose model to use. If you use one of the built-in models, you can specify the average diameter of objects to detect.\n\n4. In the Options tab, you can set a few more options:\n   - cellpose options: you can adjust the flow threshold and cell probabilities. If cells are missing try to use higher values of flow threshold (close to 1) and lower values for the cell probabilities (around -6)\n   - segmentation options: you can decide to remove segmentation touching the image border, and you can also decide to expand the segmented objects by a fixed number of pixels. If a segmentation is displayed in the viewer, adjusting this parameter will live-adjust the mask.\n\n5. You can first test the segmentation using the \"Run on current image\" button. Once segmentation is done, the corresponding mask is displayed. You can then run the segmentation over all ROIs of **all .mcd files** present in the folder by using the \"Run on folder\" button.\n\n### Post-processing\n\nIn the Segmentation tab, if you tick the box \"Run steinbock post-processing\", information will directly be extracted from images and masks at the end of segmentation. Processing is done via steinbock and generates files compatible with further downstream processing.\n\nIn the Export tab, you can select what type of information to export: object intensities, geometric properties and object neighbourhood. Note that if you have performed a segmentation without post-processing, you can still run post-processing using the \"Run steinbock postproc\" button.\n\n### Saving settings\n\nTo avoid having to re-type the same settings repeatedly, you can export a give configuration using the \"Export config\" button in the Options tab. This generates a human readable .yml file with:\n- segmentation options\n- channels selected for projections\n\nThe file is saved in the output folder. You can just copy the file in a new empty output folder to use it for an other analysis run. Once you select that folder containing a configuration file, you can import it with the \"Import config\" button. **Note that you need to have an image opened so that channels can be selected properly.**\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-steinpose\" is free and open source software\n\n## Authors\n\nThe author of this plugin is Guillaume Witz, Data Science Lab and Microscopy Imaging Center, University of Bern. This plugin is the result of a collaboration with the Imaging Mass Cytometry and Mass Cytometry Platform, University of Bern.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/guiwitz/napari-steinpose/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mcd"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Steinpose Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cryofibsem-monitor",
    "name": "napari-cryofibsem-monitor",
    "display_name": "napari-cryofibsem-monitor",
    "version": "0.0.3",
    "created_at": "2021-11-05",
    "modified_at": "2022-11-12",
    "authors": [
      "Johannes Elferich"
    ],
    "author_emails": [
      "jojotux123@hotmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-cryofibsem-monitor/",
    "home_github": "https://github.com/jojoelfe/napari-cryofibsem-monitor",
    "home_other": null,
    "summary": "A plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "tifffile",
      "imreg-dft",
      "matplotlib"
    ],
    "package_metadata_description": "# napari-cryofibsem-monitor\n\n[![License](https://img.shields.io/pypi/l/napari-cryofibsem-monitor.svg?color=green)](https://github.com/jojoelfe/napari-cryofibsem-monitor/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cryofibsem-monitor.svg?color=green)](https://pypi.org/project/napari-cryofibsem-monitor)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cryofibsem-monitor.svg?color=green)](https://python.org)\n[![tests](https://github.com/jojoelfe/napari-cryofibsem-monitor/workflows/tests/badge.svg)](https://github.com/jojoelfe/napari-cryofibsem-monitor/actions)\n[![codecov](https://codecov.io/gh/jojoelfe/napari-cryofibsem-monitor/branch/main/graph/badge.svg)](https://codecov.io/gh/jojoelfe/napari-cryofibsem-monitor)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cryofibsem-monitor)](https://napari-hub.org/plugins/napari-cryofibsem-monitor)\n\nA plugin to monitor the creation of lamella using AutoTEM on a TFS Acquilos instrument\n\n\nhttps://user-images.githubusercontent.com/6081039/201448228-fdd8b429-8ff6-4934-ad58-e80fbfcbaef0.mp4\n\n## Changelog\n\n- **v0.0.3** \n    - Update data during milling\n    - Align images to keep lamella in the center\n    - Monitor thickness\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-cryofibsem-monitor` via [pip]:\n\n    pip install napari-cryofibsem-monitor\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jojoelfe/napari-cryofibsem-monitor.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-cryofibsem-monitor\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jojoelfe/napari-cryofibsem-monitor/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Monitor"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-unicell",
    "name": "napari-unicell",
    "display_name": "unicell",
    "version": "0.0.1.post3",
    "created_at": "2022-11-12",
    "modified_at": "2022-11-12",
    "authors": [
      "Jun Ma"
    ],
    "author_emails": [
      "junma.ma@mail.utoronto.ca"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-unicell/",
    "home_github": "https://github.com/JunMa11/napari-unicell",
    "home_other": null,
    "summary": "universal cell segmentation models",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "torch",
      "imagecodecs",
      "scipy",
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "monai",
      "einops",
      "PyQt5",
      "napari",
      "napari-plugin-engine",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-unicell\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-unicell.svg?color=green)](https://github.com/JunMa11/napari-unicell/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-unicell.svg?color=green)](https://pypi.org/project/napari-unicell)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-unicell.svg?color=green)](https://python.org)\n[![tests](https://github.com/JunMa11/napari-unicell/workflows/tests/badge.svg)](https://github.com/JunMa11/napari-unicell/actions)\n[![codecov](https://codecov.io/gh/JunMa11/napari-unicell/branch/main/graph/badge.svg)](https://codecov.io/gh/JunMa11/napari-unicell)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-unicell)](https://napari-hub.org/plugins/napari-unicell)\n\nuniversal cell segmentation models\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-unicell` via [pip]:\n\n    pip install napari-unicell\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-unicell\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/JunMa11/napari-unicell/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Run UniCell"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-power-widgets",
    "name": "napari-power-widgets",
    "display_name": "napari-power-widgets",
    "version": "0.0.1",
    "created_at": "2022-11-11",
    "modified_at": "2022-11-11",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin-sc@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-power-widgets/",
    "home_github": "https://github.com/hanjinliu/napari-power-widgets",
    "home_other": null,
    "summary": "Powerful widgets and type annotations for napari plugin widgets",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "pandas",
      "typing-extensions",
      "magicgui",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-power-widgets\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-power-widgets.svg?color=green)](https://github.com/hanjinliu/napari-power-widgets/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-power-widgets.svg?color=green)](https://pypi.org/project/napari-power-widgets)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-power-widgets.svg?color=green)](https://python.org)\n[![tests](https://github.com/hanjinliu/napari-power-widgets/workflows/tests/badge.svg)](https://github.com/hanjinliu/napari-power-widgets/actions)\n[![codecov](https://codecov.io/gh/hanjinliu/napari-power-widgets/branch/main/graph/badge.svg)](https://codecov.io/gh/hanjinliu/napari-power-widgets)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-power-widgets)](https://napari-hub.org/plugins/napari-power-widgets)\n\nPowerful `magicgui` widgets and type annotations for general-purpose napari plugin development.\n\n`napari-power-widgets` makes the full use of type-to-widget mapping strategy of `magicgui` to provide napari-specific types and value-widgets, which will be very useful to improve UI/UX of your napari plugins with simple codes.\n\nCurrently, `napari-power-widgets` does not provide any reader, writer or widget. It is supposed to be used programmatically.\n\n### Examples\n\nSome types/widgets and the possible usage are picked up here ([&rarr; check all](https://github.com/hanjinliu/napari-power-widgets/blob/main/src/napari_power_widgets/types.py)). If you have any neat ideas, please open an issue.\n\n#### 1. `BoxSelection`\n\nAlias of a four-float tuple for 2D selection. You can set the value by drawing a interaction box in the viewer.\n\n*e. g. : image cropper, rectangular labeling etc.*\n\n```python\n@magicgui\ndef f(box: BoxSelection):\n    print(box)\nviewer.window.add_dock_widget(f)\n```\n\n![](images/BoxSelection.gif)\n\n#### 2. `OneOfRectangles`\n\nAlias of `np.ndarray` for one of rectangles in a `Shapes` layer.\n\n*e. g. : image cropper, rectangular labeling etc.*\n\n```python\n@magicgui\ndef f(rect: OneOfRectangles):\n    print(rect)\nviewer.window.add_dock_widget(f)\n```\n\n![](images/OneOfRectangles.gif)\n\n#### 3. `LineData`\n\nAlias of `np.ndarray` for a line data. You can obtain the data by manually drawing a line in the viewer.\n\n*e. g. : line profiling, kymograph etc.*\n\n```python\n@magicgui\ndef f(line: LineData):\n    print(line)\nviewer.window.add_dock_widget(f)\n```\n\n![](images/LineData.gif)\n\n#### 4. `OneOfLabels`\n\nAlias of boolean `np.ndarray` for a labeled region. You can choose ones by directly clicking the viewer.\n\n*e. g. : image masking, feature measurement etc.*\n\n```python\n@magicgui\ndef f(label: OneOfLabels):\n    pass\nviewer.window.add_dock_widget(f)\n```\n\n![](images/OneOfLabels.gif)\n\n\n#### 5. `ZRange`\n\nAlias of a tuple of float that represents the limit of the third dimension. You can select the values by moving the dimension slider.\n\n*e. g. : movie trimming, partial image projection etc.*\n\n```python\n@magicgui\ndef f(zrange: ZRange):\n    print(zrange)\nviewer.window.add_dock_widget(f)\n```\n\n![](images/ZRange.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-power-widgets` via [pip]:\n\n    pip install napari-power-widgets\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hanjinliu/napari-power-widgets.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-power-widgets\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hanjinliu/napari-power-widgets/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-geojson",
    "name": "napari-geojson",
    "display_name": "napari-geojson",
    "version": "0.1.3",
    "created_at": "2021-12-27",
    "modified_at": "2022-11-10",
    "authors": [
      "Tim Morello"
    ],
    "author_emails": [
      "tdmorello@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-geojson/",
    "home_github": "https://github.com/tdmorello/napari-geojson",
    "home_other": null,
    "summary": "Read and write geojson files in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "geojson",
      "numpy",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-black ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "flake8-isort ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "napari[all] ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-geojson\n\n[![License](https://img.shields.io/pypi/l/napari-geojson.svg?color=green)](https://github.com/tdmorello/napari-geojson/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-geojson.svg?color=green)](https://pypi.org/project/napari-geojson)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-geojson.svg?color=green)](https://python.org)\n[![tests](https://github.com/tdmorello/napari-geojson/workflows/tests/badge.svg)](https://github.com/tdmorello/napari-geojson/actions)\n[![codecov](https://codecov.io/gh/tdmorello/napari-geojson/branch/main/graph/badge.svg)](https://codecov.io/gh/tdmorello/napari-geojson)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-geojson)](https://napari-hub.org/plugins/napari-geojson)\n\nRead and write geojson files in napari.\n\n![](https://github.com/tdmorello/napari-geojson/raw/main/resources/output.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-geojson` via [pip]:\n\n    pip install napari-geojson\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/tdmorello/napari-geojson.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-geojson\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/tdmorello/napari-geojson/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.geojson"
    ],
    "contributions_writers_filename_extensions": [
      ".geojson"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-denoiseg",
    "name": "napari-denoiseg",
    "display_name": "DenoiSeg",
    "version": "0.0.1rc2",
    "created_at": "2022-10-31",
    "modified_at": "2022-11-01",
    "authors": [
      "Tom Burke",
      "Joran Deschamps"
    ],
    "author_emails": [
      "joran.deschamps@fht.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-denoiseg/",
    "home_github": "https://github.com/juglab/napari_denoiseg",
    "home_other": null,
    "summary": "A napari plugin performing joint denoising and segmentation of microscopy images using DenoiSeg.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "pyqtgraph",
      "denoiseg (>=0.3.0)",
      "bioimageio.core",
      "magicgui",
      "qtpy",
      "napari-time-slicer (>=0.4.9)",
      "napari (<=0.4.15)",
      "vispy (<=0.9.6)",
      "imageio (!=2.11.0,!=2.22.1,>=2.5.0)",
      "tensorflow ; platform_system != \"Darwin\" or platform_machine != \"arm64\"",
      "tensorflow-macos ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "tensorflow-metal ; platform_system == \"Darwin\" and platform_machine == \"arm64\"",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-denoiseg\n\n[![License](https://img.shields.io/pypi/l/napari-denoiseg.svg?color=green)](https://github.com/juglab/napari-denoiseg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-denoiseg.svg?color=green)](https://pypi.org/project/napari-denoiseg)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-denoiseg.svg?color=green)](https://python.org)\n[![tests](https://github.com/juglab/napari-denoiseg/workflows/build/badge.svg)](https://github.com/juglab/napari-denoiseg/actions)\n[![codecov](https://codecov.io/gh/juglab/napari-denoiseg/branch/main/graph/badge.svg)](https://codecov.io/gh/juglab/napari-denoiseg)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-denoiseg)](https://napari-hub.org/plugins/napari-denoiseg)\n\nA napari plugin performing joint denoising and segmentation of microscopy images using [DenoiSeg](https://github.com/juglab/DenoiSeg).\n\n<img src=\"https://raw.githubusercontent.com/juglab/napari-denoiseg/master/docs/images/example.png\" width=\"800\" />\n----------------------------------\n\n## Installation\n\nCheck out the [documentation](https://juglab.github.io/napari-denoiseg/installation.html) for more detailed installation \ninstructions. \n\n\n## Quick demo\n\nYou can try out a demo by loading the `DenoiSeg Demo prediction` plugin and directly clicking on `Predict`.\n\n\n<img src=\"https://raw.githubusercontent.com/juglab/napari-denoiseg/master/docs/images/prediction.gif\" width=\"800\" />\n\n\n## Documentation\n\nDocumentation is available on the [project website](https://juglab.github.io/napari-denoiseg/).\n\n\n## Contributing and feedback\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request. You can also \nhelp us improve by [filing an issue] along with a detailed description or contact us\nthrough the [image.sc](https://forum.image.sc/) forum (tag @jdeschamps).\n\n\n## Cite us\n\n\nTim-Oliver Buchholz, Mangal Prakash, Alexander Krull and Florian Jug, \"[DenoiSeg: Joint Denoising and Segmentation](https://arxiv.org/abs/2005.02987)\" _arxiv_ (2020)\n\n\n## Acknowledgements\n\nThis plugin was developed thanks to the support of the Silicon Valley Community Foundation (SCVF) and the \nChan-Zuckerberg Initiative (CZI) with the napari Plugin Accelerator grant _2021-239867_.\n\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-denoiseg\" is a free and open source software.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[filing an issue]: https://github.com/juglab/napari-denoiseg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "DenoiSeg train",
      "DenoiSeg predict",
      "DenoiSeg threshold optimizer",
      "DenoiSeg Demo prediction"
    ],
    "contributions_sample_data": [
      "Download 2D data (n0 noise)",
      "Download 2D data (n10 noise)",
      "Download 2D data (n20 noise)",
      "Download 3D data (n10 noise)",
      "Download 3D data (n20 noise)"
    ]
  },
  {
    "normalized_name": "napari-mat-file-reader",
    "name": "napari-mat-file-reader",
    "display_name": "napari mat file reader",
    "version": "0.0.2",
    "created_at": "2022-11-01",
    "modified_at": "2022-11-01",
    "authors": [
      "Ruben Lopez"
    ],
    "author_emails": [
      "rjlopez2@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-mat-file-reader/",
    "home_github": "https://github.com/rjlopez2/napari-mat-file-reader",
    "home_other": null,
    "summary": "This is a simple wraper to read .mat files from Matlab",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "mat73",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-mat-file-reader\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-mat-file-reader.svg?color=green)](https://github.com/rjlopez2/napari-mat-file-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mat-file-reader.svg?color=green)](https://pypi.org/project/napari-mat-file-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mat-file-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/rjlopez2/napari-mat-file-reader/workflows/tests/badge.svg)](https://github.com/rjlopez2/napari-mat-file-reader/actions)\n[![codecov](https://codecov.io/gh/rjlopez2/napari-mat-file-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/rjlopez2/napari-mat-file-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mat-file-reader)](https://napari-hub.org/plugins/napari-mat-file-reader)\n\nThis is a simple wraper to read .mat files from Matlab\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-mat-file-reader` via [pip]:\n\n    pip install napari-mat-file-reader\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/rjlopez2/napari-mat-file-reader.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mat-file-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/rjlopez2/napari-mat-file-reader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mat"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "napari mat file reader"
    ]
  },
  {
    "normalized_name": "napari-umap",
    "name": "napari-umap",
    "display_name": "UMAP",
    "version": "0.0.1",
    "created_at": "2022-11-01",
    "modified_at": "2022-11-01",
    "authors": [
      "Jordao Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-umap/",
    "home_github": "https://github.com/royerlab/napari-umap",
    "home_other": null,
    "summary": "A simple plugin to use with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-umap\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-umap.svg?color=green)](https://github.com/royerlab/napari-umap/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-umap.svg?color=green)](https://pypi.org/project/napari-umap)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-umap.svg?color=green)](https://python.org)\n[![tests](https://github.com/royerlab/napari-umap/workflows/tests/badge.svg)](https://github.com/royerlab/napari-umap/actions)\n[![codecov](https://codecov.io/gh/royerlab/napari-umap/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-umap)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-umap)](https://napari-hub.org/plugins/napari-umap)\n\nA simple plugin to use with napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-umap` via [pip]:\n\n    pip install napari-umap\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/royerlab/napari-umap.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-umap\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/royerlab/napari-umap/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Example QWidget",
      "Example Magic Widget",
      "Example Function Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-splinedist",
    "name": "napari-splinedist",
    "display_name": "SplineDist",
    "version": "0.3.1",
    "created_at": "2022-10-18",
    "modified_at": "2022-10-31",
    "authors": [
      "Dr. Thorsten Beier"
    ],
    "author_emails": [
      "derthorstebeier@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-splinedist/",
    "home_github": "https://github.com/DerThorsten/napari-splinedist",
    "home_other": null,
    "summary": "A napari SplineDist plugin",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "pydantic",
      "numpy",
      "magicgui",
      "qtpy",
      "stardist (>=0.8.3)",
      "splinedist (>=0.1.2)",
      "napari-splineit (>=0.3.0)",
      "requests",
      "tensorflow",
      "opencv-python-headless",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-splinedist\n\n[![License MIT](https://img.shields.io/pypi/l/napari-splinedist.svg?color=green)](https://github.com/DerThorsten/napari-splinedist/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-splinedist.svg?color=green)](https://pypi.org/project/napari-splinedist)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-splinedist.svg?color=green)](https://python.org)\n[![tests](https://github.com/DerThorsten/napari-splinedist/workflows/tests/badge.svg)](https://github.com/DerThorsten/napari-splinedist/actions)\n[![codecov](https://codecov.io/gh/DerThorsten/napari-splinedist/branch/main/graph/badge.svg)](https://codecov.io/gh/DerThorsten/napari-splinedist)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splinedist)](https://napari-hub.org/plugins/napari-splinedist)\n\nA napari SplineDist plugin\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-splinedist` via [pip]:\n\n    pip install napari-splinedist\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/DerThorsten/napari-splinedist.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-splinedist\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/DerThorsten/napari-splinedist/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Example QWidget"
    ],
    "contributions_sample_data": [
      "bbbc038",
      "conic"
    ]
  },
  {
    "normalized_name": "napari-turing",
    "name": "napari-turing",
    "display_name": "Turing Patterns",
    "version": "0.3.2",
    "created_at": "2022-08-08",
    "modified_at": "2022-10-27",
    "authors": [
      "L√©o Guignard"
    ],
    "author_emails": [
      "leo.guignard@univ-amu.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-turing/",
    "home_github": "https://github.com/leoguignard/napari-turing",
    "home_other": null,
    "summary": "A plugin to run simmple simulations of Turing patterns",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "scikit-image",
      "magicgui",
      "qtpy",
      "napari",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-turing\n\n[![License MIT](https://img.shields.io/pypi/l/napari-turing.svg?color=green)](https://github.com/leoguignard/napari-turing/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-turing.svg?color=green)](https://pypi.org/project/napari-turing)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-turing.svg?color=green)](https://python.org)\n[![tests](https://github.com/leoguignard/napari-turing/workflows/tests/badge.svg)](https://github.com/leoguignard/napari-turing/actions)\n[![codecov](https://codecov.io/gh/leoguignard/napari-turing/branch/main/graph/badge.svg)](https://codecov.io/gh/leoguignard/napari-turing)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-turing)](https://napari-hub.org/plugins/napari-turing)\n\nA plugin to run simple simulations of Turing patterns\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n![example 1](img/turing_patterns.gif)\n![example 2](img/turing_patterns2.gif)\n\n## Installation\n\nYou can install `napari-turing` via [pip] after downloading the content of\n\n    pip install napari-turing\n\n\nTo install latest version :\n\n    pip install git+https://github.com/leoguignard/napari-turing.git\n\n## Troubleshooting\n\nIf the installation does not work just with the previous command, it might be useful to first install [napari] for example that way:\n\n    conda install napari\n\n## Creating a new model\n\nTo create your own model, you can use the template for models [here](src/napari_turing/Models/ModelTemplate.py).\n\nNote that a bit of knowledge in Python is probably necessary and it might not be completely trivial at first but you'll manage :)\n\nFirst you need to name the concentrations that you will use with the following [line](src/napari_turing/Models/ModelTemplate.py#L40):\n```python\n    _concentration_names = [\"A\", \"I\"]\n```\n\nThen, you need to declare its variables. For example you can create a parameter named `mu_a` the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L53-L60)):\n```python\n    mu_a = ModelParameter(\n        name=\"mu_a\",  # Name of the parameter\n        description=\"Activator diffusion coefficient (10^-4)\",  # Description of the parameter for napari\n        value=2.8,  # Initial and default value\n        min=1,  # Minimum value the parameter can take\n        max=5,  # Maximum value the parameter can take\n        exponent=1e-4,  # All values given to this instance of the class will but multiplied by this value\n    )\n```\n\nThen you need to list the parameters that are necessary to run the model (usually all the paramaters declared previously) and the parameters that you will allow the user to tune (for example, sometimes some of the parameters are co-dependent and there is no point in being able to tune both of them). That should be done the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L86-89)):\n```python\n    # These are the parameters that are necessary to run the equations.\n    _necessary_parameters = [tau, k, mu_a, mu_i]\n    # These are the parameters that can be modified via napari\n    _tunable_parameters = _necessary_parameters\n```\n\nIf you want, you can specify what the method will return as a string, it will be displayed in the napari viewer ([here in the code](src/napari_turing/Models/ModelTemplate.py#L90-L98)):\n```python\n    # This function allows to display some information about the model\n    # in napari\n    def __str__(self) -> str:\n        return (\n            \"Equations (FitzHugh-Nagumo model):\\n\"\n            \"  Concentration of Activator (a) and Inhibitor (i)\\n\"\n            \"    - da/dt = mu_a * diffusion(a) + a - a^3 - i + k\\n\"\n            \"    - tau * di/dt = mu_i * diffusion(i) + a - i\"\n        )\n```\n\nNow that the basics are declared, you will need to declare how to initialize your concentrations the following way ([here in the code](src/napari_turing/Models/ModelTemplate.py#L100-L116)):\n```python\n    # The following allows to reset the values of the concentrations.\n    # The function takes the name of the concentration to initialize.\n    # If no name is given or if it is None all the concentrations are\n    # reinitialized.\n    #\n    # The reason why this function is useful is that some models \n    # require specific initialisations for them to work correctly\n    # In the following example the concentrations are reintinalized\n    # to a random value between -1 and 1.\n    # This is the default behavior, so if you don't need to change\n    # it you don't have to implement the function.\n    def init_concentrations(self, C: Optional[str] = None) -> None:\n        if C is None:\n            for ci in self.concentration_names():\n                self[ci] = np.random.random((self.size, self.size)) * 2 - 1\n        else:\n            self[C] = np.random.random((self.size, self.size)) * 2 - 1\n```\nIn the previous example, the all concentrations are initialized the same way. If you need to have different initializations, you can do it the following way for example ([from the GrayScott model](src/napari_turing/Models/GrayScott.py#L68-L76)):\n```python\n    def init_concentrations(self, C: Optional[str] = None) -> None:\n        if C == \"X\" or C is None:\n            self[\"X\"] = np.ones((self.size, self.size))\n        if C == \"Y\" or C is None:\n            Y = np.zeros((self.size, self.size))\n            nb_pos = 20\n            pos = (np.random.random((2, nb_pos)) * self.size).astype(int)\n            Y[pos[0], pos[1]] = 1\n            self[\"Y\"] = Y\n```\nIn this model, there are two concentrations, `X` and `Y` which are initialized differenty. Note that they can be accessed using `self[\"X\"]` or `self.X`.\n\nFinally, you of course have to define the reaction equations and the diffusion equations. The way it is defined is with two functions, one for the reaction and one for the diffusion, that take as an input the name of the concentration to apply the function to and returns the new values. Then for each of your concentrations, their new values will be computed as followed:\n```python\nnew_concentration = current_concentration + dt*(reaction + diffusion)\n```\n\nHere is an example for the reaction function ([here in the code](src/napari_turing/Models/ModelTemplate.py#L127-L136)):\n```python \n    # This function defines the equations of the reactions.\n    # It takes as an input which concentration to compute\n    # (in this example we have to define how to compute A and I)\n    def _reaction(self, c: str) -> np.ndarray:\n        if c == \"A\":\n            # Below is the reaction part of the equation (1)\n            return self.A - self.A**3 - self.I + self.k \n        elif c == \"I\":\n            # Below is the reaction part of the equation (2)\n            return (self.A - self.I) / self.tau\n```\nOf course, if you have more concentrations, you will need to define more equations.\n\nHere is an example for the reaction function ([here in the code](src/napari_turing/Models/ModelTemplate.py#L138-L166)):\n```python\n    # This function defines the equations of the diffusion.\n    # It takes as an input which concentration to compute\n    # (in this example we have to define how to compute A and I)\n    # Here we compute the diffusion as follow:\n    # A cell gives an equal fraction mu of its concentration to its neighbors\n    # A cell recieves an equal fraction mu of concentration from its neighbors\n    # Neighbors = (left, right, above, below)\n    # In the case of oriented diffusion the amount recieved and given to the neighbors\n    # is imbalanced according to the position of the neighbor.\n    def _diffusion(self, c: str) -> np.ndarray:\n        if c == \"A\":\n            arr = self.A # Define the array of concentrations to diffuse for the reageant A\n            mu = self.mu_a # Define the diffusion coefficient for the reageant A\n        elif c == \"I\":\n            arr = self.I # Define the array of concentrations to diffuse for the reageant I\n            mu = self.mu_i # Define the diffusion coefficient for the reageant I\n        \n        # Computes what is recieved from neighboring cells\n        from_cell = convolve(arr, self.kernel.value, mode=\"constant\", cval=0)\n        # Computes what is given to neighboring cells\n        to_cell = self.nb_neighbs * arr\n\n        # Computes the diffusion\n        out = mu * (from_cell - to_cell) / (self.dx * self.dy)\n\n        # In our case, the equation (2), for I specify that it has to be divided by tau\n        if c == \"I\":\n            out /= self.tau\n        return out\n```\nThe diffusion function is usually a standard one so it might not be necessary to overly change it.\n\nYou can find other model examples:\n- [Brusselator](src/napari_turing/Models/Brusselator.py)\n- [GrayScott](src/napari_turing/Models/GrayScott.py)\n- [GameOfLife](src/napari_turing/Models/GameOfLife.py)\n\nOnce all that is done, let say you've saved your new model in the folder [Models](src/napari_turing/Models) under the name `NewModel.py` and the model class created is name `NewModel`. Then you need to declare you model in the [`_model_list.py`](src/napari_turing/Models/_model_list.py) file. To do so you need to add the following lines in the file:\n```python\nfrom enum import Enum\nfrom .FitzHughNagumo import FitzHughNagumo\nfrom .Brusselator import Brusselator\nfrom .GrayScott import GrayScott\nfrom .GameOfLife import GameOfLife\nfrom .NewModel import NewModel ## THAT LINE HERE\n\nclass AvailableModels(Enum):\n    FitzHughNagumo = FitzHughNagumo\n    Brusselator = Brusselator\n    GrayScott = GrayScott\n    GameOfLife = GameOfLife\n    NewModel = NewModel ## AND THAT OTHER LINE HERE\n```\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-turing\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/leoguignard/napari-turing/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Turing Patterns"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-yolov5",
    "name": "napari-yolov5",
    "display_name": "napari-yolov5",
    "version": "0.2.14",
    "created_at": "2021-12-29",
    "modified_at": "2022-10-26",
    "authors": [
      "Richard De Mets"
    ],
    "author_emails": [
      "demets.richard@gmail.com"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-yolov5/",
    "home_github": "https://github.com/rdemets/napari-yolov5",
    "home_other": null,
    "summary": "Plugin adapted from Ultralytics to bring YOLOv5 into Napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "connected-components-3d>=3.6.0",
      "flask>=2.2.2",
      "imageio-ffmpeg>=0.4.7",
      "matplotlib>=3.2.2",
      "napari-plugin-engine>=0.1.4",
      "napari>=0.4.15",
      "numpy>=1.18.5",
      "opencv-python>=4.1.2",
      "Pillow>=7.1.2",
      "PyYAML>=5.3.1",
      "qtpy>=2.2.1",
      "requests>=2.23.0",
      "scikit-image>=0.19.3",
      "scipy>=1.4.1",
      "tensorboard>=1.15.0",
      "tensorflow>=2.10.0",
      "torch>=1.9.0",
      "torchvision>=0.8.1",
      "tqdm>=4.41.0",
      "seaborn>=0.11.2",
      "wandb>=0.13.4"
    ],
    "package_metadata_description": "# napari-yolov5\n\n[![License](https://img.shields.io/pypi/l/napari-yolov5.svg?color=green)](https://github.com/rdemets/napari-yolov5/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-yolov5.svg?color=green)](https://pypi.org/project/napari-yolov5)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-yolov5.svg?color=green)](https://python.org)\n[![tests](https://github.com/rdemets/napari-yolov5/workflows/tests/badge.svg)](https://github.com/rdemets/napari-yolov5/actions)\n[![codecov](https://codecov.io/gh/rdemets/napari-yolov5/branch/main/graph/badge.svg)](https://codecov.io/gh/rdemets/napari-yolov5)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-yolov5)](https://napari-hub.org/plugins/napari-yolov5)\n\nPlugin adapted from Ultralytics to bring YOLOv5 into Napari. \n\nTraining and detection can be done using the GUI. Training dataset must be prepared prior to using this plugin. Further development will allow users to use Napari to prepare the dataset. Follow instructions stated on [Ultralytics Github](https://github.com/ultralytics/yolov5) to prepare the dataset.\n\nThe plugin includes 3 pre-trained networks that are able to identify mitosis stages or apoptosis on soSPIM images. More details can be found on the [pre-print](https://www.biorxiv.org/content/10.1101/2021.03.26.437121v1.full).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nFirst install conda and create an environment for the plugin\n```\nconda create --prefix env-napari-yolov5 python=3.9\nconda activate env-napari-yolov5\n```\nYou can install `napari-yolov5` and `napari` via [pip]:\n\n    pip install napari-yolov5\n    pip install napari[all]\n\nFor GPU support :\n```\npip uninstall torch\npip install torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n## Usage\n\nFirst select if you would like to train a new network or detect objects.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/1.jpg?raw=true)\n\n\n***For `Training` :***\n\nData preparation should be done following [Ultralytics'](https://github.com/ultralytics/yolov5) instructions.\n\nSelect the size of the network, the number of epochs, the number of images per batch to load on the GPU, the size of the images (must be a stride of 32), and the name of the network.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/2.jpg?raw=true)\n\nAn example of the YAML config file is provided in `src/napari_yolov5/resources` folder.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/3.jpg?raw=true)\n\n\nProgress can be seen on the Terminal or on the right-hand side of the viewer.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/4.jpg?raw=true)\n\n\n***For `Detection` :***\n\nIt is possible to perform the detection on a single layer chosen in the list, all the layers opened, or by giving a folder path. For folder detection, all the images will be loaded as a single stack.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/5.jpg?raw=true)\n\nNucleus size of the prediction layer has te be filled to resize the image to the training dataset. Nucleus size of the training dataset will be asked in case of a custom network.\n\nConfidence threshold defines the minimum value for a detected object to be considered positive. \niou nms threshold (intersection-over-union non-max-suppression) defines the overlapping area of two boxes as a single object. Only the box with the maximum confidence is kept.\nProgress can be seen on the Terminal.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/6.jpg?raw=true)\n\nFew options allow for modification on how the boxes are being displayed (default : box + class + confidence score ; box + class ; box only) and if the box coordinates and the image overlay will be exported.\nPost-processing option will perform a simple 3D assignment based on 3D connected component analysis. A median filter (1x1x3 XYZ) is applied prior to the assignment. \nThe centroid of each object is then saved into a new point layer as a 3D point with a random color for each class. \n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/7.jpg?raw=true)\n\nThe localisation of each centroid is saved and the path is shown in the Terminal at the end of the detection. It is also possible now to define the export folder.\n\n![alt text](https://github.com/rdemets/napari-yolov5/blob/main/src/napari_yolov5/resources/Readme/8.jpg?raw=true)\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-yolov5\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "widget_wrapper"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-cookiecut",
    "name": "napari-cookiecut",
    "display_name": "Cookiecut",
    "version": "0.1.1",
    "created_at": "2022-10-21",
    "modified_at": "2022-10-24",
    "authors": [
      "Sean Martin"
    ],
    "author_emails": [
      "martins7@tcd.ie"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-cookiecut/",
    "home_github": "https://github.com/seankmartin/napari-cookiecut",
    "home_other": null,
    "summary": "Fixed cut",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-cookiecut\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-cookiecut.svg?color=green)](https://github.com/seankmartin/napari-cookiecut/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-cookiecut.svg?color=green)](https://pypi.org/project/napari-cookiecut)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-cookiecut.svg?color=green)](https://python.org)\n[![tests](https://github.com/seankmartin/napari-cookiecut/workflows/tests/badge.svg)](https://github.com/seankmartin/napari-cookiecut/actions)\n[![codecov](https://codecov.io/gh/seankmartin/napari-cookiecut/branch/main/graph/badge.svg)](https://codecov.io/gh/seankmartin/napari-cookiecut)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-cookiecut)](https://napari-hub.org/plugins/napari-cookiecut)\n\nA fixed version of a cookiecut napari plugin template that has been set up with all the basic functionality following the README for reference.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-cookiecut` via [pip]:\n\n    pip install napari-cookiecut\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/seankmartin/napari-cookiecut.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-cookiecut\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/seankmartin/napari-cookiecut/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Example QWidget",
      "Example Magic Widget",
      "Example Function Widget"
    ],
    "contributions_sample_data": [
      "Cookiecut"
    ]
  },
  {
    "normalized_name": "napari-splineit",
    "name": "napari-splineit",
    "display_name": "Napari SplineIt2",
    "version": "0.3.0",
    "created_at": "2022-07-05",
    "modified_at": "2022-10-24",
    "authors": [
      "Thorsten Beier"
    ],
    "author_emails": [
      "derthorstenbeier@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-splineit/",
    "home_github": "https://github.com/uhlmanngroup/napari-splineit",
    "home_other": null,
    "summary": "A napari plugin for spline manipulation",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "scikit-image",
      "scipy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-splineit\n\n[![License](https://img.shields.io/pypi/l/napari-splineit.svg?color=green)](https://github.com/uhlmanngroup/napari-splineit/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-splineit.svg?color=green)](https://pypi.org/project/napari-splineit)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-splineit.svg?color=green)](https://python.org)\n[![tests](https://github.com/uhlmanngroup/napari-splineit/workflows/tests/badge.svg)](https://github.com/uhlmanngroup/napari-splineit/actions)\n[![codecov](https://codecov.io/gh/uhlmanngroup/napari-splineit/branch/main/graph/badge.svg)](https://codecov.io/gh/uhlmanngroup/napari-splineit)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-splineit)](https://napari-hub.org/plugins/napari-splineit)\n\nA napari plugin for the interactive manipulation of spline-interpolation based geometrical models\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-splineit` via [pip]:\n\n    pip install napari-splineit\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/uhlmanngroup/napari-splineit.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-splineit\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/uhlmanngroup/napari-splineit/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.splineit"
    ],
    "contributions_writers_filename_extensions": [
      ".splineit"
    ],
    "contributions_widgets": [
      "Splineit-QWidget"
    ],
    "contributions_sample_data": [
      "Coins",
      "small toy data",
      "medium toy data",
      "large toy data",
      "extra large toy data",
      "extra large toy data"
    ]
  },
  {
    "normalized_name": "morphometrics-engine",
    "name": "morphometrics-engine",
    "display_name": "morphometrics engine",
    "version": "0.0.1",
    "created_at": "2022-10-23",
    "modified_at": "2022-10-23",
    "authors": [
      "Kevin Yamauchi"
    ],
    "author_emails": [
      "kevin.yamauchi@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/morphometrics-engine/",
    "home_github": "https://github.com/morphometrics/morphometrics-engine",
    "home_other": null,
    "summary": "A morphometrics measurement engine.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "napari-skimage-regionprops",
      "numpy",
      "magicgui",
      "pandas",
      "qtpy",
      "superqt",
      "tqdm",
      "toolz",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# morphometrics-engine\n\n[![License BSD-3](https://img.shields.io/pypi/l/morphometrics-engine.svg?color=green)](https://github.com/morphometrics/morphometrics-engine/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/morphometrics-engine.svg?color=green)](https://pypi.org/project/morphometrics-engine)\n[![Python Version](https://img.shields.io/pypi/pyversions/morphometrics-engine.svg?color=green)](https://python.org)\n[![tests](https://github.com/morphometrics/morphometrics-engine/workflows/tests/badge.svg)](https://github.com/morphometrics/morphometrics-engine/actions)\n[![codecov](https://codecov.io/gh/morphometrics/morphometrics-engine/branch/main/graph/badge.svg)](https://codecov.io/gh/morphometrics/morphometrics-engine)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/morphometrics-engine)](https://napari-hub.org/plugins/morphometrics-engine)\n\nA morphometrics measurement engine.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `morphometrics-engine` via [pip]:\n\n    pip install morphometrics-engine\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/morphometrics/morphometrics-engine.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"morphometrics-engine\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/morphometrics/morphometrics-engine/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Measure region properties"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-input-visualizer",
    "name": "napari-input-visualizer",
    "display_name": "Input Visualizer",
    "version": "0.0.1",
    "created_at": "2022-10-10",
    "modified_at": "2022-10-10",
    "authors": [
      "David Bauer"
    ],
    "author_emails": [
      "dbauer@brc.hu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-input-visualizer/",
    "home_github": "https://github.com/bauerdavid/napari-input-visualizer",
    "home_other": null,
    "summary": "Visualize keyboard and mouse button presses",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "napari",
      "imageio-ffmpeg",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-input-visualizer\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-input-visualizer.svg?color=green)](https://github.com/bauerdavid/napari-input-visualizer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-input-visualizer.svg?color=green)](https://pypi.org/project/napari-input-visualizer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-input-visualizer.svg?color=green)](https://python.org)\n[![tests](https://github.com/bauerdavid/napari-input-visualizer/workflows/tests/badge.svg)](https://github.com/bauerdavid/napari-input-visualizer/actions)\n[![codecov](https://codecov.io/gh/bauerdavid/napari-input-visualizer/branch/main/graph/badge.svg)](https://codecov.io/gh/bauerdavid/napari-input-visualizer)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-input-visualizer)](https://napari-hub.org/plugins/napari-input-visualizer)\n\nVisualize keyboard and mouse button presses\n\nA simple tool to visualize input events like keyboard presses or mouse clicking and scrolling. Use it to create tutorial videos or to demonstrate a bug you encountered!\n\n## Demo:\n\n\nhttps://user-images.githubusercontent.com/36735863/194586424-1e6288d3-2c2f-412c-a1cb-91d139f787bd.mp4\n\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-input-visualizer` via [pip]:\n\n    pip install napari-input-visualizer\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/bauerdavid/napari-input-visualizer.git\n\n\n## Contributing\n\nContributions are very welcome.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-input-visualizer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bauerdavid/napari-input-visualizer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Input Visualizer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-filament-annotator",
    "name": "napari-filament-annotator",
    "display_name": "napari 3D filament annotator",
    "version": "0.1.2",
    "created_at": "2022-10-04",
    "modified_at": "2022-10-06",
    "authors": [
      "Anna Medyukhina"
    ],
    "author_emails": [
      "anna.medyukhina@gmail.com"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-filament-annotator/",
    "home_github": "https://github.com/amedyukhina/napari-filament-annotator",
    "home_other": null,
    "summary": "Annotation of filaments / curvilinear structures in 3D",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "Geometry3D",
      "networkx",
      "numpy",
      "magicgui",
      "pandas",
      "qtpy",
      "scipy",
      "sklearn",
      "imageio (!=2.22.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# 3D Filament Annotator\n\n[![DOI](https://zenodo.org/badge/513980347.svg)](https://zenodo.org/badge/latestdoi/513980347)\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-filament-annotator.svg?color=green)](https://github.com/amedyukhina/napari-filament-annotator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-filament-annotator.svg?color=green)](https://pypi.org/project/napari-filament-annotator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-filament-annotator.svg?color=green)](https://python.org)\n[![tests](https://github.com/amedyukhina/napari-filament-annotator/workflows/tests/badge.svg)](https://github.com/amedyukhina/napari-filament-annotator/actions)\n[![codecov](https://codecov.io/gh/amedyukhina/napari-filament-annotator/branch/main/graph/badge.svg)](https://codecov.io/gh/amedyukhina/napari-filament-annotator)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-filament-annotator)](https://napari-hub.org/plugins/napari-filament-annotator)\n\n3D Filament Annotator is a tool for annotating filaments and other curvilinear structures in 3D. \nThe 3D annotation is done by annotating the filament in two different projections, \ncalculating intersection, and refining the filament position with active contours.\n\n![demo](https://raw.githubusercontent.com/amedyukhina/napari-filament-annotator/main/docs/demo_09.gif)\n\n\n## Installation\n\n### Install napari\n\n    pip install napari[all]\n\n### Install the filament annotator\n\n<!--\nYou can install `napari-filament-annotator` via [pip]:\n\n    pip install napari-filament-annotator\n\n\nTo install latest development version :\n-->\n    pip install git+https://github.com/amedyukhina/napari-filament-annotator.git\n\n## Usage\n\nFor detailed usage instructions, please refer to the [usage tutorial](docs/tutorial.md).\n\n## Contributing\n\nContributions are very welcome both with regard to plugin functionality, and\ntips on using it and setting parameters. \n\nTests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-filament-annotator\" is free and open source software\n\n## Dependencies\n\n- [napari](https://github.com/napari/napari)\n- [scikit-image](https://scikit-image.org/)\n- [scikit-learn](https://github.com/scikit-learn/scikit-learn)\n- [NetworkX](https://networkx.org/documentation/stable/index.html)\n- [Geometry3D](https://github.com/GouMinghao/Geometry3D)\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/amedyukhina/napari-filament-annotator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "3D Filament Annotator"
    ],
    "contributions_sample_data": [
      "Sample Image"
    ]
  },
  {
    "normalized_name": "napari-plot-profile",
    "name": "napari-plot-profile",
    "display_name": "napari-plot-profile",
    "version": "0.2.2",
    "created_at": "2021-08-20",
    "modified_at": "2022-10-05",
    "authors": [
      "Robert Haase",
      "Marcelo Leomil Zoccoler"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-plot-profile/",
    "home_github": "https://github.com/haesleinhuepf/napari-plot-profile",
    "home_other": null,
    "summary": "Plot intensity along a line and create topographical views in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pyqtgraph",
      "napari",
      "napari-tools-menu",
      "napari-skimage-regionprops (>=0.2.4)",
      "imageio (!=2.22.1)"
    ],
    "package_metadata_description": "# napari-plot-profile (npp)\n\n[![License](https://img.shields.io/pypi/l/napari-plot-profile.svg?color=green)](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-plot-profile.svg?color=green)](https://pypi.org/project/napari-plot-profile)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plot-profile.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-plot-profile/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-plot-profile/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plot-profile)\n[![Development Status](https://img.shields.io/pypi/status/napari-plot-profile.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-plot-profile)](https://napari-hub.org/plugins/napari-plot-profile)\n\n## Plot a Line Profile\n\nPlot intensities along a line in [napari].\n\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/napari-plot-profile-screencast.gif)\n\n* Open some images in [napari].\n  \n* Add a shapes layer.\n\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/add_shapes_layer_screenshot.png)\n  \n* Activate the line drawing tool or the path tool and draw a line.\n\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/draw_line_tool_screenshot.png)\n  \n* After drawing a line, click on the menu Plugins > Measurements (Plot Profile)\n* If you modify the line, you may want to click the \"Refresh\" button to redraw the profile.\n\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/redraw_screenshot.png)\n\nTo see how these steps can be done programmatically from python, check out the [demo notebook](https://github.com/haesleinhuepf/napari-plot-profile/blob/main/docs/demo.ipynb)\n\n## Create a Topographical View\n\nCreate a 3D view of a 2D image by warping pixel intensities to heights. It can be displayed as a 3D image layer, a points cloud layer or a surface layer.\n\n![](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/topographical_view_screencast.gif)\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-plot-profile` via [pip]:\n\n    pip install napari-plot-profile\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-plot-profile\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-plot-profile/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[image.sc]: https://image.sc\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PlotProfile",
      "topographical_view"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-timeseries-opener-plugin",
    "name": "napari-timeseries-opener-plugin",
    "display_name": "napari-timeseries-opener-plugin",
    "version": "0.1.8",
    "created_at": "2022-02-28",
    "modified_at": "2022-10-04",
    "authors": [
      "Niklas Netter"
    ],
    "author_emails": [
      "niknett@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-timeseries-opener-plugin/",
    "home_github": "https://github.com/gatoniel/napari-timeseries-opener-plugin",
    "home_other": null,
    "summary": "Simple plugin that opens separate .tif files as a 3-dimensional layer.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "qtpy",
      "magicgui",
      "tifffile",
      "stardist",
      "tensorflow"
    ],
    "package_metadata_description": "# napari-timeseries-opener-plugin\n\n[![License](https://img.shields.io/pypi/l/napari-timeseries-opener-plugin.svg?color=green)](https://github.com/gatoniel/napari-timeseries-opener-plugin/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-timeseries-opener-plugin.svg?color=green)](https://pypi.org/project/napari-timeseries-opener-plugin)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-timeseries-opener-plugin.svg?color=green)](https://python.org)\n[![tests](https://github.com/gatoniel/napari-timeseries-opener-plugin/workflows/tests/badge.svg)](https://github.com/gatoniel/napari-timeseries-opener-plugin/actions)\n[![codecov](https://codecov.io/gh/gatoniel/napari-timeseries-opener-plugin/branch/main/graph/badge.svg)](https://codecov.io/gh/gatoniel/napari-timeseries-opener-plugin)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-timeseries-opener-plugin)](https://napari-hub.org/plugins/napari-timeseries-opener-plugin)\n\nSimple plugin that opens separate .tif files as a 3-dimensional layer.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Run\n\nIn powershell run when you do not have sufficient GPU support in your environment\n```\n$env:CUDA_VISIBLE_DEVICES=-1; napari\n```\n\n## Installation\n\nYou can install `napari-timeseries-opener-plugin` via [pip]:\n\n    pip install napari-timeseries-opener-plugin\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/gatoniel/napari-timeseries-opener-plugin.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-timeseries-opener-plugin\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gatoniel/napari-timeseries-opener-plugin/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "LoadWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "the-segmentation-game",
    "name": "the-segmentation-game",
    "display_name": "the-segmentation-game",
    "version": "0.2.0",
    "created_at": "2022-05-27",
    "modified_at": "2022-10-02",
    "authors": [
      "Robert Haase",
      "Martin Sch√§tz"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/the-segmentation-game/",
    "home_github": "https://github.com/haesleinhuepf/the-segmentation-game",
    "home_other": null,
    "summary": "Gamified image segmentation quality estimation",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "napari-skimage-regionprops",
      "scikit-learn"
    ],
    "package_metadata_description": "# The segmentation game - for napari\n\n[![License](https://img.shields.io/pypi/l/the-segmentation-game.svg?color=green)](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/the-segmentation-game.svg?color=green)](https://pypi.org/project/the-segmentation-game)\n[![Python Version](https://img.shields.io/pypi/pyversions/the-segmentation-game.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/the-segmentation-game/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/the-segmentation-game/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/the-segmentation-game/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/the-segmentation-game)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/the-segmentation-game)](https://napari-hub.org/plugins/the-segmentation-game)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6588373.svg)](https://doi.org/10.5281/zenodo.6588373)\n\nGamified image segmentation quality estimation\n\n![img.png](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/screencast.gif)\n\n----------------------------------\n\n## Usage\n\nThe Segmentation Game allows to quantitatively compare segmentation results to a given ground truth annotation.\nThis allows fine-tuning parameters of image processing workflows to get optimal segmentation quality. \nIt also allows comparing different segmentation algorithms and identify which algorithm performs best objectively.\n\nThe game can be found in napari's `Tools > Games > The Segmentation Game` menu.\n\n### Ground Truth Annotation\n\nBefore you can start playing the game, some annotated cells/nuclei are necessary to later compute segmentation quality from.\nDepending on the used metric, it might be sufficient to annotate a hand full of objects. \nUse napari's annotation tools as shown below. \nUse the `+` and `-` keys on your keyboard to increase and decrease the label number that is currently drawn.\nNote: Avoid label gaps. The labels must be continuously subsequent. If there are pixels annotated with value 2, there must be pixels annotated with value 1.\n\n![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/annotation.gif)\n\n### Parameter tuning\n\nIf you work with one of [napari's segmentation plugins](https://www.napari-hub.org/?search=segmentation&sort=relevance&page=1) that produce labels layers,\nyou can tune their parameters and see how this influences segmentation quality.\n\n![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/parameter_tuning.gif)\n\n### Segmentation algorithm comparison\n\nIf you aim at comparing different segmentation algorithms, you can collect their results in label layers in the napari viewer.\nYou can then select the segmentation result from the corresponding pulldown and save quantitative comparison results in the Highscore table.\n\n![](https://github.com/haesleinhuepf/the-segmentation-game/raw/main/images/algorithm_comparison.gif)\n\n## Metrics\n\nCurrently, these metrics are implemented:\n* Jaccard Index (sparse): The [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) is a measure of overlap. \n  It lies between 0 (no overlap) and 1 (perfect overlap). \n  For each annotated ground truth label, the maximum overlap of any segmented label is determined. \n  The mean overlap of all annotated labels serves as metric result.\n* Jaccard Index (binary): The annotated ground truth labels and the segmentation result are first binarized considering all annotated pixels as positive and all other labels as negative.\n  Afterwards, the overlap between the two binary images is computed. This allows comparing binary segmentation algorithms, such as thresholding techniques.\n* Jaccard Index (binary, sparse): The annotated ground truth image can contain values 1 (negative, false) and 2 (positive, true) and\n  the segmentation result image will be binarized (0: False, otherwise: True). This allows comparing object/no-object annotations with label images.\n \n \nReceiver operating characteristic ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic))\n  \nConsider a two-class thresholding problem (binary pixel-wise classification object/background), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN) is when the prediction outcome is n while the actual value is p. We can organize result in table called confusion matrix, based on positive/neagtive results in row and true and false result in columns. From the confucsion matrix we can get many metrics with various usefulness. The curently implemented used for classification evaluation are:\n\n* Sensitivity, recall, hit rate, or true positive rate (TPR): (TP)/ (TP + FP), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \"positive\" and those for which it is not are considered \"negative\".\n* Specificity, selectivity or true negative rate (TNR): (TN)/ (TN + FN), Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered \"positive\" and those for which it is not are considered \"negative\".\n* Precision or positive predictive value (PPV): (TP)/ (TP + FP), in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. It is quantification for the TP events.\n* Accuracy: (TP + TN)/ (TP + FP + TN + FN), Accuracy measures observational error. Accuracy is how close or far off a given set of measurements are to their true value. However, it usually fails in imbalanced sets.\n* Balanced Accuracy: (TP/(TP+FN) + TN/(TN+FP))/2, Balanced Accuracy is trying to even out problems of accuracy in imbalanced sets.\n* F1 Score: 2*TP/(2*TP + FP + TN + FN), In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified.\n* Threat score (TS) or critical success index (CSI): TP/(TP + FP + FN), TC is another name for Jaccard Index (binary).\n\nThe ROC measures or confusion matrix is invaluable in cases when when our binary classifier is not ideal (which is often) and we are aiming to not get a general good result but specified low error. In that case we usually need to decide for some trade off, for example we need all (as many as possible) classified true positive objects, but we do not mind getting (usually as few as possible) false positive objects.\n\n**What we want to achieve**\n\n![Precision-versus-accuracy, source: 10.13140/RG.2.1.1668.7603](https://github.com/martinschatz-cz/the-segmentation-game/blob/main/images/Precision-versus-accuracy.png)\n\nWhen we are doing semantic segmentation, we are aiming to classify each pixel (ideally correctly) to each of our classes. But that can be hugr ammount of information, and our object might have significantly much less pixels then number of pixels belonging to background and/or other classes. Before choosing right metrics, we need to set up goal for our classification results. Idealy, we would like to have high accuracy and precission for ach class (as is on pictur above), but we might be happy getting high accuracy with good precision. Realisticaly we might need to be more specific, as to choose how big error we are prepared to accept, or decide if it is acceptable to have FN findings but no FP.\n\nPicking up a metric for highly unbalanced classification as in semantic segmentation is challenging. Most of the classic metrics wil fail (but they are stil usable object-wise). And we usually stick up with Jaccard Index/Threat score, F1 Score or anything that will tell us result for TP rate (as we expect we will have less pixels for objects then background and/or other classes).\n\n## Literature recommendation\n\nHow to choose the right metric for comparing segmentation results is explained in this paper:\n* [Metrics reloaded: Pitfalls and recommendations for image analysis validation. Maier-Hein L. and Reinke A. et al.](https://arxiv.org/abs/2206.01653)\n\n## Related plugins\n\nIf you aim at automatically optimizing segmentation quality, there are also napari plugins available with this capability:\n\n* [napari-accelerated-pixel-and-object-classification](https://www.napari-hub.org/plugins/napari-accelerated-pixel-and-object-classification)\n* [napari-workflow-optimizer](https://www.napari-hub.org/plugins/napari-workflow-optimizer)\n\n## Installation\n\nYou can install `the-segmentation-game` via [pip]:\n\n    pip install the-segmentation-game\n\n## Contributing\n\nContributions - especially new image segmentation quality metrics - are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"the-segmentation-game\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please open a thread on [image.sc](https://image.sc) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/the-segmentation-game/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TheSegmentationGameWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-brightness-contrast",
    "name": "napari-brightness-contrast",
    "display_name": "napari-brightness-contrast",
    "version": "0.1.8",
    "created_at": "2021-08-07",
    "modified_at": "2022-09-25",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-brightness-contrast/",
    "home_github": "https://github.com/haesleinhuepf/napari-brightness-contrast",
    "home_other": null,
    "summary": "Advanced layer visualization options",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "napari",
      "numpy",
      "pyqtgraph",
      "superqt",
      "napari-tools-menu",
      "pytest ; extra == 'tests'",
      "pytest-qt ; extra == 'tests'"
    ],
    "package_metadata_description": "# napari-brightness-contrast\n\n[![License](https://img.shields.io/pypi/l/napari-brightness-contrast.svg?color=green)](https://github.com/haesleinhuepf/napari-brightness-contrast/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-brightness-contrast.svg?color=green)](https://pypi.org/project/napari-brightness-contrast)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brightness-contrast.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-brightness-contrast/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-brightness-contrast/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast)\n[![Development Status](https://img.shields.io/pypi/status/napari-brightness-contrast.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-brightness-contrast)](https://napari-hub.org/plugins/napari-brightness-contrast)\n\nAdvanced layer histogram visualization options, e.g. for brightness / contrast\n![](https://github.com/haesleinhuepf/napari-brightness-contrast/blob/main/docs/images/napari-brightness-contrast3.gif?raw=true)\n\nNote: This will not work for big image data at the moment. \nIf the user interface feels slow, consider installing [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) to speed it up.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-brightness-contrast` via [pip]:\n\n    pip install napari-brightness-contrast\n\n## Contributing\n\nContributions are very welcome.  \nAfter cloning the repo, install using `pip install -e .[tests]` to enable testing via `pytest`.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-brightness-contrast\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/haesleinhuepf/napari-brightness-contrast/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BrightnessContrast"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bleach-correct",
    "name": "napari-bleach-correct",
    "display_name": "Bleaching Correction",
    "version": "0.0.1",
    "created_at": "2022-09-22",
    "modified_at": "2022-09-22",
    "authors": [
      "Alexander Marx"
    ],
    "author_emails": [
      "a.marx95@gmx.de"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-bleach-correct/",
    "home_github": "https://github.com/marx-alex/napari-bleach-correct",
    "home_other": null,
    "summary": "A napari plugin to correct time-lapse images for photobleaching.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scipy",
      "pyqtgraph",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-bleach-correct\n\n[![License](https://img.shields.io/github/license/marx-alex/napari-bleach-correct)](https://github.com/marx-alex/napari-bleach-correct)\n[![PyPI](https://img.shields.io/pypi/v/napari-bleach-correct.svg?color=green)](https://pypi.org/project/napari-bleach-correct)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bleach-correct.svg?color=green)](https://python.org)\n[![tests](https://github.com/marx-alex/napari-bleach-correct/workflows/tests/badge.svg)](https://github.com/marx-alex/napari-bleach-correct/actions)\n[![codecov](https://codecov.io/gh/marx-alex/napari-bleach-correct/branch/main/graph/badge.svg)](https://codecov.io/gh/marx-alex/napari-bleach-correct)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bleach-correct)](https://napari-hub.org/plugins/napari-bleach-correct)\n\n## Bleach correction for napari\n\nThis plugin is a python implementation of three different algorithms for bleach correction and can be used \nto correct time-lapse images that lose intensity due to photobleaching. The implementation is based on the ImageJ \nplugin Bleach Corrector by Miura et al. All methods work with 2D and 3D time series.\n\nNapari Bleach correction is easy to use:\n\n![Demo](./data/demo.gif)\n\n### Ratio Method\n\nThis is the simplest method. Every pixel in a frame is multiplied by the ratio from the mean intensity of the \nfirst frame to that of the *i-th* frame.\n\nAssumptions:\n* the mean intensity is constant through the time-lapse\n* the background fluorescence is the same for every pixel and frame\n\nParameters:\n* Background Intensity: Must be estimated\n\n### Exponential Curve Fitting\n\nDrift estimation of fluorescence signal by fitting the mean intensity to an exponential curve.\nThe image is corrected by the decay in the normalized exponential function.\n\nAssumptions:\n* time intervals between frames are equal\n\nParameters:\n* Exponential Curve: Bleaching can be modelled as a mono- or bi-exponential curve\n\n### Histogram Matching\n\nBleaching correction by matching histograms to a reference image.\nThe correct pixel values can be calculated by the cumulative distribution function\nof a frame and its reference frame. This method introduced by Miura et al.\n\nParameters:\n* Reference Frame: Match the frame's histogram with the first our neighbor frame \n\n**The Histogram Matching method using the neighbor frame as reference is a good start to correct bleaching.**\nAll methods are described in detail in Miura et al.\n\n## References\n\n* Miura K. [Bleach correction ImageJ plugin for compensating the photobleaching of time-lapse sequences.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7871415/) F1000Res. 2020 Dec 21;9:1494. doi: 10.12688/f1000research.27171.1\n* [Documentation of the ImageJ plugin](https://wiki.cmci.info/downloads/bleach_corrector)\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-bleach-correct` via [pip]:\n\n    pip install napari-bleach-correct\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/marx-alex/napari-bleach-correct.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-bleach-correct\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/marx-alex/napari-bleach-correct/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Bleaching Correction"
    ],
    "contributions_sample_data": [
      "Bleaching Mito"
    ]
  },
  {
    "normalized_name": "napari-pixel-correction",
    "name": "napari-pixel-correction",
    "display_name": "Pixel correction",
    "version": "0.1.4",
    "created_at": "2022-09-19",
    "modified_at": "2022-09-21",
    "authors": [
      "Herearii Metuarea"
    ],
    "author_emails": [
      "herearii.metuarea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-pixel-correction/",
    "home_github": "https://github.com/hereariim/napari-pixel-correction",
    "home_other": null,
    "summary": "Plugin to correct manually pixel wrongly predicted on image by annotation",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "napari",
      "matplotlib",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-pixel-correction\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-pixel-correction.svg?color=green)](https://github.com/hereariim/napari-pixel-correction/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pixel-correction.svg?color=green)](https://pypi.org/project/napari-pixel-correction)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pixel-correction.svg?color=green)](https://python.org)\n[![tests](https://github.com/hereariim/napari-pixel-correction/workflows/tests/badge.svg)](https://github.com/hereariim/napari-pixel-correction/actions)\n[![codecov](https://codecov.io/gh/hereariim/napari-pixel-correction/branch/main/graph/badge.svg)](https://codecov.io/gh/hereariim/napari-pixel-correction)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pixel-correction)](https://napari-hub.org/plugins/napari-pixel-correction)\n\nPlugin to correct manually pixel wrongly predicted on image by annotation\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\nThis plugin allows you to manually correct the images of the apple tree flowers by annotation. Below, a piece of an image shows the predicted pixels (in brown). A pixel in brown is assigned to the flower class. We can see that the brown colour does not necessarily cover a flower in this image.\n\n![Capture d‚Äô√©cran 2022-09-21 152404](https://user-images.githubusercontent.com/93375163/191530483-5ce230af-e34c-4fd5-ab91-1d611fd774d1.png)\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-pixel-correction` via [pip]:\n\n    pip install napari-pixel-correction\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hereariim/napari-pixel-correction.git\n\n## How does it work\n\nFirst, you need a compressed file (in .zip format) were you have all your images. For a compressed file named as `input.zip`, the compressed file should be built like :\n\n```\n.\n‚îî‚îÄ‚îÄ input.zip\n    ‚îî‚îÄ‚îÄ repository\n        ‚îú‚îÄ‚îÄ image\n        ‚îÇ   ‚îú‚îÄ‚îÄ im_1.JPG\n        ‚îÇ   ‚îú‚îÄ‚îÄ im_2.JPG  \n        ‚îÇ   ‚îú‚îÄ‚îÄ im_3.JPG\n        ‚îÇ   ...\n        ‚îÇ   ‚îî‚îÄ‚îÄ im_n.JPG\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ mask\n            ‚îú‚îÄ‚îÄ im_1_mask.JPG\n            ‚îú‚îÄ‚îÄ im_2_mask.JPG\n            ‚îú‚îÄ‚îÄ im_3_mask.JPG\n            ...\n            ‚îî‚îÄ‚îÄ im_n_mask.JPG\n```\nIn repository, each image folder should have two elements : image in RGB and the segmented mask in binary image (where no-flower class is 0 and flower class is 255)\n\n![napari-tutorial_simple](https://user-images.githubusercontent.com/93375163/191527225-47ba8667-e3bd-467b-b5f3-f8f7d97617a5.gif)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-pixel-correction\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hereariim/napari-pixel-correction/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Load image",
      "Save label",
      "Save zip"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-rioxarray",
    "name": "napari-rioxarray",
    "display_name": "Rioxarray Plugin",
    "version": "0.0.1",
    "created_at": "2022-09-01",
    "modified_at": "2022-09-01",
    "authors": [
      "Dr. Andrew Annex"
    ],
    "author_emails": [
      "ama6fy@virginia.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-rioxarray/",
    "home_github": "https://github.com/AndrewAnnex/napari-rioxarray",
    "home_other": null,
    "summary": "A rioxarray plugin for napari supporting GDAL raster datatypes",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari",
      "rioxarray",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-rioxarray\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-rioxarray.svg?color=green)](https://github.com/AndrewAnnex/napari-rioxarray/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-rioxarray.svg?color=green)](https://pypi.org/project/napari-rioxarray)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-rioxarray.svg?color=green)](https://python.org)\n[![tests](https://github.com/AndrewAnnex/napari-rioxarray/workflows/tests/badge.svg)](https://github.com/AndrewAnnex/napari-rioxarray/actions)\n[![codecov](https://codecov.io/gh/AndrewAnnex/napari-rioxarray/branch/main/graph/badge.svg)](https://codecov.io/gh/AndrewAnnex/napari-rioxarray)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-rioxarray)](https://napari-hub.org/plugins/napari-rioxarray)\n\nA rioxarray plugin for napari supporting GDAL raster datatypes\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-rioxarray` via [pip]:\n\n    pip install napari-rioxarray\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/AndrewAnnex/napari-rioxarray.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-rioxarray\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/AndrewAnnex/napari-rioxarray/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.tiff",
      "*.fits",
      "*.img",
      "*.tif",
      "*.IMG",
      "*.CUB",
      "*.LBL",
      "*.lbl",
      "*.cub",
      "*.FITS",
      "*.TIFF",
      "*.vrt",
      "*.TIF"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-calibration",
    "name": "napari-calibration",
    "display_name": "napari Calibration",
    "version": "0.0.14",
    "created_at": "2022-08-23",
    "modified_at": "2022-08-29",
    "authors": [
      "Tristan Cotte"
    ],
    "author_emails": [
      "tristan.cotte@sgs.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-calibration/",
    "home_github": "https://github.com/tcotte/napari-calibration",
    "home_other": null,
    "summary": "Plug in which enables to make camera calibration",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "numpy",
      "qt-material",
      "opencv-python"
    ],
    "package_metadata_description": "# napari-calibration\n\n[![License](https://img.shields.io/pypi/l/napari-calibration.svg?color=green)](https://github.com/tcotte/napari-calibration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-calibration.svg?color=green)](https://pypi.org/project/napari-calibration)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-calibration.svg?color=green)](https://python.org)\n[![tests](https://github.com/tcotte/napari-calibration/workflows/tests/badge.svg)](https://github.com/tcotte/napari-calibration/actions)\n[![codecov](https://codecov.io/gh/tcotte/napari-calibration/branch/main/graph/badge.svg)](https://codecov.io/gh/tcotte/napari-calibration)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-calibration)](https://napari-hub.org/plugins/napari-calibration)\n\nPlug in which enables to make camera calibration\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-calibration` via [pip]:\n\n    pip install napari-calibration\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/tcotte/napari-calibration.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-calibration\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/tcotte/napari-calibration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Get picture widget",
      "Calibration widget",
      "Hello World"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-quoll",
    "name": "napari-quoll",
    "display_name": "napari Quoll",
    "version": "0.0.1",
    "created_at": "2022-08-26",
    "modified_at": "2022-08-26",
    "authors": [
      "Elaine Ho"
    ],
    "author_emails": [
      "Elaine.Ho@rfi.ac.uk"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-quoll/",
    "home_github": "https://github.com/rosalindfranklininstitute/napari-quoll",
    "home_other": null,
    "summary": "Resolution estimation for electron tomography",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "quoll",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-quoll\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI](https://img.shields.io/pypi/v/napari-quoll.svg?color=green)](https://pypi.org/project/napari-quoll)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-quoll.svg?color=green)](https://python.org)\n[![tests](https://github.com/rosalindfranklininstitute/napari-quoll/workflows/tests/badge.svg)](https://github.com/rosalindfranklininstitute/napari-quoll/actions)\n[![codecov](https://codecov.io/gh/rosalindfranklininstitute/napari-quoll/branch/main/graph/badge.svg)](https://codecov.io/gh/rosalindfranklininstitute/napari-quoll)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-quoll)](https://napari-hub.org/plugins/napari-quoll)\n\nResolution estimation for electron tomography\n\nThe Python package which does the resolution estimation is [Quoll](https://github.com/rosalindfranklininstitute/quoll). This repository, `napari-quoll` is just the Napari plugin.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-quoll` via [pip] into a <b>Python 3.7</b> environment, replacing <env_name> with an environment name of your choice:\n\n    conda -n create <env_name> python=3.7\n    conda activate <env_name>\n    pip install napari-quoll\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/rosalindfranklininstitute/napari-quoll.git\n\n<b>Note:</b> Due to [miplib]() dependencies, this plugin only works on Python 3.7 environments.\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-quoll\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/rosalindfranklininstitute/napari-quoll/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Quoll"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "avidaq",
    "name": "avidaq",
    "display_name": "napari avidaq",
    "version": "0.0.5",
    "created_at": "2022-07-21",
    "modified_at": "2022-08-25",
    "authors": [
      "Riley M Shea"
    ],
    "author_emails": [
      "RileyMShea@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/avidaq/",
    "home_github": null,
    "home_other": "None",
    "summary": "controls for napari and micromanger",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "numpy",
      "pycromanager",
      "qtpy",
      "twine ; extra == 'build'",
      "black ; extra == 'testing'",
      "ipykernel ; extra == 'testing'",
      "matplotlib ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pyright ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "yappi ; extra == 'testing'"
    ],
    "package_metadata_description": "# avidaq\n\n[![PyPI](https://img.shields.io/pypi/v/avidaq.svg?color=green)](https://pypi.org/project/avidaq)\n[![Python Version](https://img.shields.io/pypi/pyversions/avidaq.svg?color=green)](https://python.org)\n[![tests](https://github.com/optimax/avidaq/workflows/tests/badge.svg)](https://github.com/optimax/avidaq/actions)\n[![codecov](https://codecov.io/gh/optimax/avidaq/branch/main/graph/badge.svg)](https://codecov.io/gh/optimax/avidaq)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/avidaq)](https://napari-hub.org/plugins/avidaq)\n\ncontrols for napari and micromanger\n\n---\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\n### Standard installation\n\nYou can install `avidaq` via [pip]:\n\n```shell\npip install napari[all] avidaq\n```\n\n### Install from plugin menu\n\nAlternatively you can install `avidaq` via the [napari] plugin menu:\n\n## ![napari-add-plugin](napari-add-plugin.png)\n\n## Running\n\nFirst start micromanager.  Make sure the server port checkbox is activated.\n\nThen to start napari with the avidaq plugin active run:\n`napari -w avidaq`\n\n![](screenshot.png)\n\n## Updating presets\n\nMDA presets are stored in a json file in the user's home directory.\n\n```shell\n\n`C:\\\\Users\\YourName\\.avidaq\\mda_presets.json`\n```\n\nThis file should exist after plugin installation with some defaults. You do not need to create the file yourself.\n\nAdd or modify the values and reload napari to see the changes.\n\nAll parameter entries are optional, if not provided the default value will be used.\n\nThe parameter names and their descriptions can be found [here] (https://github.com/micro-manager/pycro-manager/blob/main/pycromanager/acq_util.py#L102-L115)\n\nThe format is as follows:\n\n```json\n{\n    \"gui_display_name\": {\n        \"parameter_name\": value,\n        \"parameter_name\": value,\n        ...\n    },\n    \"gui_display_name\": {\n        \"parameter_name\": value,\n        \"parameter_name\": value,\n        ...\n    },\n    ...\n}\n```\n\ndefaults:\n\n```json\n{\n  \"Basic\": {\n    \"num_time_points\": 5,\n    \"z_start\": 0,\n    \"z_end\": 6,\n    \"z_step\": 0.4\n  },\n  \"Simple\": {\n    \"num_time_points\": 2,\n    \"z_start\": 0,\n    \"z_end\": 2,\n    \"z_step\": 0.1\n  },\n  \"Detailed\": {\n    \"num_time_points\": 10,\n    \"z_start\": 0,\n    \"z_end\": 12,\n    \"z_step\": 0.2\n  }\n}\n```\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Development\n\nYou should have python3.8 or higher installed.\n\n1. clone this repo\n2. create a virtual environment `python -m venv .venv && source .venv/bin/activate`\n3. run `pip install -e '.[testing,build]'`\n4. run `pre-commit install`\n\n### To run unit tests\n\n`pytest`\n\n### typical workflow\n\n1. edit code in `/src`\n2. run napari -w avidaq\n3. repeat\n\n### Releasing to pypi\n\n\nProject is automically built and deployed to pypi upon\n\n\n---\n\n[napari]: https://github.com/napari/napari\n[cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[mit]: http://opensource.org/licenses/MIT\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[pypi]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.npy"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Main AVIDAQ widget",
      "Example Function Widget"
    ],
    "contributions_sample_data": [
      "napari avidaq"
    ]
  },
  {
    "normalized_name": "napari-patchcreator",
    "name": "napari-patchcreator",
    "display_name": "napari patch creator",
    "version": "0.1.4",
    "created_at": "2022-08-23",
    "modified_at": "2022-08-24",
    "authors": [
      "Tom Burke"
    ],
    "author_emails": [
      "burke@mpi-cbg.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-patchcreator/",
    "home_github": "https://github.com/juglab/napari-patchcreator",
    "home_other": null,
    "summary": "A simple plugin to use with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-patchcreator\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-patchcreator.svg?color=green)](https://github.com/juglab/napari-patchcreator/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-patchcreator.svg?color=green)](https://pypi.org/project/napari-patchcreator)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-patchcreator.svg?color=green)](https://python.org)\n[![tests](https://github.com/juglab/napari-patchcreator/workflows/tests/badge.svg)](https://github.com/juglab/napari-patchcreator/actions)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-patchcreator)](https://napari-hub.org/plugins/napari-patchcreator)\n\nA simple plugin to create quadratic patches from images through selection and clicking with the left mouse button.\nThe patches can then be exported to a folder of your own choosing.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-patchcreator` via [pip]:\n\n    pip install napari-patchcreator\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/juglab/napari-patchcreator.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-patchcreator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/juglab/napari-patchcreator/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Patch creation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-aicsimageio",
    "name": "napari-aicsimageio",
    "display_name": "napari-aicsimageio",
    "version": "0.7.2",
    "created_at": "2020-12-14",
    "modified_at": "2022-08-22",
    "authors": [
      "Eva Maxfield Brown",
      "Talley Lambert"
    ],
    "author_emails": [
      "Eva Maxfield Brown <evamaxfieldbrown@gmail.com>",
      "Talley Lambert <talley.lambert@gmail.com>"
    ],
    "license": "GPLv3",
    "home_pypi": "https://pypi.org/project/napari-aicsimageio/",
    "home_github": "https://github.com/AllenCellModeling/napari-aicsimageio#README.md",
    "home_other": null,
    "summary": "AICSImageIO bindings for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "aicsimageio[all] (>=4.6.3)",
      "fsspec[http] (>=2022.7.1)",
      "napari (>=0.4.11)",
      "psutil (>=5.7.0)",
      "aicspylibczi (>=3.0.5)",
      "bioformats-jar",
      "readlif (>=0.6.4)",
      "black (>=19.10b0) ; extra == 'dev'",
      "coverage (>=5.1) ; extra == 'dev'",
      "docutils (<0.16,>=0.10) ; extra == 'dev'",
      "flake8-debugger (>=3.2.1) ; extra == 'dev'",
      "flake8-pyprojecttoml ; extra == 'dev'",
      "flake8 (>=3.8.3) ; extra == 'dev'",
      "ipython (>=7.15.0) ; extra == 'dev'",
      "isort (>=5.7.0) ; extra == 'dev'",
      "mypy (>=0.800) ; extra == 'dev'",
      "pytest-runner (>=5.2) ; extra == 'dev'",
      "twine (>=3.1.1) ; extra == 'dev'",
      "wheel (>=0.34.2) ; extra == 'dev'",
      "PyQt5 ; extra == 'test'",
      "pytest (>=5.4.3) ; extra == 'test'",
      "pytest-qt (~=4.0) ; extra == 'test'",
      "pytest-cov (>=2.9.0) ; extra == 'test'",
      "pytest-raises (>=0.11) ; extra == 'test'",
      "pytest-xvfb (~=2.0) ; extra == 'test'",
      "quilt3 (~=3.4.0) ; extra == 'test'"
    ],
    "package_metadata_description": "# napari-aicsimageio\n\n[![License](https://img.shields.io/pypi/l/napari-aicsimageio.svg?color=green)](https://github.com/AllenCellModeling/napari-aicsimageio/raw/main/LICENSE)\n[![Build Status](https://github.com/AllenCellModeling/napari-aicsimageio/workflows/Build%20Main/badge.svg)](https://github.com/AllenCellModeling/napari-aicsimageio/actions)\n[![Code Coverage](https://codecov.io/gh/AllenCellModeling/napari-aicsimageio/branch/main/graph/badge.svg)](https://codecov.io/gh/AllenCellModeling/napari-aicsimageio)\n\nAICSImageIO bindings for napari\n\n---\n\n## Features\n\n-   Supports reading metadata and imaging data for:\n    -   `OME-TIFF`\n    -   `TIFF`\n    -   `CZI` (Zeiss)\n    -   `LIF` (Leica)\n    -   `ND2` (Nikon)\n    -   `DV` (DeltaVision)\n    -   Any formats supported by [aicsimageio](https://github.com/AllenCellModeling/aicsimageio)\n    -   Any formats supported by [bioformats](https://github.com/tlambert03/bioformats_jar)\n        -   `SLD` (Slidebook)\n        -   `SVS` (Aperio)\n        -   [Full List](https://docs.openmicroscopy.org/bio-formats/6.5.1/supported-formats.html)\n    -   Any additional format supported by [imageio](https://github.com/imageio/imageio)\n        -   `PNG`\n        -   `JPG`\n        -   `GIF`\n        -   `AVI`\n        -   [Full List](https://imageio.readthedocs.io/en/v2.4.1/formats.html)\n\n_While upstream `aicsimageio` is released under BSD-3 license, this plugin is released under GPLv3 license because it installs all format reader dependencies._\n\n## Installation\n\n**Stable Release:** `pip install napari-aicsimageio` or `conda install napari-aicsimageio -c conda-forge`<br>\n**Development Head:** `pip install git+https://github.com/AllenCellModeling/napari-aicsimageio.git`\n\n### Reading Mode Threshold\n\nThis image reading plugin will load the provided image directly into memory if it meets\nthe following two conditions:\n\n1. The filesize is less than 4GB.\n2. The filesize is less than 30% of machine memory available.\n\nIf either of these conditions isn't met, the image is loaded in chunks only as needed.\n\n### Use napari-aicsimageio as the Reader for All File Formats\n\nIf you want to force napari to always use this plugin as the reader for all file formats,\ntry running this snippet:\n\n```python\nfrom napari.settings import get_settings\n\nget_settings().plugins.extension2reader = {'*': 'napari-aicsimageio', **get_settings().plugins.extension2reader}\n```\n\nFor more details, see [#37](https://github.com/AllenCellModeling/napari-aicsimageio/issues/37).\n\n## Examples of Features\n\n#### General Image Reading\n\nAll image file formats supported by\n[aicsimageio](https://github.com/AllenCellModeling/aicsimageio) will be read and all\nraw data will be available in the napari viewer.\n\nIn addition, when reading an OME-TIFF, you can view all OME metadata directly in the\nnapari viewer thanks to `ome-types`.\n\n![screenshot of an OME-TIFF image view, multi-channel, z-stack, with metadata viewer](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/ome-tiff-with-metadata-viewer.png)\n\n#### Multi-Scene Selection\n\nWhen reading a multi-scene file, a widget will be added to the napari viewer to manage\nscene selection (clearing the viewer each time you change scene or adding the\nscene content to the viewer) and a list of all scenes in the file.\n\n![gif of drag and drop file to scene selection and management](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/scene-selection.gif)\n\n#### Access to the AICSImage Object and Metadata\n\n![napari viewer with console open showing `viewer.layers[0].metadata`](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/console-access.png)\n\nYou can access the `AICSImage` object used to load the image pixel data and\nimage metadata using the built-in napari console:\n\n```python\nimg = viewer.layers[0].metadata[\"aicsimage\"]\nimg.dims.order  # TCZYX\nimg.channel_names  # [\"Bright\", \"Struct\", \"Nuc\", \"Memb\"]\nimg.get_image_dask_data(\"ZYX\")  # dask.array.Array\n```\n\nThe napari layer metadata dictionary also stores a shorthand\nfor the raw image metadata:\n\n```python\nviewer.layers[0].metadata[\"raw_image_metadata\"]\n```\n\nThe metadata is returned in whichever format is used by the underlying\nfile format reader, i.e. for CZI the raw metadata is returned as\nan `xml.etree.ElementTree.Element`, for OME-TIFF the raw metadata is returned\nas an `OME` object from `ome-types`.\n\nLastly, if the underlying file format reader has an OME metadata conversion function,\nyou may additionally see a key in the napari layer metadata dictionary\ncalled `\"ome_types\"`. For example, because the AICSImageIO\n`CZIReader` and `BioformatsReader` both support converting raw image metadata\nto OME metadata, you will see an `\"ome_types\"` key that stores the metadata transformed\ninto the OME metadata model.\n\n```python\nviewer.layers[0].metadata[\"ome_types\"]  # OME object from ome-types\n```\n\n#### Mosaic Reading\n\nWhen reading CZI or LIF images, if the image is a mosaic tiled image, `napari-aicsimageio`\nwill return the reconstructed image:\n\n![screenshot of a reconstructed / restitched mosaic tile LIF](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/tiled-lif.png)\n\n## Development\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information related to developing the code.\n\nFor additional file format support, contributed directly to\n[AICSImageIO](https://github.com/AllenCellModeling/aicsimageio).\nNew file format support will become directly available in this\nplugin on new `aicsimageio` releases.\n\n## Citation\n\nIf you find `aicsimageio` _(or `napari-aicsimageio`)_ useful, please cite as:\n\n> AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\n\n_Free software: GPLv3_\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.lms",
      "*.oif",
      "*.pcoraw",
      "*.seq",
      "*.c01",
      "*.k25",
      "*.pr3",
      "*.fff",
      "*.rec",
      "*.spc",
      "*.labels",
      "*.rw2",
      "*.ids",
      "*.mod",
      "*.jng",
      "*.jif",
      "*.am",
      "*.dng",
      "*.ics",
      "*.tfr",
      "*.img",
      "*.cine",
      "*.pxr",
      "*.cr2",
      "*.cap",
      "*.g3",
      "*.sdt",
      "*.pcx",
      "*.dds",
      "*.pict",
      "*.bay",
      "*.nd2",
      "*.im",
      "*.nii.gz",
      "*.r3d",
      "*.vsi",
      "*.jp2",
      "*.mhd",
      "*.gdcm",
      "*.msr",
      "*.afi",
      "*.his",
      "*.nrw",
      "*.xdce",
      "*.lim",
      "*.xys",
      "*.msp",
      "*.pnl",
      "*.zvi",
      "*.sr2",
      "*.array-like",
      "*.nrrd",
      "*.al3d",
      "*.koa",
      "*.htm",
      "*.mov",
      "*.sld",
      "*.lei",
      "*.wap",
      "*.cut",
      "*.klb",
      "*.mic",
      "*.gbr",
      "*.html",
      "*.iff",
      "*.sxm",
      "*.jpe",
      "*.lsm",
      "*.wpi",
      "*.ftc",
      "*.avi",
      "*.xpm",
      "*.ffr",
      "*.sti",
      "*.cur",
      "*.wav",
      "*.2fl",
      "*.bmp",
      "*.mdb",
      "*.aim",
      "*.ipl",
      "*.acff",
      "*.hdr",
      "*.mvd2",
      "*.txt",
      "*.ct.img",
      "*.pfm",
      "*.arw",
      "*.pgm",
      "*.frm",
      "*.grey",
      "*.grib",
      "*.oib",
      "*.rdc",
      "*.j2c",
      "*.erf",
      "*.ch5",
      "*.flc",
      "*.mri",
      "*.dm3",
      "*.fit",
      "*.fake",
      "*.mnc2",
      "*.ome.tif",
      "*.ppm",
      "*.srf",
      "*.imggz",
      "*.mpeg",
      "*.xv",
      "*.ims",
      "*.scan",
      "*.jpk",
      "*.pic",
      "*.im3",
      "*.bif",
      "*.bmq",
      "*.gel",
      "*.rwl",
      "*.nd",
      "*.cs1",
      "*.psd",
      "*.zfp",
      "*.gif",
      "*.nii",
      "*.wdp",
      "*.ipm",
      "*.mp4",
      "*.htd",
      "*.par",
      "*.niigz",
      "*.fdf",
      "*.dcr",
      "*.kc2",
      "*.zpo",
      "*.rcpnl",
      "*.apl",
      "*.acqp",
      "*.bip",
      "*.bsdf",
      "*.ome.tiff",
      "*.gipl",
      "*.naf",
      "*.mos",
      "*.exp",
      "*.png",
      "*.exr",
      "*.wat",
      "*.hdp",
      "*.3fr",
      "*.mkv",
      "*.zfr",
      "*.vtk",
      "*.nia",
      "*.dv",
      "*.tnb",
      "*.j2k",
      "*.tim",
      "*.cxd",
      "*.ps",
      "*.ia",
      "*.qptiff",
      "*.fts",
      "*.lbm",
      "*.thm",
      "*.wbm",
      "*.ndpi",
      "*.jpeg",
      "*.ano",
      "*.nef",
      "*.raw",
      "*.kdc",
      "*.csv",
      "*.liff",
      "*.vff",
      "*.xbm",
      "*.jpx",
      "*.mrc",
      "*.orf",
      "*.sif",
      "*.tiff",
      "*.dsc",
      "*.bin",
      "*.dicom",
      "*.stp",
      "*.targa",
      "*.wmv",
      "*.1sc",
      "*.tif",
      "*.amiramesh",
      "*.nhdr",
      "*.pcd",
      "*.i2i",
      "*.ftu",
      "*.pct",
      "*.dcx",
      "*.pxn",
      "*.raf",
      "*.jfif",
      "*.vms",
      "*.xqd",
      "*.rgb",
      "*.lfr",
      "*.inf",
      "*.scn",
      "*.tga",
      "*.cfg",
      "*.iiq",
      "*.lif",
      "*.dti",
      "*.wlz",
      "*.l2d",
      "*.ome",
      "*.svs",
      "*.st",
      "*.cat",
      "*.webp",
      "*.flex",
      "*.wmf",
      "*.ptx",
      "*.df3",
      "*.srw",
      "*.fpx",
      "*.mgh",
      "*.hdf",
      "*.mtb",
      "*.mef",
      "*.arf",
      "*.sm2",
      "*.ico",
      "*.lfp",
      "*.hdf5",
      "*.ct",
      "*.inr",
      "*.spi",
      "*.db",
      "*.fits",
      "*.mpo",
      "*.afm",
      "*.cif",
      "*.mng",
      "*.zip",
      "*.ras",
      "*.jpc",
      "*.dat",
      "*.xqf",
      "*.epsi",
      "*.swf",
      "*.bufr",
      "*.fli",
      "*.dcm",
      "*.crw",
      "*.iim",
      "*.jpf",
      "*.oir",
      "*.dc2",
      "*.pef",
      "*.icns",
      "*.ipw",
      "*.vws",
      "*.dm2",
      "*.rwz",
      "*.sm3",
      "*.stk",
      "*.ali",
      "*.h5",
      "*.czi",
      "*.rgba",
      "*.wbmp",
      "*.xml",
      "*.ecw",
      "*.fz",
      "*.eps",
      "*.hed",
      "*.mha",
      "*.bw",
      "*.hx",
      "*.obf",
      "*.v",
      "*.pbm",
      "*.qtk",
      "*.fid",
      "*.top",
      "*.drf",
      "*.mpg",
      "*.ndpis",
      "*.jpg",
      "*.emf",
      "*.jxr",
      "*.spe",
      "*.mdc",
      "*.npz",
      "*.mnc",
      "*.mrw"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-wsireg",
    "name": "napari-wsireg",
    "display_name": "napari-wsireg",
    "version": "0.1.2",
    "created_at": "2022-04-27",
    "modified_at": "2022-08-16",
    "authors": [
      "Nathan Heath Patterson"
    ],
    "author_emails": [
      "heath.patterson@vanderbilt.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-wsireg/",
    "home_github": "https://github.com/nhpatterson/napari-wsireg",
    "home_other": null,
    "summary": "plugin to perform whole slide image registration with wsireg",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "wsireg (>=0.3.6)",
      "SimpleITK",
      "czifile",
      "dask",
      "imagecodecs",
      "napari",
      "numpy",
      "ome-types",
      "pint",
      "qtpy",
      "tifffile",
      "zarr (>=2.10.3)",
      "napari-geojson",
      "networkx",
      "matplotlib"
    ],
    "package_metadata_description": "# napari-wsireg\n\n![Alt text](https://github.com/NHPatterson/napari-wsireg/blob/main/src/napari_wsireg/gui/resources/wsireg-logo-light.svg?raw=true \"wsireg\")\n\n[//]: # ([![License]&#40;https://img.shields.io/pypi/l/napari-wsireg.svg?color=green&#41;]&#40;https://github.com/nhpatterson/napari-wsireg/raw/main/LICENSE&#41;)\n\n[//]: # ([![PyPI]&#40;https://img.shields.io/pypi/v/napari-wsireg.svg?color=green&#41;]&#40;https://pypi.org/project/napari-wsireg&#41;)\n\n[//]: # ([![Python Version]&#40;https://img.shields.io/pypi/pyversions/napari-wsireg.svg?color=green&#41;]&#40;https://python.org&#41;)\n\n[//]: # ([![tests]&#40;https://github.com/nhpatterson/napari-wsireg/workflows/tests/badge.svg&#41;]&#40;https://github.com/nhpatterson/napari-wsireg/actions&#41;)\n\n[//]: # ([![napari hub]&#40;https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-wsireg&#41;]&#40;https://napari-hub.org/plugins/napari-wsireg&#41;)\n\n\nPlugin to perform whole slide image registration based on wsireg.\n\nPlease see [wsireg](https://github.com/nhpatterson/wsireg) for more info image formats, features and how registration works.\n\n\n## Usage\n\nAdd images from napari layers or from file and set up \"registration paths\" between them. OME-TIFF is best supported format.\n\n### Constructed registration graph in action\n\n![Alt Text](assets/graph_in_action.gif)\n\n\n_Solid arrows_: direct registration between two images.\n\n_Dashed arrows_: indriect registration paths.\n\n## Installation\n\nYou can install `napari-wsireg` via [pip]:\n\n    pip install napari-wsireg\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/nhpatterson/napari-wsireg.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-wsireg\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/nhpatterson/napari-wsireg/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "wsireg2D Main"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-boids",
    "name": "napari-boids",
    "display_name": "Boids",
    "version": "0.0.1",
    "created_at": "2022-08-08",
    "modified_at": "2022-08-08",
    "authors": [
      "L√©o Guignard"
    ],
    "author_emails": [
      "leo.guignard@univ-amu.fr"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-boids/",
    "home_github": "https://github.com/leoguignard/napari-boids",
    "home_other": null,
    "summary": "A plugin to look at boids",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "scipy",
      "magicgui",
      "qtpy",
      "napari",
      "tox; extra == \"testing\"",
      "pytest; extra == \"testing\"",
      "pytest-cov; extra == \"testing\"",
      "pytest-qt; extra == \"testing\"",
      "napari; extra == \"testing\"",
      "pyqt5; extra == \"testing\""
    ],
    "package_metadata_description": "# napari-boids\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-boids.svg?color=green)](https://github.com/leoguignard/napari-boids/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-boids.svg?color=green)](https://pypi.org/project/napari-boids)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-boids.svg?color=green)](https://python.org)\n[![tests](https://github.com/leoguignard/napari-boids/workflows/tests/badge.svg)](https://github.com/leoguignard/napari-boids/actions)\n[![codecov](https://codecov.io/gh/leoguignard/napari-boids/branch/main/graph/badge.svg)](https://codecov.io/gh/leoguignard/napari-boids)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-boids)](https://napari-hub.org/plugins/napari-boids)\n\nA plugin to look at boids\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-boids` via [pip]:\n\n    pip install napari-boids\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/leoguignard/napari-boids.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-boids\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/leoguignard/napari-boids/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "View Boids"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tissuumaps",
    "name": "napari-tissuumaps",
    "display_name": "Napari TissUUmaps",
    "version": "1.1.2",
    "created_at": "2021-09-02",
    "modified_at": "2022-08-05",
    "authors": [
      "Nicolas Pielawski"
    ],
    "author_emails": [
      "nicolas@pielawski.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-tissuumaps/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin to export Napari projects to TissUUmaps.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "napari ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# üèù napari-tissuumaps üß´\n\n[![License MIT](https://img.shields.io/pypi/l/napari-tissuumaps.svg?color=green)](https://github.com/npielawski/napari-tissuumaps/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tissuumaps.svg?color=green)](https://pypi.org/project/napari-tissuumaps)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tissuumaps.svg?color=green)](https://python.org)\n[![tests](https://github.com/npielawski/napari-tissuumaps/workflows/tests/badge.svg)](https://github.com/npielawski/napari-tissuumaps/actions)\n[![codecov](https://codecov.io/gh/npielawski/napari-tissuumaps/branch/main/graph/badge.svg)](https://codecov.io/gh/npielawski/napari-tissuumaps)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tissuumaps)](https://napari-hub.org/plugins/napari-tissuumaps)\n\nA plugin to export Napari projects to [TissUUmaps](https://tissuumaps.research.it.uu.se/).\n\n----------------------------------\n\nThis plugins adds a new writer to [Napari] to export projects to [TissUUmaps](https://github.com/TissUUmaps/TissUUmaps). Exported projects can then be open on the browser or on a standalone GUI with [TissUUmaps](https://github.com/TissUUmaps/TissUUmaps). More information and demonstrations are available on the [TissUUmaps webpage](https://tissuumaps.research.it.uu.se/).\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## üöÄ Features\n\n<p align=\"center\">\n  <img src=\"images/screenshot.jpg\" alt=\"Demonstration of a project exported from Napari to TissUUmaps.\" width=\"500\" />\n</p>\n\nThe plugin now supports:\n\n* Exporting images\n* Exporting labels\n* Exporting points\n* Exporting shapes, including:\n    * Polygons\n    * Rectangles\n    * Lines\n    * Paths\n    * Ellipses\n\nThe plugin exports the right color for the points, shapes and labels and also saves the visibility/opacity of each layers. The shapes are exported in the GeoJSON format, the points in CSV files, and images as TIFFs.\n\n## üì∫ Installation\n\nYou can install `napari-tissuumaps` via [pip]:\n\n    pip install napari-tissuumaps\n\nYou can also install `napari-tissumaps` via [napari]:\n\nIn Napari, access the menubar, Plugins > Install/Uninstall Plugins.\nSearch for napari-tissuumaps in the list and choose install, or type\n`napari-tissuumaps` in the \"install by name/url, or drop file...\" text area and choose\ninstall.\n\n## ‚õè Usage\n\nTo export a project for TissUUmaps, access the menubar, File > Save All Layers... and\ntype in a filename. Choose the `.tmap` extension in the dropdown and click on the Save\nbutton, It will create a folder containing all the necessary files for TissUUmaps.\n\n## üë©‚Äçüíª Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## ‚öñÔ∏è License\n\nDistributed under the terms of the [MIT] license,\n\"napari-tissuumaps\" is free and open source software\n\n## üöí Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".tmap"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-help",
    "name": "napari-help",
    "display_name": "napari Help",
    "version": "0.1.0",
    "created_at": "2022-07-28",
    "modified_at": "2022-07-28",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "brisvag@gmail.com"
    ],
    "license": "GPL-3.0",
    "home_pypi": "https://pypi.org/project/napari-help/",
    "home_github": "https://github.com/brisvag/napari-help",
    "home_other": null,
    "summary": "Helpful tooltips for napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari",
      "qtpy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-help\n\n[![License GNU GPL v3.0](https://img.shields.io/pypi/l/napari-help.svg?color=green)](https://github.com/brisvag/napari-help/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-help.svg?color=green)](https://pypi.org/project/napari-help)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-help.svg?color=green)](https://python.org)\n[![tests](https://github.com/brisvag/napari-help/workflows/tests/badge.svg)](https://github.com/brisvag/napari-help/actions)\n[![codecov](https://codecov.io/gh/brisvag/napari-help/branch/main/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-help)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-help)](https://napari-hub.org/plugins/napari-help)\n\nHelpful tooltips for napari.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-help` via [pip]:\n\n    pip install napari-help\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/brisvag/napari-help.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-help\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/brisvag/napari-help/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Help Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sift-registration",
    "name": "napari-sift-registration",
    "display_name": "SIFTReg",
    "version": "0.1.2",
    "created_at": "2022-07-27",
    "modified_at": "2022-07-27",
    "authors": [
      "John Fozard"
    ],
    "author_emails": [
      "john.fozard@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-sift-registration/",
    "home_github": "https://github.com/jfozard/napari-sift-registration",
    "home_other": null,
    "summary": "Simple plugin for SIFT keypoint detection, and affine registration with RANSAC, based on scikit-image",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# skimage-sift-registration\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-sift-registration.svg?color=green)](https://github.com/jfozard/napari-sift-registration/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sift-registration.svg?color=green)](https://pypi.org/project/napari-sift-registration)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sift-registration.svg?color=green)](https://python.org)\n[![tests](https://github.com/jfozard/napari-sift-registration/workflows/tests/badge.svg)](https://github.com/jfozard/napari-sift-registration/actions)\n[![codecov](https://codecov.io/gh/jfozard/napari-sift-registration/branch/main/graph/badge.svg)](https://codecov.io/gh/jfozard/napari-sift-registration)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sift-registration)](https://napari-hub.org/plugins/napari-sift-registration)\n\nSimple plugin for 2D keypoint detection and affine registration with RANSAC.\n\n----------------------------------\n\n![moving image](test_data/test1.png)\n![fixed image](test_data/test2.png)\n\nArtificial data \n\n![moving image with inlier keypoints](doc/moving_keypoints.png)\n![fixed image with inlier keypoints](doc/fixed_keypoints.png)\n\nMoving and fixed images showing inlier keypoints after RANSAC\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\nIt uses the [scikit-image] SIFT keypoint detection routines to find distinctive image points and generate local descriptions of the image around them.\nCorrespondences between the two images are then found by looking for pairs of keypoints, one in each of the two images, with closely matching descriptors.\n\n\n\nFor typical images, many of these correspondences will be wrong. To reduce these false correspondences, the plugin applies the RANSAC algorithm. This randomly selects a small subset of the matching pairs of keypoints, estimates the affine transformation between this subset of keypoints, and then evaluates how many of the other pairs of keypoints also closely agree with this affine transformation (\"inliers\"). A large number of random samples are tested, and the transformation with the most inliers retained.\n\nThe plugin outputs two points layers, one for each image, containing all the corresponding (inlier) SIFT keypoints. It also uses the estimated affine transformation between the two images to deform the \"moving\" image layer onto the \"fixed\" image layer.\n\nThis approach is an attempt to provide similar functionality to the Stephan Saalfeld's Fiji \"Extract SIFT Correspondences\" plugin [extract], and more-or-less\njust provides a napari interface to the existing routines in scikit-image. There are great examples in the scikit-image documentation (e.g. [SIFT-example] and [RANSAC-example]) that can be used if you would like to use these routines in your own analysis scripts.\n\n\n## Installation\n\nYou can install `napari-sift-registration` via [pip]:\n\n    pip install napari-sift-registration\n\nTo install the latest development version :\n\n    pip install git+https://github.com/jfozard/napari-sift-registration.git\n\n## Usage\n\n### Basic usage\n\n- Load two 2D single channel images in Napari.\n- Select the menu item Plugins > napari-sift-registration\n- Select these two images as the \"Moving image layer\" and the \"Fixed image layer\". The moving image will be deformed by the transformation to look like the fixed image.\n- The remaining parameters are the default settings from scikit-image; try these default values first.\n\n### Advanced usage\n\nThe parameter values for SIFT feature detection, keypoint matching and RANSAC are accessible from the plugin gui. For further information about their use, see the appropriate scikit-image documentation:\n\nUpsampling before feature detection, maximum number of octaves, maximum number of scales in every octave, blur level of seed image, feature descriptor size, feature descriptor orientation bins: see [scikit-image-SIFT].\n\nClosest/next closest ratio: see [scikit-image-match_descriptors]\n\nMinimum number of points sampled for each RANSAC model, distance for points to be inliers in RANSAC model, maximum number of trials in RANSAC model: see [scikit-image-RANSAC]\n\nOnly show inlier keypoints: If checked, only show corresponding keypoints that are inliers after RANSAC. If unchecked, show all corresponding keypoints.\n\n### Limitations\n\nOnly 2D, single channel images (for now).\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-sift-registration\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[extract]: https://imagej.net/plugins/feature-extraction\n[scikit-image]: https://scikit-image.org/\n[SIFT-example]: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_sift.html\n[RANSAC-example]: https://scikit-image.org/docs/stable/auto_examples/transform/plot_matching.html\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[scikit-image-SIFT]: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.SIFT\n[scikit-image-match_descriptors]: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.match_descriptors\n[scikit-image-RANSAC]: https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.ransac\n\n[file an issue]: https://github.com/jfozard/napari-sift-registration/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Affine registration with SIFT keypoints"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nlm",
    "name": "napari-nlm",
    "display_name": "napari NLM",
    "version": "0.0.4",
    "created_at": "2022-07-25",
    "modified_at": "2022-07-26",
    "authors": [
      "Martin Weigert"
    ],
    "author_emails": [
      "marweigert@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-nlm/",
    "home_github": "https://github.com/maweigert/napari-nlm",
    "home_other": null,
    "summary": "NLM (non local means) denoising",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pyopencl (==2022.1.5)",
      "gputools",
      "scikit-image",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-nlm\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-nlm.svg)](https://github.com/maweigert/napari-nlm/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nlm.svg)](https://pypi.org/project/napari-nlm)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nlm.svg)](https://python.org)\n[![tests](https://github.com/maweigert/napari-nlm/workflows/tests/badge.svg)](https://github.com/maweigert/napari-nlm/actions)\n[![codecov](https://codecov.io/gh/maweigert/napari-nlm/branch/main/graph/badge.svg)](https://codecov.io/gh/maweigert/napari-nlm)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nlm)](https://napari-hub.org/plugins/napari-nlm)\n\n----------------------------------\n\n\nGPU accelerated non local means (NLM) denoising plugin for napari (WIP)\n\n* currently only supports single-channel 2D or 3D images\n* requires a OpenCL capable GPU\n\n![Screenshot](images/screenshot.jpg)\n\n\n## Installation\n\nYou can install `napari-nlm` via [pip]:\n\n    pip install napari-nlm\n\n## Usage\n\n1. Open example image `Open Sample > napari-nlm: noisy bricks`\n2. Adjust parameters \n   * `sigma`: denoising strength (the larger sigma, the greater the smoothing)\n   * `patch_radius`: size of local patches, 2 or 3 is a good default\n   * `search_radius`: size of search area around each pixel to find similar patches, 7-11 is a good default\n3. Denoise by pressing `run`\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-nlm\" is free and open source software\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "denoise_nlm"
    ],
    "contributions_sample_data": [
      "2D (noisy bricks)",
      "3D (noisy cells)"
    ]
  },
  {
    "normalized_name": "napari-trait2d",
    "name": "napari-trait2d",
    "display_name": "napari TRAIT2D",
    "version": "0.1.4",
    "created_at": "2022-07-09",
    "modified_at": "2022-07-21",
    "authors": [
      "Jacopo Abramo"
    ],
    "author_emails": [
      "jacopo.abramo@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-trait2d/",
    "home_github": "https://github.com/jacopoabramo/napari-trait2d",
    "home_other": null,
    "summary": "A napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "napari[pyqt5]",
      "dacite",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-trait2d\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-trait2d.svg?color=green)](https://github.com/jacopoabramo/napari-trait2d/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-trait2d.svg?color=green)](https://pypi.org/project/napari-trait2d)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-trait2d.svg?color=green)](https://python.org)\n[![tests](https://github.com/jacopoabramo/napari-trait2d/workflows/tests/badge.svg)](https://github.com/jacopoabramo/napari-trait2d/actions)\n[![codecov](https://codecov.io/gh/jacopoabramo/napari-trait2d/branch/main/graph/badge.svg)](https://codecov.io/gh/jacopoabramo/napari-trait2d)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-trait2d)](https://napari-hub.org/plugins/napari-trait2d)\n\nA napari plugin for TRAIT2D, a software for quantitative analysis of single particle diffusion data\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-trait2d` via [pip]:\n\n    pip install napari-trait2d\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jacopoabramo/napari-trait2d.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-trait2d\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jacopoabramo/napari-trait2d/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TRAIT2D tracker GUI"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-metroid",
    "name": "napari-metroid",
    "display_name": "napari METROID",
    "version": "0.0.5",
    "created_at": "2022-03-24",
    "modified_at": "2022-07-20",
    "authors": [
      "Marcelo Leomil Zoccoler"
    ],
    "author_emails": [
      "marcelo.zoccoler@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-metroid/",
    "home_github": "https://github.com/zoccoler/napari-metroid",
    "home_other": null,
    "summary": "This napari plugin creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).",
    "categories": [],
    "package_metadata_requires_python": "<3.9,>=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "scikit-learn",
      "scikit-image",
      "statsmodels",
      "scipy",
      "matplotlib",
      "napari-skimage-regionprops (>=0.3.1)"
    ],
    "package_metadata_description": "# napari-metroid\n\n[![License](https://img.shields.io/pypi/l/napari-metroid.svg?color=green)](https://github.com/zoccoler/napari-metroid/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-metroid.svg?color=green)](https://pypi.org/project/napari-metroid)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-metroid.svg?color=green)](https://python.org)\n[![tests](https://github.com/zoccoler/napari-metroid/workflows/tests/badge.svg)](https://github.com/zoccoler/napari-metroid/actions)\n[![codecov](https://codecov.io/gh/zoccoler/napari-metroid/branch/main/graph/badge.svg)](https://codecov.io/gh/zoccoler/napari-metroid)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-metroid)](https://napari-hub.org/plugins/napari-metroid)\n\nThis napari plugin is an adaptation of [metroid](https://github.com/zoccoler/metroid). It creates several regions of interest of similar area over cells in a fluorescence video (2D+time). It then gets ROIs means over time and performs signal denoising: fixes photobleaching and separates signal from noise by means of blind source separation (with or without wavelet filtering).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## A Picture (to boil down a thousand words)\n\nBelow is the graphical abstract of the Metroid software. This napari plugin works very similarly.\n\n![](https://github.com/zoccoler/metroid/blob/master/Metroid_flowchart.png)\n\n## Table of Contents\n\n- [Quick Walktrough](#quick-walkthrough)\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Open Sample Data](#open-sample-data)\n  - [Open Plugin Main Interface](#open-plugin-main-interface)\n  - [Auto-generate Cell Mask](#auto-generate-cell-mask)\n  - [Split Mask into ROIs](#split-mask-into-rois)\n  - [Get ROI Means over Time](#get-roi-means-over-time)\n  - [Remove Photobleaching](#remove-photobleaching)\n  - [Filter Signals](#filter-signals)\n  - [Save outputs](#save-outputs)\n- [Contributing](#contributing)\n- [Citing napari-metroid](#citing-napari-metroid)\n- [License](#license)\n- [Issues](#issues)\n\n## Quick Walkthrough\n\nBelow is a full demonstration of using napari-metroid. It shows the following:\n  * Open sample data;\n  * Create cell mask;\n  * Split mask into ROIs of similar area;\n  * Get ROIs signals over time and plots two of them;\n  * Remove photobleaching;\n  * Remove noise:\n    * Use ICA to decompose ROIs signals into independent components;\n    * Plot 4 components;\n    * Manually select the component of interest (source);\n    * Perform inverse transformation with selected source;\n        \n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/napari_metroid_demo.gif)\n\n## Installation\n\nDownload and install [Anaconda](https://www.anaconda.com/products/individual) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html#).\n\nCreate a new conda environment:\n\n    conda create -n metroid-env python=3.8\n\nInstall napari, e.g. via pip:\n\n    pip install \"napari[all]\"\n\nInstall `napari-metroid` via [pip]:\n\n    pip install napari-metroid\n\nTo install latest development version :\n\n    pip install git+https://github.com/zoccoler/napari-metroid.git\n\n## Usage\n### Open Sample Data\n\nThis plugin comes with two sample videos:\n- Cell1 Video Action Potential: 2D + time fluorescence video of a rat isolated cardiomyocyte labeled with a membrane potential dye upon which an external electrical field pulse is applied.\n- Cell1 Video Electroporation: Same cell, but submitted to a strong external electrical field pulse.\n\nYou can open them under \"File -> Open Sample -> napari-metroid\", as shown below. Both videos are loaded from the [metroid main repository](https://github.com/zoccoler/metroid). To know more about the experimental conditions, please refer to the [original publication](https://doi.org/10.1186/s12859-020-03661-9).\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/load_sample_data.gif)\n\n### Open Plugin Main Interface\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/open_plugin.gif)\n\n### Auto-generate Cell Mask\n\nMetroid can generate cell binary masks automatically by cumulative sum of images until any pixel saturation happens. It then applies Otsu thresholding and removes small objects.\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/auto_create_mask.png)\n\n### Split Mask into ROIs\n\nBy default, a cell mask is split into 32 regions of interest (ROIs) in a double-layer fashion: An outer layer of ROIs and an inner layer. \nThe method is solely based on the shape of the cell mask and the main criteria is that ROIs must have similar areas. The number of ROIs in each layer can be editted. \n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/mess.png)\n\n### Get ROI Means over Time\n\nThe 'Get Signals' button serves to collect each ROI mean fluorescence over time and enable plotting. There, you can optionally provide the frame rate so that the time axis is properly displayed.\nDouble click over a ROI to have its signal plotted. Hold the 'ALT' key to plot multiple signals together.\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/get_signals.gif)\n\n### Remove Photobleaching\n\nMetroid removes photobleaching by curve fitting over time periods that lack the cellular signal (which can be an action potential or an electroporation signal). That is why the 'Transitory' parameter is important. Action potentials are transitory signals whereas electroporation (at least for the duration of this experiment) are not, and the algorithm must be informed about that for proper trend removal.\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/remov_photob.gif)\n\n### Filter Signals\n\nCellular signals are filtered by separating signal components with either PCA or ICA (plus optional wavelet filtering). It then chooses one (or several) components and it applies the inverse transform using only the selected components. Metroid can do this component/source selection automatically based on estimations of signal power. Instead, we show below the manual selection procedure, where 4 components are plotted and the user selects one of them.\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/bssd.gif)\n\n### Save Outputs\n\nRaw, corrected and filtered signals, as well as time and components, are arranged in a table with values for each time point. The table is displayed as a widget after each Run button click. Estimated signal-to-noise (SNR) in dB for each label/ROI are also provided (in this case, each line corresponds to a ROI, not a time point).\nThe user can save these data by clicking on the buttons \"Copy to clipboard\" or \"Save as csv...\".\n\n![](https://github.com/zoccoler/napari-metroid/raw/main/figures/table_widget.png)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Citing napari-metroid\n\nIf you use this plugin in your research, please be kind to cite the original paper below:\n\nZoccoler, M., de Oliveira, P.X. METROID: an automated method for robust quantification of subcellular fluorescence events at low SNR. BMC Bioinformatics 21, 332 (2020). https://doi.org/10.1186/s12859-020-03661-9\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-metroid\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/zoccoler/napari-metroid/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Main Interface"
    ],
    "contributions_sample_data": [
      "Cell1 Video_Action_Potential",
      "Cell1 Video_Electroporation"
    ]
  },
  {
    "normalized_name": "napari-nasa-samples",
    "name": "napari-nasa-samples",
    "display_name": "NASA sample images",
    "version": "0.0.5",
    "created_at": "2022-07-12",
    "modified_at": "2022-07-16",
    "authors": [
      "Loic A. Royer"
    ],
    "author_emails": [
      "royerloic@gmail.com"
    ],
    "license": "MPL-2.0",
    "home_pypi": "https://pypi.org/project/napari-nasa-samples/",
    "home_github": "https://github.com/royerloic/napari-nasa-samples",
    "home_other": null,
    "summary": "This napari plugin provides sample datasets from NASA.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "requests",
      "pillow",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-nasa-samples\n\n[![License Mozilla Public License 2.0](https://img.shields.io/pypi/l/napari-nasa-samples.svg?color=green)](https://github.com/royerlab/napari-nasa-samples/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nasa-samples.svg?color=green)](https://pypi.org/project/napari-nasa-samples)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nasa-samples.svg?color=green)](https://python.org)\n[![tests](https://github.com/royerloic/napari-nasa-samples/workflows/tests/badge.svg)](https://github.com/royerlab/napari-nasa-samples/actions)\n[![codecov](https://codecov.io/gh/royerloic/napari-nasa-samples/branch/main/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-nasa-samples)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-nasa-samples)](https://napari-hub.org/plugins/napari-nasa-samples)\n\nThis napari plugin written [by Loic A. Royer](https://twitter.com/loicaroyer) provides sample datasets from NASA.\nIn particular, you can access directly from napari the recently released images for the [James Webb Space Telescope](https://webb.nasa.gov/), as well as\nsome of the classic and most beautiful images obtained by the venerable and still strong [Hubble Space Telescope](https://hubblesite.org/). \nMore images will be added over time.\n\nThanks to (NASA)[https://www.nasa.gov/] for releasing these incredible images!\n\n![](https://github.com/royerloic/napari-nasa-samples/raw/main/docs/images/teaser.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-nasa-samples` via [pip]:\n\n    pip install napari-nasa-samples\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/royerloic/napari-nasa-samples.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Mozilla Public License 2.0] license,\n\"napari-nasa-samples\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/royerlab/napari-nasa-samples/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "Messier 101, Hubble",
      "Pillars of Creation, Hubble",
      "Large Magellanic Cloud, Hubble",
      "Cosmic Cliffs, Carina Nebula, NIRCam Image, JWST",
      "Deep Field SMACS 0723, NIRCam Image, JWST",
      "Stephan's Quintet NIRCam and MIRI, JWST",
      "Southern Ring Nebula NIRCam Image, JWST"
    ]
  },
  {
    "normalized_name": "napari-pdr-reader",
    "name": "napari-pdr-reader",
    "display_name": "PDS reader plugin for Napari",
    "version": "0.0.1",
    "created_at": "2022-07-14",
    "modified_at": "2022-07-14",
    "authors": [
      "Dr. Andrew Annex"
    ],
    "author_emails": [
      "ama6fy@virginia.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-pdr-reader/",
    "home_github": "https://github.com/AndrewAnnex/napari-pdr-reader",
    "home_other": null,
    "summary": "A reader plugin for Napari for PDS data powered by the PDR library",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "astropy",
      "dustgoggles",
      "napari",
      "numpy",
      "pandas",
      "pdr",
      "pds4-tools",
      "pillow",
      "pvl",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-pdr-reader\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-pdr-reader.svg?color=green)](https://github.com/AndrewAnnex/napari-pdr-reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pdr-reader.svg?color=green)](https://pypi.org/project/napari-pdr-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pdr-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/AndrewAnnex/napari-pdr-reader/workflows/tests/badge.svg)](https://github.com/AndrewAnnex/napari-pdr-reader/actions)\n[![codecov](https://codecov.io/gh/AndrewAnnex/napari-pdr-reader/branch/main/graph/badge.svg)](https://codecov.io/gh/AndrewAnnex/napari-pdr-reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pdr-reader)](https://napari-hub.org/plugins/napari-pdr-reader)\n\nA reader plugin for Napari for PDS data powered by the PDR library\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-pdr-reader` via [pip]:\n\n    pip install napari-pdr-reader\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/AndrewAnnex/napari-pdr-reader.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-pdr-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/AndrewAnnex/napari-pdr-reader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.fits",
      "*.img",
      "*.IMG",
      "*.LBL",
      "*.lbl",
      "*.FITS"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "sample m2020 data for napari pdr plugin"
    ]
  },
  {
    "normalized_name": "napari-bio-sample-data",
    "name": "napari-bio-sample-data",
    "display_name": "napari bio sample data",
    "version": "0.0.4",
    "created_at": "2022-07-12",
    "modified_at": "2022-07-13",
    "authors": [
      "Chi-Li Chiu"
    ],
    "author_emails": [
      "cchiu@chanzuckerberg.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-bio-sample-data/",
    "home_github": "https://github.com/chili-chiu/napari-bio-sample-data",
    "home_other": null,
    "summary": "a sample data plugin for bio-related demos",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "fsspec",
      "zarr (>=2.12.0)",
      "dask",
      "s3fs",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-bio-sample-data\n\n[![License](https://img.shields.io/pypi/l/napari-bio-sample-data.svg?color=green)](https://github.com/chili-chiu/napari-bio-sample-data/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bio-sample-data.svg?color=green)](https://pypi.org/project/napari-bio-sample-data)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bio-sample-data.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bio-sample-data)](https://napari-hub.org/plugins/napari-bio-sample-data)\n\na sample data plugin for bio-related demos\n\n----------------------------------\nThis plugin contains 5 sample datasets with additional napari layer types:\n\n(1) 3D EM dataset (image + points + vectors)  \nImage credit: Alister Burt  \nThe [original data](https://github.com/alisterburt/napari-cryo-et-demo) is down-sampled to have smaller file size.  \n<img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569428-7daa2eb8-a3ff-4c0e-8e5f-4f615a55684f.png\">\n\n(2) 2D skin RGB dataset (image + shape)  \nImage credit: skimage.data.skin  \n<img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569580-bf77e55c-71cc-4883-9fe5-ed94e05f2a29.png\">\n  \n(3) 3D nuclei dataset (image + label + surface)  \nImage credit: skimage.data.cells3d  \n<img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569701-7c9b1cc3-c1c3-4e54-8ca0-fb2b530f858e.png\">\n\n(4) 2D timelapse dataset (image + points + tracks)  \nImage credit: [Cell Tracking Challenge](http://celltrackingchallenge.net/2d-datasets/)  \nThe original data is cropped to have smaller file size.  \n<img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178569846-b995d1cb-c1ec-4363-ba1a-71243ffea4e0.png\">\n\n(5) large multi-resolution 3D EM dataset  \nImage credit: [Janelia Open Organelle](https://openorganelle.janelia.org/datasets/jrc_hela-1)   \nThis plugin only accesses 2 lower resolution levels.  \n<img width=\"300\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89602983/178570136-6f59ba3c-d687-446c-9f5e-1df567a62948.png\">\n\nDatasets (1)-(4) are stored locally.   \nDataset (5) is downloaded and temporarily stored on RAM when accessed.    \n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-bio-sample-data` via [pip]:\n\n    pip install napari-bio-sample-data\n\nTo install latest development version :\n\n    pip install git+https://github.com/chili-chiu/napari-bio-sample-data.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bio-sample-data\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/chili-chiu/napari-bio-sample-data/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": [
      "3D cryo-tomography",
      "2D skin HE",
      "3D nuclei",
      "2D cell timelapse",
      "large multi-resolution 3D EM"
    ]
  },
  {
    "normalized_name": "napari-vesicles-segmentation",
    "name": "napari-vesicles-segmentation",
    "display_name": "Vesicles Segmentation",
    "version": "0.0.1",
    "created_at": "2022-07-08",
    "modified_at": "2022-07-08",
    "authors": [
      "Alexis Japas"
    ],
    "author_emails": [
      "alexis.japas@proton.me"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-vesicles-segmentation/",
    "home_github": "https://github.com/alexisjapas/napari-vesicles-segmentation",
    "home_other": null,
    "summary": "A simple plugin to detect vesicles in cells images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "scikit-image",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "scikit-image ; extra == 'testing'",
      "scipy ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-vesicles-segmentation\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-vesicles-segmentation.svg?color=green)](https://github.com/alexisjapas/napari-vesicles-segmentation/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-vesicles-segmentation.svg?color=green)](https://pypi.org/project/napari-vesicles-segmentation)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-vesicles-segmentation.svg?color=green)](https://python.org)\n[![tests](https://github.com/alexisjapas/napari-vesicles-segmentation/workflows/tests/badge.svg)](https://github.com/alexisjapas/napari-vesicles-segmentation/actions)\n[![codecov](https://codecov.io/gh/alexisjapas/napari-vesicles-segmentation/branch/main/graph/badge.svg)](https://codecov.io/gh/alexisjapas/napari-vesicles-segmentation)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-vesicles-segmentation)](https://napari-hub.org/plugins/napari-vesicles-segmentation)\n\nA simple plugin to detect vesicles in cells images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-vesicles-segmentation` via [pip]:\n\n    pip install napari-vesicles-segmentation\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/alexisjapas/napari-vesicles-segmentation.git\n\n## Usage\n1. Open napari\n2. Open your data\n![usage-open-data](images/usage-open-data.png)\n3. Launch the vesicles-segmentation plugin\n4. Select the data you want to segment and set the parameters of the segmentation\n![usage-setup](images/usage-setup.png)\n    * **image**: The image to segment vesicles in. The image can be a 2D or 3D temporal stack of images.\n    * **minimum vesicles size**: The minimum size of the vesicles to detect. Smaller detected vesicles are removed.\n    * **membrane erosion**: The size of the disk radius used for eroding the cell. This is used to remove the external membrane. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.\n    * **closing size**: The size of the disk radius used for closing the cell. This is used to fill holes in the cell. This parameter scales when downsizing the image, for more information see 'downsizing ratio' parameter.\n    * **clip**: If set to zero, no standardization is performed. Otherwise, the standard deviation of the image is set to n_sigma * the standard deviation of the image, the image is standardized and its values are clipped to the range [-1, 1] in order to remove outliers. The higher the value of n_sigma, the less outliers are removed. This operation can lead to a better detection of the cell.\n    * **downsampling ratio**: The downsampling ratio used for the downsampled image. This is used to speed up the computation. Downsampling the image have impact in reducing the resolution of erosion and closing e.g. for a downsize ratio of 2, setting the erosion size to 3 will result in an erosion size of 6.\n    * **display cell detection**: If set to True, the cell detection is displayed in the viewer instead of the vesicle detection.\n5. Click on the \"Segment\" button to start the segmentation. This can take few seconds or minutes depending on the size of the data. The result is added to the viewer as below.\n![usage-segmentation](images/usage-segmentation.png)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-vesicles-segmentation\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/alexisjapas/napari-vesicles-segmentation/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Vesicles Segmentation"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bioimageio",
    "name": "napari-bioimageio",
    "display_name": "BioImage.IO Model Manager",
    "version": "0.1.3",
    "created_at": "2022-07-05",
    "modified_at": "2022-07-07",
    "authors": [],
    "author_emails": [],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/napari-bioimageio/",
    "home_github": null,
    "home_other": "None",
    "summary": null,
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "bioimageio.core (>=0.5.1)",
      "PyYAML (>=6.0)"
    ],
    "package_metadata_description": "# napari-bioimageio\n\nnapari plugin for managing AI models in the [BioImage Model Zoo](https://bioimage.io).\n\n> **WARNING**: This is an alpha release. The API may change in future versions, and please feel free to create issues to report bugs or provide feedbacks.\n\n![](assets/screenshot-model-manager-1.png)\n\n## Installation\n\n```\npip install napari-bioimageio\n```\n\n(If you don't have napari installed, run `pip install napari[pyqt5]`)\n\n## Usage\n\nThis library is meant for helping developers to ease the handling of models in napari.\n\nWe provide a set of API functions for managing and selecting models.\n### `show_model_manager()`\nShow the model manager with a model list pulled from the BioImage Model Zoo, the user can explore all the available models, download or remove models.\n\n### `show_model_selector(filter=None)`\nDisplay a dialog for selecting models from the BioImage Model Zoo, the user can either select an existing model or download from the BioImage Model Zoo.\n\nThe selecte model information (a dictionary) will be returned if the user selected a model, otherwise it returns `None`.\n\nOnce the user selected the model, you can access the name, and also the file path to the model resource description file (via the `rdf_source` key). With the `bioimageio.core` library (installed via `pip install bioimageio.core` or `conda install -c conda-forge bioimageio.core`), you can run inference directly, the following examples shows how to implement it:\n\n```python\n# Popup a model selection dialog for choosing the model\nmodel_info = show_model_selector(filter=nuclear_segmentation_model_filter)\n\nif model_info:\n  self.nucseg_model_source = model_info[\"rdf_source\"]\n  # Load model \n  model_description = bioimageio.core.load_resource_description(model_info[\"rdf_source\"])\n  input_image = imageio.imread(\"./my-image.tif\")\n\n  with bioimageio.core.create_prediction_pipeline(\n      bioimageio_model=model_description\n  ) as pipeline:\n    output_image = bioimageio.core.prediction.predict_with_padding(\n        pipeline, input_image, padding=padding\n    )\n```\nNote: To run the models, you need to setup the conda environment properly according to the [installation guide of bioimageio.core](https://github.com/bioimage-io/core-bioimage-io-python#installation).\n\nFor more examples, see [this example notebook](https://github.com/bioimage-io/core-bioimage-io-python/blob/main/example/bioimageio-core-usage.ipynb) for `bioimageio.core`.\n\nYou can also access the weight files directly by searching the model folder (e.g. extract the model folder path via `os.path.dirname(model_description[\"rdf_source\"])`), this will be useful if you prefer to use your own model inference logic.\n### `show_model_uploader()`\nDisplay a dialog to instruct the user to upload a model package to the BioImage Model Zoo.\nCurrently, it only shows a message, in the future, we will try to support direct uploading with user's credentials obtained from Zenodo (a public data repository used by the BioImage Model Zoo to store models).\n\nTo create a BioImageIO-compatible model package, you can use the `build_model` function as demonstrated in [this notebook]((https://github.com/bioimage-io/core-bioimage-io-python/blob/main/example/bioimageio-core-usage.ipynb)).\n\n## Development\n\n- Install and set up development environment.\n\n  ```sh\n  pip install -r requirements_dev.txt\n  ```\n\n  This will install all requirements.\nIt will also install this package in development mode, so that code changes are applied immediately without reinstall necessary.\n\n- Here's a list of development tools we use.\n  - [black](https://pypi.org/project/black/)\n  - [flake8](https://pypi.org/project/flake8/)\n  - [mypy](https://pypi.org/project/mypy/)\n  - [pydocstyle](https://pypi.org/project/pydocstyle/)\n  - [pylint](https://pypi.org/project/pylint/)\n  - [pytest](https://pypi.org/project/pytest/)\n  - [tox](https://pypi.org/project/tox/)\n- It's recommended to use the corresponding code formatter and linters also in your code editor to get instant feedback. A popular editor that can do this is [`vscode`](https://code.visualstudio.com/).\n- Run all tests, check formatting and linting.\n\n  ```sh\n  tox\n  ```\n\n- Run a single tox environment.\n\n  ```sh\n  tox -e lint\n  ```\n\n- Reinstall all tox environments.\n\n  ```sh\n  tox -r\n  ```\n\n- Run pytest and all tests.\n\n  ```sh\n  pytest\n  ```\n\n- Run pytest and calculate coverage for the package.\n\n  ```sh\n  pytest --cov-report term-missing --cov=napari-bioimageio\n  ```\n\n- Continuous integration is by default supported via [GitHub actions](https://help.github.com/en/actions). GitHub actions is free for public repositories and comes with 2000 free Ubuntu build minutes per month for private repositories.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BioImage.IO Model Manager"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-deepmeta",
    "name": "napari-deepmeta",
    "display_name": "napari DeepMeta",
    "version": "2.1",
    "created_at": "2021-06-02",
    "modified_at": "2022-07-07",
    "authors": [
      "Edgar Lefevre"
    ],
    "author_emails": [
      "lefevreedg@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-deepmeta/",
    "home_github": "https://github.com/EdgarLefevre/napari-deepmeta",
    "home_other": null,
    "summary": "Mice lungs and metastases segmentation tool.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "connected-components-3d",
      "magicgui",
      "napari",
      "numpy",
      "opencv-python",
      "qtpy",
      "scikit-image",
      "torch",
      "connected-components-3d ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "opencv-python ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-deepmeta\n\n[![License MIT](https://img.shields.io/pypi/l/napari-deepmeta.svg?color=green)](https://github.com/EdgarLefevre/napari-deepmeta/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-deepmeta.svg?color=green)](https://pypi.org/project/napari-deepmeta)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-deepmeta.svg?color=green)](https://python.org)\n[![tests](https://github.com/EdgarLefevre/napari-deepmeta/workflows/tests/badge.svg)](https://github.com/EdgarLefevre/napari-deepmeta/actions)\n[![codecov](https://codecov.io/gh/EdgarLefevre/napari-deepmeta/branch/main/graph/badge.svg)](https://codecov.io/gh/EdgarLefevre/napari-deepmeta)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deepmeta)](https://napari-hub.org/plugins/napari-deepmeta)\n\nMice lungs and metastases segmentation tool.\nThis tool is a demo tool for DeepMeta network.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-deepmeta` via [pip]:\n\n    pip install napari-deepmeta\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/EdgarLefevre/napari-deepmeta.git\n\n\n## Usage\n\nThis plugin is designed to process your mouse MRI images with our dataset. It comes with a demo, including one of our\ntest images.\n\nBy opening the deepmeta demo plugin, you will see an interface with one unique button, by clicking on it, it will load an image,\nrun prediction and then draw the masks contours on each slice.\n\nIf you open the deepmeta plugin, you will see an interface with one button and 3 checkboxes.\nBy checking the checkboxes, you add steps to the pipeline (enhance contrast, do postprocessing, segment metastases).\nOnce everything is setup, just click on the button and wait (the waiting time depends on your computer performance.)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-deepmeta\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/EdgarLefevre/napari-deepmeta/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "DeepmetaWidget",
      "DeepmetaDemoWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sdeconv",
    "name": "napari-sdeconv",
    "display_name": "napari sdeconv",
    "version": "1.0.1",
    "created_at": "2021-09-02",
    "modified_at": "2022-07-06",
    "authors": [
      "Sylvain Prigent"
    ],
    "author_emails": [
      "meriadec.prigent@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-sdeconv/",
    "home_github": "https://github.com/sylvainprigent/napari-sdeconv",
    "home_other": null,
    "summary": "2D and 3D deconvolution",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "sdeconv (>=1.0.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "sdeconv (>=1.0.1) ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-sdeconv\n\n[![License BSD-3](https://img.shields.io/pypi/l/napari-sdeconv.svg?color=green)](https://github.com/sylvainprigent/napari-sdeconv/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sdeconv.svg?color=green)](https://pypi.org/project/napari-sdeconv)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sdeconv.svg?color=green)](https://python.org)\n[![tests](https://github.com/sylvainprigent/napari-sdeconv/workflows/tests/badge.svg)](https://github.com/sylvainprigent/napari-sdeconv/actions)\n[![codecov](https://codecov.io/gh/sylvainprigent/napari-sdeconv/branch/main/graph/badge.svg)](https://codecov.io/gh/sylvainprigent/napari-sdeconv)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sdeconv)](https://napari-hub.org/plugins/napari-sdeconv)\n\n2D and 3D deconvolution\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-sdeconv` via [pip]:\n\n    pip install napari-sdeconv\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/sylvainprigent/napari-sdeconv.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-sdeconv\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/sylvainprigent/napari-sdeconv/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PSF Gaussian",
      "PSF Gibson-Lanni",
      "Wiener deconvolution",
      "Richardson-Lucy deconvolution",
      "Spitfire deconvolution"
    ],
    "contributions_sample_data": [
      "napari sdeconv"
    ]
  },
  {
    "normalized_name": "napari-picasso",
    "name": "napari-PICASSO",
    "display_name": "napari-PICASSO",
    "version": "0.3.0",
    "created_at": "2022-06-01",
    "modified_at": "2022-06-29",
    "authors": [
      "Kunal Pandit"
    ],
    "author_emails": [
      "kpandit@nygenome.org"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-picasso/",
    "home_github": null,
    "home_other": null,
    "summary": "Blind fluorescence unmixing",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "dask",
      "psutil",
      "tox ; extra == 'testing'",
      "napari[all] ; extra == 'testing'",
      "torch ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "xarray ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-PICASSO\n\n[![License](https://img.shields.io/pypi/l/napari-curtain.svg?color=green)](https://github.com/nygctech/PICASSO/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-PICASSO.svg?color=green)](https://pypi.org/project/napari-PICASSO)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-PICASSO.svg?color=green)](https://python.org)\n[![tests](https://github.com/nygctech/PICASSO/actions/workflows/test_and_deploy.yml/badge.svg?event=push)](https://github.com/nygctech/PICASSO/actions/workflows/test_and_deploy.yml)\n[![codecov](https://codecov.io/gh/nygctech/napari-PICASSO/branch/main/graph/badge.svg)](https://codecov.io/gh/nygctech/napari-PICASSO)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-PICASSO)](https://napari-hub.org/plugins/napari-PICASSO)\n\nUnmix spectral spillover\n\n![](https://user-images.githubusercontent.com/72306584/176486552-50e1bca9-65fd-4466-8c92-a114e48d2278.gif)\n\n## Automatic Usage\n\nYou can find the `PICASSO` plugin in the menu `Plugins > napari-PICASSO: PICASSO`. Select sink images that have spectral spillover from corresponding source images, then click run to optimise the mixing parameters with PICASSO. \n\n## Manual Usage\n\n![](https://user-images.githubusercontent.com/72306584/176505151-572bd762-abe6-47b1-9821-4f3aaa4704c9.gif)\n\nSelect the manual button in options pop up window. Then select sink images that have spectral spillover from corresponding source images. In the source images window, sliders for each $source$ control the mixing spillover, $m$ (top), and background, $b$ (bottom, optional).\n\n## Mixing model\n\n$$ sink = \\sum_{i} m_i(source - b_i) $$\n\n## Installation\n\nYou can install `napari-PICASSO` via [pip]:\n\n    pip install napari-PICASSO\n\n## Details\n\nnapari-PICASSO is a napari widget to blindly unmix fluorescence images of known members using PICASSO<sup>1</sup>. \n\nFor example, if 2 fluorophores with overlapping spectra are imaged, spillover fluorescesce from a channel into an adjacent channel could be removed if you know which channel is the source of the spillover fluorescence and which channel is the sink of the spillover fluorescence. \n\nPICASSO is an algorithm to remove spillover fluorescence by minimizing the mutual information between sink and source images. The original algorithm described by Seo et al, minimized the mutual information between pairs of sink and source images using a Nelson-Mead simplex algorithm and computing the mutual information outright with custom written MATLAB code<sup>1</sup>. The napari plugin uses a neural net to estimate and minimize the mutual information (MINE<sup>2</sup>) between pairs of sink and source images using stochastic gradient descent with GPU acceleration.\n\n## References\n\n1. Seo, J. et al. PICASSO allows ultra-multiplexed fluorescence imaging of spatially overlapping proteins without reference spectra measurements. Nat Commun 13, 2475 (2022).\n2. Belghazi, M. I. et al. MINE: Mutual Information Neural Estimation. arXiv:1801.04062 [cs, stat] (2018).\n\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PICASSO"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-dexp",
    "name": "napari-dexp",
    "display_name": "DEXP",
    "version": "0.0.7",
    "created_at": "2021-07-01",
    "modified_at": "2022-06-23",
    "authors": [
      "Jordao Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-dexp/",
    "home_github": "https://github.com/royerlab/napari-dexp",
    "home_other": null,
    "summary": "A simple plugin to use with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "dexp",
      "numpy"
    ],
    "package_metadata_description": "# napari-DEXP\n\n[![License](https://img.shields.io/pypi/l/napari-dexp.svg?color=green)](https://github.com/royerlab/napari-dexp/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-dexp.svg?color=green)](https://pypi.org/project/napari-dexp)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dexp.svg?color=green)](https://python.org)\n[![tests](https://github.com/royerlab/napari-dexp/workflows/tests/badge.svg)](https://github.com/royerlab/napari-dexp/actions)\n[![codecov](https://codecov.io/gh/royerlab/napari-dexp/branch/master/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-dexp)\n\nA plugin to interface [DEXP](https://github.com/royerlab/dexp) with [napari](https://github.com/napari/napari).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-dexp` via [pip]:\n\n    pip install napari-dexp\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-dexp\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/royerlab/napari-dexp/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.zarr",
      "*.zarr.zip"
    ],
    "contributions_writers_filename_extensions": [
      ".zarr"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-labels-overlap",
    "name": "napari-labels-overlap",
    "display_name": "napari labels overlap",
    "version": "0.0.3",
    "created_at": "2021-11-30",
    "modified_at": "2022-06-22",
    "authors": [
      "Chi-Li Chiu"
    ],
    "author_emails": [
      "cchiu@chanzuckerberg.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-labels-overlap/",
    "home_github": "https://github.com/chili-chiu/napari-labels-overlap",
    "home_other": null,
    "summary": "create an overlap labels layer from two labels layers",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "scikit-image"
    ],
    "package_metadata_description": "# napari-labels-overlap\n\n[![License](https://img.shields.io/pypi/l/napari-labels-overlap.svg?color=green)](https://github.com/chili-chiu/napari-labels-overlap/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-labels-overlap.svg?color=green)](https://pypi.org/project/napari-labels-overlap)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labels-overlap.svg?color=green)](https://python.org)\n[![tests](https://github.com/chili-chiu/napari-labels-overlap/workflows/tests/badge.svg)](https://github.com/chili-chiu/napari-labels-overlap/actions)\n[![codecov](https://codecov.io/gh/chili-chiu/napari-labels-overlap/branch/main/graph/badge.svg)](https://codecov.io/gh/chili-chiu/napari-labels-overlap)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labels-overlap)](https://napari-hub.org/plugins/napari-labels-overlap)\n\ncreate an overlap labels layer from two labels layers\n\n## Description\n\nThis plugin takes two labels layers (layerA, layerB) as inputs, and generate the overlapped regions as a binary labels layer.\nThree modes:<br>\n(1) A_OR_B: new layer = layerA OR layerB (union)<br>\n(2) A_AND_B: new layer = layerA AND layerB (intersection)<br>\n(3) A_OUTSIDE_B: new layer = layerA OUTSIDE layerB (complement)<br>\n\n[comment]: <need to update the gif>\n\n![labels_overlap](https://user-images.githubusercontent.com/89602983/144129087-9a88d55f-f1a0-4825-bd01-770909bfc64f.gif)\n\n## Applicaions\n- Object colocalization\n- Merge separately identified objects\n\n## Future work\n- Support N labels layers\n- Basic coloc stats (% volume overlap)\n- Output Labels with distinct IDs and links to original label IDs\n\n## Release log\n- 0.0.2<br>\n-- Run on npe2<br>\n-- Add output types: binary/connected component<br>\n- 0.0.1<br>\n-- Run on npe1<br>\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-labels-overlap` via [pip]:\n\n    pip install napari-labels-overlap\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/chili-chiu/napari-labels-overlap.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-labels-overlap\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/chili-chiu/napari-labels-overlap/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "labels overlap"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "beetlesafari",
    "name": "beetlesafari",
    "display_name": "beetlesafari",
    "version": "0.4.0",
    "created_at": "2022-04-21",
    "modified_at": "2022-06-21",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/beetlesafari/",
    "home_github": "https://github.com/haesleinhuepf/beetlesafari",
    "home_other": null,
    "summary": "A napari plugin for loading and working with light sheet imaging data of developing embryos acquired using ClearControl, e.g. _Tribolium castaneum_.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "pyopencl",
      "toolz",
      "scikit-image",
      "requests",
      "pyclesperanto-prototype",
      "napari",
      "magicgui",
      "dask",
      "cachetools",
      "napari-tools-menu"
    ],
    "package_metadata_description": "A library for working with light sheet imaging data of developing embryos acquired using [ClearControl](https://github.com/ClearControl) at the [Center for Systems Biology Dresden](https://www.csbdresden.de/), e.g. _Tribolium castaneum_.\n\n# Installation\n```\nconda install -c conda-forge pyopencl\npip install beetlesafari\n```\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-error-reporter",
    "name": "napari-error-reporter",
    "display_name": "Napari Error Reporter",
    "version": "0.3.1",
    "created_at": "2022-02-13",
    "modified_at": "2022-06-21",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "talley.lambert@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-error-reporter/",
    "home_github": "https://github.com/tlambert03/napari-error-reporter",
    "home_other": null,
    "summary": "Opt-in automated bug/error reporting for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "appdirs",
      "qtpy",
      "sentry-sdk",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "ipython ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "jedi (<0.18.0) ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pydocstyle ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'",
      "tox-conda ; extra == 'testing'"
    ],
    "package_metadata_description": "# üêõ napari-error-reporter\n\n[![License](https://img.shields.io/pypi/l/napari-error-reporter.svg?color=green)](https://github.com/tlambert03/napari-error-reporter/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-error-reporter.svg?color=green)](https://pypi.org/project/napari-error-reporter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-error-reporter.svg?color=green)](https://python.org)\n[![CI](https://github.com/tlambert03/napari-error-reporter/actions/workflows/ci.yml/badge.svg)](https://github.com/tlambert03/napari-error-reporter/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/tlambert03/napari-error-reporter/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-error-reporter)\n\nWant to help out napari?  Install this plugin!\n\nThis plugin will automatically send error reports to napari (via\n[sentry.io](https://sentry.io)) whenever an exception occurs while you are using\nnapari.\n\nThe first time you run napari after installing this plugin an opt-in\nnotification will appear (Be sure to click \"yes\", otherwise no reports will be\ncollected or sent).  You may opt back out at any time in napari's help menu.\n\nEvery effort is made to strip these reports of personally identifiable\ninformation.  Here is an example exception event:\n\n<details>\n\n<summary>Example bug report</summary>\n\n```python\n{\n    'breadcrumbs': {\n        'values': [\n            {\n                'category': 'subprocess',\n                'data': {},\n                'message': 'sw_vers -productVersion',\n                'timestamp': '2022-02-02T01:30:00.216738Z',\n                'type': 'subprocess'\n            }\n        ]\n    },\n    'contexts': {\n        'runtime': {\n            'build': '3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:37) \\n[Clang 11.1.0 ]',\n            'name': 'CPython',\n            'version': '3.9.9'\n        }\n    },\n    'environment': 'macOS-10.15.7-x86_64-i386-64bit',\n    'event_id': '02dd8ddd3a4b4743af3d7d7a09949df4',\n    'exception': {\n        'values': [\n            {\n                'mechanism': None,\n                'module': None,\n                'stacktrace': {\n                    'frames': [\n                        {\n                            'context_line': '                x = 1 / 0',\n                            'filename': 'napari_error_reporter/_util.py',\n                            'function': 'get_sample_event',\n                            'in_app': True,\n                            'lineno': 130,\n                            'module': 'napari_error_reporter._util',\n                            'post_context': [\n                                '            except Exception:',\n                                '                with sentry_sdk.push_scope() as scope:',\n                                '                    for k, v in _get_tags().items():',\n                                '                        scope.set_tag(k, v)',\n                                '                    del v, k, scope'\n                            ],\n                            'pre_context': [\n                                \"            # remove locals that wouldn't really be there\",\n                                '            del settings, _trans, kwargs, client, EVENT',\n                                '            try:',\n                                '                some_variable = 1',\n                                '                another_variable = \"my_string\"'\n                            ]\n                        }\n                    ]\n                },\n                'type': 'ZeroDivisionError',\n                'value': 'division by zero'\n            }\n        ]\n    },\n    'extra': {'sys.argv': ['napari']},\n    'level': 'error',\n    'modules': {\n        'aicsimageio': '4.5.2',\n        'aicspylibczi': '3.0.4',\n        'aiohttp': '3.8.1',\n        'aiosignal': '1.2.0',\n        'alabaster': '0.7.12',\n        'anyio': '3.5.0',\n        'appdirs': '1.4.4',\n        'appnope': '0.1.2',\n        'argon2-cffi': '21.3.0',\n        'argon2-cffi-bindings': '21.2.0',\n        'arrow': '1.2.1',\n        'asciitree': '0.3.3',\n        'asttokens': '2.0.5',\n        'async-timeout': '4.0.2',\n        'atomium': '1.0.11',\n        'attrs': '21.4.0',\n        'autopep8': '1.6.0',\n        'babel': '2.9.1',\n        'backcall': '0.2.0',\n        'bcrypt': '3.2.0',\n        'beautifulsoup4': '4.10.0',\n        'binaryornot': '0.4.4',\n        'black': '20.8b1',\n        'bleach': '4.1.0',\n        'bracex': '2.2.1',\n        'build': '0.7.0',\n        'cachey': '0.2.1',\n        'cellpose': '0.6.5',\n        'certifi': '2021.10.8',\n        'cffi': '1.15.0',\n        'cfgv': '3.3.1',\n        'chardet': '4.0.0',\n        'charset-normalizer': '2.0.10',\n        'check-manifest': '0.47',\n        'click': '7.1.2',\n        'click-option-group': '0.5.3',\n        'cloudpickle': '2.0.0',\n        'colorama': '0.4.4',\n        'commonmark': '0.9.1',\n        'cookiecutter': '1.7.3',\n        'coverage': '6.2',\n        'cryptography': '36.0.1',\n        'cycler': '0.11.0',\n        'dask': '2022.1.0',\n        'debugpy': '1.5.1',\n        'decorator': '5.1.1',\n        'defusedxml': '0.7.1',\n        'distlib': '0.3.4',\n        'dnspython': '2.2.0',\n        'docstring-parser': '0.13',\n        'docutils': '0.16',\n        'elementpath': '2.4.0',\n        'email-validator': '1.1.3',\n        'entrypoints': '0.3',\n        'executing': '0.8.2',\n        'fancycompleter': '0.9.1',\n        'fasteners': '0.17.2',\n        'fastremap': '1.12.2',\n        'filelock': '3.4.2',\n        'flake8': '3.8.4',\n        'fonttools': '4.28.5',\n        'freetype-py': '2.2.0',\n        'frozenlist': '1.3.0',\n        'fsspec': '2022.1.0',\n        'furo': '2022.1.2',\n        'gitdb': '4.0.9',\n        'gitpython': '3.1.26',\n        'greenlet': '1.1.2',\n        'heapdict': '1.0.1',\n        'hsluv': '5.0.2',\n        'hypothesis': '6.35.1',\n        'identify': '2.4.4',\n        'idna': '3.3',\n        'imagecodecs': '2021.11.20',\n        'imageio': '2.10.5',\n        'imageio-ffmpeg': '0.4.5',\n        'imagesize': '1.3.0',\n        'importlib-metadata': '4.10.1',\n        'iniconfig': '1.1.1',\n        'install': '1.3.5',\n        'intervaltree': '3.1.0',\n        'ipykernel': '6.7.0',\n        'ipython': '8.0.0',\n        'ipython-genutils': '0.2.0',\n        'ipywidgets': '7.6.5',\n        'jedi': '0.18.1',\n        'jinja2': '3.0.3',\n        'jinja2-time': '0.2.0',\n        'jsonschema': '3.2.0',\n        'jupyter': '1.0.0',\n        'jupyter-book': '0.12.1',\n        'jupyter-cache': '0.4.3',\n        'jupyter-client': '7.1.1',\n        'jupyter-console': '6.4.0',\n        'jupyter-core': '4.9.1',\n        'jupyter-server': '1.13.3',\n        'jupyter-server-mathjax': '0.2.3',\n        'jupyter-sphinx': '0.3.2',\n        'jupyterlab-pygments': '0.1.2',\n        'jupyterlab-widgets': '1.0.2',\n        'jupytext': '1.11.5',\n        'kiwisolver': '1.3.2',\n        'latexcodec': '2.0.1',\n        'linkify-it-py': '1.0.3',\n        'llvmlite': '0.38.0',\n        'locket': '0.2.1',\n        'loguru': '0.5.3',\n        'lxml': '4.7.1',\n        'magicgui': '0.3.5.dev18+g78d1687',\n        'markdown-it-py': '1.1.0',\n        'markupsafe': '2.0.1',\n        'matplotlib': '3.5.1',\n        'matplotlib-inline': '0.1.3',\n        'mccabe': '0.6.1',\n        'mdit-py-plugins': '0.2.8',\n        'meshzoo': '0.9.2',\n        'mistune': '0.8.4',\n        'mrc': '0.2.0',\n        'msgpack': '1.0.3',\n        'multidict': '5.2.0',\n        'mypy': '0.931',\n        'mypy-extensions': '0.4.3',\n        'myst-nb': '0.13.1',\n        'myst-parser': '0.15.2',\n        'napari': '0.4.14rc1.dev4+gcdf58d44b',\n        'napari-aicsimageio': '0.4.1',\n        'napari-console': '0.0.4',\n        'napari-dv': '0.2.7.dev0+g54e1691.d20220128',\n        'napari-error-reporter': '0.1.dev1+g1b388f2.d20220201',\n        'napari-hello': '0.0.1',\n        'napari-math': '0.0.1a0',\n        'napari-micromanager': '0.0.1rc6.dev14+g5149788.d20220128',\n        'napari-molecule-reader': '0.1.2.dev1+gc2ec2de',\n        'napari-plugin-engine': '0.2.0',\n        'napari-pyclesperanto-assistant': '0.12.0',\n        'napari-skimage-regionprops': '0.2.9',\n        'napari-svg': '0.1.6',\n        'napari-time-slicer': '0.4.2',\n        'napari-workflows': '0.1.2',\n        'natsort': '8.0.2',\n        'nbclient': '0.5.10',\n        'nbconvert': '6.4.0',\n        'nbdime': '3.1.1',\n        'nbformat': '5.1.3',\n        'nd2': '0.1.4',\n        'nest-asyncio': '1.5.4',\n        'networkx': '2.6.3',\n        'nodeenv': '1.6.0',\n        'notebook': '6.4.7',\n        'npe2': '0.1.1',\n        'numba': '0.55.0',\n        'numcodecs': '0.9.1',\n        'numpy': '1.20.3',\n        'numpydoc': '1.1.0',\n        'ome-types': '0.2.10',\n        'opencv-python-headless': '4.5.5.62',\n        'packaging': '21.3',\n        'pandas': '1.3.5',\n        'pandocfilters': '1.5.0',\n        'paramiko': '2.9.2',\n        'parso': '0.8.3',\n        'partd': '1.2.0',\n        'pathspec': '0.9.0',\n        'pdbpp': '0.10.3',\n        'peewee': '3.14.8',\n        'pep517': '0.12.0',\n        'pexpect': '4.8.0',\n        'pickleshare': '0.7.5',\n        'pillow': '8.4.0',\n        'pint': '0.18',\n        'pip': '21.3.1',\n        'platformdirs': '2.4.1',\n        'pluggy': '1.0.0',\n        'pooch': '1.5.2',\n        'poyo': '0.5.0',\n        'pre-commit': '2.16.0',\n        'prometheus-client': '0.12.0',\n        'prompt-toolkit': '3.0.24',\n        'psutil': '5.9.0',\n        'psygnal': '0.2.0',\n        'ptyprocess': '0.7.0',\n        'pure-eval': '0.2.1',\n        'py': '1.11.0',\n        'pybtex': '0.24.0',\n        'pybtex-docutils': '1.0.1',\n        'pyclesperanto-prototype': '0.12.0',\n        'pycodestyle': '2.8.0',\n        'pycparser': '2.21',\n        'pydantic': '1.9.0',\n        'pydata-sphinx-theme': '0.7.2',\n        'pyflakes': '2.2.0',\n        'pygments': '2.11.2',\n        'pymmcore': '10.1.1.70.5',\n        'pymmcore-plus': '0.1.8',\n        'pynacl': '1.5.0',\n        'pyopencl': '2021.2.13',\n        'pyopengl': '3.1.5',\n        'pyparsing': '3.0.6',\n        'pyperclip': '1.8.2',\n        'pyrepl': '0.9.0',\n        'pyro5': '5.13.1',\n        'pyrsistent': '0.18.1',\n        'pyside2': '5.15.2.1',\n        'pytest': '6.2.5',\n        'pytest-cookies': '0.6.1',\n        'pytest-cov': '3.0.0',\n        'pytest-faulthandler': '2.0.1',\n        'pytest-order': '1.0.1',\n        'pytest-qt': '4.0.2',\n        'python-dateutil': '2.8.2',\n        'python-dotenv': '0.19.2',\n        'python-slugify': '5.0.2',\n        'pytomlpp': '1.0.10',\n        'pytools': '2021.2.9',\n        'pytz': '2021.3',\n        'pywavelets': '1.2.0',\n        'pyyaml': '6.0',\n        'pyzmq': '22.3.0',\n        'qtconsole': '5.2.2',\n        'qtpy': '2.0.0',\n        'regex': '2021.11.10',\n        'requests': '2.27.1',\n        'rich': '11.0.0',\n        'rmsd': '1.4',\n        'ruamel.yaml': '0.17.20',\n        'ruamel.yaml.clib': '0.2.6',\n        'scikit-image': '0.19.1',\n        'scipy': '1.7.3',\n        'semgrep': '0.78.0',\n        'send2trash': '1.8.0',\n        'sentry-sdk': '1.5.4',\n        'serpent': '1.40',\n        'setuptools': '60.5.0',\n        'shiboken2': '5.15.2.1',\n        'six': '1.16.0',\n        'smmap': '5.0.0',\n        'sniffio': '1.2.0',\n        'snowballstemmer': '2.2.0',\n        'sortedcontainers': '2.4.0',\n        'soupsieve': '2.3.1',\n        'sourcery-cli': '0.10.0',\n        'sphinx': '4.4.0',\n        'sphinx-autodoc-typehints': '1.12.0',\n        'sphinx-book-theme': '0.1.10',\n        'sphinx-comments': '0.0.3',\n        'sphinx-copybutton': '0.4.0',\n        'sphinx-external-toc': '0.2.3',\n        'sphinx-jupyterbook-latex': '0.4.6',\n        'sphinx-multitoc-numbering': '0.1.3',\n        'sphinx-panels': '0.6.0',\n        'sphinx-tabs': '3.2.0',\n        'sphinx-thebe': '0.0.10',\n        'sphinx-togglebutton': '0.2.3',\n        'sphinxcontrib-applehelp': '1.0.2',\n        'sphinxcontrib-bibtex': '2.2.1',\n        'sphinxcontrib-devhelp': '1.0.2',\n        'sphinxcontrib-htmlhelp': '2.0.0',\n        'sphinxcontrib-jsmath': '1.0.1',\n        'sphinxcontrib-qthelp': '1.0.3',\n        'sphinxcontrib-serializinghtml': '1.1.5',\n        'sqlalchemy': '1.4.29',\n        'stack-data': '0.1.4',\n        'superqt': '0.2.5.post2.dev7+ga49bcd7',\n        'tensorstore': '0.1.16',\n        'terminado': '0.12.1',\n        'testpath': '0.5.0',\n        'text-unidecode': '1.3',\n        'tifffile': '2021.11.2',\n        'toml': '0.10.2',\n        'tomli': '2.0.0',\n        'toolz': '0.11.2',\n        'torch': '1.10.1',\n        'tornado': '6.1',\n        'tox': '3.24.5',\n        'tox-conda': '0.9.1',\n        'tqdm': '4.62.3',\n        'traitlets': '5.1.1',\n        'transforms3d': '0.3.1',\n        'transitions': '0.8.10',\n        'typed-ast': '1.5.1',\n        'typer': '0.4.0',\n        'typing-extensions': '4.0.1',\n        'uc-micro-py': '1.0.1',\n        'urllib3': '1.26.8',\n        'useq-schema': '0.1.1.dev13+g01d1b46.d20220120',\n        'valerius': '0.2.0',\n        'virtualenv': '20.13.0',\n        'vispy': '0.9.4',\n        'watchdog': '2.1.6',\n        'wcmatch': '8.3',\n        'wcwidth': '0.2.5',\n        'webencodings': '0.5.1',\n        'websocket-client': '1.2.3',\n        'wheel': '0.37.1',\n        'widgetsnbextension': '3.5.2',\n        'wmctrl': '0.4',\n        'wrapt': '1.13.3',\n        'wurlitzer': '3.0.2',\n        'xarray': '0.20.2',\n        'xmlschema': '1.9.2',\n        'yarl': '1.7.2',\n        'zarr': '2.10.3',\n        'zipp': '3.7.0'\n    },\n    'platform': 'python',\n    'release': '0.4.14rc1.dev4+gcdf58d44b',\n    'sdk': {\n        'integrations': [\n            'aiohttp',\n            'argv',\n            'atexit',\n            'dedupe',\n            'excepthook',\n            'logging',\n            'modules',\n            'sqlalchemy',\n            'stdlib',\n            'threading',\n            'tornado'\n        ],\n        'name': 'sentry.python',\n        'packages': [{'name': 'pypi:sentry-sdk', 'version': '1.5.4'}],\n        'version': '1.5.4'\n    },\n    'server_name': '',\n    'tags': {\n        'platform.name': 'MacOS 10.15.7',\n        'platform.system': 'Darwin',\n        'qtpy.API_NAME': 'PySide2',\n        'qtpy.QT_VERSION': '5.15.2'\n    },\n    'timestamp': '2022-02-02T01:30:00.229122Z'\n}\n```\n\n</details>\n\n> ***NOTE**: in the opt-in dialog, there is a checkbox labeled \"include local variables\",\nchecking this will include the value of variables in the local scope when an exception\noccurs.  While these can be very useful when interpreting a bug report, they may\noccasionally include local file path strings.  If that concerns you, please leave this\nbox unchecked*\n\n## Install\n\nThis plugin requires napari version 0.4.15 or greater, or the `main` branch with PR\n[napari/napari#4055](https://github.com/napari/napari/pull/4055).\n\nInstall via pip with:\n\n```sh\npip install napari-error-reporter\n```\n\nor in the built-in plugin installer (a restart will be required):\n\n<img width=\"503\" alt=\"Untitled\" src=\"https://user-images.githubusercontent.com/1609449/153915128-09a5e3d7-8561-4c17-b543-5ea172e3e860.png\">\n\n\nThank you!!\n\n## Privacy FAQ\n\nEven with the multiple layers of opt-ins, and the attempts to wipe all personal info\nprior to sending reports, we understand that privacy is always a concern.\n\n### Do you collect personal info?\n\nWe make every attempt to collect ***no*** personally identifiable information.  No\nname, location, IP address, etc...  We do collect your\n([`uuid.getnode()`](https://docs.python.org/3.10/library/uuid.html#uuid.getnode)) to\nbe able to track bug resolution over time. As mentioned above, allowing local\nvariables to be collected may occasionally include a file path in the log.\nIf that concerns you, please leave that unchecked.\n\n### Is this shipped with napari?\n\n`napari-error-reporter` is **not** bundled with napari or listed as a napari dependency.\nIn order for reports to be sent, you must first install this plugin yourself, and then\nopt in on the next launch.  If you uninstall the plugin, no more reports can be sent.\n\n### Who can access these reports?\n\nOnly the following napari core developers have access to these reports.\nIf [this](https://raw.githubusercontent.com/tlambert03/napari-error-reporter/main/ADMINS)\nlist changes in the future, you will be asked to opt-in again in napari:\n\n- Juan Nunez-Iglesias ([@jni](https://github.com/jni))\n- Talley Lambert ([@tlambert03](https://github.com/tlambert03))\n\n*This plugin is **not** associated with the Chan Zuckerberg Initiative*.\n\n### How will these reports be used?\n\nCommonly occuring errors will be will be manually purged of file paths and\nlocal variables and posted to https://github.com/napari/napari/issues\n\n### How long is data retained\n\nSentry retains event data for 90 days by default.  For complete details,\nsee Sentry's page on [Security & Compliance](https://sentry.io/security/)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-aideveloper",
    "name": "napari-aideveloper",
    "display_name": "napari AIDeveloper",
    "version": "0.0.4",
    "created_at": "2022-05-20",
    "modified_at": "2022-06-17",
    "authors": [
      "Chenqi Zhang"
    ],
    "author_emails": [
      "cqzhang@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-aideveloper/",
    "home_github": "https://github.com/zcqwh/napari-aideveloper",
    "home_other": null,
    "summary": "napari_aideveloper is a napari-plugin deived from AIDeveloper that allows you to train, evaluate and apply deep neural nets for image classification within a graphical user-interface (GUI).",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "dclab (>=0.39.9)",
      "h5py (>=3.6.0)",
      "Keras-Applications (>=1.0.8)",
      "keras (>=2.8.0)",
      "keras-metrics (>=1.1.0)",
      "napari-plugin-engine (>=0.2.0)",
      "napari (>=0.4.14)",
      "onnx (>=1.11.0)",
      "opencv-contrib-python-headless (>=4.5.5.62)",
      "openpyxl (>=3.0.9)",
      "pandas (>=1.4.1)",
      "Pillow (>=9.1.1)",
      "psutil (>=5.9.0)",
      "pyqtgraph (>=0.12.3)",
      "pytest (>=7.1.2)",
      "scikit-learn (>=1.1.1)",
      "scipy (>=1.8.0)",
      "setuptools (>=58.0.4)",
      "six (>=1.16.0)",
      "tensorboard (>=2.8.0)",
      "tensorflow (>=2.8.0)",
      "tf2onnx (>=1.9.3)",
      "xlrd (>=2.0.1)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-aideveloper\n\n[![License](https://img.shields.io/pypi/l/napari-aideveloper.svg?color=green)](https://github.com/zcqwh/napari-aideveloper/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-aideveloper.svg?color=green)](https://pypi.org/project/napari-aideveloper)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-aideveloper.svg?color=green)](https://python.org)\n[![tests](https://github.com/zcqwh/napari-aideveloper/workflows/tests/badge.svg)](https://github.com/zcqwh/napari-aideveloper/actions)\n[![codecov](https://codecov.io/gh/zcqwh/napari-aideveloper/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/napari-aideveloper)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-aideveloper)](https://napari-hub.org/plugins/napari-aideveloper)\n\n[napari_aideveloper](https://www.napari-hub.org/plugins/napari-aideveloper) is a napari-plugin derived from [AIDeveloper](https://github.com/maikherbig/AIDeveloper) that allows you to train, evaluate, and apply deep neural nets for image classification within a graphical user-interface (GUI).\n\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-aideveloper` via [pip]:\n\n    pip install napari-aideveloper\n\n## Introduction\n### Main functions\n* [Build](#build)\n* [History](#history)\n\n****\n\n### Build \n#### 1. Load data\nDrag and drop your data in .rtdc (HDF5) format into the file table and set the class and training/validation.\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/00_Load_data.gif?raw=true)\n\n#### 2. Choose Neural Networks\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/01_choose%20NN.gif?raw=true)\n\n#### 3. Set model storage path\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/02_save_model.gif?raw=true)\n\n#### 4. Start fitting\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/03_start_fitting.gif?raw=true)\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/04_fitting.gif?raw=true)\n\n#### Preview image\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/05_preview.gif?raw=true)\n\n#### Image augmentation\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/main/Tutorial/06_augmentation.gif?raw=true)\n\n****\n\n### History\n#### 1. Load meta data\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/01_Load_metadata.gif?raw=true)\n\n#### 2. Check model details\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/02_model_detail.gif?raw=true)\n\n#### 3. Rolling median & Linear fit\n![alt Load_data](https://github.com/zcqwh/napari-aideveloper/blob/development/Tutorial/GIF/History/03_rolling_linear.gif?raw=true)\n\n\n\n\n\n\n## Contributing\n\nContributions are very welcome. You can submit your pull request on [GitHub](https://github.com/zcqwh/napari-aideveloper/pulls). Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-aideveloper\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/zcqwh/napari-aideveloper/issues) along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "AIDeveloper"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "disease-classifier",
    "name": "disease-classifier",
    "display_name": "Disease classifier",
    "version": "0.0.1",
    "created_at": "2022-06-07",
    "modified_at": "2022-06-07",
    "authors": [
      "Chenqi Zhang"
    ],
    "author_emails": [
      "cqzhang@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/disease-classifier/",
    "home_github": "https://github.com/zcqwh/disease-classifier",
    "home_other": null,
    "summary": "A disease classifier based on iPAC images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "matplotlib",
      "h5py (>=3.6.0)",
      "napari (>=0.4.15)",
      "numpy (>=1.22.4)",
      "opencv-contrib-python-headless (>=4.5.5.64)",
      "pytranskit (>=0.2.3)",
      "statsmodels (>=0.13.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# disease-classifier\n\n[![License](https://img.shields.io/pypi/l/disease-classifier.svg?color=green)](https://github.com/zcqwh/disease-classifier/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/disease-classifier.svg?color=green)](https://pypi.org/project/disease-classifier)\n[![Python Version](https://img.shields.io/pypi/pyversions/disease-classifier.svg?color=green)](https://python.org)\n[![tests](https://github.com/zcqwh/disease-classifier/workflows/tests/badge.svg)](https://github.com/zcqwh/disease-classifier/actions)\n[![codecov](https://codecov.io/gh/zcqwh/disease-classifier/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/disease-classifier)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/disease-classifier)](https://napari-hub.org/plugins/disease-classifier)\n\nA napari plugin for disease classification based on iPAC images.\n\n\n\n## Installation\n\nYou can install `disease-classifier` via [pip]:\n\n    pip install disease-classifier\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/zcqwh/disease-classifier.git\n\n## Introduction\n#### Load data (.rtdc or .bin)\n* Drag and drop the data in .rtdc or .bin into the files table.\n* Click eye button to preview images.\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/01_Load_preview.gif?raw=true)\n\n\n#### Choose model and classify\n\n* Choose the model folder including CNN and RF/PLDA.\n* Check the data.\n* Click classify.\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/02_model_classify.gif?raw=true)\n\n#### Preview classification results\n* Click the eye button to preview the result.\n* Click the header to show all.\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/03_preview_result.gif?raw=true)\n\n\n#### Save results\n* Click ‚ÄúAdd classification to .rtdc file‚Äù button to save results.\n![](https://github.com/zcqwh/disease-classifier/blob/main/Tutorial/Gif/04_save.gif?raw=true)\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"disease-classifier\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/zcqwh/disease-classifier/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "iPAC disease classifier"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-sairyscan",
    "name": "napari-sairyscan",
    "display_name": "napari sairyscan",
    "version": "0.0.2",
    "created_at": "2022-06-03",
    "modified_at": "2022-06-07",
    "authors": [
      "Sylvain Prigent"
    ],
    "author_emails": [
      "meriadec.prigent@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-sairyscan/",
    "home_github": "https://github.com/sylvainprigent/napari-sairyscan",
    "home_other": null,
    "summary": "Airyscan image reconstruction",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "sairyscan (>=0.0.2)",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-sairyscan\n\n[![License](https://img.shields.io/pypi/l/napari-sairyscan.svg?color=green)](https://github.com/sylvainprigent/napari-sairyscan/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-sairyscan.svg?color=green)](https://pypi.org/project/napari-sairyscan)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-sairyscan.svg?color=green)](https://python.org)\n[![tests](https://github.com/sylvainprigent/napari-sairyscan/workflows/tests/badge.svg)](https://github.com/sylvainprigent/napari-sairyscan/actions)\n[![codecov](https://codecov.io/gh/sylvainprigent/napari-sairyscan/branch/main/graph/badge.svg)](https://codecov.io/gh/sylvainprigent/napari-sairyscan)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-sairyscan)](https://napari-hub.org/plugins/napari-sairyscan)\n\nAiryscan image reconstruction\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-sairyscan` via [pip]:\n\n    pip install napari-sairyscan\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/sylvainprigent/napari-sairyscan.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-sairyscan\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/sylvainprigent/napari-sairyscan/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.czi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Airyscan reconstruction"
    ],
    "contributions_sample_data": [
      "SAiryscan"
    ]
  },
  {
    "normalized_name": "napari-czifile2",
    "name": "napari-czifile2",
    "display_name": "napari-czifile2",
    "version": "0.2.7",
    "created_at": "2021-02-02",
    "modified_at": "2022-06-01",
    "authors": [
      "Jonas Windhager"
    ],
    "author_emails": [
      "jonas.windhager@uzh.ch"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-czifile2/",
    "home_github": "https://github.com/BodenmillerGroup/napari-czifile2",
    "home_other": null,
    "summary": "Carl Zeiss Image (.czi) file support for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "czifile",
      "imagecodecs",
      "numpy",
      "tifffile"
    ],
    "package_metadata_description": "# napari-czifile2\n\n<a href=\"https://pypi.org/project/napari-czifile2/\">\n    <img src=\"https://img.shields.io/pypi/v/napari-czifile2\" alt=\"PyPI\" />\n</a>\n<a href=\"https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md\">\n    <img src=\"https://img.shields.io/pypi/l/napari-czifile2\" alt=\"License\" />\n</a>\n<a href=\"https://www.python.org/\">\n    <img src=\"https://img.shields.io/pypi/pyversions/napari-czifile2\" alt=\"Python\" />\n</a>\n<a href=\"https://github.com/BodenmillerGroup/napari-czifile2/issues\">\n    <img src=\"https://img.shields.io/github/issues/BodenmillerGroup/napari-czifile2\" alt=\"Issues\" />\n</a>\n<a href=\"https://github.com/BodenmillerGroup/napari-czifile2/pulls\">\n    <img src=\"https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-czifile2\" alt=\"Pull requests\" />\n</a>\n\nCarl Zeiss Image (.czi) file type support for napari\n\nOpen .czi files and interactively view scenes co-registered in the machine's coordinate system using napari\n\n## Installation\n\nYou can install napari-czifile2 via [pip](https://pypi.org/project/pip/):\n\n    pip install napari-czifile2\n\nAlternatively, you can install napari-czifile2 via [conda](https://conda.io/):\n\n    conda install -c conda-forge napari-czifile2\n\n## Authors\n\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\n\n## Contributing\n\n[Contributing](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CONTRIBUTING.md)\n\n## Changelog\n\n[Changelog](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CHANGELOG.md)\n\n## License\n\n[MIT](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.czi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-imc",
    "name": "napari-imc",
    "display_name": "napari-imc",
    "version": "0.6.5",
    "created_at": "2021-01-19",
    "modified_at": "2022-06-01",
    "authors": [
      "Jonas Windhager"
    ],
    "author_emails": [
      "jonas.windhager@uzh.ch"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-imc/",
    "home_github": "https://github.com/BodenmillerGroup/napari-imc",
    "home_other": null,
    "summary": "Imaging Mass Cytometry (IMC) file type support for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "qtpy",
      "readimc",
      "superqt"
    ],
    "package_metadata_description": "# napari-imc\n\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-imc)](https://napari-hub.org/plugins/napari-imc)\n[![PyPI](https://img.shields.io/pypi/v/napari-imc.svg?color=green)](https://pypi.org/project/napari-imc)\n[![License](https://img.shields.io/pypi/l/napari-imc.svg?color=green)](https://github.com/BodenmillerGroup/napari-imc/raw/main/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-imc.svg?color=green)](https://python.org)\n[![Issues](https://img.shields.io/github/issues/BodenmillerGroup/napari-imc)](https://github.com/BodenmillerGroup/napari-imc/issues)\n[![Pull requests](https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-imc)](https://github.com/BodenmillerGroup/napari-imc/pulls)\n\nImaging Mass Cytometry (IMC) file type support for napari\n\nLoad panoramas and multi-channel acquisitions co-registered within the machine's coordinate system from Fluidigm TXT/MCD files\n\n## Installation\n\nYou can install napari-imc via [pip](https://pypi.org/project/pip/):\n\n    pip install napari-imc\n    \nAlternatively, you can install napari-imc via [conda](https://conda.io/):\n\n    conda install -c conda-forge napari-imc\n    \nFor example, to install napari and napari-imc in a fresh conda environment using pip:\n\n    conda create -n napari-imc python=3.9\n    conda activate napari-imc\n    pip install \"napari[all]\" napari-imc\n    \n## Usage\n\nSimply open your Fluidigm TXT/MCD file using napari.\n\n## Authors\n\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\n\n## Citation\n\nPlease cite the following paper when using napari-imc in your work:\n\n> Windhager J, Bodenmiller B, Eling N (2021). An end-to-end workflow for multiplexed image processing and analysis. bioRxiv. doi: https://doi.org/10.1101/2021.11.12.468357.\n\n    @article{Windhager2021,\n      author = {Windhager, Jonas and Bodenmiller, Bernd and Eling, Nils},\n      title = {An end-to-end workflow for multiplexed image processing and analysis},\n      year = {2021},\n      doi = {10.1101/2021.11.12.468357},\n      URL = {https://www.biorxiv.org/content/early/2021/11/13/2021.11.12.468357},\n      journal = {bioRxiv}\n    }\n\n## Contributing\n\n[Contributing](https://github.com/BodenmillerGroup/napari-imc/blob/main/CONTRIBUTING.md)\n\n## Changelog\n\n[Changelog](https://github.com/BodenmillerGroup/napari-imc/blob/main/CHANGELOG.md)\n\n## License\n\n[MIT](https://github.com/BodenmillerGroup/napari-imc/blob/main/LICENSE.md)\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mcd",
      "*.txt"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Imaging Mass Cytometry"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-kics",
    "name": "napari-kics",
    "display_name": "napari-kics",
    "version": "0.0.3rc6",
    "created_at": "2022-05-02",
    "modified_at": "2022-05-22",
    "authors": [
      "Alexandr Dibrov"
    ],
    "author_emails": [
      "dibrov@mpi-cbg.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-kics/",
    "home_github": null,
    "home_other": "None",
    "summary": "A plugin to estimate chromosome sizes from karyotype images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari[all]",
      "scikit-image",
      "pandas",
      "pulp",
      "pyqtgraph"
    ],
    "package_metadata_description": "# napari-kics\n\n![napari-kics](https://github.com/mpicbg-csbd/napari-kics/raw/main/docs/banner.png?sanitize=true&raw=true)\n\n[![standard-readme compliant](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg)](https://github.com/RichardLitt/standard-readme)\n[![License](https://img.shields.io/pypi/l/napari-kics.svg?color=green)](./LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-kics.svg?color=green)](https://pypi.org/project/napari-kics)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-kics.svg?color=green)](https://python.org)\n[![Python package](https://github.com/mpicbg-csbd/napari-kics/actions/workflows/python-package.yml/badge.svg)](https://github.com/mpicbg-csbd/napari-kics/actions/workflows/python-package.yml)\n\n\n> A plugin to estimate chromosome sizes from karyotype images.\n\n<small>*This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.*</small>\n\n\n## Table of Contents\n\n- Install\n- Usage\n- Example\n- Citation\n- Maintainer\n- Contributing\n- License\n\n\n## Install\n\nYou can install `napari-kics` via [pip]:\n\n```sh\npip install napari-kics\n```\n\nThis will install all required dependencies as well. We recommend installing it in a virtual environment, e.g. using [conda]:\n\n```sh\nconda create -n kics python\nconda activate kics\npip install napari-kics\n```\n\nWe recommend using [mamba] as a faster alternative to conda.\n\n\n## Usage\n\n1. Launch Napari via command line (`napari`).\n2. Activate the plugin via menu `Plugins -> napari-kics: Karyotype Widget`.\n3. Select file via `File -> Open File`.\n4. Follow instructions in the panel on the right.\n\nYou may use the interactive analysis plots directly via command line:\n\n```sh\nkaryotype-analysis-plots\n```\n\n\n## Example\n\n1. Launch Napari via command line (`napari`).\n2. Activate the plugin via menu `Plugins -> napari-kics: Karyotype Widget`.\n3. Select file via `File -> Open Sample -> napari-kics: sample`.\n4. Follow instructions in the panel on the right.\n\nTry out the interactive analysis plots directly via command line:\n\n```sh\nkaryotype-analysis-plots --example\n```\n\n\n## Citation\n\n> Arne Ludwig, Alexandr Dibrov, Gene Myers, Martin Pippel.\n> Estimating chromosome sizes from karyotype images enables validation of\n> *de novo* assemblies. To be published.\n\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-kics\" is free and open source software\n\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n## Contributing\n\nContributions are very welcome. Please [file a pull request] with your\ncontribution.\n\nYou can setup a local development environment for `napari-kics` via [pip]:\n\n```sh\ngit clone https://github.com/mpicbg-csbd/napari-kics.git\ncd napari-kics\npip install -e .\n```\n\n\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[@napari]: https://github.com/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[conda]: https://www.anaconda.com/products/distribution\n[mamba]: https://github.com/mamba-org/mamba\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[file an issue]: https://github.com/mpicbg-csbd/napari-kics/issues\n[file a pull request]: https://github.com/mpicbg-csbd/napari-kics/pulls\n\n## Overview\nhttps://user-images.githubusercontent.com/17703905/139654249-685703b5-2196-4a73-a036-d40d578ebcdf.mp4\n\n\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "KaryotypeWidget"
    ],
    "contributions_sample_data": [
      "sample"
    ]
  },
  {
    "normalized_name": "napari-lazy-openslide",
    "name": "napari-lazy-openslide",
    "display_name": "napari-lazy-openslide",
    "version": "0.3.0",
    "created_at": "2020-10-26",
    "modified_at": "2022-05-19",
    "authors": [
      "Trevor Manz"
    ],
    "author_emails": [
      "trevor.j.manz@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-lazy-openslide/",
    "home_github": "https://github.com/manzt/napari-lazy-openslide",
    "home_other": null,
    "summary": "A plugin to lazily load multiscale whole-slide images with openslide and dask",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "zarr (>=2.11.0)",
      "numpy",
      "dask[array]",
      "openslide-python"
    ],
    "package_metadata_description": "# napari-lazy-openslide\n\n[![License](https://img.shields.io/pypi/l/napari-lazy-openslide.svg?color=green)](https://github.com/manzt/napari-lazy-openslide/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-lazy-openslide.svg?color=green)](https://pypi.org/project/napari-lazy-openslide)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-lazy-openslide.svg?color=green)](https://python.org)\n[![tests](https://github.com/manzt/napari-lazy-openslide/workflows/tests/badge.svg)](https://github.com/manzt/napari-lazy-openslide/actions)\n\nAn experimental plugin to lazily load multiscale whole-slide tiff images with openslide and dask.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\n**Step 1.)** Make sure you have OpenSlide installed. Download instructions [here](https://openslide.org/download/).\n\n> NOTE: Installation on macOS is easiest via Homebrew: `brew install openslide`. Up-to-date and multiplatform \n> binaries for `openslide` are also avaiable via `conda`: `conda install -c sdvillal openslide-python`\n\n**Step 2.)** Install `napari-lazy-openslide` via `pip`:\n\n    pip install napari-lazy-openslide\n\n## Usage\n\n### Napari plugin\n\n```bash\n$ napari tumor_004.tif\n```\nBy installing this package via `pip`, the plugin should be recognized by `napari`. The plugin\nattempts to read image formats recognized by `openslide` that are multiscale \n(`openslide.OpenSlide.level_count > 1`). \n\nIt should be noted that `napari-lazy-openslide` is experimental and has primarily \nbeen tested with `CAMELYON16` and `CAMELYON17` datasets, which can be \ndownloaded [here](https://camelyon17.grand-challenge.org/Data/).\n\n![Interactive deep zoom of whole-slide image](tumor_004.gif)\n\n\n### Using `OpenSlideStore` with Zarr and Dask\n\nThe `OpenSlideStore` class wraps an `openslide.OpenSlide` object as a valid Zarr store. \nThe underlying `openslide` image pyramid is translated to the Zarr multiscales extension,\nwhere each level of the pyramid is a separate 3D `zarr.Array` with shape `(y, x, 4)`.\n\n```python\nimport dask.array as da\nimport zarr\n\nfrom napari_lazy_openslide import OpenSlideStore\n\nstore = OpenSlideStore('tumor_004.tif')\ngrp = zarr.open(store, mode=\"r\")\n\n# The OpenSlideStore implements the multiscales extension\n# https://forum.image.sc/t/multiscale-arrays-v0-1/37930\ndatasets = grp.attrs[\"multiscales\"][0][\"datasets\"]\n\npyramid = [grp.get(d[\"path\"]) for d in datasets]\nprint(pyramid)\n# [\n#   <zarr.core.Array '/0' (23705, 29879, 4) uint8 read-only>,\n#   <zarr.core.Array '/1' (5926, 7469, 4) uint8 read-only>,\n#   <zarr.core.Array '/2' (2963, 3734, 4) uint8 read-only>,\n# ]\n\npyramid = [da.from_zarr(store, component=d[\"path\"]) for d in datasets]\nprint(pyramid)\n# [\n#   dask.array<from-zarr, shape=(23705, 29879, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>,\n#   dask.array<from-zarr, shape=(5926, 7469, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>,\n#   dask.array<from-zarr, shape=(2963, 3734, 4), dtype=uint8, chunksize=(512, 512, 4), chunktype=numpy.ndarray>,\n# ]\n\n# Now you can use numpy-like indexing with openslide, reading data into memory lazily!\nlow_res = pyramid[-1][:]\nregion = pyramid[0][y_start:y_end, x_start:x_end]\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with `tox`, please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/manzt/napari-lazy-openslide/issues\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ip-workflow",
    "name": "napari-IP-workflow",
    "display_name": "Image Processing Workflow",
    "version": "0.0.3",
    "created_at": "2022-05-17",
    "modified_at": "2022-05-18",
    "authors": [
      "Jay Unruh"
    ],
    "author_emails": [
      "jru@stowers.org"
    ],
    "license": "GPL-3.0-only",
    "home_pypi": "https://pypi.org/project/napari-ip-workflow/",
    "home_github": "https://github.com/jayunruh/napari-IP-workflow",
    "home_other": null,
    "summary": "A plugin to do image preprocessing, segmentation, and measurements on other images.",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "pandas",
      "skimage",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-IP-workflow\n\n[![License](https://img.shields.io/pypi/l/napari-IP-workflow.svg?color=green)](https://github.com/jayunruh/napari-IP-workflow/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-IP-workflow.svg?color=green)](https://pypi.org/project/napari-IP-workflow)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-IP-workflow.svg?color=green)](https://python.org)\n[![tests](https://github.com/jayunruh/napari-IP-workflow/workflows/tests/badge.svg)](https://github.com/jayunruh/napari-IP-workflow/actions)\n[![codecov](https://codecov.io/gh/jayunruh/napari-IP-workflow/branch/main/graph/badge.svg)](https://codecov.io/gh/jayunruh/napari-IP-workflow)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-IP-workflow)](https://napari-hub.org/plugins/napari-IP-workflow)\n\nA plugin to do image preprocessing, segmentation, and measurements on other images.  The typical workflow is background subtraction followed by smoothing, thresholding, and size filtering.  This is typically done on nuclear stained images.  Segmentation can optionally be followed by circular label expansion to find cytoplasmic signals. The labeled signals are then measured on background subtracted images.\n\n##General organization\n\nThe code is separated into non-interactive processing functions (ipfunctions module) and an interactive widget (segwidget module).  Please look at the code on github for examples: [Github](https://github.com/jayunruh/napari-ip-workflow). The expected workflow is from jupyter notebooks with an interactive workflow shown in src/napari-ip-workflow/_tests/standard_segementation_widget.ipynb and a non-interactive workflow shown in src/napari-ip-workflow/_tests/standard_segmentation.ipynb.  The expectation is to find the best parameters in an interactive way (ideally testing on several images) and then use the non-interactive workflow to batch through more data sets.  All image processing algorithms are in the ipfunctions module and the segwidget module has the Napari widget code.  Below I describe the strategies that are utilized in the workflow.\n\n## Background subtraction strategy\n\nAutomated background subtraction (e.g. as in Fiji) is often accomplished with a low pass filter-style approach like rolling ball background subtraction.  This approach fails as feature sizes grow larger or measurements approach the background.  Manual selection of the background is more robust but introduces human variability and isn't compatible with high throughput analyses.  Our approach is to attempt to automate regional selection of background as follows.  First the image is smoothed with a Gaussian filter to eliminate background noise.  Next, minimum values are subtracted from each channel and the resulting images are summed.   Next, a uniform 2D boxcar smoothing is applied to the image--background level regions in the resulting image are at least the boxcar size distance away from foreground objects.  The minimum pixel in that resulting image is a good approximation for the background region of the image.  A thick border is specified to avoid lower intensity regions at the border of the image.  This algorithm is implemented in the ipfunctions module as findBackground.  Once the background region is found, it can be measured with measureCirc.\n\n## Segmentation and thresholding strategy\n\nThere are many automated thresholding algorithms available via python and, by extension, Napari.  This program uses a very simplistic but powerful method.  Most segmentable images consist of foreground and background components.  In imaging, the foreground is more noisy than the background.  Ideally a smoothed background subtracted image will have a maximum intensity that represents the foreground well and a background intensity of 0.  In that case, the threshold level can be easily defined as a fraction of that smoothed maximum intensity.  A threshold fraction of 0.25 tends to work well but lower values may be more robust if background is fairly smooth and the foreground is noisier.  In some cases the foreground has anomalous high values that will skew the estimation.  In that case it may be better to estimate the foreground as e.g. the 99th percentile of the intensity.  In some cases it may be useful to use the average intensity as a reference point instead or use the raw intensity value (statistic is Identity).  Those last options tend to be less robust and it may be desired in those cases to use some of the more complex autothresholding methods.  After thresholding, objects on the image edge are eliminated and objects are filtered according to size.  The minimum area can easy remove small debris that can contaminate a measurement.  The maximum area can be used for large contaminants or poorly segmented clusters of cells that might not be desired in the analysis.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-IP-workflow` via [pip]:\n\n    pip install napari-IP-workflow\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/jayunruh/napari-IP-workflow.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-IP-workflow\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/jayunruh/napari-IP-workflow/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Make Segmentation Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-workflow-inspector",
    "name": "napari-workflow-inspector",
    "display_name": "napari-workflow-inspector",
    "version": "0.2.2",
    "created_at": "2021-12-04",
    "modified_at": "2022-05-15",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-workflow-inspector/",
    "home_github": "https://github.com/haesleinhuepf/napari-workflow-inspector",
    "home_other": null,
    "summary": "Inspect relationships between image processing operations in active workflows in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "networkx",
      "matplotlib",
      "napari-workflows"
    ],
    "package_metadata_description": "# napari-workflow-inspector\n\n[![License](https://img.shields.io/pypi/l/napari-workflow-inspector.svg?color=green)](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-workflow-inspector.svg?color=green)](https://pypi.org/project/napari-workflow-inspector)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-workflow-inspector.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-workflow-inspector/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-workflow-inspector/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-workflow-inspector/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-workflow-inspector)\n[![Development Status](https://img.shields.io/pypi/status/napari-workflow-inspector.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workflow-inspector)](https://napari-hub.org/plugins/napari-workflow-inspector)\n\nInspect relationships between image processing operations in active workflows in napari. Open the inspector by clicking the menu `Tools > Visualization > Workflow Inspector`.\n\n![img_1.png](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/docs/screenshot_graph.png)\n\nAlso install the [napari-script-editor](https://www.napari-hub.org/plugins/napari-script-editor) \nto generate code from active workflows.\n\n![img_2.png](https://github.com/haesleinhuepf/napari-workflow-inspector/raw/main/docs/screenshot_script_editor.png)\n\nFor recording workflows, all napari image processing plugins that use the `@time_slicer` interface are supported. See\n[napari-time-slicer](https://www.napari-hub.org/plugins/napari-time-slicer) for a list. More to come, stay tuned.\n\n## Installation\n\nYou can install `napari-workflow-inspector` via [pip]:\n\n```\npip install napari-workflow-inspector\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-workflow-inspector\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-workflow-inspector/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "WorkflowWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-labeling",
    "name": "napari-labeling",
    "display_name": "napari Labeling",
    "version": "0.1.2",
    "created_at": "2022-05-09",
    "modified_at": "2022-05-10",
    "authors": [
      "Tom Burke"
    ],
    "author_emails": [
      "burke@mpi-cbg.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-labeling/",
    "home_github": "https://github.com/Labelings/napari-labeling",
    "home_other": null,
    "summary": "A napari plugin for handling overlapping labeling data",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "labeling"
    ],
    "package_metadata_description": "# napari-labeling\n\n[![License](https://img.shields.io/pypi/l/napari-labeling.svg?color=green)](https://github.com/tomburke-rse/napari-labeling/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-labeling.svg?color=green)](https://pypi.org/project/napari-labeling)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labeling.svg?color=green)](https://python.org)\n[![tests](https://github.com/tomburke-rse/napari-labeling/workflows/tests/badge.svg)](https://github.com/tomburke-rse/napari-labeling/actions)\n[![codecov](https://codecov.io/gh/tomburke-rse/napari-labeling/branch/main/graph/badge.svg)](https://codecov.io/gh/tomburke-rse/napari-labeling)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labeling)](https://napari-hub.org/plugins/napari-labeling)\n\nThis is a napari-plugin based on the [labeling project].\n\nIt allows the generation of overlapping labels in one layer, save and load of this layer in a json-based file format and\nit contains a widget to explore the overlapping labels layer and select specific segments with a mouse click .\n\nPlease note that currently, the widget part only works by adding it through code with:\n\n    from napari_labeling import edit_widget\n    viewer = napari.Viewer()\n    viewer.window.add_dock_widget(edit_widget)\n\nAn example on how to achieve this can be found in the [main.py] on GitHub.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-labeling` via [pip]:\n\n    pip install napari-labeling\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-labeling\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n[labeling project]: https://github.com/Labelings/Labeling\n[main.py]: https://github.com/Labelings/Labeling/blob/main/main.py\n[file an issue]: https://github.com/Labelings/napari-labeling/issues\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.lbl.json"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Labeling Explorer",
      "Create multi-layer labels"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pram",
    "name": "napari-pram",
    "display_name": "napari PRAM",
    "version": "0.1.3",
    "created_at": "2022-04-29",
    "modified_at": "2022-05-03",
    "authors": [
      "Hieu Hoang"
    ],
    "author_emails": [
      "hthieu@illinois.edu"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-pram/",
    "home_github": null,
    "home_other": "None",
    "summary": "plugin for PRAM data annotation and processing",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "opencv-python",
      "scipy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-pram\n\n[![License](https://img.shields.io/pypi/l/napari-pram.svg?color=green)](https://github.com/hthieu166/napari-pram/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-pram.svg?color=green)](https://pypi.org/project/napari-pram)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pram.svg?color=green)](https://python.org)\n[![tests](https://github.com/hthieu166/napari-pram/workflows/tests/badge.svg)](https://github.com/hthieu166/napari-pram/actions)\n[![codecov](https://codecov.io/gh/hthieu166/napari-pram/branch/main/graph/badge.svg)](https://codecov.io/gh/hthieu166/napari-pram)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-pram)](https://napari-hub.org/plugins/napari-pram)\n\nPlugin for PRAM data annotation and processing.\n\n![PRAM Demo](https://raw.githubusercontent.com/hthieu166/napari-pram/main/docs/figs/demo.jpg)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Usage\n\n### Open `napari-pram` toolbox:\n\nOn the toolbar, select ``[Plugins] > napari-pram: Open PRAM's toolbox``\n\n### Load PRAM image and annotations:\n\nPress <kbd>Command/Control</kbd> + <kbd>O</kbd>: \n- Select `*.json` files for annotations (from either [VGG Annotator](https://www.robots.ox.ac.uk/~vgg/software/via/) or `napari-pram`)\n- Select `*.png` files for PRAM image\n\n### Annotate\n- Press <kbd>Annotate</kbd>\n- Click the plus-in-circle icon on the top-left panel and start editing\n\n### Run PRAM particles detector\n- Select a proper threshold between 1 (ultra sensitive) - 10 (less sensitive)\n- Press <kbd>Run Detector</kbd>\n\n### Evaluate\n- Press <kbd>Evaluate</kbd>\n- Hide/Unhide true positive/ false postive/false negative layers\n\n### Load new image\n- Press <kbd>Clear All</kbd> to remove all layers\n\n### Export to JSON\n- Press <kbd>Save to File</kbd> to export all annotations, predictions from the algorithm to a JSON file\n## Installation\nFollowing this [tutorial](https://napari.org/tutorials/fundamentals/quick_start.html) to install `napari`. \n\nAlternatively, you can follow my instructions as follows:\n\nYou will need a python environment. I recommend [Conda](https://docs.conda.io/en/latest/miniconda.html). Create a new environment, for example:\n    \n    conda create --name napari-env python=3.7 pip \n\nActivate the new environment:\n\n    conda activate napari-env \n\nInstall [napari](https://napari.org/tutorials/fundamentals/installation) via [pip]:\n\n    pip install napari[all]\n\nThen you can finally install our plugin `napari-pram` via [pip]:\n\n    pip install napari-pram\n\nAlternatively, the plugin can be installed using napari-GUI\n\n``[Plugins] > Install/Uninstall Plugins`` and search for `napari-pram`\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-pram\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.json",
      "*.png"
    ],
    "contributions_writers_filename_extensions": [
      ".npy"
    ],
    "contributions_widgets": [
      "Open PRAM's toolbox"
    ],
    "contributions_sample_data": [
      "napari PRAM"
    ]
  },
  {
    "normalized_name": "napari-manual-transforms",
    "name": "napari-manual-transforms",
    "display_name": "Manual Transforms",
    "version": "0.0.3",
    "created_at": "2022-04-28",
    "modified_at": "2022-04-29",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "talley.lambert@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-manual-transforms/",
    "home_github": "https://github.com/tlambert03/napari-manual-transforms",
    "home_other": null,
    "summary": "Interface to manually edit layer affine transforms",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magicgui",
      "napari",
      "numpy",
      "pytransform3d",
      "qtpy",
      "scipy",
      "vispy",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "tox ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-manual-transforms\n\n[![License](https://img.shields.io/pypi/l/napari-manual-transforms.svg?color=green)](https://github.com/tlambert03/napari-manual-transforms/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-manual-transforms.svg?color=green)](https://pypi.org/project/napari-manual-transforms)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-manual-transforms.svg?color=green)](https://python.org)\n[![tests](https://github.com/tlambert03/napari-manual-transforms/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-manual-transforms/actions)\n[![codecov](https://codecov.io/gh/tlambert03/napari-manual-transforms/branch/main/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-manual-transforms)\n\nInterface to manually edit layer affine transforms.\n\n- express rotations as quaternion, euler angle, or axis + angle.\n- allows rotation around arbitrary origin\n- currently, focusing on rigid rotations\n- Alt-Drag to rotate a layer independently of the rest.\n- image resampling (i.e. \"apply\" the transformation to create new dataset that can be saved)\n\n![Plugin Preview](/preview.jpeg)\n\ncaveats:\n\n- only works on 3D Image layers for now, open a feature request for other dims/layers.\n- will likely result in \"Non-orthogonal slicing is being requested\" warnings in 2D view.\n\n## Try it out\n\n```python\n\nimport napari\n\nv = napari.Viewer()\nv.dims.ndisplay = 3\nv.open_sample('napari', 'cells3d')\nv.window.add_plugin_dock_widget('napari-manual-transforms')\n\nnapari.run()\n\n```\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-manual-transforms` via [pip]:\n\n```sh\npip install napari-manual-transforms\n```\n\nTo install latest development version :\n\n```sh\npip install git+https://github.com/tlambert03/napari-manual-transforms.git\n```\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-manual-transforms\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/tlambert03/napari-manual-transforms/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Rotation Helper"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-blob-detection",
    "name": "napari-blob-detection",
    "display_name": "napari blob detection",
    "version": "0.0.2",
    "created_at": "2022-04-22",
    "modified_at": "2022-04-22",
    "authors": [
      "Andy Sweet",
      "Chi-Li Chiu"
    ],
    "author_emails": [
      "andrewdsweet@gmail.com",
      "cchiu@chanzuckerberg.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-blob-detection/",
    "home_github": "https://github.com/andy-sweet/napari-blob-detection",
    "home_other": null,
    "summary": "Detects blobs in images",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari (>=0.4.13)",
      "numpy",
      "scikit-image",
      "magicgui",
      "pytest ; extra == 'test'"
    ],
    "package_metadata_description": "# napari-blob-detection\n\n[![License](https://img.shields.io/pypi/l/napari-blob-detection.svg?color=green)](https://github.com/andy-sweet/napari-blob-detection/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-blob-detection.svg?color=green)](https://pypi.org/project/napari-blob-detection)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-blob-detection.svg?color=green)](https://python.org)\n[![tests](https://github.com/andy-sweet/napari-blob-detection/workflows/tests/badge.svg)](https://github.com/andy-sweet/napari-blob-detection/actions)\n[![codecov](https://codecov.io/gh/andy-sweet/napari-blob-detection/branch/main/graph/badge.svg)](https://codecov.io/gh/andy-sweet/napari-blob-detection)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-blob-detection)](https://napari-hub.org/plugins/napari-blob-detection)\n\nDetects blobs in images\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\nThis plugin consists of two widgets:\n\n1. Detects blobs on images\n2. Convert points layer to labels layer\n\n----------------------------------\n\n### Detects blobs on images\n\nThis widget uses [scikit-image's blob detection algorithms](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_blob.html) to detect bright blobs on dark backgrounds.\n\nParameters\n\n- method: Laplacian of Gaussian (most accurate) or Difference of Gaussian (faster approximation) \n- image: Image layer for blob detection. Can be a 2D, 3D, or higher dimensionality image.\n- dimensionality: users can specify if the image is 2D(+t) or 3D(+t).\n- min sigma: the smallest blob size to detect\n- max sigma: the largest blob size to detect\n- threshold: the lower the threshold, the more low intensity blobs are detected. \n\nOutput\n\nBlobs are represented by the Points layer.\nThe size of each blob is proportional to `Points.feature['sigma']`,\nwhich signifies the scale at which the feature point was found.\n\n### Convert points layer to labels layer\n\nThis widget takes a points layer and converts it into a labels layer, with the image dimension matching the selected image layer.\nBy converting points to labels, users can leverage feature extraction functions that are available to labels to the detected points.\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-blob-detection` via [pip]:\n\n    pip install napari-blob-detection\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/andy-sweet/napari-blob-detection.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-blob-detection\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/andy-sweet/napari-blob-detection/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Detects blobs on images",
      "Convert points layer to labels layer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-power-spectrum",
    "name": "napari-power-spectrum",
    "display_name": "Power Spectrum",
    "version": "0.0.6",
    "created_at": "2022-04-08",
    "modified_at": "2022-04-22",
    "authors": [
      "Giorgia Tortora"
    ],
    "author_emails": [
      "giorgiatortora2@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-power-spectrum/",
    "home_github": "https://github.com/GiorgiaTortora/napari-power-spectrum",
    "home_other": null,
    "summary": "A simple plugin to get the power spectrum of frames of a stack image",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "numpy",
      "magicgui",
      "qtpy",
      "tox ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "napari ; extra == 'testing'",
      "pyqt5 ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-power-spectrum\n\n[![License](https://img.shields.io/pypi/l/napari-power-spectrum.svg?color=green)](https://github.com/GiorgiaTortora/napari-power-spectrum/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-power-spectrum.svg?color=green)](https://pypi.org/project/napari-power-spectrum)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-power-spectrum.svg?color=green)](https://python.org)\n[![tests](https://github.com/GiorgiaTortora/napari-power-spectrum/workflows/tests/badge.svg)](https://github.com/GiorgiaTortora/napari-power-spectrum/actions)\n[![codecov](https://codecov.io/gh/GiorgiaTortora/napari-power-spectrum/branch/main/graph/badge.svg)](https://codecov.io/gh/GiorgiaTortora/napari-power-spectrum)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-power-spectrum)](https://napari-hub.org/plugins/napari-power-spectrum)\n\nA simple plugin to get the power spectrum of frames of a stack image\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-power-spectrum` via [pip]:\n\n    pip install napari-power-spectrum\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/GiorgiaTortora/napari-power-spectrum.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-power-spectrum\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/GiorgiaTortora/napari-power-spectrum/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Power Spectrum Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-animated-gif-io",
    "name": "napari-animated-gif-io",
    "display_name": "napari-animated-gif-io",
    "version": "0.1.2",
    "created_at": "2021-12-03",
    "modified_at": "2022-04-15",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-animated-gif-io/",
    "home_github": "https://github.com/haesleinhuepf/napari-animated-gif-io",
    "home_other": null,
    "summary": "Save 3D image stacks as animated gifs",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "imageio",
      "napari-tools-menu",
      "napari",
      "microfilm"
    ],
    "package_metadata_description": "# napari-animated-gif-io\n\n[![License](https://img.shields.io/pypi/l/napari-animated-gif-io.svg?color=green)](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-animated-gif-io.svg?color=green)](https://pypi.org/project/napari-animated-gif-io)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-animated-gif-io.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-animated-gif-io/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-animated-gif-io/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-animated-gif-io/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-animated-gif-io)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-animated-gif-io)](https://napari-hub.org/plugins/napari-animated-gif-io)\n\nOpen and save 3D image stacks as animated gifs\n\nYou find the menus for opening and saving animated gifs in the `Tools > File Import/Export` menu:\n\n![img.png](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/docs/screenshot.png)\n\nFurthermore, if in 3D view, you can save the current view with a little tilt animation as animated gif.\nUnder the hood this uses the [microfilm](https://github.com/guiwitz/microfilm) library.\n\n![img.png](https://github.com/haesleinhuepf/napari-animated-gif-io/raw/main/docs/video.gif)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-animated-gif-io` via [pip]:\n\n    pip install napari-animated-gif-io\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/haesleinhuepf/napari-animated-gif-io.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-animated-gif-io\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-animated-gif-io/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "save_as_animated_gif"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-workflow-optimizer",
    "name": "napari-workflow-optimizer",
    "display_name": "napari-workflow-optimizer",
    "version": "0.1.4",
    "created_at": "2021-12-24",
    "modified_at": "2022-04-15",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-workflow-optimizer/",
    "home_github": "https://github.com/haesleinhuepf/napari-workflow-optimizer",
    "home_other": null,
    "summary": "Optimize image processing workflows in napari for segmentation quality",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pyclesperanto-prototype",
      "scikit-learn",
      "napari-time-slicer",
      "matplotlib",
      "scipy",
      "napari-workflows",
      "napari-assistant (>=0.1.9)"
    ],
    "package_metadata_description": "# napari-workflow-optimizer\n\n[![License](https://img.shields.io/pypi/l/napari-workflow-optimizer.svg?color=green)](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-workflow-optimizer.svg?color=green)](https://pypi.org/project/napari-workflow-optimizer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-workflow-optimizer.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-workflow-optimizer/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-workflow-optimizer/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-workflow-optimizer/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-workflow-optimizer)\n[![Development Status](https://img.shields.io/pypi/status/napari-workflow-optimizer.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-workflow-optimizer)](https://napari-hub.org/plugins/napari-workflow-optimizer)\n\nOptimize image processing workflows in napari for segmentation quality\n\n![img.png](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/napari-workflow-optimizer.gif)\n\n## Usage\n\nThe starting point for workflow optimization is a workflow and some reference (\"ground truth\") labels image. \nThe label image can be a sparse annotation, which means only some objects and also parts of objets are annotated (see [hints](https://github.com/haesleinhuepf/napari-workflow-optimizer#optimization-hints)). \nThese datasets should be ready. You can reproduce the following procedure by downloading an \n[examle raw image](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membranes_2d.tif) (derived from the \n[scikit-image cells3d example data set](https://scikit-image.org/docs/dev/api/skimage.data.html#skimage.data.cells3d)) and a corresponding \n[sparse annotation label image](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membranes_2d_sparse_labels.tif).\nFor reproducing the following procedure, also follow the [installation instructions](https://github.com/haesleinhuepf/napari-workflow-optimizer#optimization-hints) below.\nThe whole procedure is [also shown in this video](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/napari-workflow-optimizer.mp4), an extended version of the trailer above.\n\n### Step 0: Loading data and setting up the workflow\n\nLoad the \"membranes_2d.tif\" data set, e.g. by drag&drop on napari and start the Assistant from the `Tools > Utilities > Assistant (clEsperanto)` menu.\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot1_start_raw.png)\n\nClick the `Label` button and select as operation \"Seeded watershed using local minima as seeds and an intensity threshold (nsbatwm)\".\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot2_labeled_beginning.png)\n\nDraw an annotation in a new labels layer or load the example spare annotation \"membranes_2d_sparse_labels.tif\". \n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot4_loaded_manual_annotation.png)\n\nIn case the image is not displayed as label image, convert it to a label image by right-clicking on the entry in the layers list:\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot3_load_manual_annotation.png)\n\n### Step 1: The Workflow Optimizer\n\nStart the Workflow Optimizer from the `Tools > Utilities > Workflow optimizer (Labels)` menu. \nConfigure the target layer, showing the label image that should be optimized.\nSelect the manual annotation as reference layer for the optimization. \nConsider increasing the number of iterations. This number depends on your segmenation problem. \nIn the present example, 100 iterations should be enough.\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot5_start_optimization.png)\n\nThe optimizer will plot quality over the number of iterations to show the progress of optimization. \nTo determine the quality, the optimizer will measure the maximum overlap ([Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)) \nof any labeled object over the manually annotated objects and calculate the mean of this value over all annotated objects.\nAfter a moment, optimization will finish and update the labeled image. \nIf your starting point for the optimization was already good, the result may now look better than before.\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot6_finished_optimization.png)\n\n### Step 2: Manual parameter space plotting\n\nIn case the result is not perfect yet (as the fringed segmentation above suggests), consider manual plotting of the \nindividual parameters and their relation to segmentation quality to get an idea about the surrounding parameter space.\nTherefore, click the `Plot` button next to one of the workflow parameters.\nSelect the range in which the labeling quality should be determined (green arrows). In our example, the optimizer was setting the parameter to 2.34. \nThus, to demonstrate the procedure we plot the parameter space beween 0 and 10. \nThe quality plotted over this parameter obviously has a local maxium at 2.34, which was detected by the optimizer.\nHowever, it also has another local maxium at 8 and actually a plateau in the quality plot (orange arrows).\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot7_parameter_quality_plot.png)\n\nFor further optimization, we re-configure the algorithm and set a new starting point for optimization of the parameter to 8.\nAfterwards, we restart the optimization. It will then optimize the settings again from the new starting point.\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot8_start_optimization_again.png)\n\nAfter another moment, optimization will finish again, potentially leading to an even better result.\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot9_finished_optimization_again.png)\n\n### Step 3: Visualization of results\n\nMake sure the segmentation has high quality by inspecting the result visually. Use the `contour` setting of the labels layer\nand hide/show the outlines of the labeled layer:\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot10_contours_on.png)\n\n![](https://github.com/haesleinhuepf/napari-workflow-optimizer/raw/main/docs/screenshot11_contours_off.jpg)\n\n### Optimization Hints\n\nThe Workflow Optimizer uses the [Nelder-Mead simplex method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)\nfor optimizing parameters. This algorithm varies individual parameters and makes steps in the parameter space ideally following a gradient \nto a local optimum. Hence, this algorithm may not be capable of determining a global optimum in parameter space. \nParameter optimization is no magic. If it does not immediately work on your data, plot the parameters as introduced in Step 2 \nand identify parameters with a clear gradient and those with many local maxima. \nConsider optimizing the parameters with many local maxima manually and de-selecting their checkboxes for the optimization.\nThe optimizer will then only optimize the parameters showing the clear gradient. \nRepeat these steps a couple of times to get a feeling for your parameter space. \n\nFurthermore, parameter optimization works well if\n* the initial settings are close to a good segmentation,\n* a small number of parameters (a short workflow) are optimized,\n* the reference annotation is prepared carefully and\n* the dataset is small. Consider using a small representative crop in case of bigger datasets.\n\n### Workflow optimization scripting\n\nFor optimizing workflows from within a jupyter notebook, check out our [example notebook for optimization using spare labels](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/sparse_label_image_optimizer.ipynb). \nThe examples are more flexible than the graphical user interface and allow for example [optimizing intensity images](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/intensity_image_optimizer.ipynb)\nand [binary images](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/binary_image_optimizer.ipynb).\nThe membrane segmentation workflow optimization similar to the one shown above is also available as [jupyter notebook](https://github.com/haesleinhuepf/napari-workflow-optimizer/blob/main/demo/membrane_segmentation.ipynb).\n\n### Known issues\n\nIf you change the workflow architecture after the optimizer window was opened, please re-open it\nto select the parameters that should be optimized. Changing parameters is ok and re-opening is not necessary.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nFurthermore, to reproduce the procedure above, please download and install \n[napari](https://napari.org/),\n[pyopencl](https://documen.tician.de/pyopencl/),\nthe [napari-pyclesperanto-assistant](https://www.napari-hub.org/plugins/napari-pyclesperanto-assistant) and\nthe [napari-segment-blobs-and-things-with-membranes](https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes) plugin. E.g. using \n[conda](https://docs.conda.io/en/latest/) and [pip](https://pypi.org/project/pip/):\n\n```\nconda create --name napari-opti python=3.8\nconda activate napari-opti\n\nconda install pyopencl napari\npip install napari-pyclesperanto-assistant napari-segment-blobs-and-things-with-membranes\npip install napari-workflow-optimizer\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-workflow-optimizer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-workflow-optimizer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "WorkflowOptimizer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-imaris-loader",
    "name": "napari-imaris-loader",
    "display_name": "napari-imaris-loader",
    "version": "0.1.8",
    "created_at": "2021-10-21",
    "modified_at": "2022-04-13",
    "authors": [
      "Alan M Watson"
    ],
    "author_emails": [
      "alan.watson@pitt.edu"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-imaris-loader/",
    "home_github": "https://github.com/AlanMWatson/napari-imaris-loader",
    "home_other": null,
    "summary": "Napari plugin for loading Bitplane imaris files '.ims'",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "napari[all]",
      "napari-plugin-engine (>=0.1.4)",
      "imaris-ims-file-reader (>=0.1.5)",
      "numpy",
      "h5py",
      "dask"
    ],
    "package_metadata_description": "\n\n# Description\n\nThis plugin enables viewing of Bitplane Imaris files, including very large datasets.  The GIFs below demonstrate rendering of a ~2TB IMS file containing a 2 color whole mouse brain.  The plugin has been tested on datasets as large as 20TB.\n\n**NOTE: For this plugin to work \"File/Preferences/Experimental/Render Images Asynchronously\" must be selected.**\n\n### Open IMS file:\n\n![Open IMS file GIF](https://i.imgur.com/ByHb0wI.gif \"Open IMS file\")\n\n\n\n### Render in 3D:\n\nA plugin is provided to dynamically reload the data after selecting the lowest resolution level to be included in the viewer.  Since napari only renders the lowest resolution, the user can use this plugin to control the quality of 3D rendering.  See features and limitations for tips on suggested usage.\n\n![3D Rendering and Quality Adjustment GIF](https://i.imgur.com/MZNlWtM.gif \"3D Rendering and Quality Adjustment\")\n\n### Features\n\n* Multiscale Rendering\n  * Image pyramids which are present in the native IMS format are automatically added to napari during file loading.\n* Chunks are implemented by dask and matched to the chunk sizes stored in each dataset.  (Napari appears to only ask for 2D chunks - unclear how helpful this feature is currently)\n* Successfully handles multi-terabyte multi-timepoint multi-channel datasets.\n* Tested with all sample files provided by Bitplane.\n* Higher 3D rendering quality is enabled by a widget that reloads data after specifying the lowest resolution level (higher number = lower resolution) to be included in the multiscale series.\n\n### Known Issues / limitations\n\n* Currently, this is **only an image loader**, and there are no features for loading or viewing objects\n* Napari sometimes throws errors indicating that it expected a 3D or 5D array but receives the other.\n  * This sometimes *but relatively rarely* causes napari to crash\n  * Would like to enable Asynchronous Tiling of Images, but this results in more instability and causes crashes.\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-imaris-loader` via [pip]:\n\n    pip install napari-imaris-loader\n\n## Change Log:\n\n##### <u>v0.1.2:</u>\n\n**Fixed:** Issue where .ims files containing a single color 2D image would not open.\n\n**Fixed:** Issue where using the widget to change resolutions while in 3D rendering would cause a crash.  Now the viewer is automatically forced into 2D rendering mode when the widget is used.\n\n**Dependency change:** The loader is now dependent in a separate package for loading IMS files.  https://pypi.org/project/imaris-ims-file-reader/\n\n**v0.1.3:**\n\nDocumentation\n\n**v0.1.4:**\n\nAdd napari to install requirements for plugin compatibility\n\n**v0.1.5:**\n\nChanges to napari:\n\n- now requires napari[all] upon install.\n- requires >=v0.1.5 of imaris-ims-file-reader\n\n**v0.1.6:**\n\n- Fix issue #7 where contrastLimits possibly unbound in reader\n\n**v0.1.7:**\n\n- For squeeze_output=False when opening .ims file for Napari compatibility\n\n**v0.1.8:**\n\n- Add automatic determination of contrast_limits\n- Fix bug in squeeze_output\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-imaris-loader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/AlanMWatson/napari-imaris-loader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "resolution_change"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "iacs-ipac-reader",
    "name": "iacs-ipac-reader",
    "display_name": "iacs-ipac-reader",
    "version": "0.0.13",
    "created_at": "2022-01-21",
    "modified_at": "2022-04-12",
    "authors": [
      "Chenqi Zhang"
    ],
    "author_emails": [
      "cqzhang@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/iacs-ipac-reader/",
    "home_github": "https://github.com/zcqwh/iacs_ipac_reader",
    "home_other": null,
    "summary": "A reader plugin for read iacs/ipac images and export .rtdc files.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "h5py (>=3.5.0)",
      "napari (>=0.4.12)",
      "napari-plugin-engine (>=0.2.0)",
      "numpy (>=1.21.4)",
      "opencv-contrib-python-headless (>=4.4.0.46)",
      "openpyxl (>=3.0.9)",
      "sklearn (>=0.0)",
      "PyQt5 (==5.12.3)",
      "pandas (>=1.4.0)"
    ],
    "package_metadata_description": "# iacs_ipac_reader\n\n[![License](https://img.shields.io/pypi/l/iacs_ipac_reader.svg?color=green)](https://github.com/zcqwh/iacs_ipac_reader/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/iacs_ipac_reader.svg?color=green)](https://pypi.org/project/iacs_ipac_reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/iacs_ipac_reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/zcqwh/iacs_ipac_reader/workflows/tests/badge.svg)](https://github.com/zcqwh/iacs_ipac_reader/actions)\n[![codecov](https://codecov.io/gh/zcqwh/iacs_ipac_reader/branch/main/graph/badge.svg)](https://codecov.io/gh/zcqwh/iacs_ipac_reader)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/iacs_ipac_reader)](https://napari-hub.org/plugins/iacs_ipac_reader)\n\nA plugin used a convolutional neural network (CNN) to distinguish single platelets, platelet clusters, and white blood cells and performed classical image analysis for each subpopulation individually. Based on the derived single-cell features for each population, a Random Forest (RF) model was trained and used to classify COVID-19 associated thrombosis and non-COVID-19 associated thrombosis.\n\nMore information about IACS/iPAC.  \n__IACS__: DOI: [10.1016/j.cell.2018.08.028](https://www.sciencedirect.com/science/article/pii/S0092867418310444)   \n__iPAC__: DOI: [10.7554/eLife.52938](https://elifesciences.org/articles/52938)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `iacs_ipac_reader` via [pip]:\n\n    pip install iacs_ipac_reader\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/zcqwh/iacs_ipac_reader.git\n\n\n## Introduction\n\nThe iacs-ipac-reader plugin mainly include 3 functional tabs:\n\n* iPAC\n* IACS\n* AID classif.\n\n\n\n### iPAC image contour tracker\n<center>Interface of iPAC contour tracker</center>    \n\n![ipac.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/ipac.png?raw=true \"iPAC\")\n\n### IACS image contour tracker\n<center>Interface of IACS contour tracker</center>    \n\n![iacs.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/iacs.png?raw=true \"IACS\")\n\n### AID classif.\n<center>Interface of AID classif.</center>     \n \n![AID_classif.](https://github.com/zcqwh/iacs_ipac_reader/blob/main/Tutorial/pictures/classifier.jpg?raw=true \"AID classif\")\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"iacs_ipac_reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/zcqwh/iacs_ipac_reader/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "iacs_ipac_reader"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-math",
    "name": "napari-math",
    "display_name": "napari math",
    "version": "0.0.1b0",
    "created_at": "2022-01-18",
    "modified_at": "2022-03-29",
    "authors": [
      "Zach Marin",
      "Talley Lambert"
    ],
    "author_emails": [
      "zach.marin@yale.edu"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-math/",
    "home_github": "https://github.com/zacsimile/napari-math",
    "home_other": null,
    "summary": "Simple mathematical operations on image, point and surface layers.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "numpy"
    ],
    "package_metadata_description": "# napari-math\n\n[![License](https://img.shields.io/pypi/l/napari-math.svg?color=green)](https://github.com/zacsimile/napari-math/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-math.svg?color=green)](https://pypi.org/project/napari-math)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-math.svg?color=green)](https://python.org)\n[![tests](https://github.com/zacsimile/napari-math/workflows/tests/badge.svg)](https://github.com/zacsimile/napari-math/actions)\n[![codecov](https://codecov.io/gh/zacsimile/napari-math/branch/main/graph/badge.svg)](https://codecov.io/gh/zacsimile/napari-math)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-math)](https://napari-hub.org/plugins/napari-math)\n\nThis package provides a GUI interfrace for simple mathematical operations on image, point and surface layers.\n\n- addition\n- subtraction\n- multiplication\n- division\n- logical and, or, xor\n- z-projection (mean and sum)\n\nOperations can be peformed on a single layer or between Image layers (functionaly pending for Surface and Point layers), \nfor example adding one layer to another.\n\nWhen performing operations on two images of different sizes, the result will be the size of the smallest\nof the two images.\n\n----------------------------------\n\n<!--\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n-->\n\n## Installation\n\nYou can install `napari-math` via [pip]:\n\n    pip install napari-math\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-math\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please file an [issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Math operations",
      "Add images",
      "Add image and scalar",
      "Subtract images",
      "Subtract image and scalar",
      "Multiply images",
      "Multiply image by scalar",
      "Divide images",
      "Divide image by scalar",
      "Logical AND images",
      "Logical AND image and value",
      "Logical OR images",
      "Logical OR image and value",
      "Logical XOR images",
      "Logical XOR image and value",
      "z-project sum",
      "z-project mean",
      "z-project max"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-plot",
    "name": "napari-plot",
    "display_name": "napari-plot",
    "version": "0.1.5",
    "created_at": "2022-01-09",
    "modified_at": "2022-03-22",
    "authors": [
      "Lukasz G. Migas"
    ],
    "author_emails": [
      "lukas.migas@yahoo.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-plot/",
    "home_github": "https://github.com/lukasz-migas/napari-1d",
    "home_other": null,
    "summary": "Plugin providing support for 1d plotting in napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "qtpy",
      "qtawesome",
      "napari (<0.4.15,>=0.4.13)",
      "matplotlib",
      "vispy (>=0.9.6)",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'all'",
      "pre-commit (>=2.9.0) ; extra == 'dev'",
      "black (==22.1.0) ; extra == 'dev'",
      "flake8 (==4.0.1) ; extra == 'dev'",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-qt ; extra == 'dev'",
      "scikit-image ; extra == 'dev'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'pyqt5'",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'pyside'",
      "PySide2 (!=5.15.0,>=5.13.2) ; extra == 'pyside2'",
      "PyQt5 (!=5.15.0,>=5.12.3) ; extra == 'qt'",
      "pytest ; extra == 'testing'",
      "pytest-qt ; extra == 'testing'",
      "scikit-image ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-plot\n\n[![License](https://img.shields.io/pypi/l/napari-plot.svg?color=green)](https://github.com/lukasz-migas/napari-1d/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-plot.svg?color=green)](https://pypi.org/project/napari-plot)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plot.svg?color=green)](https://python.org)\n[![tests](https://github.com/lukasz-migas/napari-1d/workflows/tests/badge.svg)](https://github.com/lukasz-migas/napari-1d/actions)\n[![codecov](https://codecov.io/gh/lukasz-migas/napari-1d/branch/main/graph/badge.svg)](https://codecov.io/gh/lukasz-migas/napari-1d)\n\nPlugin providing support for 1d plotting in napari.\n\nThis plugin is in very early stages of development and many things are still in a state of disarray. New features and bug fixes\nwill be coming over the coming months. \n\n## Note\n\n`napari-plot` provides several custom icons and stylesheets to take advantage of the `Qt` backend. Since it would be a bit busy to add multiple layer lists,\nI opted to include a toolbar that quickly pulls the layer list whenever requested. Simple use the toolbar to access several commonly accessed elements.\n\n## Usage\n\nYou can use `napari-plot` alongside `napari` where it is embedded as a dock widget. If using this option, controls are relegated to toolbar\nwhere you can adjust layer properties like you would do in `napari`.\n\n![embedded](https://github.com/lukasz-migas/napari-1d/blob/main/misc/embedded.png)\n\nOr as a standalone app where only one-dimensional plotting is enabled. In this mode, controls take central stage and reflect `napari's` own\nbehaviour where layer controls are embedded in the main application.\n\n![live-view](https://github.com/lukasz-migas/napari-1d/blob/main/misc/napariplot-live-line.gif)\n\n## Roadmap:\n\nThis is only provisional list of features that I would like to see implemented. It barely scratches the surface of what plotting tool should cover so as soon as the basics are covered,\nfocus will be put towards adding more exotic features. If there are features that you certainly wish to be included,\nplease modify the list below or create a [new issue](https://github.com/lukasz-migas/napari-1d/issues/new)\n\n- [ ] Support for new layer types. Layers are based on `napari's` `Layer`, albeit in a two-dimensional setting. Supported and planned layers:\n  - [x] Line Layer - simple line plot.\n  - [x] Scatter Layer - scatter plot (similar to `napari's Points` layer).\n  - [x] Centroids/Segments Layer - horizontal or vertical line segments.\n  - [x] InfLine Layer - infinite horizontal or vertical lines that span over very broad range. Useful for defining regions of interest.\n  - [x] Region Layer - infinite horizontal or vertical rectangular boxes that span over very broad range. Useful for defining regions of interest.\n  - [x] Shapes Layer - `napari's` own `Shapes` layer\n  - [x] Points Layer - `napari's` own `Points` layer\n  - [x] Multi-line Layer - more efficient implementation of `Line` layer when multiple lines are necessary.\n  - [ ] Bar - horizontal and vertical barchart (TODO)\n- [x] Proper interactivity of each layer type (e.g. moving `Region` or `InfLine`, adding points, etc...)\n- [x] Intuitive interactivity. `napari-plot` will provide excellent level of interactivity with the plotted data. We plan to support several types of `Tools` that permit efficient interrogation of the data. We currently provide several `zoom` and `select` tools and hope to add few extras in the future.\n  - [x] Box-zoom - standard zooming rectangle. Simply `left-mouse + drag/release` in the canvas on region of interest\n  - [x] Horizontal span - zoom-in only in the y-axis by `Ctrl + left-mouse + drag/release` in the canvas.\n  - [x] Vertical span - span-in only in the x-axis by `Shift + left-mouse + drag/release` in the canvas.\n  - [x] Rectangle select - rectangle tool allowing sub-selection of data in the canvas. Similar to the `Box-zoom` but without the zooming part.\n  - [x] Polygon select - polygon tool allowing sub-selection of data in the canvas.\n  - [x] Lasso select - lasso tool allowing sub-selection of data in the canvas.\n- [ ] Interactive plot legend\n- [ ] Customizable axis visuals.\n  - [x] Plot axis enabling customization of tick/label size and color\n  - [ ] Support for non-linear scale\n- [ ] Add convenient plotting interface:\n  - [ ] Add `.plot` functionality\n  - [ ] Add `.scatter` functionality\n  - [ ] Add `.hbar` and `.vbar` functionality\n  - [ ] Add `.imshow` functionality\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-plot` directly from PyPI via:\n\n```python\npip install napari-plot\n```\n\nor from the git repo:\n\n```python\ngit clone https://github.com/lukasz-migas/napari-1d.git\ncd napari-1d\npip install -e '.[all]'\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-plot\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/lukasz-migas/napari-1d/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "NapariPlotWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-dv",
    "name": "napari-dv",
    "display_name": "napari-dv",
    "version": "0.3.0",
    "created_at": "2021-01-27",
    "modified_at": "2022-03-19",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "talley.lambert@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-dv/",
    "home_github": "https://github.com/tlambert03/napari-dv",
    "home_other": null,
    "summary": "Deltavision/MRC file reader for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "mrc (>=0.2.0)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy ; extra == 'testing'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-dv\n\n[![License](https://img.shields.io/pypi/l/napari-dv.svg?color=green)](https://github.com/tlambert03/napari-dv/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-dv.svg?color=green)](https://pypi.org/project/napari-dv)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dv.svg?color=green)](https://python.org)\n[![tests](https://github.com/tlambert03/napari-dv/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-dv/actions)\n[![codecov](https://codecov.io/gh/tlambert03/napari-dv/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-dv)\n\nDeltavision/MRC file reader for napari.\n\nThis wraps the [mrc](https://github.com/tlambert03/mrc) library.\n\nSee also [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), which also uses the [mrc](https://github.com/tlambert03/mrc) to provide dv file support,\nalong with many other common file formats.\n\n## Installation\n\nYou can install `napari-dv` via [pip]:\n\n    pip install napari-dv\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-dv\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[file an issue]: https://github.com/tlambert03/napari-dv/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mrc",
      "*.dv"
    ],
    "contributions_writers_filename_extensions": [
      ".mrc",
      ".dv"
    ],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "workshop-demo",
    "name": "workshop-demo",
    "display_name": "workshop demo",
    "version": "0.0.2",
    "created_at": "2021-12-15",
    "modified_at": "2022-03-10",
    "authors": [
      "Draga Doncila Pop"
    ],
    "author_emails": [
      "ddoncilapop@contractor.chanzuckerberg.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/workshop-demo/",
    "home_github": "https://github.com/DragaDoncila/workshop-demo",
    "home_other": null,
    "summary": "A demo napari plugin incorporating reader, writer and dock widget contributions using the new npe2 plugin architecture.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "dask[array]",
      "imagecodecs",
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "npe2",
      "numpy",
      "scikit-image",
      "tifffile"
    ],
    "package_metadata_description": "# workshop-demo\n\n[![License](https://img.shields.io/pypi/l/workshop-demo.svg?color=green)](https://github.com/DragaDoncila/workshop-demo/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/workshop-demo.svg?color=green)](https://pypi.org/project/workshop-demo)\n[![Python Version](https://img.shields.io/pypi/pyversions/workshop-demo.svg?color=green)](https://python.org)\n[![tests](https://github.com/DragaDoncila/workshop-demo/workflows/tests/badge.svg)](https://github.com/DragaDoncila/workshop-demo/actions)\n[![codecov](https://codecov.io/gh/DragaDoncila/workshop-demo/branch/main/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/workshop-demo)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/workshop-demo)](https://napari-hub.org/plugins/workshop-demo)\n\nA demo napari plugin incorporating reader, writer and dock widget contributions using the new npe2 plugin architecture.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `workshop-demo` via [pip]:\n\n    pip install workshop-demo\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/DragaDoncila/workshop-demo.git\n\n## What is this?\n\nThis plugin was created to serve as a semi-meaningful example of a plugin using\nthe new napari [npe2](https://pypi.org/project/npe2/) architecture.\n\nIt provides a reader, a writer and two dock widgets to support opening, processing\nand writing out [cell tracking challenge](https://celltrackingchallenge.net/) data.\n\nWe've provided comments and example tests that can be used as a reference\nwhen building your own plugin.\n\n## Using this plugin\n\n### Sample Data\nYou can download sample data for this plugin from the tracking challenge website. Any 2D+T\nsequence should work, but this plugin has been tested only with the \n[Human hepatocarcinoma-derived cells expressing the fusion protein YFP-TIA-1](http://data.celltrackingchallenge.net/training-datasets/Fluo-C2DL-Huh7.zip) \ndataset.\n### Reading Data\nThis plugin's reader is designed for tracking challenge segmentation gold standard ground truth\ndata conforming to the file format described in the [data specification](https://public.celltrackingchallenge.net/documents/Naming%20and%20file%20content%20conventions.pdf).\n\nGround truth data is only provided for a subset of the frames of the entire sequence. This\nreader will attempt to find the number of frames of the associated sequence in a sister\ndirectory of the ground truth data directory and open a labels layer with the same number\nof frames, thus ensuring the labelled data is correctly overlaid onto the original sequence.\n\n\n\nhttps://user-images.githubusercontent.com/17995243/146114062-36124c05-f44a-488e-8991-f39a702c917f.mov\n\n\n\n### Segmenting Data\nOne of the dock widgets provided by this plugin is \"Segment by Threshold\". The widget\nallows you to select a 2D+T image layer in the viewer (e.g. any of the sequences in the Human \nhepatocarcinoma dataset above) and segment it using a selection of scikit-image thresholding functions.\n\nThe segmentation is then returned as a `Labels` layer into the viewer.\n\n\nhttps://user-images.githubusercontent.com/17995243/146114088-f6fb645e-8d78-4880-827b-2f0334dad859.mov\n\n\n\n### Highlighting Segmentation Differences\nThe second dock widget provided by this plugin allows you to visually compare your segmentation\nagainst the ground truth data by computing the difference between the two and adding this as a\nlayer in the napari viewer.\n\nTo use this widget, open it from the Plugins menu and select the two layers you wish to compare.\n\n\n\nhttps://user-images.githubusercontent.com/17995243/146114112-c891723f-8640-4708-8014-c78731fb3396.mov\n\n\n\n### Writing to Zip\nFinally, you can save your segmentation to a zip file whose internal directory structure\nwill closely mimic that of the tracking challenge datasets, so that it may be opened \nagain in the viewer.\n\nTo save your layer, choose File -> Save selected layer(s) with *one* labels layer selected,\nthen select label zipper from the dropdown choices.\n\n\n\nhttps://user-images.githubusercontent.com/17995243/146114163-ee886990-979c-4756-97c5-aaf2c39dccde.mov\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"workshop-demo\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/DragaDoncila/workshop-demo/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [
      ".zip"
    ],
    "contributions_widgets": [
      "Segment by Thresholding",
      "Highlight Segmentation Differences"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-video-cvdask",
    "name": "napari-video-cvdask",
    "display_name": "napari VideoCVDask",
    "version": "0.2.1",
    "created_at": "2022-02-24",
    "modified_at": "2022-02-25",
    "authors": [
      "Nicholas A. Del Grosso"
    ],
    "author_emails": [
      "delgrosso.nick@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-video-cvdask/",
    "home_github": "https://github.com/nickdelgrosso/napari-video-cvdask",
    "home_other": null,
    "summary": "A Video File Reader that uses OpenCV2 and Dask Arrays",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "dask-image",
      "av"
    ],
    "package_metadata_description": "# napari-video-cvdask\n\n[![License](https://img.shields.io/pypi/l/napari-video-cvdask.svg?color=green)](https://github.com/nickdelgrosso/napari-video-cvdask/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-video-cvdask.svg?color=green)](https://pypi.org/project/napari-video-cvdask)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-video-cvdask.svg?color=green)](https://python.org)\n[![tests](https://github.com/nickdelgrosso/napari-video-cvdask/workflows/tests/badge.svg)](https://github.com/nickdelgrosso/napari-video-cvdask/actions)\n[![codecov](https://codecov.io/gh/nickdelgrosso/napari-video-cvdask/branch/main/graph/badge.svg)](https://codecov.io/gh/nickdelgrosso/napari-video-cvdask)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-video-cvdask)](https://napari-hub.org/plugins/napari-video-cvdask)\n\nA Video File Reader that used to use OpenCV2 and Dask Arrays, and now uses dask-image, which does the same thing but better.  (Pro-tip, never name a package after its dependencies!)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-video-cvdask` via [pip]:\n\n    pip install napari-video-cvdask\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/nickdelgrosso/napari-video-cvdask.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-video-cvdask\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*.mov",
      "*.mp4",
      "*.avi"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ids",
    "name": "napari-IDS",
    "display_name": "napari IDS",
    "version": "0.0.8",
    "created_at": "2022-02-17",
    "modified_at": "2022-02-23",
    "authors": [
      "Tristan Cotte"
    ],
    "author_emails": [
      "tristan.cotte@sgs.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-ids/",
    "home_github": "https://github.com/tcotte/napari-IDS",
    "home_other": null,
    "summary": "Plug in which enables to take photo with IDS uEye camera",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "opencv-python",
      "numpy"
    ],
    "package_metadata_description": "# napari-IDS\n\n[![License](https://img.shields.io/pypi/l/napari-IDS.svg?color=green)](https://github.com/githubuser/napari-IDS/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-IDS.svg?color=green)](https://pypi.org/project/napari-IDS)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-IDS.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-IDS)](https://napari-hub.org/plugins/napari-IDS)\n\nA simple plugin to use with napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-IDS` via [pip]:\n\n    pip install napari-IDS\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/githubuser/napari-IDS.git\n\n\n## First utilisation\n\nSuggested environment : \n- Python 3.8\n- IDS 1.2.0.5 version installed\n\nTo use this package for the first time :\n1. Install Napari `pip install \"napari[all]\"`\n2. Install napari-IDS package\n3. Install IDS Python api thanks to the command `ids_packages`\n\nIf your environment is not the suggested environment, you have to install IDS packages manually. \n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-IDS\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/githubuser/napari-IDS/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Live IDS QWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "nfinder",
    "name": "nfinder",
    "display_name": "nfinder",
    "version": "0.3",
    "created_at": "2021-12-06",
    "modified_at": "2022-02-05",
    "authors": [
      "Santiago N. Rodriguez Alvarez"
    ],
    "author_emails": [
      "rodriguezsantiago96@gmail.com"
    ],
    "license": "Unavailable",
    "home_pypi": "https://pypi.org/project/nfinder/",
    "home_github": "https://github.com/santi-rodriguez/nfinder",
    "home_other": null,
    "summary": "Automatic inference of neighboring cells based on their Delaunay triangulation.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": null,
    "package_metadata_description": "# Nfinder\nAutomatic inference of neighboring cells based on their Delaunay triangulation.\n\n## Dependencies \nnfinder was tested with:\n\n- python = 3.8.5\n- napari = 0.4.12\n- numpy = 1.21.2\n- pandas = 1.3.4\n- scikit-image = 0.18.3\n- scipy = 1.7.1\n- importlib-resources 5.4.0\n\n\n## Installation\n\nIt can be installed with `pip` from PyPI:\n\n```\npip install nfinder\n```\n\n\n## Usage\nFor usage examples, please check out the [notebook](https://github.com/santi-rodriguez/nfinder/blob/main/examples.ipynb) in our GitHub repository.\n\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "find"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-image-stacker",
    "name": "napari-image-stacker",
    "display_name": "napari-image-stacker",
    "version": "0.1.10",
    "created_at": "2022-01-11",
    "modified_at": "2022-02-04",
    "authors": [
      "Robin Koch",
      "Marc Boucsein"
    ],
    "author_emails": [
      "robin.koch@dkfz-heidelberg.de"
    ],
    "license": "BSD3",
    "home_pypi": "https://pypi.org/project/napari-image-stacker/",
    "home_github": "https://github.com/RobAnKo/napari-image-stacker",
    "home_other": null,
    "summary": "A plugin designed to convert multiple open layers into a stack or vice versa",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy"
    ],
    "package_metadata_description": "# napari-image-stacker\n\n[![License](https://img.shields.io/pypi/l/napari-image-stacker.svg?color=green)](https://github.com/RobAnKo/napari-image-stacker/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-image-stacker.svg?color=green)](https://pypi.org/project/napari-image-stacker)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-image-stacker.svg?color=green)](https://python.org)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-image-stacker)](https://napari-hub.org/plugins/napari-image-stacker)\n\nA plugin designed to convert multiple open layers into a stack or vice versa\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-image-stacker` via [pip]:\n\n    pip install napari-image-stacker\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/RobAnKo/napari-image-stacker.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-image-stacker\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/RobAnKo/napari-image-stacker/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "image_stacker_widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-yapic-prediction",
    "name": "napari-yapic-prediction",
    "display_name": "napari-yapic-prediction",
    "version": "0.2.0",
    "created_at": "2021-04-19",
    "modified_at": "2022-02-04",
    "authors": [
      "Duway Nicolas Lesmes Leon",
      "Pranjal Dhole"
    ],
    "author_emails": [
      "dlesmesleon@hotmail.com",
      "dhole.pranjal@gmail.com"
    ],
    "license": "GNU GPL v3.0",
    "home_pypi": "https://pypi.org/project/napari-yapic-prediction/",
    "home_github": "https://github.com/yapic/napari-yapic-prediction",
    "home_other": null,
    "summary": "napari widget that performs image segmentation with yapic model in the napari window. Install TENSORFLOW to use this plugin.",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari[all]",
      "yapic",
      "scikit-image"
    ],
    "package_metadata_description": "# napari-yapic-prediction\n\n[![License](https://img.shields.io/pypi/l/napari-yapic-prediction.svg?color=green)](https://github.com/yapic/napari-yapic-prediction/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-yapic-prediction.svg?color=green)](https://pypi.org/project/napari-yapic-prediction)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-yapic-prediction.svg?color=green)](https://python.org)\n[![tests](https://github.com/yapic/napari-yapic-prediction/workflows/tests/badge.svg)](https://github.com/yapic/napari-yapic-prediction/actions)\n[![codecov](https://codecov.io/gh/yapic/napari-yapic-prediction/branch/master/graph/badge.svg?token=amah2YwOpx)](https://codecov.io/gh/yapic/napari-yapic-prediction)\n\nA napari widget plugin to perform YAPiC model segmentation prediction in the napari window. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Description\n\nThis napari plugin provides a widget to upload a [YAPiC] trained model and perform segmentation over all the present images in the napari window. The segmentation results are uploaded as napari layers into the viewer automatically with the name structure of *imgename_prediction*.\n\n## Installation\n\n1. Please install either GPU or CPU version of tensorflow that is compatible with your `cuda` and `cudnn` libraries before installing the plugin depending on your system.\nOne of the plugin dependency is `yapic` that currently has sensitivity to tensorflow versions.\n\n2. You can install `napari-yapic-prediction` via [pip]:\n\n    ```pip install napari-yapic-prediction```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-yapic-prediction\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/yapic/napari-yapic-prediction/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[YAPiC]: https://yapic.github.io/yapic/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MyWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bigwarp",
    "name": "napari-bigwarp",
    "display_name": "napari-bigwarp",
    "version": "0.0.1",
    "created_at": "2022-01-26",
    "modified_at": "2022-01-26",
    "authors": [
      "Ben Kantor"
    ],
    "author_emails": [
      "benkantor@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-bigwarp/",
    "home_github": "https://github.com/bkntr/napari-bigwarp",
    "home_other": null,
    "summary": "BigWarp-like interface for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "opencv-contrib-python",
      "opencv-python"
    ],
    "package_metadata_description": "# napari-bigwarp\n\n[![License](https://img.shields.io/pypi/l/napari-bigwarp.svg?color=green)](https://github.com/bkntr/napari-bigwarp/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bigwarp.svg?color=green)](https://pypi.org/project/napari-bigwarp)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bigwarp.svg?color=green)](https://python.org)\n[![tests](https://github.com/bkntr/napari-bigwarp/workflows/tests/badge.svg)](https://github.com/bkntr/napari-bigwarp/actions)\n[![codecov](https://codecov.io/gh/bkntr/napari-bigwarp/branch/main/graph/badge.svg)](https://codecov.io/gh/bkntr/napari-bigwarp)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-bigwarp)](https://napari-hub.org/plugins/napari-bigwarp)\n\nBigWarp-like interface for napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-bigwarp` via [pip]:\n\n    pip install napari-bigwarp\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/bkntr/napari-bigwarp.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-bigwarp\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/bkntr/napari-bigwarp/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "BigWarpQWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-labelling-assistant",
    "name": "napari-labelling-assistant",
    "display_name": "napari-labelling-assistant",
    "version": "0.0.5",
    "created_at": "2022-01-19",
    "modified_at": "2022-01-24",
    "authors": [
      "Pranjal Dhole"
    ],
    "author_emails": [
      "dhole.pranjal@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-labelling-assistant/",
    "home_github": "https://github.com/pranjaldhole/napari-labelling-assistant",
    "home_other": null,
    "summary": "A lightweight plugin for visualizing labelling statistics.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "matplotlib"
    ],
    "package_metadata_description": "# napari-labelling-assistant\n\n[![License](https://img.shields.io/pypi/l/napari-labelling-assistant.svg?color=green)](https://github.com/pranjaldhole/napari-labelling-assistant/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-labelling-assistant.svg?color=green)](https://pypi.org/project/napari-labelling-assistant)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labelling-assistant.svg?color=green)](https://python.org)\n[![tests](https://github.com/pranjaldhole/napari-labelling-assistant/workflows/tests/badge.svg)](https://github.com/pranjaldhole/napari-labelling-assistant/actions)\n[![codecov](https://codecov.io/gh/pranjaldhole/napari-labelling-assistant/branch/main/graph/badge.svg)](https://codecov.io/gh/pranjaldhole/napari-labelling-assistant)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelling-assistant)](https://napari-hub.org/plugins/napari-labelling-assistant)\n\nA lightweight plugin for visualizing labelling statistics.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-labelling-assistant` via [pip]:\n\n    pip install napari-labelling-assistant\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/pranjaldhole/napari-labelling-assistant.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-labelling-assistant\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/pranjaldhole/napari-labelling-assistant/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "LabellingAssistant"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "label-creator",
    "name": "Label-Creator",
    "display_name": "Label-Creator",
    "version": "0.0.9",
    "created_at": "2022-01-12",
    "modified_at": "2022-01-21",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/label-creator/",
    "home_github": "https://github.com/MBPhys/Label-Creator",
    "home_other": null,
    "summary": "A napari plugin for generation of Label-Layers according to selected image data shapes",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "dask"
    ],
    "package_metadata_description": "# Label-Creator\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Label-Creator/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Label-Creator.svg?color=green)](https://pypi.org/project/Label-Creator)\n[![Python Version](https://img.shields.io/pypi/pyversions/Label-Creator.svg?color=green)](https://python.org)\n\n\nA napari plugin for generation of Label-Layers according to selected image data shapes.\n\n----------------------------------\n\n## Installation\n\nYou can install `Label-Creator` via [pip]:\n\n    pip install Label-Creator\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"Label-Creator\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/Label-Creator/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Creator"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-elementary-numpy-operations",
    "name": "napari-elementary-numpy-operations",
    "display_name": "napari-elementary-numpy-operations",
    "version": "0.0.5",
    "created_at": "2022-01-12",
    "modified_at": "2022-01-21",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-elementary-numpy-operations/",
    "home_github": "https://github.com/MBPhys/napari-elementary-numpy-operations",
    "home_other": null,
    "summary": "A napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90¬∞ steps",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "qtpy",
      "superqt"
    ],
    "package_metadata_description": "# napari-elementary-numpy-operations\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-elementary-numpy-operations/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-elementary-numpy-operations.svg?color=green)](https://pypi.org/project/napari-elementary-numpy-operations)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-elementary-numpy-operations.svg?color=green)](https://python.org)\n\n\nA napari plugin covers elementary numpy operations like swap axes, flip, sqeeze an array or rotate an arrays by 90¬∞ steps.\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-elementary-numpy-operations` via [pip]:\n\n    napari-elementary-numpy-operations\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-elementary-numpy-operations\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/napari-elementary-numpy-operations/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "elementary_numpy"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "partial-aligner",
    "name": "Partial-Aligner",
    "display_name": "Partial-Aligner",
    "version": "0.0.1",
    "created_at": "2022-01-14",
    "modified_at": "2022-01-14",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/partial-aligner/",
    "home_github": "https://github.com/DKFZ-TMTRR/Partial-Aligner",
    "home_other": null,
    "summary": "A napari plugin for manual registration of (a part of) an image",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "packaging",
      "dask"
    ],
    "package_metadata_description": "# Partial-Aligner\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Partial-Aligner/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Partial-Aligner.svg?color=green)](https://pypi.org/project/Partial-Aligner)\n[![Python Version](https://img.shields.io/pypi/pyversions/Partial-Aligner.svg?color=green)](https://python.org)\n\n\nA napari plugin to affine transform images and parts of images in 2D and 3D. It was developed in the context of brain slice registration and solves multiple, related problems when working with histology slices.\n\n----------------------------------\n\n## Installation\n\nYou can install `Partial-Aligner` via [pip]:\n\n    pip install Partial-Aligner\n    \nTo make full use of this plugin, please also install the sister plugins:\n\n    pip install Label-Creator\n    pip install Layer-Data-Replace\n    pip install World2Data\n\n## Usage\n\nIt is important to note that this plugin is part of a group of plugins ([Label-Creator](https://github.com/DKFZ-TMTRR/Label-Creator, \"Creates Labels\"), [Layer-Data-Replace](https://github.com/DKFZ-TMTRR/Layer-Data-Replace, \"Replaces the data of a layer with other data\"), [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \"Applies a transformation to an image\")) which are intended to be used together. \n\nThe principle workflow with this plugin is as follows:\n\n1. Load an image of interest (ioi) using standard napari.\n2. Find out meaningful transformation parameters for the ioi (or part of it) based on what you see in the viewer.\n3. (optional) Save the affine transformation matrix (can later be applied to other modalities)\n4. Apply the transformation to create a new, altered version of the ioi (use plugin [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \"Applies a transformation to an image\"))\n\nDecisions on the parameters (step 2) are made based on the problem at hand:\n\n- Registration: You have a second (fixed) image and you want to align your ioi to that image? Transform your whole ioi! Just play with the transformation parameters until you are happy with the alignment of ioi and fixed image.\n\n<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/36212786/149524198-9a25b6dc-4169-4546-85b3-7c2f57fccc97.png\" width=\"50%\" height=\"50%\">  <br /> \n     <i>DAPI staining (red) before (left) and after (right) manual registration on an MRI image (green).</i> \n</p>\n\n- Histology artifact repair: Parts of your histology slice are misplaced? Transform the misplaced parts! Label them and change the transformation parameters for the misplaced parts until you are happy with their alignment with the rest of the image.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/36212786/149526385-09aeebe2-d03e-4dd4-a424-d0f3af207529.png\" width=\"50%\" height=\"50%\">  <br /> \n     <i> Original slice with misplaced region (left), marked using the label function (middle) and after manual adjustment (right), where the misplaced region (green) was cut and newly positioned.</i> \n</p>\n\nTo make this plugin run reasonably fast, the affine transformations are not applied to the image data in real time. Instead, the internal napari viewing parameters are changed according to the transformation parameters. Therefore, to save transformed image data, the [World2Data](https://github.com/DKFZ-TMTRR/World2Data, \"Applies a transformation to an image\") plugin is used, which calculates and saves the resulting image based on the internal napari viewing parameters.\n\n\nHere we showcase a resulting multimodal 3D alignment of a whole mouse brain. The modalities are CT, MRI, simulated radiation dose distributions, DAPI staining and DNA-damage repair foci, with a Nissl-staining mouse atlas as template.\n\nhttps://user-images.githubusercontent.com/36212786/149530462-51a53631-bf74-459b-ab4e-572c52cf2692.mov\n\n\n\n\n\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"Partial-Aligner\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/Partial-Aligner/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Aligner"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "world2data",
    "name": "World2Data",
    "display_name": "World2Data",
    "version": "0.0.3",
    "created_at": "2022-01-14",
    "modified_at": "2022-01-14",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/world2data/",
    "home_github": "https://github.com/MBPhys/World2Data",
    "home_other": null,
    "summary": "A napari plugin in order to convert the world information to the data of a 2D/3D layer",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "gryds",
      "dask",
      "scikit-image"
    ],
    "package_metadata_description": "# World2Data\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/World2Data/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/World2Data.svg?color=green)](https://pypi.org/project/World2Data)\n[![Python Version](https://img.shields.io/pypi/pyversions/World2Data.svg?color=green)](https://python.org)\n\n\nA napari plugin in order to convert the world information to the data of a 2D/3D layer.\n\n----------------------------------\n\n## Installation\n\nYou can install `World2Data` via [pip]:\n\n    pip install World2Data\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"World2Data\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/World2Data/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "World2Data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "layer-data-replace",
    "name": "Layer-Data-Replace",
    "display_name": "Layer-Data-Replace",
    "version": "0.0.5",
    "created_at": "2022-01-13",
    "modified_at": "2022-01-13",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/layer-data-replace/",
    "home_github": "https://github.com/MBPhys/Layer-Data-Replace",
    "home_other": null,
    "summary": "A napari plugin in order to replace parts of the data of a layer by another one",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "dask"
    ],
    "package_metadata_description": "# Layer-Data-Replace\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Layer-Data-Replace/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Layer-Data-Replace.svg?color=green)](https://pypi.org/project/Layer-Data-Replace)\n[![Python Version](https://img.shields.io/pypi/pyversions/Layer-Data-Replace.svg?color=green)](https://python.org)\n\n\nA napari plugin in order to replace parts of the data of a layer by another one.\n\n----------------------------------\n\n## Installation\n\nYou can install `Layer-Data-Replace` via [pip]:\n\n    pip install Layer-Data-Replace\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"Layer-Data-Replace\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/Layer-Data-Replace/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Replace"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "offset-subtraction",
    "name": "Offset-Subtraction",
    "display_name": "Offset-Subtraction",
    "version": "0.0.5",
    "created_at": "2022-01-13",
    "modified_at": "2022-01-13",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/offset-subtraction/",
    "home_github": "https://github.com/MBPhys/Offset-Subtraction",
    "home_other": null,
    "summary": "A napari plugin in oder to subtract an intensity offset such as autofluorescence",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "dask"
    ],
    "package_metadata_description": "# Offset-Subtraction\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Offset-Subtraction/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Offset-Subtraction.svg?color=green)](https://pypi.org/project/Offset-Subtraction)\n[![Python Version](https://img.shields.io/pypi/pyversions/Offset-Subtraction.svg?color=green)](https://python.org)\n\n\nA napari plugin in oder to subtract an intensity offset such as autofluorescence\n\n----------------------------------\n\n## Installation\n\nYou can install `Offset-Subtraction` via [pip]:\n\n    pip install Offset-Subtraction\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"Offset-Subtraction\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/Offset-Subtraction/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Subtraction"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "image-composer",
    "name": "Image-Composer",
    "display_name": "Image-Composer",
    "version": "0.0.19",
    "created_at": "2022-01-10",
    "modified_at": "2022-01-12",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/image-composer/",
    "home_github": "https://github.com/MBPhys/Image-Composer",
    "home_other": null,
    "summary": "A napari plugin in order to compose a background image with a foreground image",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy"
    ],
    "package_metadata_description": "# Image-Composer\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Image-Composer/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Image-Composer.svg?color=green)](https://pypi.org/project/Image-Composer)\n[![Python Version](https://img.shields.io/pypi/pyversions/Image-Composer.svg?color=green)](https://python.org)\n\n\nA napari plugin in order to compose a background image with a foreground image.\n\n----------------------------------\n\n## Installation\n\nYou can install `Image-Composer` via [pip]:\n\n    pip install Image-Composer\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"Image-Composer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/Image-Composer/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Composer"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "image-part-selecter",
    "name": "Image-Part-Selecter",
    "display_name": "Image-Part-Selecter",
    "version": "0.0.7",
    "created_at": "2022-01-12",
    "modified_at": "2022-01-12",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/image-part-selecter/",
    "home_github": "https://github.com/MBPhys/Image-Part-Selecter",
    "home_other": null,
    "summary": "A napari plugin in order to select parts of images",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy"
    ],
    "package_metadata_description": "# Image-Part-Selecter\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/Image-Part-Selecter/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/Image-Part-Selecter.svg?color=green)](https://pypi.org/project/Image-Part-Selecter)\n[![Python Version](https://img.shields.io/pypi/pyversions/Image-Part-Selecter.svg?color=green)](https://python.org)\n\n\nA napari plugin in order to select parts of images\n\n----------------------------------\n\n## Installation\n\nYou can install `Image-Part-Selecter` via [pip]:\n\n    pip install Image-Part-Selecter\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"Image-Part-Selecter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/Image-Part-Selecter/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Selecter"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nd-cropper",
    "name": "napari-nd-cropper",
    "display_name": "napari-nd-cropper",
    "version": "0.1.3",
    "created_at": "2022-01-12",
    "modified_at": "2022-01-12",
    "authors": [
      "Marc Boucsein",
      "Robin Koch"
    ],
    "author_emails": [],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-nd-cropper/",
    "home_github": "https://github.com/MBPhys/napari-nd-cropper",
    "home_other": null,
    "summary": "A napari plugin in order to crop nd-images via different modes",
    "categories": [],
    "package_metadata_requires_python": ">=3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu",
      "qtpy",
      "superqt",
      "magicgui"
    ],
    "package_metadata_description": "# napari-nd-cropper\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-nd-cropper/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nd-cropper.svg?color=green)](https://pypi.org/project/napari-nd-cropper)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nd-cropper.svg?color=green)](https://python.org)\n\n\nA napari plugin in order to crop nd-images via different modes:\n\n- Cropping via Drag&Drop interaction box (available for napari releases > 0.4.12)\n- Cropping of double-clicked regions based on predefined size (Integer or Tuple of integer) \n- Cropping based on view \n- Cropping via Sliders \n\n\n----------------------------------\n\n## Installation\n\nYou can install `napari-nd-cropper` via [pip]:\n\n    pip install napari-nd-cropper\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-nd-cropper\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/napari-nd-cropper/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "nd_Cropper"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-medical-image-formats",
    "name": "napari-medical-image-formats",
    "display_name": "napari-medical-image-formats",
    "version": "0.3.8",
    "created_at": "2021-04-24",
    "modified_at": "2022-01-11",
    "authors": [
      "Marc Boucsein",
      "Marc Buckmakowski"
    ],
    "author_emails": [],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-medical-image-formats/",
    "home_github": "https://github.com/MBPhys/napari-medical-image-formats",
    "home_other": null,
    "summary": "A Plugin in order to read medical image formats such as DICOM and NIfTI",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pydicom",
      "SimpleITK",
      "itk",
      "itk-napari-conversion"
    ],
    "package_metadata_description": "# napari-medical-image-formats\n\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-medical-image-formats/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-medical-image-formats.svg?color=green)](https://pypi.org/project/napari-medical-image-formats)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-medical-image-formats.svg?color=green)](https://python.org)\n\n\nA Plugin in order to read and write medical image formats such as DICOM, DICOM Series and NIfTI. The meta information is supported by the package napari-itk-io. \n\n----------------------------------\n\n## Installation\n\nYou can install `napari-medical-image-formats` via [pip]:\n\n    pip install napari-medical-image-formats\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-medical-image-formats\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/MBPhys/napari-medical-image-formats/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-curtain",
    "name": "napari-curtain",
    "display_name": "napari-curtain",
    "version": "0.1.1",
    "created_at": "2021-11-07",
    "modified_at": "2022-01-01",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-curtain/",
    "home_github": "https://github.com/haesleinhuepf/napari-curtain",
    "home_other": null,
    "summary": "View one image over another as curtain",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# napari-curtain\n\n[![License](https://img.shields.io/pypi/l/napari-curtain.svg?color=green)](https://github.com/haesleinhuepf/napari-curtain/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-curtain.svg?color=green)](https://pypi.org/project/napari-curtain)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-curtain.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-curtain/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-curtain/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-curtain/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-curtain)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-curtain)](https://napari-hub.org/plugins/napari-curtain)\n\nView one image over another as curtain\n\n![](https://github.com/haesleinhuepf/napari-curtain/raw/main/docs/curtain_screencast.gif)\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Usage\n\nYou find the `Curtain` plugin in the menu `Tools > Visualization > Curtain`. Move the position of the slider left/right \nas shown in the video above. In case one image is much bright than the other, you can modify the `factors` above the \nslider until visualization pleases.\n\n## Installation\n\nYou can install `napari-curtain` via [pip]:\n\n    pip install napari-curtain\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-curtain\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-curtain/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "curtain"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mouse-controls",
    "name": "napari-mouse-controls",
    "display_name": "napari-mouse-controls",
    "version": "0.1.3",
    "created_at": "2021-10-30",
    "modified_at": "2022-01-01",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-mouse-controls/",
    "home_github": "https://github.com/haesleinhuepf/napari-mouse-controls",
    "home_other": null,
    "summary": "Control napari using a touch screen",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# napari-mouse-controls\n\n[![License](https://img.shields.io/pypi/l/napari-mouse-controls.svg?color=green)](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mouse-controls.svg?color=green)](https://pypi.org/project/napari-mouse-controls)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mouse-controls.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-mouse-controls/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-mouse-controls/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-mouse-controls/branch/main/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-mouse-controls)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-mouse-controls)](https://napari-hub.org/plugins/napari-mouse-controls)\n\nControl zoom, slicing and contrast windowing with mouse and touch screen\n\n----------------------------------\n\n## Usage\n\nYou find the mouse control panel in the menu `Tools > Utilities > Mouse controls`\n\n### Zoom\n\nAfter clicking the Zoom button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Zoom.png), you can click in the napari canvas and move the mouse up and down to zoom in and out.\n\n![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/zoom.gif)\n\n### Slicing\n\nAfter clicking the Slicing button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Slicing.png), you can control the currently displayed slice by moving the mouse.\nBy moving the mouse up and down, you control the currently selected Z-plane.\nBy moving the mouse left and right, you control the currently seleted time point.\n\n![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/slicing.gif)\n\n### Windowing\n\nAfter clicking the Windowing button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Windowing.png), you can modify the brightness and contrast by moving the mouse. \nBy moving the mouse up and down, you control window width of the range of displayed grey values (max - min).\nBy moving the mouse left and right, you control the center of the grey value window. \n\n![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/docs/windowing.gif)\n\n### Normal / default mode\n\nClick the Default button ![](https://github.com/haesleinhuepf/napari-mouse-controls/raw/main/src/napari_mouse_controls/icons/Default.png)\nto return to napari's normal mode.\n\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n\n## Installation\n\nYou can install `napari-mouse-controls` via [pip]:\n\n    pip install napari-mouse-controls\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mouse-controls\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-mouse-controls/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[image.sc]: https://image.sc\n\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MouseControls"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tabu",
    "name": "napari-tabu",
    "display_name": "napari-tabu",
    "version": "0.1.5",
    "created_at": "2021-10-14",
    "modified_at": "2022-01-01",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-tabu/",
    "home_github": "https://github.com/haesleinhuepf/napari-tabu",
    "home_other": null,
    "summary": "A plugin for handling multiple napari windows",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# napari-tabu\n\n[![License](https://img.shields.io/pypi/l/napari-tabu.svg?color=green)](https://github.com/haesleinhuepf/napari-tabu/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tabu.svg?color=green)](https://pypi.org/project/napari-tabu)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tabu.svg?color=green)](https://python.org)\n[![tests](https://github.com/haesleinhuepf/napari-tabu/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-tabu/actions)\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-tabu/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-tabu)\n\nA plugin for handling multiple napari windows\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/napari-tabu-screencast.gif)\n\n----------------------------------\n\n## Usage\n\nTo open a new window, first click the menu `Plugins > napari-tabu: open new window`\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/new_window_menu.png)\n\nAfterwards, select the layer which should be opened in the new window and click on `Run`:\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/new_window_dialog.png)\n\nWhen you're done with working with the new window, you can send back the result of your work using the `Send current layer back to main napari` butoon:\n![](https://github.com/haesleinhuepf/napari-tabu/raw/main/docs/send_back.png)\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `napari-tabu` via [pip]:\n\n    pip install napari-tabu\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-tabu\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please open a thread on [image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/napari-tabu/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "open_in_new_window"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tiler",
    "name": "napari-tiler",
    "display_name": "napari-tiler",
    "version": "0.0.9",
    "created_at": "2021-12-13",
    "modified_at": "2021-12-29",
    "authors": [
      "Tim Morello"
    ],
    "author_emails": [
      "tdmorello@gmail.com"
    ],
    "license": "LICENCE",
    "home_pypi": "https://pypi.org/project/napari-tiler/",
    "home_github": "https://github.com/tdmorello/napari-tiler",
    "home_other": null,
    "summary": "N-dimensional tiling and merging support for napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7,<3.11",
    "package_metadata_requires_dist": [
      "importlib-metadata (<4.3); python_version < \"3.8\"",
      "napari-plugin-engine (>=0.2.0,<0.3.0)",
      "napari-tools-menu (>=0.1.7,<0.2.0)",
      "numpy (>=1.21.4,<2.0.0)",
      "tiler (>=0.4.1,<0.5.0)"
    ],
    "package_metadata_description": "# napari-tiler\n\n[![License](https://img.shields.io/pypi/l/napari-tiler.svg?color=green)](https://github.com/tdmorello/napari-tiler/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tiler.svg?color=green)](https://pypi.org/project/napari-tiler)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tiler.svg?color=green)](https://python.org)\n[![tests](https://github.com/tdmorello/napari-tiler/workflows/tests/badge.svg)](https://github.com/tdmorello/napari-tiler/actions)\n[![codecov](https://codecov.io/gh/tdmorello/napari-tiler/branch/main/graph/badge.svg)](https://codecov.io/gh/tdmorello/napari-tiler)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-tiler)](https://napari-hub.org/plugins/napari-tiler)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/napari-tiler.svg)](https://pypistats.org/packages/napari-tiler)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n[![Development Status](https://img.shields.io/pypi/status/napari-tiler.svg)](https://github.com/tdmorello/napari-tiler)\n\nN-dimensional tiling and merging support for napari\n\nThis plugin allows the user to split an image into a stack of tiles and subsequently merge the tiles to reconstruct the orignal image.\nSee [Tiler](https://pypi.org/project/tiler/) by [@thelay](https://github.com/the-lay) for more details.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\n### Option 1 (recommended)\n\nYou can install `napari-tiler` from the napari plugin manager. Go to `Plugins -> Install/Uninstall Package(s)`, then search for `napari-tiler`. Click `Install`.\n\n### Option 2\n\nYou can also install `napari-tiler` via [pip]:\n\n    pip install napari-tiler\n\nTo install latest development version:\n\n    pip install git+https://github.com/tdmorello/napari-tiler.git\n\n## Quick Start\n\n1. Open a file in napari. The file may have any number of dimensions (e.g. z-stack, time series, ...)\n2. Start the plugin ( `Plugins -> napari-tiler: make_tiles` )\n3. Select the input layer from the dropdown box\n4. Select parameters for tiling\n5. Click `Run`\n\n## Contributing\n\nThis project uses [Poetry](https://github.com/python-poetry/poetry) for dependency management.\nTo set up the development environment, it is recommended to use:\n\n    conda env create -f environment.yaml\n\nContributions are very welcome. Tests can be run with [tox], please ensure the coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-tiler\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/tdmorello/napari-tiler/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TilerWidget",
      "MergerWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "natari",
    "name": "natari",
    "display_name": "natari",
    "version": "0.2.7",
    "created_at": "2021-10-17",
    "modified_at": "2021-12-22",
    "authors": [
      "Robert Haase"
    ],
    "author_emails": [
      "robert.haase@tu-dresden.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/natari/",
    "home_github": "https://github.com/haesleinhuepf/natari",
    "home_other": null,
    "summary": "Napari gaming",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari",
      "scipy",
      "napari-tools-menu"
    ],
    "package_metadata_description": "# natari\n\n[![License](https://img.shields.io/pypi/l/natari.svg?color=green)](https://github.com/haesleinhuepf/natari/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/natari.svg?color=green)](https://pypi.org/project/natari)\n[![Python Version](https://img.shields.io/pypi/pyversions/natari.svg?color=green)](https://python.org)\n\nNapari gaming\n\n## Sliding puzzle\n\nRestore the image by reordering the superpixels using the `W`, `A`, `S`, `D` keys! \n\n![](https://github.com/haesleinhuepf/natari/raw/master/images/sliding_puzzle.gif)\n\n## Cell counting arcade\nCommander! Cells are intruding our dish! Control your tiny space ship using `1` and `2` keys to move it left/right.\nUse the `9` key to shoot a anti-cell bullet.\n\n![](https://github.com/haesleinhuepf/natari/raw/master/images/cell_counting_arcade.gif)\n\nThe image originates from [BBBC022v1](https://bbbc.broadinstitute.org/BBBC022) (Gustafsdottir et al., PLOS ONE, 2013), available from the Broad Bioimage Benchmark Collection (Ljosa et al., Nature Methods, 2012).\n\n## Snake\nTwo mitochondria navigating in a cell searching for stress granules. \nThe two players can control their mito using the `W`, `A`, `S`, `D` and `I`, `J`, `K`, `L`  keys, respectively.\n\n![](https://github.com/haesleinhuepf/natari/raw/master/images/snake.gif)\n\n## Ping pong\nDon't drop the organoid! Use your racket and hit it back to your colleague!\nThe two players can use `W`, `S` and `I`, `K` to control their racket, respectively.\n\n![](https://github.com/haesleinhuepf/natari/raw/master/images/ping_pong.gif)\n\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Installation\n\nYou can install `natari` via [pip]:\n\n    pip install natari\n\n## Known issues\n\n* To make the keyboard buttons work, you sometimes have to click within the image after starting the game.\n\n## Contributing\n\nContributions are very welcome. \n\n## License\n\n\"natari\" is free and open source software. The code is in the public domain.\n\n[See also: unlicense.org](https://unlicense.org)\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/haesleinhuepf/natari/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "cell_counting_arcade_with_default_image",
      "ping_pong",
      "snake",
      "sliding_puzzle"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "grabber-ift",
    "name": "grabber-ift",
    "display_name": "grabber-ift",
    "version": "0.2.2",
    "created_at": "2021-12-12",
    "modified_at": "2021-12-14",
    "authors": [
      "Jord√£o Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/grabber-ift/",
    "home_github": null,
    "home_other": "UNKNOWN",
    "summary": "A tool for contour-based segmentation correction (2D only).",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "pyift (>=0.0.4)",
      "opencv-python-headless (>=4.4.0)",
      "scipy (>=1.7.2)"
    ],
    "package_metadata_description": "# Grabber: A Tool to Improve Convergence in Interactive Image Segmentation\n\n[![License](https://img.shields.io/pypi/l/grabber.svg?color=green)](https://github.com/LIDS-UNICAMP/grabber/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/grabber.svg?color=green)](https://pypi.org/project/grabber)\n[![Python Version](https://img.shields.io/pypi/pyversions/grabber.svg?color=green)](https://python.org)\n[![tests](https://github.com/LIDS-UNICAMP/grabber/workflows/tests/badge.svg)](https://github.com/LIDS-UNICAMP/grabber/actions)\n[![codecov](https://codecov.io/gh/LIDS-UNICAMP/grabber/branch/master/graph/badge.svg)](https://codecov.io/gh/LIDS-UNICAMP/grabber)\n\nA tool for contour-based segmentation correction (2D only).\n\nThis repository provides a demo code of the paper:\n> **Grabber: A Tool to Improve Convergence in Interactive Image Segmentation**\n> [Jord√£o Bragantini](https://jookuma.github.io/), Bruno Moura, [Alexandre X. Falc√£o](http://lids.ic.unicamp.br/), [F√°bio A. M. Cappabianco](https://scholar.google.com/citations?user=qmH9VEEAAAAJ&hl=en&oi=ao)\n\nhttps://user-images.githubusercontent.com/21022743/145699960-57da06a5-668f-4e81-82b5-7f3d3ddf8ee3.mp4\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `grabber-ift` via [pip]:\n\n    pip install grabber-ift\n\n\n## Known Limitations\n\nThis implementation doesn't support the items below, feel free to open a PR to add them.\n\n- It only support 2D image, supporting 3D images isn't trivial, but it could be applied per slice with minor changes.\n\n## Citation\n\nIf this work was useful for your research, please cite our paper:\n\n```\n@article{bragantini2020grabber,\n  title={Grabber: A Tool to Improve Convergence in Interactive Image Segmentation,\n  author={Bragantini, Jord{\\~a}o and Bruno Moura, Falc{\\~a}o, Alexandre Xavier and Cappabianco, F{\\'a}bio AM,\n  journal={Pattern Recognition Letters},\n  year={2020}\n}\n```\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"grabber\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "GrabberWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "bbii-decon",
    "name": "bbii-decon",
    "display_name": "bbii-decon",
    "version": "0.0.1",
    "created_at": "2021-12-13",
    "modified_at": "2021-12-13",
    "authors": [
      "Graham Dellaire",
      "Robert Haase"
    ],
    "author_emails": [
      "dellaire@Dal.Ca"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/bbii-decon/",
    "home_github": "https://github.com/gdellaire/bbii-decon",
    "home_other": null,
    "summary": "Projected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pypher"
    ],
    "package_metadata_description": "# BBii-Decon\n\n[![License](https://img.shields.io/pypi/l/bbii-decon.svg?color=green)](https://github.com/gdellaire/bbii-decon/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/bbii-decon.svg?color=green)](https://pypi.org/project/bbii-decon)\n[![Python Version](https://img.shields.io/pypi/pyversions/bbii-decon.svg?color=green)](https://python.org)\n[![tests](https://github.com/gdellaire/bbii-decon/workflows/tests/badge.svg)](https://github.com/gdellaire/bbii-decon/actions)\n[![codecov](https://codecov.io/gh/gdellaire/bbii-decon/branch/main/graph/badge.svg)](https://codecov.io/gh/gdellaire/bbii-decon)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/bbii-decon)](https://napari-hub.org/plugins/bbii-decon)\n\nProjected Barzilai-Borwein Image Deconvolution with Infeasible Iterates (BBii-Decon)\n\n\nThe projected Barzilai-Borwein method of image deconvolution utilizing infeasible iterates (BBii-Decon), utilizes Barzilai-Borwein (BB) or projected BB (PBB) method and enforces a nonnegativity constraint, but allows for infeasible iterates between projections. This algorithm (BBii) results in faster convergence than the basic PBB method, while achieving better quality images, with reduced background than the unconstrained BB method (1). \n\nThe code represented is based on the original BBii algorithm written in MatLab by Kathleen Fraser and Dirk Arnold, which was ported to python 3.8 by Graham Dellaire, Dirk Arnold and Kathleen Fraser for non-commercial use.\n\nThe first implementation shown here is for 2D deconvolution using a known 2D PSF of 256 X 256 pixels, and images of at least 256 pixels in one dimension. One file implements just the deconvolution of a blurred image, while the second file contains a modification of the BBii-Decon algorithm that has a built in heuristic for measuring image reconstruction error relative to a ground truth image. For general 2D deconvolution, either a theoretical 2D PSF (if you know the optical properties of your system) or the central in focus image of a fluorescent bead taken with the same imaging setup (lens, magnification, camera) can produce a suitable PSF.\n\n### GPU-acceleration\n\nFor most 2D deconvolution, optimal results are obtained with 10 iterations of the algorithm. However, if processing takes too long, acceleration using graphics processing units (GPUs) may make sense, especially for processing larger images with >10 iterations or 3D images. (Note: At this time BBii-Decon is optimized for 2D deconvolution, with a 3D implementation planned in future). \n\nThis plugin supports accelerated processing using the [cupy](https://cupy.dev) library. To make use of it, please follow \n[the instructions](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-conda-forge) to install cupy. \nInstallation may look like this:\n```\nconda create --name cupy_p38 python=3.8\nconda activate cupy_p38\nconda install -c conda-forge cupy cudatoolkit=10.2\n```\n\nIf cupy installation worked out, you will find another checkbox in the user interface. By activating it, processing \nshould become faster by factor 5-10, depending on processed image data and use GPU hardware.\n\n![img.png](https://github.com/gdellaire/BBii-Decon/raw/main/demo/use_GPU_checkbox.png)\n\n## Usage - napari\n\nYou can use the BBii deconvolution from within napari by clicking the menu `Plugins > bbii-decon > bbii deconvolution`. \nIn the dialog, select the PSF, the image to process (a) and click on `Run`. After a moment, the deconvolved image (b) \nwill show up.\n\n![img.png](https://github.com/gdellaire/BBii-Decon/raw/main/demo/screenshot_napari.png)\n\n## Usage from python\n\nYou can also call the function from python. There is a full working example in [this notebook](demo/BBii_Decon_2D_2021.ipynb).\n\n```\nfrom bbii_decon import bbii\n\nbbii(PSF, image, number_of_iterations = 15, tau = 1.0e-08, rho = 0.98)\n```\n\n\n## Citation\n1) [Kathleen Fraser, Dirk V. Arnold, and Graham Dellaire (2014). Projected Barzilai-Borwein\nmethod with infeasible iterates for nonnegative least-squares image deblurring. In Proceedings\nof the Eleventh Conference on Computer and Robot Vision (CRV 2014), Montreal, Canada, pp.\n189--194.](https://ieeexplore.ieee.org/abstract/document/6816842)\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `bbii-decon` via [pip]:\n\n    pip install bbii-decon\n\n\n## Installation for developers\n\nClone the github repository:\n\n```\nconda install git\n\ngit clone https://github.com/gdellaire/BBii-Decon.git\n\ncd BBii-Decon\n\npip install -e .\n```\n\n## Deployment to pypi\n\nFor deploying the plugin to the python package index (pypi), one needs a [pypi user account](https://pypi.org/account/register/) \nfirst. For deploying the plugin to pypi, one needs to install some tools:\n\n```\npython -m pip install --user --upgrade setuptools wheel\npython -m pip install --user --upgrade twine\n```\n\nThe following command allows us to package the souce code as a python wheel. Make sure that the 'dist' and 'build' folders are deleted before doing this:\n\n```\npython setup.py sdist bdist_wheel\n```\n\nThis command ships the just generated to pypi:\n\n```\npython -m twine upload --repository pypi dist/*\n```\n\n[Read more about distributing your python package via pypi](https://realpython.com/pypi-publish-python-package/#publishing-to-pypi).\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"bbii-decon\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/gdellaire/bbii-decon/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "bbii_deconvolution"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-multitask",
    "name": "napari-multitask",
    "display_name": "napari-multitask",
    "version": "0.0.2",
    "created_at": "2021-12-06",
    "modified_at": "2021-12-13",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin-sc@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-multitask/",
    "home_github": null,
    "home_other": "UNKNOWN",
    "summary": "Multitasking in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.8",
    "package_metadata_requires_dist": [
      "magic-class (>=0.5.11)"
    ],
    "package_metadata_description": "# napari-multitask\n\nMultitasking on napari.\n\n![](https://github.com/hanjinliu/napari-multitask/raw/main/Figs/output.gif)\n\nLayers and opened dock widgets are stored in the task panels below. Switch your tasks at any time.\n\n# Installation\n\n```\npip install napari-multitask\n```\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TaskView"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-text-layer",
    "name": "napari-text-layer",
    "display_name": "napari-text-layer",
    "version": "0.1.2",
    "created_at": "2021-12-04",
    "modified_at": "2021-12-13",
    "authors": [
      "Hanjin Liu"
    ],
    "author_emails": [
      "liuhanjin-sc@g.ecc.u-tokyo.ac.jp"
    ],
    "license": "BSD 3-Clause",
    "home_pypi": "https://pypi.org/project/napari-text-layer/",
    "home_github": null,
    "home_other": "UNKNOWN",
    "summary": "Text layer for bio-image annotation.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": null,
    "package_metadata_description": "# napari-text-layer\n\nNapari text layer for bio-image annotation.\n\n![](https://github.com/hanjinliu/napari-text-layer/raw/main/GIFs/annot.gif)\n\n![](https://github.com/hanjinliu/napari-text-layer/raw/main/GIFs/age.gif)\n\n### Installation\n\nYou can install using pip:\n\n```\npip install napari-text-layer\n```\n\n### Keybindings and mouse callbacks\n\n- \"&rarr;\", \"&larr;\", \"&uarr;\", \"&darr;\" ... Move selected shapes by 1 pixel.\n- \"F2\" ... Enter edit mode at the selected shape (or the last one if no shape is selected).\n- \"Enter\" ... Finish edit mode or add a new shape at the same interval.\n- \"Ctrl\" + \"Shift\" + \"<\" or \">\" ... Change font size.\n- double click ... Enter edit mode at the clicked shape.\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "TextLayerOverview"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spatial-omics",
    "name": "napari-spatial-omics",
    "display_name": "napari-spatial-omics",
    "version": "0.0.8",
    "created_at": "2021-12-10",
    "modified_at": "2021-12-12",
    "authors": [
      "Sebastian Gonzalez-Tirado"
    ],
    "author_emails": [
      "sebgoti8@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-spatial-omics/",
    "home_github": "https://github.com/sebgoti/napari-spatial-omics",
    "home_other": null,
    "summary": "A simple plugin to visualize spatial omic data stored in CSV format",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas"
    ],
    "package_metadata_description": "# napari-spatial-omics\n\n[![License](https://img.shields.io/pypi/l/napari-spatial-omics.svg?color=green)](https://github.com/sebgoti/napari-spatial-omics/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spatial-omics.svg?color=green)](https://pypi.org/project/napari-spatial-omics)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spatial-omics.svg?color=green)](https://python.org)\n[![tests](https://github.com/sebgoti/napari-spatial-omics/workflows/tests/badge.svg)](https://github.com/sebgoti/napari-spatial-omics/actions)\n[![codecov](https://codecov.io/gh/sebgoti/napari-spatial-omics/branch/main/graph/badge.svg)](https://codecov.io/gh/sebgoti/napari-spatial-omics)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-spatial-omics)](https://napari-hub.org/plugins/napari-spatial-omics)\n\nA simple plugin to visualize spatial omic data stored in CSV format\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/plugins/stable/index.html\n-->\n\n## Installation\n\nYou can install `napari-spatial-omics` via [pip]:\n\n    pip install napari-spatial-omics\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/sebgoti/napari-spatial-omics.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-spatial-omics\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/sebgoti/napari-spatial-omics/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "example_magic_widget",
      "ExampleQWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-spacetx-explorer",
    "name": "napari-spacetx-explorer",
    "display_name": "napari-spacetx-explorer",
    "version": "0.1.8",
    "created_at": "2021-09-28",
    "modified_at": "2021-12-10",
    "authors": [
      "Sebastian Gonzalez-Tirado"
    ],
    "author_emails": [
      "sebastian.gonzalez@embl.de"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-spacetx-explorer/",
    "home_github": "https://github.com/sebgoti/napari-spacetx-explorer",
    "home_other": null,
    "summary": "visualizer for spatial omic data",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas"
    ],
    "package_metadata_description": "# napari-spacetx-explorer\n\n[![License](https://img.shields.io/pypi/l/napari-spacetx-explorer.svg?color=green)](https://github.com/sebgoti/napari-spacetx-explorer/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-spacetx-explorer.svg?color=green)](https://pypi.org/project/napari-spacetx-explorer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-spacetx-explorer.svg?color=green)](https://python.org)\n[![tests](https://github.com/sebgoti/napari-spacetx-explorer/workflows/tests/badge.svg)](https://github.com/sebgoti/napari-spacetx-explorer/actions)\n[![codecov](https://codecov.io/gh/sebgoti/napari-spacetx-explorer/branch/master/graph/badge.svg)](https://codecov.io/gh/sebgoti/napari-spacetx-explorer)\n\nA napari plugin for interactive visualization of decoded spots from spatial transcriptomic data stored as CSV\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\nThe plugin code was written by Sebastian Gonzalez-Tirado.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n## Reader hookspec\n\n`napari-spacetx-explorer` allows the user to open and visualize CSV files that\nhave point-data stored in a given format. The main target is for users who\nwant to analyze decoded spot maps from spatial omics experiments but it can\nused as well for any other type of coordinate data where each point has assigned\na label (e. g. a gene) as a string and the x and y-coordinates of the point's center.\nThe header for these data must be 'target', 'xc', and 'yc', respectively.\n\n![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/Read_Hookspec.png)\n\n## Selecting genes\n\nAfter loading the gene/target maps it is possible to select specific groups for better visualization.\nThis creates a new \"Points\" layer in napari with the selected groups displayed in different colors.\n\n![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/_function_hookspec.png)\n\n## Loading data in OME.ZARR format\n\nThe plugin napari-ome-zarr can be used to display whole-tissue images in addition to the spot maps produced with the \n`napari-spacetx-explorer` plugin.\n\n![img.png](https://github.com/sebgoti/napari-spacetx-explorer/raw/main/docs/_ome_zarr_napari_spacetx_explorer.png)\n\n## Installation\n\nThe easiest installation is via the \"Install/Uninstall Plugins...\" under the Plugins menu in napari.  \nAnother way is through [pip] \n\n    pip install napari-spacetx-explorer\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-spacetx-explorer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems or would like some support, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/sebgoti/napari-spacetx-explorer/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "read_spots"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-labelimg4classification",
    "name": "napari-labelimg4classification",
    "display_name": "napari-labelimg4classification",
    "version": "0.1.1",
    "created_at": "2021-12-02",
    "modified_at": "2021-12-03",
    "authors": [
      "Hiroki Kawai"
    ],
    "author_emails": [
      "h.kawai888@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-labelimg4classification/",
    "home_github": "https://github.com/hiroalchem/napari-labelimg4classification",
    "home_other": null,
    "summary": "Image-Level labeling tool",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "napari",
      "numpy",
      "napari-tools-menu",
      "pandas"
    ],
    "package_metadata_description": "# napari-labelimg4classification\n\n[![License](https://img.shields.io/pypi/l/napari-labelimg4classification.svg?color=green)](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-labelimg4classification.svg?color=green)](https://pypi.org/project/napari-labelimg4classification)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-labelimg4classification.svg?color=green)](https://python.org)\n[![tests](https://github.com/hiroalchem/napari-labelimg4classification/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-labelimg4classification/actions)\n[![codecov](https://codecov.io/gh/hiroalchem/napari-labelimg4classification/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-labelimg4classification)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-labelimg4classification)](https://napari-hub.org/plugins/napari-labelimg4classification)\n\nA simple image-level annotation tool supporting multi-channel images for napari.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Usage\nStart the labeling tool from the menu `Utilities > label tool for classification`.   \nFirst, click on the Choose directory button to open the folder selection window, and select the folder that contains the\n images you want to label and annotate.   \nIt will automatically list and display the images of tif, png, jpg, and bmp formats.\nIf you want to view the channels of a multi-channel image separately, check the split channels checkbox.\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/open.gif)\n\nInitially, all channels will be opened in grayscale, but the pseudo-color and contrast adjustments you specified will be\n carried over when you open the next image.   \nThanks to napari, you can freely merge channels and turn them on and off.   \nLabel classes can be added, and can be removed by typing the same name as an already added class.\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/color_and_label.gif)\n\n\nIt will automatically save the labels.csv file with the image path and label, and the class.txt file with the class of the label.\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/class_and_labels.png)\n\nIf labels.csv and class.txt are already in the folder, they will be loaded and reflected automatically.\n![](https://github.com/hiroalchem/napari-labelimg4classification/raw/main/docs/reopen.gif)\n\n## Installation\n\nYou can install `napari-labelimg4classification` via [pip]:\n\n    pip install napari-labelimg4classification\n\n\n\nTo install latest development version :\n\n    pip install git+https://github.com/hiroalchem/napari-labelimg4classification.git\n\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-labelimg4classification\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/hiroalchem/napari-labelimg4classification/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "L4CWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-properties-plotter",
    "name": "napari-properties-plotter",
    "display_name": "napari-properties-plotter",
    "version": "0.2.2",
    "created_at": "2021-05-26",
    "modified_at": "2021-12-01",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "lorenzo.gaifas@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-properties-plotter/",
    "home_github": "https://github.com/brisvag/napari-properties-plotter",
    "home_other": null,
    "summary": "A napari plugin that automatically generates interactive plots based on layer properties.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas",
      "pyqtgraph",
      "qtpy"
    ],
    "package_metadata_description": "# napari-properties-plotter\n\n[![License](https://img.shields.io/pypi/l/napari-properties-plotter.svg?color=green)](https://github.com/brisvag/napari-properties-plotter/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-properties-plotter.svg?color=green)](https://pypi.org/project/napari-properties-plotter)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-plotter.svg?color=green)](https://python.org)\n[![tests](https://github.com/brisvag/napari-properties-plotter/workflows/tests/badge.svg)](https://github.com/brisvag/napari-properties-plotter/actions)\n[![codecov](https://codecov.io/gh/brisvag/napari-properties-plotter/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-properties-plotter)\n\nA napari plugin that automatically generates interactive plots based on layer properties.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-properties-plotter` via [pip]:\n\n    pip install napari-properties-plotter\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-properties-plotter\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/brisvag/napari-properties-plotter/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PropertyPlotter"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-deepspot",
    "name": "napari-DeepSpot",
    "display_name": "napari-DeepSpot",
    "version": "0.0.7",
    "created_at": "2021-11-29",
    "modified_at": "2021-11-30",
    "authors": [
      "Emmanuel Bouilhol"
    ],
    "author_emails": [
      "emmanuel.bouilhol@u-bordeaux.fr"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-deepspot/",
    "home_github": "https://github.com/ebouilhol/napari-DeepSpot",
    "home_other": null,
    "summary": "RNA spot enhancement for fluorescent microscopy images",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pytest",
      "pytest-cov",
      "pytest-xvfb",
      "pytest-qt",
      "napari",
      "qtpy (==1.9.0)",
      "pyqt5",
      "tensorflow",
      "scikit-image",
      "opencv-python"
    ],
    "package_metadata_description": "# napari-DeepSpot\n\n[![License](https://img.shields.io/pypi/l/napari-DeepSpot.svg?color=green)](https://github.com/ebouilhol/napari-DeepSpot/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-DeepSpot.svg?color=green)](https://pypi.org/project/napari-DeepSpot)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-DeepSpot.svg?color=green)](https://python.org)\n[![tests](https://github.com/ebouilhol/napari-DeepSpot/workflows/tests/badge.svg)](https://github.com/ebouilhol/napari-DeepSpot/actions)\n[![codecov](https://codecov.io/gh/ebouilhol/napari-DeepSpot/branch/main/graph/badge.svg)](https://codecov.io/gh/ebouilhol/napari-DeepSpot)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-DeepSpot)](https://napari-hub.org/plugins/napari-DeepSpot)\n\nRNA spot enhancement for fluorescent microscopy images.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-DeepSpot` via [pip]:\n\n    pip install napari-DeepSpot\n\n## Build from source\n\nThis plugin is using Tensorflow, make sure your Python environment has Tensorflow, on create a new environment using the following commands:\n* Conda:  \n`conda env create -f environment.yml`  \n`conda activate deepspot-napari`\n* Or pip:   \n`pip install -r requirements.txt`\n\n## Usage\n\nOpen one or multiple images using Napari GUI : \nFile > Open > Select your image\n\nThe images are then displayed on Napari\n\nLoad the Plugin:\nPlugins > Napari-DeepSpot:Enhance Spot\n\n![Usage](./image/napari.png)\n\nClick on the right panel Button \"Enhance\"\n\nWait a few seconds for the magic to happen :\n\n![Usage](./image/napari_enhance.png)\n\nYou can see the original images and the enhanced version in the left panel in the layer section.\n\nTo save the images : File > Save all layers or File > Save selected layers.\n\n\n![Usage](./image/napari_video.gif)\n\n\n\n## Citation\nIf you use this plugin please cite the [paper](https://www.biorxiv.org/content/10.1101/2021.11.25.469984v1):\n\n>@article {Bouilhol2021DeepSpot,  \n>\t author = {Bouilhol, Emmanuel and Lefevre, Edgar and Dartigues, Benjamin and Brackin, Robyn and Savulescu, Anca Flavia and Nikolski, Macha},  \n>\t title = {DeepSpot: a deep neural network for RNA spot enhancement in smFISH microscopy images},  \n>\t elocation-id = {2021.11.25.469984},  \n>\t year = {2021},  \n>\t doi = {10.1101/2021.11.25.469984},  \n>\t publisher = {Cold Spring Harbor Laboratory},  \n>\t URL = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984},  \n>\t eprint = {https://www.biorxiv.org/content/early/2021/11/25/2021.11.25.469984.full.pdf},  \n>\t journal = {bioRxiv}  \n>}  \n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-DeepSpot\" is free and open source software\n\n## Known Issues\n\nIf you have troubles with the Python packages `typing extensions`, use the command :  \n`pip install typing-extensions --upgrade`  \n\nWhen using \"Enhance\" on multiple images, Napari may freeze. Just wait until it comes to life again, the images will still be enhanced. This is due to Napari memory usage and will be fix one day.\n\n\n## Other Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/ebouilhol/napari-DeepSpot/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "EnhanceSpot"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-hdf5-labels-io",
    "name": "napari-hdf5-labels-io",
    "display_name": "napari-hdf5-labels-io",
    "version": "0.3.dev16",
    "created_at": "2021-03-04",
    "modified_at": "2021-11-30",
    "authors": [
      "Duway Nicolas Lesmes Leon",
      "Pranjal Dhole"
    ],
    "author_emails": [
      "dlesmesleon@hotmail.com",
      "dhole.pranjal@gmail.com"
    ],
    "license": "GNU GPL v3.0",
    "home_pypi": "https://pypi.org/project/napari-hdf5-labels-io/",
    "home_github": "https://github.com/yapic/napari-hdf5-labels-io",
    "home_other": null,
    "summary": "Napari plugin to store set of layers in a .h5 file. Label layer are stored in a sparse representation.",
    "categories": [],
    "package_metadata_requires_python": "<3.9",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "typing",
      "numpy",
      "sparse",
      "h5py (==2.10.0)",
      "zarr"
    ],
    "package_metadata_description": "# napari-hdf5-labels-io\n\n[![License](https://img.shields.io/pypi/l/napari-hdf5-labels-io.svg?color=green)](https://github.com/yapic/napari-hdf5-labels-io/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-hdf5-labels-io.svg?color=green)](https://pypi.org/project/napari-hdf5-labels-io)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-hdf5-labels-io.svg?color=green)](https://python.org)\n[![tests](https://github.com/yapic/napari-hdf5-labels-io/workflows/tests/badge.svg)](https://github.com/yapic/napari-hdf5-labels-io/actions)\n[![codecov](https://codecov.io/gh/yapic/napari-hdf5-labels-io/branch/master/graph/badge.svg)](https://codecov.io/gh/yapic/napari-hdf5-labels-io)\n\nnapari plugin to store napari projects in a .h5 file. Label layer are stored in a sparse representation (COO list).\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Description\n\nThis napari plugin provides a writer and reader to store existing layers in the current napari window, all the metadata is stored as well in a HDF5 file. All the stored preferences are included when a project file is opened.\n\nLabel layers are stored in a coordinate list sparse representation with the [Sparse module](https://sparse.pydata.org/) to keep the project file size minimum when possible (aiming to implement this in other layers in the future).\n\n## HDF5 file architecture\n\nThe project file is a HDF5 generated with the [h5py module](https://docs.h5py.org). The file groups correspond to the different napari layer types and the layer metadata is stored as attributes of each layer.\n\nIn the case of the meta dictionary which is nested in the LayerData meta dictionary (napari IO), new keys are generated in the outer dictionary to use them as h5 dataset attributes. This nested dictionary architecture is reconstructed by the reader to ensure format compatibility.\n\n## Installation\n\nYou can install `napari-hdf5-labels-io` via [pip]:\n\n    pip install napari-hdf5-labels-io\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-hdf5-labels-io\" is free and open source software.\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/yapic/napari-hdf5-labels-io/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-microscope",
    "name": "napari-microscope",
    "display_name": "napari-microscope",
    "version": "0.0.3",
    "created_at": "2021-11-23",
    "modified_at": "2021-11-23",
    "authors": [
      "David Miguel Susano Pinto"
    ],
    "author_emails": [
      "david.pinto@bioch.ox.ac.uk"
    ],
    "license": "GPL-3.0-or-later",
    "home_pypi": "https://pypi.org/project/napari-microscope/",
    "home_github": null,
    "home_other": "None",
    "summary": "Napari plugin for Microscope.",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "Pyro4",
      "microscope",
      "napari_plugin_engine"
    ],
    "package_metadata_description": "Microscope control plugin for Napari via Python Microscope.\n\nCurrent development stage is whatever comes before alpha and \"proof of\nconcept\".\n\nTo test\n-------\n\nI haven't had access to real hardware yet, so this has all been\ndeveloped with simulated devices.\n\n1. Start the device server with simulated devices.\n\n    a. Create a device server configuration file like so::\n\n        import microscope\n        from microscope.device_server import device\n        from microscope.simulators import (\n            SimulatedCamera,\n            SimulatedFilterWheel,\n            SimulatedLightSource,\n            SimulatedStage,\n        )\n\n        DEVICES = [\n            device(SimulatedCamera, \"localhost\", 8000,),\n            device(SimulatedLightSource, \"localhost\", 8001),\n            device(SimulatedFilterWheel, \"localhost\", 8002,\n                   {\"positions\": 6}),\n            device(SimulatedStage, \"localhost\", 8003,\n                   {\"limits\": {\"X\": microscope.AxisLimits(0, 25000),\n                               \"Y\": microscope.AxisLimits(0, 12000)}}),\n        ]\n\n    b. Start the device server (ensure port 8000-8003 are unused)::\n\n        $ device-server path-to-microscope-config.py\n\n2. Start napari\n\n3. Plugins > Add Dock Widget > microscope: MicroscopeWidget\n\n4. Connect to the camera:\n\n    a. On the new widget, click on the \"Add device\" button.\n\n    b. Enter the camera URI `PYRO:SimulatedCamera@localhost:8000`\n\n5. Tick the `Enabled` box to enable the camera and then press the\n\"Snap\" button.\n\n6. A random values image will appear displayed on the napari viewer.\nKeep pressing the \"Snap\" button to get new images.  The top left\ncorner of the image is the simulated image number.\n\n7. Connect to the other simulated devices.  Their URIs are:\n\n    a. PYRO:SimulatedLightSource@localhost:8001\n    b. PYRO:SimulatedFilterWheel@localhost:8002\n    c. PYRO:SimulatedStage@localhost:8003\n\n8. Changing the other simulated devices, doesn't really do much (but\ndoes change state of the devices, as can be seen in the logs)\n\n\nTest with stage aware camera\n----------------------------\n\nThis is pretty much the same as before but one can use a large RGB\nTIFF (histology samples are perfect) to simulate a camera that returns\nsubsections of the image file based on the simulated stage position.\n\nFor quick example, try::\n\n    wget https://zenodo.org/record/1445489/files/B0002.tif\n\nAnd use the following device server configuration file::\n\n    from microscope.device_server import device\n    from microscope.simulators.stage_aware_camera import simulated_setup_from_image\n\n    DEVICES = [\n        device(simulated_setup_from_image, \"localhost\", 8000,\n               conf={\"filepath\": \"B0002.tif\"}),\n    ]\n\nThe URI for the devices will be::\n\n    PYRO:camera@localhost:8000\n    PYRO:filterwheel@localhost:8000\n    PYRO:stage@localhost:8000\n\nChanging the filterwheel changes which channel from the image is\nreturned.  Changing the stage coordinates changes the image that is\nreturned (but beware of the corners, pixels outside the image size are\nnot handled yet and will give an error).\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "MicroscopeWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-subboxer",
    "name": "napari-subboxer",
    "display_name": "napari-subboxer",
    "version": "0.0.1",
    "created_at": "2021-11-22",
    "modified_at": "2021-11-22",
    "authors": [
      "Alister Burt"
    ],
    "author_emails": [
      "alisterburt@gmail.com"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-subboxer/",
    "home_github": "https://github.com/alisterburt/napari-subboxer",
    "home_other": null,
    "summary": "A napari plugin for interacting with electron cryotomograms",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "napari[pyqt5] (==0.4.12)",
      "mrcfile",
      "typer",
      "eulerangles",
      "starfile",
      "einops",
      "pydantic"
    ],
    "package_metadata_description": "# napari-subboxer\n\n[![License](https://img.shields.io/pypi/l/napari-subboxer.svg?color=green)](https://github.com/alisterburt/napari-subboxer/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-subboxer.svg?color=green)](https://pypi.org/project/napari-subboxer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-subboxer.svg?color=green)](https://python.org)\n[![tests](https://github.com/alisterburt/napari-subboxer/workflows/tests/badge.svg)](https://github.com/alisterburt/napari-subboxer/actions)\n[![codecov](https://codecov.io/gh/alisterburt/napari-subboxer/branch/master/graph/badge.svg)](https://codecov.io/gh/alisterburt/napari-subboxer)\n\nA napari plugin for visualising and interacting with electron cryotomograms.\n\n\n## Installation\n\nYou can install `napari-subboxer` via [pip]:\n\n    pip install napari-subboxer\n\n## Usage\n\nThis plugin provides a user interface for opening electron cryotomograms in \nnapari as both volumes and slices through volumes.\n\n![demo](https://user-images.githubusercontent.com/7307488/138575305-b05c4735-9c03-4629-bfb0-9612ea8f26fd.gif)\n\nThe plugin can be opened from the `plugins` menu in napari, or with \n`napari-subboxer` at the command line.\n\n![plugins-menu](https://user-images.githubusercontent.com/7307488/138575015-00ea78d9-02c1-44bc-9034-0c0a7fa8d973.png)\n\n```yaml\nUsage: napari-subboxer [TOMOGRAM_FILE]\n\n  An interactive tool for defining and applying relative transforms\n  on sets of particles in napari.\n\nArguments:\n  [TOMOGRAM_FILE]\n\nOptions:\n  --help                          Show this message and exit.\n\n```\n\n## Contributing\n\nContributions are very welcome. \n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-subboxer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/alisterburt/napari-subboxer/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "SubboxingWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-pdf-reader",
    "name": "napari-pdf-reader",
    "display_name": "napari-pdf-reader",
    "version": "0.0.1a3",
    "created_at": "2021-11-05",
    "modified_at": "2021-11-05",
    "authors": [
      "Daniel Krentzel"
    ],
    "author_emails": [
      "dkrentzel@pm.me"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-pdf-reader/",
    "home_github": "https://github.com/krentzd/napari-pdf-reader",
    "home_other": null,
    "summary": "Reader for PDF files",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pillow",
      "pdf2image"
    ],
    "package_metadata_description": "# PDF reader for napari\nReads PDF files into napari\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-ccp4map",
    "name": "napari-ccp4map",
    "display_name": "napari-ccp4map",
    "version": "1.0",
    "created_at": "2021-10-04",
    "modified_at": "2021-10-04",
    "authors": [
      "Simon Biberger"
    ],
    "author_emails": [
      "dev@biberger.xyz"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-ccp4map/",
    "home_github": "https://github.com/biberger/napari-ccp4map",
    "home_other": null,
    "summary": "Enables napari to read .map files in the ccp4 format. Drag&Drop or press Ctrl+O to read files.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "gemmi"
    ],
    "package_metadata_description": "# napari-ccp4map\n\n[![License](https://img.shields.io/pypi/l/napari-ccp4map.svg?color=green)](https://github.com/biberger/napari-ccp4map/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-ccp4map.svg?color=green)](https://pypi.org/project/napari-ccp4map)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ccp4map.svg?color=green)](https://python.org)\n[![tests](https://github.com/biberger/napari-ccp4map/workflows/tests/badge.svg)](https://github.com/biberger/napari-ccp4map/actions)\n[![codecov](https://codecov.io/gh/biberger/napari-ccp4map/branch/master/graph/badge.svg)](https://codecov.io/gh/biberger/napari-ccp4map)\n\nEnables napari to read .map files in the ccp4 format.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-ccp4map` via [pip]:\n\n    pip install napari-ccp4map\n\n## Usage\nIf the plugin was installed correctly, it will pop up in a napari window under Plugins->Install/Uninstall Plugins.\nYou can either drag&drop filed into the window to read them, or search for a folder/file using Ctrl+O.\n\n## How it works\nThis plugin simply reads a file and allows [gemmi](https://github.com/project-gemmi/gemmi) to interact with it. Then, numpy turns the file into an array.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-ccp4map\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/biberger/napari-ccp4map/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "platelet-unet-watershed",
    "name": "platelet-unet-watershed",
    "display_name": "platelet-unet-watershed",
    "version": "0.0.3",
    "created_at": "2021-07-01",
    "modified_at": "2021-09-20",
    "authors": [
      "Juan Nunez-Iglesias & Abigail McGovern"
    ],
    "author_emails": [
      "juan.nunez-iglesias@monash.edu"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/platelet-unet-watershed/",
    "home_github": "https://github.com/jni/platelet-unet-watershed",
    "home_other": null,
    "summary": "Segment platelets with pretrained unet and affinity watershed",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "magicgui>=0.2.11",
      "napari>=0.4.11",
      "napari-plugin-engine>=0.1.4",
      "numba>=0.50",
      "numpy",
      "scikit-image",
      "scipy",
      "toolz",
      "torch",
      "torchvision",
      "tqdm"
    ],
    "package_metadata_description": "# platelet-unet-watershed\n\n[![License](https://img.shields.io/pypi/l/platelet-unet-watershed.svg?color=green)](https://github.com/jni/platelet-unet-watershed/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/platelet-unet-watershed.svg?color=green)](https://pypi.org/project/platelet-unet-watershed)\n[![Python Version](https://img.shields.io/pypi/pyversions/platelet-unet-watershed.svg?color=green)](https://python.org)\n[![tests](https://github.com/jni/platelet-unet-watershed/workflows/tests/badge.svg)](https://github.com/jni/platelet-unet-watershed/actions)\n[![codecov](https://codecov.io/gh/jni/platelet-unet-watershed/branch/master/graph/badge.svg)](https://codecov.io/gh/jni/platelet-unet-watershed)\n\nSegment platelets with pretrained unet and affinity watershed\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `platelet-unet-watershed` via [pip]:\n\n    pip install platelet-unet-watershed\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"platelet-unet-watershed\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/jni/platelet-unet-watershed/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "UNetPredictWidget",
      "copy_data"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mrcfile-handler",
    "name": "napari-mrcfile-handler",
    "display_name": "napari-mrcfile-handler",
    "version": "0.0.6",
    "created_at": "2021-08-30",
    "modified_at": "2021-09-01",
    "authors": [
      "Philipp Schoennenbeck"
    ],
    "author_emails": [
      "p.schoennenbeck@fz-juelich.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-mrcfile-handler/",
    "home_github": "https://github.com/Croxa/napari-mrcfile_handler",
    "home_other": null,
    "summary": "A simple plugin to read, write and adjust mrcfiles in napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "mrcfile"
    ],
    "package_metadata_description": "# napari-mrcfile_handler\n\n[![License](https://img.shields.io/pypi/l/napari-mrcfile_handler.svg?color=green)](https://github.com/Croxa/napari-mrcfile_handler/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-mrcfile_handler.svg?color=green)](https://pypi.org/project/napari-mrcfile_handler)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mrcfile_handler.svg?color=green)](https://python.org)\n[![tests](https://github.com/Croxa/napari-mrcfile_handler/workflows/tests/badge.svg)](https://github.com/Croxa/napari-mrcfile_handler/actions)\n[![codecov](https://codecov.io/gh/Croxa/napari-mrcfile_handler/branch/master/graph/badge.svg)](https://codecov.io/gh/Croxa/napari-mrcfile_handler)\n\nA simple plugin to read, write and adjust mrcfiles in napari.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-mrcfile_handler` via [pip]:\n\n    pip install napari-mrcfile_handler\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-mrcfile_handler\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/Croxa/napari-mrcfile_handler/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "PixelSpacing"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-brushsettings",
    "name": "napari-brushsettings",
    "display_name": "napari-brushsettings",
    "version": "0.0.2",
    "created_at": "2021-08-30",
    "modified_at": "2021-08-30",
    "authors": [
      "Philipp Schoennenbeck"
    ],
    "author_emails": [
      "p.schoennenbeck@fz-juelich.de"
    ],
    "license": "BSD-3-Clause",
    "home_pypi": "https://pypi.org/project/napari-brushsettings/",
    "home_github": "https://github.com/Croxa/napari-brushsettings",
    "home_other": null,
    "summary": "A simple plugin to set the brush settings for segmentation in napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy"
    ],
    "package_metadata_description": "# napari-brushsettings\n\n[![License](https://img.shields.io/pypi/l/napari-brushsettings.svg?color=green)](https://github.com/Croxa/napari-brushsettings/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-brushsettings.svg?color=green)](https://pypi.org/project/napari-brushsettings)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brushsettings.svg?color=green)](https://python.org)\n[![tests](https://github.com/Croxa/napari-brushsettings/workflows/tests/badge.svg)](https://github.com/Croxa/napari-brushsettings/actions)\n[![codecov](https://codecov.io/gh/Croxa/napari-brushsettings/branch/master/graph/badge.svg)](https://codecov.io/gh/Croxa/napari-brushsettings)\n\nA simple plugin to set the brush settings for segmentation in napari\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-brushsettings` via [pip]:\n\n    pip install napari-brushsettings\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-brushsettings\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n\n[file an issue]: https://github.com/Croxa/napari-brushsettings/issues\n\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Brushsize"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-features",
    "name": "napari-features",
    "display_name": "napari-features",
    "version": "0.1.4",
    "created_at": "2021-06-17",
    "modified_at": "2021-08-24",
    "authors": [
      "Allen Goodman"
    ],
    "author_emails": [
      "allen.goodman@icloud.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-features/",
    "home_github": "https://github.com/0x00b1/napari-features",
    "home_other": null,
    "summary": "extracts image and object features",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "magicgui (>=0.2.9)",
      "napari (>=0.4.10)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.19.5)",
      "pandas (>=1.2.4)",
      "qtpy (>=1.9.0)",
      "scikit-image (>=0.18.1)",
      "scipy (>=1.4.1)"
    ],
    "package_metadata_description": "# napari-features\n\n[![License](https://img.shields.io/pypi/l/napari-features.svg?color=green)](https://github.com/0x00b1/napari-features/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-features.svg?color=green)](https://pypi.org/project/napari-features)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-features.svg?color=green)](https://python.org)\n[![tests](https://github.com/0x00b1/napari-features/workflows/tests/badge.svg)](https://github.com/0x00b1/napari-features/actions)\n[![codecov](https://codecov.io/gh/0x00b1/napari-features/branch/master/graph/badge.svg)](https://codecov.io/gh/0x00b1/napari-features)\n\nAn extensible, general-purpose feature extraction plug-in for the [Napari](https://napari.org) image viewer.\n\n## Features\n\n### Color\n\n#### Image\n\n    color_image_integrated_intensity\n    color_image_maximum_intensity\n    color_image_mean_intensity\n    color_image_median_absolute_deviation_intensity\n    color_image_median_intensity\n    color_image_minimum_intensity\n    color_image_quantile_1_intensity\n    color_image_quantile_3_intensity\n    color_image_standard_deviation_intensity\n\n#### Object\n\n    color_object_center_mass_intensity_x\n    color_object_center_mass_intensity_y\n    color_object_integrated_intensity\n    color_object_integrated_intensity_edge\n    color_object_mass_displacement\n    color_object_maximum_intensity\n    color_object_maximum_intensity_edge\n    color_object_maximum_intensity_x\n    color_object_maximum_intensity_y\n    color_object_mean_intensity\n    color_object_mean_intensity_edge\n    color_object_median_absolute_deviation_intensity\n    color_object_median_intensity\n    color_object_median_intensity_edge\n    color_object_minimum_intensity\n    color_object_minimum_intensity_edge\n    color_object_quantile_1_intensity\n    color_object_quantile_1_intensity_edge   \n    color_object_quantile_3_intensity\n    color_object_quantile_3_intensity_edge\n    color_object_standard_deviation_intensity\n    color_object_standard_deviation_intensity_edge\n    Object distribution\n    color_object_distribution_coefficient_of_variation_intensity\n    color_object_distribution_integrated_intensity\n    Color_object_distribution_mean_intensity\n\n### Location\n\n#### Object neighborhood\n\n    location_object_neighborhood_angle\n    location_object_neighborhood_closest_0_distance\n    location_object_neighborhood_closest_0_object_index\n    location_object_neighborhood_closest_1_distance\n    location_object_neighborhood_closest_1_object_index\n    location_object_neighborhood_closest_2_distance\n    location_object_neighborhood_closest_2_object_index\n    location_object_neighborhood_neighbors\n    location_object_neighborhood_touching\n\n### Metadata\n\n#### Image\n\n    metadata_image_checksum\n    metadata_image_filename\n\n#### Layer\n\n    metadata_layer_name\n    metadata_layer_type\n\n#### Object\n\n    metadata_object_index\n\n### Shape\n\n#### Image\n\n    shape_image_area\n\n#### Image skeleton\n\n    shape_image_skeleton_branches\n    shape_image_skeleton_endpoints\n    shape_image_skeleton_length\n    shape_image_skeleton_trunks\n\n#### Object\n\n    shape_object_area\n    shape_object_bounding_box_area\n    shape_object_bounding_box_maximum_x\n    shape_object_bounding_box_maximum_y\n    shape_object_bounding_box_maximum_z\n    shape_object_bounding_box_minimum_x\n    shape_object_bounding_box_minimum_y\n    shape_object_bounding_box_minimum_z\n    shape_object_bounding_box_volume\n    shape_object_central_moment_0_0_0\n    shape_object_central_moment_0_0_1\n    shape_object_central_moment_0_1_2\n    shape_object_central_moment_0_1_3\n    shape_object_central_moment_1_2_0\n    shape_object_central_moment_1_2_1\n    shape_object_central_moment_1_3_2\n    shape_object_central_moment_1_3_3\n    shape_object_central_moment_2_0_0\n    shape_object_central_moment_2_0_1\n    shape_object_central_moment_2_1_2\n    shape_object_central_moment_2_1_3\n    shape_object_central_moment_3_2_0\n    shape_object_central_moment_3_2_1\n    shape_object_central_moment_3_3_2\n    shape_object_central_moment_3_3_3\n    shape_object_centroid_x\n    shape_object_centroid_y\n    shape_object_centroid_z\n    shape_object_compactness\n    shape_object_eccentricity\n    shape_object_equivalent_diameter\n    shape_object_euler_number\n    shape_object_extent\n    shape_object_form_factor\n    shape_object_hu_moment_0\n    shape_object_hu_moment_1\n    shape_object_hu_moment_2\n    shape_object_hu_moment_3\n    shape_object_hu_moment_4\n    shape_object_hu_moment_5\n    shape_object_hu_moment_6\n    shape_object_inertia_tensor_eigenvalues_x\n    shape_object_inertia_tensor_eigenvalues_y\n    shape_object_inertia_tensor_eigenvalues_z\n    shape_object_inertia_tensor_x_x\n    shape_object_inertia_tensor_x_y\n    Shape_object_inertia_tensor_x_z\n    shape_object_inertia_tensor_y_x\n    shape_object_inertia_tensor_y_y\n    shape_object_inertia_tensor_y_z\n    shape_object_inertia_tensor_z_x\n    shape_object_inertia_tensor_z_y\n    shape_object_inertia_tensor_z_z\n    shape_object_major_axis_length\n    shape_object_maximum_feret_diameter\n    shape_object_maximum_radius\n    shape_object_mean_radius\n    shape_object_median_radius\n    shape_object_minimum_feret_diameter\n    shape_object_minor_axis_length\n    shape_object_normalized_moment_x_y\n    shape_object_orientation\n    shape_object_perimeter\n    shape_object_solidity\n    shape_object_spatial_moment_0_0_0\n    shape_object_spatial_moment_0_0_1\n    shape_object_spatial_moment_0_1_2\n    shape_object_spatial_moment_0_1_3\n    shape_object_spatial_moment_1_2_0\n    shape_object_spatial_moment_1_2_1\n    shape_object_spatial_moment_1_3_2\n    shape_object_spatial_moment_1_3_3\n    shape_object_spatial_moment_2_0_0\n    shape_object_spatial_moment_2_0_1\n    shape_object_spatial_moment_2_1_2\n    shape_object_spatial_moment_2_1_3\n    shape_object_spatial_moment_3_2_0\n    shape_object_spatial_moment_3_2_1\n    shape_object_spatial_moment_3_3_2\n    shape_object_spatial_moment_3_3_3\n    shape_object_surface_area\n    shape_object_volume\n    shape_object_zernike shape features\n    Object skeleton\n    shape_object_skeleton_endpoints\n    shape_object_skeleton_branches\n    shape_object_skeleton_length\n    shape_object_skeleton_trunks\n\n### Texture\n\n#### Object\n\n    texture_object_haralick_angular_second_moment\n    texture_object_haralick_contrast\n    texture_object_haralick_coorelation\n    texture_object_haralick_sum_of_squares_variance\n    texture_object_haralick_inverse_difference_moment\n    texture_object_haralick_sum_average\n    texture_object_haralick_sum_variance\n    texture_object_haralick_sum_entropy\n    texture_object_haralick_entropy\n    texture_object_haralick_difference_variance\n    texture_object_haralick_measure_of_correlation_0\n    texture_object_haralick_measure_of_correlation_1\n    texture_object_haralick_maximum_correlation_coefficient\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "waver",
    "name": "waver",
    "display_name": "waver",
    "version": "0.0.4",
    "created_at": "2021-05-15",
    "modified_at": "2021-08-15",
    "authors": [
      "Nicholas Sofroniew"
    ],
    "author_emails": [
      "sofroniewn@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/waver/",
    "home_github": "https://github.com/sofroniewn/waver",
    "home_other": null,
    "summary": "Wave simulations",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "magicgui (>=0.2.10)",
      "napari (>=0.4.10)",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "zarr"
    ],
    "package_metadata_description": "# waver\n\n[![License](https://img.shields.io/pypi/l/waver.svg?color=green)](https://github.com/sofroniewn/waver/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/waver.svg?color=green)](https://pypi.org/project/waver)\n[![Python Version](https://img.shields.io/pypi/pyversions/waver.svg?color=green)](https://python.org)\n[![tests](https://github.com/sofroniewn/waver/workflows/tests/badge.svg)](https://github.com/sofroniewn/waver/actions)\n[![codecov](https://codecov.io/gh/sofroniewn/waver/branch/main/graph/badge.svg?token=QBP7K6YUT7)](https://codecov.io/gh/sofroniewn/waver)\n\nRun simulations of the [wave equation](https://en.wikipedia.org/wiki/Wave_equation) in nD on grids of variable speed in Python. This library owes a lot of its design and approach to the [fdtd](https://github.com/flaport/fdtd) library, a Python 3D electromagnetic FDTD simulator.\n\nThis package allows for a fair amount of customization over your wave simulation. You can\n - specify the size and spacing of the grid\n - specify the time step for the simulation, which will be checked to ensure stability of the simulation\n - specify the duration of the simulation\n - setting a variable speed array (one value per grid point) to allow for \"objects\" in your environment\n - set the source of the wave, which can be a point, line, or any (n-1)D subarray\n - record the wave with a detector, which can be the full grid, the full boundary, or a particular boundary\n - use convenience methods to run many simulations with different sources on the same grid and detector combination\n\nYou can use [napari](https://napari.org/), a multi-dimensional image viewer for Python, to allow for easy visualization of the detected wave. Some functionality is also available as a napari plugin to allow for running simulations from a graphical user interface.\n\nResults can look like\n\nhttps://user-images.githubusercontent.com/6531703/128283012-a784ec06-4df9-4ddf-bf4f-e21b927fe4a3.mov\n\n----------------------------------\n\n## Installation\n\nYou can install `waver` via [pip]:\n\n    pip install waver\n\n## Usage\n\n### Convenience Methods\n\nThe most convenient way to use waver is to use one of two convenience methods that will create and run a simulation\nfor you and return the results.\n\nThe first method `run_single_source` allows you to run a single simulation with a single source on one grid and \nrecord the results using a detector. For example\n\n```python\nfrom waver.simulation import run_single_source\n\nsingle_sim_params = {\n    'size': (12.8e-3, 12.8e-3),\n    'spacing': 100e-6,\n    'duration': 80e-6,\n    'min_speed': 343,\n    'max_speed': 686,\n    'speed': 686,\n    'time_step': 50e-9,\n    'temporal_downsample': 2,\n    'location': (6.4e-3, 6.4e-3),\n    'period': 5e-6,\n    'ncycles':1,\n}\n\ndetected_wave, speed_grid = run_single_source(**single_sim_params)\n```\n\nThe second method `run_multiple_sources` allows you to run multiple simulations with multiple sources on the same\ngrid and with the same detector and return the results. For example\n\n```python\nfrom waver.simulation import run_multiple_sources\n\nmulti_sim_params = {\n    'size': (12.8e-3, 12.8e-3),\n    'spacing': 100e-6,\n    'duration': 80e-6,\n    'min_speed': 343,\n    'max_speed': 686,\n    'speed': 686,\n    'time_step': 50e-9,\n    'temporal_downsample': 2,\n    'sources': [{\n        'location': (6.4e-3, 6.4e-3),\n        'period': 5e-6,\n        'ncycles':1,\n    }]\n}\n\ndetected_wave, speed_grid = run_multiple_sources(**multi_sim_params)\n```\n\nThe main difference between these two methods is that `run_multiple_sources` takes a `sources` parameter which takes a list \nof dictionaries with keys corresponding to source related keyword arguments found in `run_single_source`.\n\n### Visualization\n\nIf you want to quickly visualize the results of `run_multiple_sources`, you can use the `run_and_visualize` command which will \nrun the simulation and then launch napari with the results, as seen in [examples/2D/point_source.py](./examples/2D/point_source.py)\n\n```python\nfrom waver.datasets import run_and_visualize\n\nrun_and_visualize(**multi_sim_params)\n```\n\n### Datasets\n\nIf you want to run simulations with on many different speed grids you can use the `generate_simulation_dataset` method as a convenience. The results will be saved to a [zarr](https://zarr.readthedocs.io/en/stable/) file of your chosing. You can then use the `load_simulation_dataset` to load the dataset.\n\n```python\nfrom waver.datasets import generate_simulation_dataset\n\n# Define root path for simulation\npath = './simulation_dataset.zarr'\nruns = 5\n\n# Define a simulation, 12.8mm, 100um spacing\ndataset_sim_params = {\n    'size': (12.8e-3, 12.8e-3),\n    'spacing': 100e-6,\n    'duration': 80e-6,\n    'min_speed': 343,\n    'max_speed': 686,\n    'speed': 'mixed_random_ifft',\n    'time_step': 50e-9,\n    'sources': [{\n        'location': (None, 0),\n        'period': 5e-6,\n        'ncycles':1,\n    }],\n    'temporal_downsample': 2,\n    'boundary': 1,\n    'edge': 1,\n}\n\n# Run and save simulation\ngenerate_simulation_dataset(path, runs, **dataset_sim_params)\n```\n\nThe `generate_simulation_dataset` allows the `speed` to be a string that will specify a particular method of randomly generating speed values for the simulation grid.\n\n### The Simulation Object\n\nIf you'd like to understand in a little bit more detail how a simulation is defined then you might want to use the unerlying simulation object `Simulation` and manually set key objects like the `Source` and `Detector`. A full example of this is as follows\n\n```python\n# Create a simulation\nsim = Simulation(size=size, spacing=spacing, max_speed=max_speed, time_step=time_step)\n\n# Set speed array\nsim.set_speed(speed=speed, min_speed=min_speed, max_speed=max_speed)\n\n# Add source\nsim.add_source(location=location, period=period, ncycles=ncycles, phase=phase)\n\n# Add detector grid\nsim.add_detector(spatial_downsample=spatial_downsample,\n                    boundary=boundary, edge=edge)\n\n# Run simulation\nsim.run(duration=duration, temporal_downsample=temporal_downsample, progress=progress, leave=leave)\n\n# Print simulation wave and speed data\nprint('wave: ', sim.detected_wave)\nprint('speed: ', sim.grid_speed)\n```\n\nNote these steps are done inside the `run_single_source` method for you as a convenience.\n\n## Known Limitations\n\nA [perfectly matched layer](https://en.wikipedia.org/wiki/Perfectly_matched_layer) boundary has recently been added, but might not perform well under all conditions. Additional contributions would be welcome here.\n\nRight now the simulations are quite slow. I'd like to add a [JAX](https://github.com/google/jax) backend, but \nhavn't done so yet. Contributions would be welcome.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"waver\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/sofroniewn/waver/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "simulation",
      "sample_fourier"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-bioformats",
    "name": "napari-bioformats",
    "display_name": "napari-bioformats",
    "version": "0.2.1",
    "created_at": "2021-06-25",
    "modified_at": "2021-08-11",
    "authors": [
      "Talley Lambert"
    ],
    "author_emails": [
      "talley.lambert@gmail.com"
    ],
    "license": "GPL-3.0",
    "home_pypi": "https://pypi.org/project/napari-bioformats/",
    "home_github": "https://github.com/tlambert03/napari-bioformats",
    "home_other": null,
    "summary": "Bioformats for napari, using pims",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "jpype1",
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "ome-types",
      "pims",
      "requests",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'"
    ],
    "package_metadata_description": "# napari-bioformats\n\n[![License](https://img.shields.io/pypi/l/napari-bioformats.svg?color=green)](https://github.com/napari/napari-bioformats/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-bioformats.svg?color=green)](https://pypi.org/project/napari-bioformats)\n[![Conda](https://img.shields.io/conda/v/conda-forge/napari-bioformats)](https://anaconda.org/conda-forge/napari-bioformats)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bioformats.svg?color=green)](https://python.org)\n[![tests](https://github.com/tlambert03/napari-bioformats/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-bioformats/actions)\n[![codecov](https://codecov.io/gh/tlambert03/napari-bioformats/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-bioformats)\n\nBioformats plugin for napari using\n[pims-bioformats](http://soft-matter.github.io/pims/v0.5/bioformats.html)\n\n----------------------------------\n\n## Use this plugin as a fallback!\n\nAnyone coming to napari from the Fiji/ImageJ world will likely be aware of the\n_incredible_ [Bio-Formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/index.html)\nlibrary.  A heroic effort, built over years, to read\n[more than a 100 file formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/supported-formats.html).  Naturally, we want some of that goodness for `napari` ... hence this plugin.\n\n**However:** it's important to note that this plugin _still_\nrequires having a java runtime engine installed.  This is easy enough to do\n(the plugin will ask to install it for you if you're in a `conda` environment), but\nit definitely makes for a more complicated environment setup, it's not very\n\"pythonic\", and the performance will likely not feel as snappy as a native \"pure\"\npython module.\n\nSo, before you reflexively install this plugin to fill that bio-formats\nsized hole in your python heart, consider trying some of the other pure-python\nplugins designed to read your format of interest:\n\n- **Zeiss (.czi)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), [napari-czifile2](https://github.com/BodenmillerGroup/napari-czifile2)\n- **Nikon (.nd2)**: [napari-nikon-nd2](https://github.com/cwood1967/napari-nikon-nd2), [nd2-dask](https://github.com/DragaDoncila/nd2-dask)\n- **Leica (.lif)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio)\n- **Olympus (.oif)**: no plugin?  (but see [oiffile](https://pypi.org/project/oiffile/) )\n- **DeltaVision (.dv, .mrc)**: [napari-dv](https://github.com/tlambert03/napari-dv)\n\n> *if you have a pure-python reader for a bio-formats-supported file format that\nyou'd like to see added to this list, please open an issue*\n\n## Installation\n\nThe easiest way to install `napari-bioformats` is via [conda], from the\n[conda-forge] channel:\n\n    conda install -c conda-forge napari-bioformats\n\nIt is also possible to install via [pip], but you will need to have a working\nJVM installed, and may need to set the `JAVA_HOME` environment variable\n\n    pip install napari-bioformats\n\n### First Usage\n\nThe first time you attempt to open a file with napari-bioformats, you will\nlikely notice a long delay as pims downloads the `loci_tools.jar` (speed will\ndepend on your internet connection). Subsequent files should open more quickly.\n\n## License\n\nDistributed under the terms of the [GPLv3] license,\n\"napari-bioformats\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n_This [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template._\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[GPLv3]: https://opensource.org/licenses/GPL-3.0\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/tlambert03/napari-bioformats/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[conda]: https://docs.conda.io/en/latest/\n[conda-forge]: https://conda-forge.org\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-mat-images",
    "name": "napari-mat-images",
    "display_name": "napari-mat-images",
    "version": "0.1.3",
    "created_at": "2021-06-03",
    "modified_at": "2021-06-30",
    "authors": [
      "Hector Munoz"
    ],
    "author_emails": [
      "hectormz.git@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-mat-images/",
    "home_github": "https://github.com/hectormz/napari-mat-images",
    "home_other": null,
    "summary": "A plugin to load images stored in MATLAB .mat files with napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "dask[delayed]",
      "h5py",
      "numpy",
      "pluggy",
      "scipy"
    ],
    "package_metadata_description": "# napari-mat-images\n\n[![PyPI version](https://img.shields.io/pypi/v/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)\n\n[![Python versions](https://img.shields.io/pypi/pyversions/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)\n\n[![See Build Status on Azure Pipelines](https://dev.azure.com/hectormz-1/napari-mat-images/_apis/build/status/hectormz.napari-mat-images?branchName=main)](https://dev.azure.com/hectormz-1/napari-mat-images/_build/latest?definitionId=1&branchName=main)\n\n## Features\n\nThis plugin loads image variables stored in `MATLAB` `.mat` files into [napari](https://github.com/napari/napari).\n\nIt loads any variable that looks like an image.\nPresently, that includes any array with more than two dimensions with size greater than 20 pixels (determined by `shape_is_image()`).\n\nIf loading a variable with 3 or more dimensions, the plugin assumes that it is a stack of images, and the dimension with greatest size is the axis of the stack.\n\n### Loading Large Files\n\nIf loading a large `.mat` file saved in `HDF5`/`v7.3` format, chunks of the images are loaded as needed, resulting in fast initial load, but potentially slower scrolling.\n\nSlices of the image stacks are randomly sampled to determine min/max contrast values.\n\n## Requirements\n\nThis plugin relies on `scipy` to load small `.mat` files and `h5py` (with `dask`) to load larger `HDF5`/`v7.3` `.mat` files.\n\nIt implicitly requires `napari` for use.\n\n## Installation\n\n`napari-mat-images` requires [napari](https://github.com/napari/napari) to be installed, although it is not listed as a requirement for installation.\nThis plugin relies on plugin functionality found in `napari` version \\> `0.2.12`. This can be installed via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):\n\n    $ pip install napari>0.2.12\n\nYou can install `napari-mat-images` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):\n\n    $ pip install napari-mat-images\n\n## Usage\n\nOnce installed, the plugin will be used whenever trying to load a `.mat` file.\nThis can be done from the `napari` GUI or commandline:\n\n    $ napari my_file.mat\n\n## Contributing\n\nContributions are very welcome.\nTests can be run with [pytest](https://docs.pytest.org/en/latest/),\nplease ensure the coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3](http://opensource.org/licenses/BSD-3-Clause) license, `napari-mat-images` is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue](https://github.com/hectormz/napari-mat-images/issues) along with a detailed description.\n\n---\n\nThis [napari](https://github.com/napari/napari) plugin was generated with [Cookiecutter](https://github.com/audreyr/cookiecutter) along with [napari](https://github.com/napari/napari)\\'s [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) template.\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-properties-viewer",
    "name": "napari-properties-viewer",
    "display_name": "napari-properties-viewer",
    "version": "0.0.2",
    "created_at": "2021-04-28",
    "modified_at": "2021-06-30",
    "authors": [
      "Kevin Yamauchi"
    ],
    "author_emails": [
      "kevin.yamauchi@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-properties-viewer/",
    "home_github": "https://github.com/kevinyamauchi/napari-properties-viewer",
    "home_other": null,
    "summary": "A viewer for napari layer properties",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy"
    ],
    "package_metadata_description": "# napari-properties-viewer\n\n[![License](https://img.shields.io/pypi/l/napari-properties-viewer.svg?color=green)](https://github.com/napari/napari-properties-viewer/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-properties-viewer.svg?color=green)](https://pypi.org/project/napari-properties-viewer)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-viewer.svg?color=green)](https://python.org)\n[![tests](https://github.com/kevinyamauchi/napari-properties-viewer/workflows/tests/badge.svg)](https://github.com/kevinyamauchi/napari-properties-viewer/actions)\n[![codecov](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer/branch/master/graph/badge.svg)](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer)\n\nA viewer for napari layer properties\n\n![image](resources/properties_viewer.gif)\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-properties-viewer` via [pip]:\n\n    pip install napari-properties-viewer\n    \n## Using the properties viewer table\n\n1. Open a a napari viewer with a layer with properties (e.g., Points)\n2. View the properties by opening the properties viewer plugin from Plugins menu -> Add dock widget -> napari-propertiews-viewer: properties table\n3. The layer property values are now displayed in the table widget. You can edit the values by double clicking the cell of interest and entering a new value.\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-properties-viewer\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/kevinyamauchi/napari-properties-viewer/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "QtPropertiesTable"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "ortho-view-napari",
    "name": "ortho-view-napari",
    "display_name": "ortho-view-napari",
    "version": "0.1.1",
    "created_at": "2021-06-15",
    "modified_at": "2021-06-15",
    "authors": [
      "Jordao Bragantini"
    ],
    "author_emails": [
      "jordao.bragantini@czbiohub.org"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/ortho-view-napari/",
    "home_github": "https://github.com/JoOkuma/ortho-view-napari",
    "home_other": null,
    "summary": "It displays the lateral view of the current 3D stack. This could be a starting point for a orthorviewer.",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy"
    ],
    "package_metadata_description": "# ortho-view-napari\n\n[![License](https://img.shields.io/pypi/l/ortho-view-napari.svg?color=green)](https://github.com/JoOkuma/ortho-view-napari/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/ortho-view-napari.svg?color=green)](https://pypi.org/project/ortho-view-napari)\n[![Python Version](https://img.shields.io/pypi/pyversions/ortho-view-napari.svg?color=green)](https://python.org)\n[![tests](https://github.com/JoOkuma/ortho-view-napari/workflows/tests/badge.svg)](https://github.com/JoOkuma/ortho-view-napari/actions)\n[![codecov](https://codecov.io/gh/JoOkuma/ortho-view-napari/branch/master/graph/badge.svg)](https://codecov.io/gh/JoOkuma/ortho-view-napari)\n\nIt displays the lateral view of the current 3D stack. This could be a starting point for a orthorviewer.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `ortho-view-napari` via [pip]:\n\n    pip install ortho-view-napari\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"ortho-view-napari\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/JoOkuma/ortho-view-napari/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "Widget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-dvid",
    "name": "napari-dvid",
    "display_name": "napari-dvid",
    "version": "0.2.0",
    "created_at": "2021-06-09",
    "modified_at": "2021-06-09",
    "authors": [
      "Emma Zhou"
    ],
    "author_emails": [
      "emma@emmazhou.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-dvid/",
    "home_github": "https://github.com/emmazhou/napari-dvid",
    "home_other": null,
    "summary": "DVID loader for napari, from a url",
    "categories": [],
    "package_metadata_requires_python": ">=3.7",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "requests"
    ],
    "package_metadata_description": "# napari-dvid\n\n[![License](https://img.shields.io/pypi/l/napari-dvid.svg?color=green)](https://github.com/emmazhou/napari-dvid/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-dvid.svg?color=green)](https://pypi.org/project/napari-dvid)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dvid.svg?color=green)](https://python.org)\n[![tests](https://github.com/emmazhou/napari-dvid/workflows/tests/badge.svg)](https://github.com/emmazhou/napari-dvid/actions)\n[![codecov](https://codecov.io/gh/emmazhou/napari-dvid/branch/master/graph/badge.svg)](https://codecov.io/gh/emmazhou/napari-dvid)\n\nDVID loader for napari, from a url\n\n---\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-dvid` via [pip]:\n\n    pip install napari-dvid\n\n## Examples\n\nOnce installed, run `napari --with napari-dvid` to get the plugin sidebar:\n\n![Screenshot](screenshot.png)\n\nPaste in a URL to a DVID volume and hit \"Load\" to load the volume! As an example, try:\n\n`https://emdata.janelia.org/api/node/ab6e610d4/grayscale/raw/0_1_2/256_256_256/7500_7000_4400`\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-dvid\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[mit]: http://opensource.org/licenses/MIT\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/emmazhou/napari-dvid/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[pypi]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "UrlWidget"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "platymatch",
    "name": "PlatyMatch",
    "display_name": "PlatyMatch",
    "version": "0.0.3",
    "created_at": "2021-05-28",
    "modified_at": "2021-06-08",
    "authors": [
      "Manan Lalit"
    ],
    "author_emails": [
      "lalit@mpi-cbg.de"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/platymatch/",
    "home_github": "https://github.com/juglab/PlatyMatch",
    "home_other": null,
    "summary": "PlatyMatch allows registration of volumetric images of embryos by establishing correspondences between cells",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "scikit-image",
      "scikit-learn",
      "tqdm",
      "simpleitk",
      "napari[all]",
      "pandas",
      "pytest"
    ],
    "package_metadata_description": "[![DOI:10.1007/978-3-030-66415-2_30](https://zenodo.org/badge/DOI/10.1007/978-3-030-66415-2_30.svg)](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![PyPI](https://img.shields.io/pypi/v/PlatyMatch.svg?color=green)](https://pypi.org/project/PlatyMatch)\n[![Python Version](https://img.shields.io/pypi/pyversions/PlatyMatch.svg?color=green)](https://python.org)\n[![tests](https://github.com/juglab/PlatyMatch/workflows/tests/badge.svg)](https://github.com/juglab/PlatyMatch/actions)\n[![codecov](https://codecov.io/gh/juglab/PlatyMatch/branch/master/graph/badge.svg)](https://codecov.io/gh/juglab/PlatyMatch)\n\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/34229641/117537510-b26ee500-b001-11eb-9642-3baa461bfc94.png\" width=400 />\n</p>\n<h2 align=\"center\">Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence</h2>\n\n## Table of Contents\n\n- **[Introduction](#introduction)**\n- **[Dependencies](#dependencies)**\n- **[Getting Started](#getting-started)**\n- **[Datasets](#datasets)**\n- **[Registering your data](#registering-your-data)**\n- **[Contributing](#contributing)**\n- **[Issues](#issues)**\n- **[Citation](#citation)**\n\n### Introduction\nThis repository hosts the version of the code used for the **[publication](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)** **Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence**. \n\nWe refer to the techniques elaborated in the publication, here as **PlatyMatch**. `PlatyMatch` performs a linear registration of volumetric, microscopy images of embryos by establishing correspondences between cells. \n\n`PlatyMatch` first detects nuclei in the two images being considered, next calculates unique `shape context` features for each nucleus detection which encapsulates the neighborhood as seen by that nucleus, and finally identifies pairs of matching nuclei through maximum bipartite matching applied to the pairwise distance matrix generated from these features. \n\n### Dependencies \n\nYou can install `PlatyMatch` via **[pip]**:\n\n```\nconda create -y -n PlatyMatchEnv python==3.8\nconda activate PlatyMatchEnv\npython3 -m pip install PlatyMatch\n```\n\n### Getting Started\n\nType in the following commands in a new terminal window.\n\n```\nconda activate PlatyMatchEnv\nnapari\n```\n\nNext, select `PlatyMatch` from `Plugins> Add Dock Widget`.\n\n### Datasets\n\nDatasets are available in **`bic_eccv_data.zip`** as release assets **[here](https://github.com/juglab/PlatyMatch/releases/tag/v0.0.1)**.\nThese comprise of images, nuclei detections and keypoint locations for confocal images of 12 individual specimens under the `01-insitus` directory and static snapshots of a live embryo imaged through Light Sheet Microscopy under the `02-live` directory. \nFolders with the same name in these two directories correspond in their developmental age, for example, `01-insitus/02` corresponds to `02-live/02`, `01-insitus/03` corresponds to `02-live/03` and so on.   \n\n\n### Registering your data\n\n- **Detect Nuclei** \n\t- Drag and drop your images in the viewer \n\t- Click on `Sync with Viewer` button to refresh the drop-down menus \n\t- Select the appropriate image in the drop down menu (for which nuclei detections are desired)\n\t- Select **`Detect Nuclei`** from the drop-down menu\n\t- Specify the anisotropy factor (`Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\n\t- Ideally min scales and max scales should be estimated from your data (`min_scale` should be set as `min_radius/sqrt(3)` and `max_scale` should be set as `max_radius/sqrt(3)`. The default values of `min_scale=5` and `max_scale=9` generally works well).  \n\t- Click `Run Scale Space Log` button. Please note that this step takes a few minutes.\n\t- Wait until a confirmation message suggesting that nuclei detection is over shows up on the terminal\n\t- Export the nuclei locations (`Export detections to csv`) to a csv file\n\t- Repeat this step for all images which need to be matched\n\n\n\n\nhttps://user-images.githubusercontent.com/34229641/120660618-cd5d3980-c487-11eb-8996-326264a4df87.mp4\n\n\n- **Estimate Transform**\n\t- In case, nuclei were exported to a csv in the `Detect Nuclei` panel, tick `csv` checkbox\n\t- If the nuclei detected were specified in the order id, z, y and x in the csv file, then tick `IZYXR` checkbox\n\t- Additionally if there is a header in the csv file, tick `Header` checkbox\n\t- Load the detections for the `Moving Image`, which is defined as the image which will be transformed to later match another `fixed` image\n\t- Load the detections for the `Fixed Image`\n\t- Click on `Run` pushbutton. Once the calculation is complete, a confirmation message shows up in the terminal. Export the transform matrix to a csv (Note that this step can take a few minutes)\n\t- It is also possible to estimate the transform in a `supervised` fashion. For this, upload the locations of a few matching keypoints in both images. These locations serve to provide a good starting point for the transform calculation. Once the keypoint files have been uploaded for both the images, then click `Run` and then export the transform matrix to a csv file \n\n\nhttps://user-images.githubusercontent.com/34229641/120685628-53857a00-c4a0-11eb-8f92-7ffac730e28a.mp4\n\n\n\n- **Evaluate Metrics**\n\t- Drag images which need to be transformed, in the viewer\n\t- Click on `Sync with Viewer` button to refresh the drop-down menus\n\t- Specify the anisotropy factor (`Moving Image Anisotropy (Z)` and `Fixed Image Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\n\t- Load the transform which was calculated in the previous steps\n\t- If you simply wish to export a transformed version of the moving image, click on `Export Transformed Image`\n\t- Additionally, one could quantify metrics such as average registration error evaluated on a few keypoints. To do so, tick the `csv` checkbox, if keypoints and detections are available as a csv file. Then load the keypoints for the moving image (`Moving Kepoints`) and the fixed image (`Fixed Keypoints`)\n\t- Also, upload the detections calculated in the previous steps (`Detect Nuclei`)  by uploading the `Moving Detections` and the `Fixed Detections`\n\t- Click on the `Run` push button\n\t- The text fields such as `Matching Accuracy`(0 to 1, with 1 being the best) and `Average Registration Error` (the lower the better) should become populated once the results are available\n\n\n\nhttps://user-images.githubusercontent.com/34229641/120685654-5b451e80-c4a0-11eb-8d7d-de58b8b8304d.mp4\n\n\n### Contributing\n\nContributions are very welcome. Tests can be run with **[tox]**.\n\n### Issues\n\nIf you encounter any problems, please **[file an issue]** along with a detailed description.\n\n[file an issue]: https://github.com/juglab/PlatyMatch/issues\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/EmbedSeg/\n\n\n### Citation\nIf you find our work useful in your research, please consider citing:\n\n```bibtex\n@InProceedings{10.1007/978-3-030-66415-2_30,\nauthor=\"Lalit, Manan and Handberg-Thorsager, Mette and Hsieh, Yu-Wen and Jug, Florian and Tomancak, Pavel\",\neditor=\"Bartoli, Adrien\nand Fusiello, Andrea\",\ntitle=\"Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence\",\nbooktitle=\"Computer Vision -- ECCV 2020 Workshops\",\nyear=\"2020\",\npublisher=\"Springer International Publishing\",\naddress=\"Cham\",\npages=\"458--473\",\nisbn=\"978-3-030-66415-2\"\n}\n```\n\n`PlatyMatch` plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/juglab/PlatyMatch/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "DetectNuclei",
      "EstimateTransform",
      "EvaluateMetrics"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-checkerboard",
    "name": "napari-checkerboard",
    "display_name": "napari-checkerboard",
    "version": "0.0.3",
    "created_at": "2021-05-11",
    "modified_at": "2021-05-31",
    "authors": [
      "Viktor van der Valk"
    ],
    "author_emails": [
      "v.o.van_der_valk@lumc.nl"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-checkerboard/",
    "home_github": "https://github.com/ViktorvdValk/napari-checkerboard",
    "home_other": null,
    "summary": "Compare two images with the itk checkerboard filter",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=1.19.0)",
      "napari (>=0.4.6)",
      "magicgui (>=0.2.6)",
      "itk-elastix (>=0.11.1)",
      "itk-napari-conversion (>=0.3.1)",
      "napari-itk-io (>=0.1.0)"
    ],
    "package_metadata_description": "# napari-checkerboard\n\n[![License](https://img.shields.io/pypi/l/napari-checkerboard.svg?color=green)](https://github.com/ViktorvdValk/napari-checkerboard/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-checkerboard.svg?color=green)](https://pypi.org/project/napari-checkerboard)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-checkerboard.svg?color=green)](https://python.org)\n[![tests](https://github.com/ViktorvdValk/napari-checkerboard/workflows/tests/badge.svg)](https://github.com/ViktorvdValk/napari-checkerboard/actions)\n[![codecov](https://codecov.io/gh/ViktorvdValk/napari-checkerboard/branch/master/graph/badge.svg)](https://codecov.io/gh/ViktorvdValk/napari-checkerboard)\n\nCompare two images with the itk checkerboard filter\n\n\n<img width=\"1430\" alt=\"Screenshot 2021-05-12 at 15 03 17\" src=\"https://user-images.githubusercontent.com/33719474/117979519-48bd4680-b333-11eb-874c-d9ec09681d93.png\">\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-checkerboard` via [pip]:\n\n    pip install napari-checkerboard\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-checkerboard\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/ViktorvdValk/napari-checkerboard/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [
      "checkerboard"
    ],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-tracks-reader",
    "name": "napari-tracks-reader",
    "display_name": "napari-tracks-reader",
    "version": "0.1.3",
    "created_at": "2021-05-11",
    "modified_at": "2021-05-26",
    "authors": [
      "Sylvain Prigent"
    ],
    "author_emails": [
      "sylvain.prigent@inria.fr"
    ],
    "license": "GNU GPL v3.0",
    "home_pypi": "https://pypi.org/project/napari-tracks-reader/",
    "home_github": "https://github.com/sylvainprigent/napari-tracks-reader",
    "home_other": null,
    "summary": "Read tracks from txt (xml, csv) files to napari",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "pandas (>=1.2.4)"
    ],
    "package_metadata_description": "# napari-tracks-reader\n\n[![License](https://img.shields.io/pypi/l/napari-tracks-reader.svg?color=green)](https://github.com/sylvainprigent/napari-tracks-reader/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-tracks-reader.svg?color=green)](https://pypi.org/project/napari-tracks-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-tracks-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/sylvainprigent/napari-tracks-reader/workflows/tests/badge.svg)](https://github.com/sylvainprigent/napari-tracks-reader/actions)\n[![codecov](https://codecov.io/gh/sylvainprigent/napari-tracks-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/sylvainprigent/napari-tracks-reader)\n\nRead tracks from various tracking softwares output files to napari tracks layer.\nSupported formats are:\n- Trakmate model (xml)\n- Icy (xml)\n- TrackContestISBI2012 (xml) \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-tracks-reader` via [pip]:\n\n    pip install napari-tracks-reader\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [GNU GPL v3.0] license,\n\"napari-tracks-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/sylvainprigent/napari-tracks-reader/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-dzi-zarr",
    "name": "napari-dzi-zarr",
    "display_name": "napari-dzi-zarr",
    "version": "0.1.2",
    "created_at": "2020-10-26",
    "modified_at": "2021-04-06",
    "authors": [
      "Trevor Manz"
    ],
    "author_emails": [
      "trevor.j.manz@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-dzi-zarr/",
    "home_github": "https://github.com/manzt/napari-dzi-zarr",
    "home_other": null,
    "summary": "An experimental plugin for viewing Deep Zoom Images (DZI) with napari and zarr.",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy (>=0.1.19)",
      "zarr (>=0.2.4)",
      "dask[array] (>=2.23.0)",
      "fsspec (>=0.8.0)",
      "requests (>=2.24.0)",
      "aiohttp (>=3.6.2)",
      "imageio (>=2.9.0)"
    ],
    "package_metadata_description": "# napari-dzi-zarr\n\n[![License](https://img.shields.io/pypi/l/napari-dzi-zarr.svg?color=green)](https://github.com/napari/napari-dzi-zarr/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-dzi-zarr.svg?color=green)](https://pypi.org/project/napari-dzi-zarr)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dzi-zarr.svg?color=green)](https://python.org)\n[![tests](https://github.com/manzt/napari-dzi-zarr/workflows/tests/badge.svg)](https://github.com/manzt/napari-dzi-zarr/actions)\n\nAn experimental plugin for viewing Deep Zoom Images (DZI) with napari + zarr + dask.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n## Description \n\nThe [DZI File Format](https://github.com/openseadragon/openseadragon/wiki/The-DZI-File-Format) \nis a pyramidal tile source specification where individual tiles are RGB/RGBA JPEG/PNG images. \nDZI is a very popular tile source for zoomable web-viewers like \n[OpenSeadragon](https://openseadragon.github.io/), and as a result many tile sources are available over \nHTTP. This plugin wraps a DZI tile source (local or remote) as a multiscale Zarr, where each pyramidal level is a `zarr.Array` of shape `(level_height, level_width, 3/4)`, allowing the same images to be viewed \nin `napari` + `dask`.\n\n## Installation\n\nYou can install `napari-dzi-zarr` via [pip]:\n\n    pip install napari-dzi-zarr\n\n\n## Usage\n\nThis high-resolution scan of Rembrandt's Night Watch is available thanks to [R.G Erdmann](https://twitter.com/erdmann). More examples can be found on [boschproject.org](http://boschproject.org).\n\n    $ napari http://hyper-resolution.org/dzi/Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21.dzi\n\n![Rembrandt's Night Watch in napari](./night_watch_napari.png)\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox].\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-dzi-zarr\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/manzt/napari-dzi-zarr/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-compressed-labels-io",
    "name": "napari-compressed-labels-io",
    "display_name": "napari-compressed-labels-io",
    "version": "0.0.2",
    "created_at": "2021-02-23",
    "modified_at": "2021-03-03",
    "authors": [
      "Draga Doncila"
    ],
    "author_emails": [
      "ddoncila@gmail.com"
    ],
    "license": "MIT",
    "home_pypi": "https://pypi.org/project/napari-compressed-labels-io/",
    "home_github": "https://github.com/DragaDoncila/napari-compressed-labels-io",
    "home_other": null,
    "summary": "Plugin exploring different options for reading and writing compressed and portable labels layers in napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "zarr",
      "dask[complete]"
    ],
    "package_metadata_description": "# napari-compressed-labels-io\n\n[![License](https://img.shields.io/pypi/l/napari-compressed-labels-io.svg?color=green)](https://github.com/DragaDoncila/napari-compressed-labels-io/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-compressed-labels-io.svg?color=green)](https://pypi.org/project/napari-compressed-labels-io)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-compressed-labels-io.svg?color=green)](https://python.org)\n[![tests](https://github.com/DragaDoncila/napari-compressed-labels-io/workflows/tests/badge.svg)](https://github.com/DragaDoncila/napari-compressed-labels-io/actions)\n[![codecov](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io/branch/master/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io)\n\n\n## Description\n\nThis napari plugin provides readers and writers for labels and their corresponding image layers into zarr format for compression and portability. Each reader/writer pair supports a round trip of saving and loading image and labels layers.\n\n## Writers\nTwo writers are provided by this plugin, each with its own reader.\n\n### `labels_to_zarr`\nThis writer is an alternative to napari's default label writer and will write an entire labels layer, regardless of its dimensions, into a single zarr file. This writer provides the best compression option and its associated reader `get_zarr_labels` will read the layer back into napari.\n\nThis writer will be called when the user tries to save a selected labels layer into a path ending with .zarr\n\n### `label_image_pairs_to_zarr`\nThis writer will save 3-dimensional labels and image layers from the viewer into individual zarrs for portability and convenience. For example, given one labels and one image layer of the shape (10, 200, 200) saved to my_stacks.zarr, 10 subdirectories will be created, each with two zarrs inside of shape (200, 200) corresponding to the labels and image layer.\n\nThis writer allows users to load stacks of associated images, label them, and then quickly save these stacks out into individual slices for easy loading, viewing and interaction. Its associated reader supports the loading into napari of the whole stack, all layers at one slice of the stack, and an individual layer of a given slice of the stack.\n\nThe writer currently supports only 3D layers, with the exception of RGB images of the form (z, y, x, 3), which are also supported.\n\n\n## Readers\n\nTwo readers are provided by this plugin for loading the formats saved by each writer. These are detailed below.\n\n### `get_zarr_labels`\n\nThis reader will open any zarr file with a .zarray at the top level in `path` as a labels layer. This is to be used in conjunction with `labels_to_zarr`.\n\n\n### `get_label_image_stack`\n\nThis reader will open any zarr containing a `.zmeta` file as layers into napari. Depending on what is being opened, the reader will either load a full stack of labels and images, one slice of a stack of images and labels or an individual layer within a slice. This is to be used in conjunction with `label_image_pairs_to_zarr`.\n\n## .zmeta\n\nThis metadata file contains information about the layer types in the stack and in each individual slice, as well as the number of image/label slices. This allows the reader plugin to load the correct layer types with appropriate names both at a stack level and at the individual slice level.\n\n### An example .zmeta specification\n\n```json\n{\n    \"meta\": {\n        \"stack\": 7                               # number of slices in the entire stack (1 for an individual slice, 0 for a layer within a slice)\n    },\n    \"data\": {\n        \"image\" : [                              # all image layers must be listed here\n            {\n                \"name\": \"leaves_example_data\",\n                \"shape\": [790, 790, 3],\n                \"dtype\": \"uint8\",\n                \"rgb\": true                      # where rgb is false the image will be loaded as greyscale (colormap support has not yet been implemented)\n            }\n        ],\n        \"labels\" : [\n            {\n                \"name\": \"oak\",\n                \"shape\": [790, 790],\n                \"dtype\": \"int64\"\n            },\n            {\n                \"name\": \"bg\",\n                \"shape\": [790, 790],\n                \"dtype\": \"int64\"\n            }\n        ]\n    }\n}\n\n```\n\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-compressed-labels-io` via [pip]:\n\n    pip install napari-compressed-labels-io\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [MIT] license,\n\"napari-compressed-labels-io\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/DragaDoncila/napari-compressed-labels-io/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-em-reader",
    "name": "napari-em-reader",
    "display_name": "napari-em-reader",
    "version": "0.1.0",
    "created_at": "2021-02-23",
    "modified_at": "2021-02-23",
    "authors": [
      "Lorenzo Gaifas"
    ],
    "author_emails": [
      "brisvag@gmail.com"
    ],
    "license": "BSD-3",
    "home_pypi": "https://pypi.org/project/napari-em-reader/",
    "home_github": "https://github.com/brisvag/napari-em-reader",
    "home_other": null,
    "summary": "A napari plugin to read .em files",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "emfile (>=0.2)"
    ],
    "package_metadata_description": "# napari-em-reader\n\n[![License](https://img.shields.io/pypi/l/napari-em-reader.svg?color=green)](https://github.com/brisvag/napari-em-reader/raw/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-em-reader.svg?color=green)](https://pypi.org/project/napari-em-reader)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-em-reader.svg?color=green)](https://python.org)\n[![tests](https://github.com/brisvag/napari-em-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-em-reader/actions)\n[![codecov](https://codecov.io/gh/brisvag/napari-em-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-em-reader)\n\nA napari plugin to read .em files\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-em-reader` via [pip]:\n\n    pip install napari-em-reader\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [BSD-3] license,\n\"napari-em-reader\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/brisvag/napari-em-reader/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  },
  {
    "normalized_name": "napari-nikon-nd2",
    "name": "napari-nikon-nd2",
    "display_name": "napari-nikon-nd2",
    "version": "0.1.3",
    "created_at": "2021-02-03",
    "modified_at": "2021-02-03",
    "authors": [
      "Chris Wood"
    ],
    "author_emails": [
      "cwood1967@gmail.com"
    ],
    "license": "Apache",
    "home_pypi": "https://pypi.org/project/napari-nikon-nd2/",
    "home_github": "https://github.com/cwood1967/napari-nikon-nd2",
    "home_other": null,
    "summary": "Opens Nikon ND2 files into napari.",
    "categories": [],
    "package_metadata_requires_python": ">=3.6",
    "package_metadata_requires_dist": [
      "napari-plugin-engine (>=0.1.4)",
      "numpy",
      "nd2reader"
    ],
    "package_metadata_description": "# napari-nikon-nd2\n\n[![License](https://img.shields.io/pypi/l/napari-nikon-nd2.svg?color=green)](https://github.com/cwood1967/napari-nikon-nd2/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-nikon-nd2.svg?color=green)](https://pypi.org/project/napari-nikon-nd2)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nikon-nd2.svg?color=green)](https://python.org)\n[![tests](https://github.com/cwood1967/napari-nikon-nd2/workflows/tests/badge.svg)](https://github.com/cwood1967/napari-nikon-nd2/actions)\n[![codecov](https://codecov.io/gh/cwood1967/napari-nikon-nd2/branch/main/graph/badge.svg)](https://codecov.io/gh/cwood1967/napari-nikon-nd2)\n\nOpens Nikon ND2 files into napari. This plugin uses the [nd2reader] and [pims] python packages. \n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/docs/plugins/index.html\n-->\n\n## Installation\n\nYou can install `napari-nikon-nd2` via [pip]:\n\n    pip install napari-nikon-nd2\n\n## Contributing\n\nContributions are very welcome. Tests can be run with [tox], please ensure\nthe coverage at least stays the same before you submit a pull request.\n\n## License\n\nDistributed under the terms of the [Apache Software License 2.0] license,\n\"napari-nikon-nd2\" is free and open source software\n\n## Issues\n\nIf you encounter any problems, please [file an issue] along with a detailed description.\n\n## Credits\n\nThis [napari] plugin was created using [Napari Delta Vision Reader] and\nthe [Allen Institute IO] plugin as examples.\n\n\n[napari]: https://github.com/napari/napari\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\n[@napari]: https://github.com/napari\n[MIT]: http://opensource.org/licenses/MIT\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\n[file an issue]: https://github.com/cwood1967/napari-nikon-nd2/issues\n[napari]: https://github.com/napari/napari\n[tox]: https://tox.readthedocs.io/en/latest/\n[pip]: https://pypi.org/project/pip/\n[PyPI]: https://pypi.org/\n[nd2reader]: https://github.com/rbnvrw/nd2reader\n[pims]: https://github.com/soft-matter/pims\n[Allen Institute IO]: https://github.com/AllenCellModeling/napari-aicsimageio\n[Napari Delta Vision Reader]: https://github.com/tlambert03/napari-dv\n\n",
    "package_metadata_classifiers": [],
    "contributions_readers_filename_patterns": [
      "*"
    ],
    "contributions_writers_filename_extensions": [],
    "contributions_widgets": [],
    "contributions_sample_data": []
  }
]